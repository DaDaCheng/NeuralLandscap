{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of FBB.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u5JvKOkWPeuF"},"source":["## Setting"]},{"cell_type":"code","metadata":{"id":"_vLPIx9bcT0x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597597924857,"user_tz":-120,"elapsed":1641,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"4866fb05-e398-497c-b234-d3e04c21fafc"},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import sys\n","\n","import os\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Dataset\n","import matplotlib.pyplot as plt\n","import imp\n","import importlib\n","sys.path.append(\"/content/drive/My Drive/LCNN/\")\n"],"execution_count":119,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RXOaJjTycT00","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1597598359644,"user_tz":-120,"elapsed":859,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"35f39dcf-77b2-43b7-a892-6d69eb2535d7"},"source":["sys.path.append(\"./../\")\n","\n","import LCNN\n","imp.reload(LCNN)\n","from LCNN import adjust,accuracy,one_hot,adjust_\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print(torch.cuda.device_count())\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"],"execution_count":123,"outputs":[{"output_type":"stream","text":["Using device: cuda\n","\n","Tesla P100-PCIE-16GB\n","1\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.1 GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"laqt8kVacT04","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597581060554,"user_tz":-120,"elapsed":878,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qatyyFYBOJug","colab":{},"executionInfo":{"status":"ok","timestamp":1597581060903,"user_tz":-120,"elapsed":811,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ])\n","\n","batchsize=64\n","dataset_dir = '/home/liu0003/Desktop/datasets'\n","\n","trainset = datasets.MNIST(dataset_dir, download=True, train=True, transform=transform)\n","valset = datasets.MNIST(dataset_dir, download=True, train=False, transform=transform)\n","\n","\n","\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=False, sampler=SubsetRandomSampler(range(batchsize)), worker_init_fn=np.random.seed(0))\n","test_loader = torch.utils.data.DataLoader(valset, batch_size=batchsize, shuffle=False, worker_init_fn=np.random.seed(0))"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_Wvk0CW17-lZ"},"source":["# 2 layers network"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kT0d0nDqTXME","colab":{},"executionInfo":{"status":"ok","timestamp":1597581064506,"user_tz":-120,"elapsed":728,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["class NeuralNet2layer(nn.Module):\n","    def __init__(self, input_size, hidden_size1,num_classes, initialize='NTK',batchnorm=False):\n","        super(NeuralNet2layer, self).__init__()\n","        self.initialize=initialize\n","        self.input_size = input_size\n","        self.hidden_size1 = hidden_size1\n","        self.batchnorm = batchnorm\n","\n","        self.fc1 = nn.Linear(input_size, hidden_size1) \n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size1, num_classes,bias=False)\n","\n","\n","        if self.batchnorm:\n","            self.bn1 = nn.BatchNorm1d(num_features=hidden_size1)\n","        \n","        if self.initialize=='NTK':\n","\n","            self.fc1.weight.detach().uniform_(-1, 1)\n","            self.fc1.bias.detach().uniform_(-1, 1)\n","            self.fc2.weight.detach().uniform_(-1,1)\n","\n","            \n","    def forward(self, x):\n","        if self.batchnorm:\n","            if self.initialize=='LeCun':\n","                out = self.fc1(x)\n","                out = self.bn1(out)\n","                out = self.relu1(out)\n","                out = self.fc2(out)\n","\n","            if self.initialize=='NTK':\n","                out = self.fc1(x)\n","                out = out/np.sqrt(self.input_size)\n","                out = self.bn1(out)\n","                out = self.relu1(out)\n","                out = self.fc2(out)\n","                out = out/np.sqrt(self.hidden_size1)\n","        else:\n","\n","            if self.initialize=='LeCun':\n","                out = self.fc1(x)\n","                out = self.relu1(out)\n","                out = self.fc2(out)\n","\n","            if self.initialize=='NTK':\n","                out = self.fc1(x)\n","                out = out/np.sqrt(self.input_size)\n","                out = self.relu1(out)\n","                out = self.fc2(out)\n","                out = out/np.sqrt(self.hidden_size1)\n","        return out\n","    \n","    \n","     \n","\n","def loss_and_accuracy(model,loader, criterion, acc_bool = False):\n","# def loss_and_accuracy(model,loader, criterion):\n","    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","    t=0\n","    c=0\n","    total_loss = 0\n","    \n","    model.eval()\n","\n","    with torch.no_grad():    \n","        if not acc_bool:\n","            for i, (images, labels) in enumerate(loader):\n","                images = images.view(images.shape[0], -1).to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, one_hot(labels, 10)).to(device)\n","                total_loss += loss\n","\n","                c=c+len(outputs)\n","            return total_loss/c\n","        \n","        else: \n","            for i, (images, labels) in enumerate(loader):\n","                images = images.view(images.shape[0], -1).to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, one_hot(labels, 10)).to(device)\n","                total_loss += loss\n","                \n","                t=t+(torch.argmax(outputs,dim=1) == labels).float().sum()\n","                c=c+len(outputs)\n","            return t/c*100 , total_loss/c\n","\n","\n","def simpleaxis(ax):\n","    ax.spines['top'].set_visible(False)\n","    ax.spines['right'].set_visible(False)\n","    ax.get_xaxis().tick_bottom()\n","    ax.get_yaxis().tick_left()\n","    \n","    \n","import matplotlib as mpl\n","mpl.rcParams['xtick.labelsize'] = 12 \n","mpl.rcParams['ytick.labelsize'] = 12 \n","    \n","def plot_loss_acc(step_list, loss_dict, acc_dict, title, fig_save_path):\n","    \n","    fig, axs = plt.subplots(1, 2, figsize=(9, 3))\n","\n","\n","    axs[0].plot(step_list, loss_dict['train'], label = 'train', linewidth = 3 ,  linestyle =  '--')\n","    axs[0].plot(step_list,loss_dict['test'], label = 'test', linewidth = 3)\n","    axs[0].legend(frameon = False, fontsize = 12)\n","    axs[0].set_xlabel('Iter num', fontsize = 15)\n","    axs[0].set_ylabel('Loss', fontsize = 15)\n","\n","\n","    axs[1].plot(step_list, acc_dict['train'], label = 'train', linewidth = 3, linestyle =  '--')\n","    axs[1].plot(step_list, acc_dict['test'], label = 'test', linewidth = 3)\n","    axs[1].legend(frameon = False, fontsize = 12)\n","    axs[1].set_xlabel('Iter num', fontsize = 15)\n","    axs[1].set_ylabel('Acc', fontsize = 15)\n","\n","\n","#     title = '784-800-10; no scaling; no bn; full batch; train data size = 64'\n","    plt.suptitle(title, fontsize = 17)\n","\n","    axs[0] = simpleaxis(axs[0])\n","    axs[1] = simpleaxis(axs[1])\n","    plt.subplots_adjust(bottom=0.2)\n","    plt.savefig(fig_save_path + title, format='png')\n","    \n","    return fig\n","\n","\n","def plot_loss(step_list, loss_dict, title, fig_save_path):\n","    \n","    fig, axs = plt.subplots(1, figsize=(4, 3))\n","\n","\n","    axs.plot(step_list, loss_dict['train'], label = 'train', linewidth = 3 ,  linestyle =  '--')\n","    axs.plot(step_list,loss_dict['test'], label = 'test', linewidth = 3)\n","    axs.legend(frameon = False, fontsize = 12)\n","    axs.set_xlabel('Iter num', fontsize = 15)\n","    axs.set_ylabel('Loss', fontsize = 15)\n","\n","    plt.suptitle(title, fontsize = 17)\n","\n","    axs = simpleaxis(axs)\n","    plt.subplots_adjust(bottom=0.2)\n","#     plt.savefig(fig_save_path + title, format='png')\n","    \n","    return fig"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAeX0ci3cT0_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597600737630,"user_tz":-120,"elapsed":856,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["total_run_steps = 100\n","\n","store_every = 1\n","\n","shared_model_param_dict = {'input_size': 784, 'hidden_size1': 800, 'num_classes': 10, \n","                           'total_run_steps': total_run_steps, 'criterion': nn.MSELoss(), \n","                         'train_loader': train_loader, 'test_loader': test_loader, \n","                          'store_every': store_every, 'weight_decay': 0}\n","\n","fig_save_path = './Figures/fullbatch/'"],"execution_count":133,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYPsShDtcT1C","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597608593101,"user_tz":-120,"elapsed":1463,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["def full_batch_train(initialize, batchnorm, learning_rate, weight_decay, input_size, hidden_size1, num_classes, criterion, total_run_steps, train_loader, test_loader, store_every  ):\n","    images, labels = next(iter(train_loader))\n","    images = images.view(images.shape[0], -1).to(device)\n","    labels =one_hot(labels, 10).to(device)\n","\n","    torch.manual_seed(0)\n","    model = NeuralNet2layer(input_size = input_size, hidden_size1 = hidden_size1, num_classes = num_classes, initialize=initialize,batchnorm = batchnorm).to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, weight_decay = weight_decay)\n","\n","\n","    train_losslist=[]\n","#     train_aclist=[]\n","\n","    test_losslist=[]\n","#     test_aclist=[]\n","    \n","    iter_list = []\n","    \n","    for runsteps in range(total_run_steps):\n","        outputs = model(images).to(device)\n","        loss = criterion(outputs, labels).to(device)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if runsteps%store_every==0 :  \n","            iter_list.append(runsteps)\n","            train_loss = loss_and_accuracy(model, train_loader , criterion)\n","            test_loss = loss_and_accuracy(model, test_loader , criterion)\n","\n","            train_losslist.append(train_loss)\n","\n","            test_losslist.append(test_loss)\n","\n","#             print ('Iter: %03d/%03d | Train Loss: %.8f | Train acc: %.4f' %(runsteps, total_run_steps, train_loss, train_ac))   \n","            print ('Iter: %03d/%03d | Train Loss: %.8f' %(runsteps, total_run_steps, train_loss))   \n","\n","    loss_dict = {'train': train_losslist, 'test': test_losslist}\n","    \n","    train_ac, train_loss = loss_and_accuracy(model, train_loader , criterion, acc_bool = True)\n","    test_ac, test_loss = loss_and_accuracy(model, test_loader , criterion, acc_bool = True)\n","    \n","#     acl_dict = {'train': train_aclist, 'test': test_aclist}\n","    print()\n","    print('Iter: %03d/%03d | Test Loss: %.8f | Test acc: %.4f' %(runsteps, total_run_steps, test_loss, test_ac) )\n","#     return iter_list, loss_dict, acl_dict\n","    return iter_list, loss_dict, train_loss"],"execution_count":175,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRKaJ4tXefB9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597609720224,"user_tz":-120,"elapsed":5180,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}}},"source":["def full_batch_train_(scale,therd,mode,testlossflage,initialize, batchnorm, learning_rate, weight_decay, input_size, hidden_size1, num_classes, criterion, total_run_steps, train_loader, test_loader, store_every):\n","    images, labels = next(iter(train_loader))\n","    images = images.view(images.shape[0], -1).to(device)\n","    labels =one_hot(labels, 10).to(device)\n","\n","    torch.manual_seed(0)\n","    model = NeuralNet2layer(input_size = input_size, hidden_size1 = hidden_size1, num_classes = num_classes, initialize=initialize,batchnorm = batchnorm).to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, weight_decay = weight_decay)\n","\n","\n","    train_losslist=[]\n","#     train_aclist=[]\n","\n","    test_losslist=[]\n","#     test_aclist=[]\n","    \n","    iter_list = []\n","    activation = {}\n","    def get_activation(name):\n","        def hook(model, input, output):\n","            activation[name] = output.detach()\n","        return hook\n","    lold=1000\n","    train_loss=100\n","    for runsteps in range(total_run_steps):\n","        #if (lold<train_loss and runsteps<20) or (runsteps==0):\n","        if (lold<train_loss) or (runsteps==0):\n","            model.fc1.register_forward_hook(get_activation('fc1'))\n","            outputs=model(images).to(device)\n","            adjust_(model,activation['fc1'],images,threshold_u=1.0/therd,threshold_l=therd,scale=scale,ln=1,oflag=1,mode=mode,shuff=0)\n","        \n","        lold=train_loss\n","        outputs = model(images).to(device)\n","        loss = criterion(outputs, labels).to(device)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if runsteps%store_every==0: \n","            iter_list.append(runsteps)\n","            train_loss = loss_and_accuracy(model, train_loader , criterion)\n","            if testlossflage:\n","                test_loss = loss_and_accuracy(model, test_loader , criterion)\n","                test_losslist.append(test_loss)\n","\n","            train_losslist.append(train_loss)\n","\n","            \n","\n","#             print ('Iter: %03d/%03d | Train Loss: %.8f | Train acc: %.4f' %(runsteps, total_run_steps, train_loss, train_ac))   \n","            print ('Iter: %03d/%03d | Train Loss: %.8f' %(runsteps, total_run_steps, train_loss))   \n","\n","    loss_dict = {'train': train_losslist, 'test': test_losslist}\n","    \n","    train_ac, train_loss = loss_and_accuracy(model, train_loader , criterion, acc_bool = True)\n","    test_ac, test_loss = loss_and_accuracy(model, test_loader , criterion, acc_bool = True)\n","    \n","#     acl_dict = {'train': train_aclist, 'test': test_aclist}\n","    print()\n","    print('Iter: %03d/%03d | Test Loss: %.8f | Test acc: %.4f' %(runsteps, total_run_steps, test_loss, test_ac) )\n","#     return iter_list, loss_dict, acl_dict\n","    return iter_list, loss_dict,train_loss"],"execution_count":190,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4GaFQXjBGjV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597597349723,"user_tz":-120,"elapsed":4592,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"ec813e19-5f90-4319-d821-b85de2af0a8e"},"source":["step_list, loss_no_scale_no_bn,train_loss = full_batch_train_(1,0.8,0,initialize = 'LeCun', batchnorm = False, learning_rate = 0.1, ** shared_model_param_dict)\n","print(train_loss)"],"execution_count":117,"outputs":[{"output_type":"stream","text":["Adjusting Layer 1, Kernel Nodes: 662, Adptive Nodes:138\n","Iter: 000/050 | Train Loss: 0.00400724\n","Iter: 001/050 | Train Loss: 0.00260391\n","Iter: 002/050 | Train Loss: 0.00136370\n","Iter: 003/050 | Train Loss: 0.00095701\n","Iter: 004/050 | Train Loss: 0.00098666\n","Adjusting Layer 1, Kernel Nodes: 688, Adptive Nodes:112\n","Iter: 005/050 | Train Loss: 0.00076271\n","Iter: 006/050 | Train Loss: 0.00059651\n","Iter: 007/050 | Train Loss: 0.00057251\n","Iter: 008/050 | Train Loss: 0.00058819\n","Adjusting Layer 1, Kernel Nodes: 719, Adptive Nodes:81\n","Iter: 009/050 | Train Loss: 0.00053403\n","Iter: 010/050 | Train Loss: 0.00042141\n","Iter: 011/050 | Train Loss: 0.00035162\n","Iter: 012/050 | Train Loss: 0.00035212\n","Adjusting Layer 1, Kernel Nodes: 722, Adptive Nodes:78\n","Iter: 013/050 | Train Loss: 0.00034337\n","Iter: 014/050 | Train Loss: 0.00027041\n","Iter: 015/050 | Train Loss: 0.00021140\n","Iter: 016/050 | Train Loss: 0.00021192\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 017/050 | Train Loss: 0.00019342\n","Iter: 018/050 | Train Loss: 0.00013493\n","Iter: 019/050 | Train Loss: 0.00012155\n","Iter: 020/050 | Train Loss: 0.00012293\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 021/050 | Train Loss: 0.00007915\n","Iter: 022/050 | Train Loss: 0.00008044\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 023/050 | Train Loss: 0.00006531\n","Iter: 024/050 | Train Loss: 0.00005775\n","Iter: 025/050 | Train Loss: 0.00005606\n","Iter: 026/050 | Train Loss: 0.00004515\n","Iter: 027/050 | Train Loss: 0.00004776\n","Adjusting Layer 1, Kernel Nodes: 710, Adptive Nodes:90\n","Iter: 028/050 | Train Loss: 0.00004233\n","Iter: 029/050 | Train Loss: 0.00003472\n","Iter: 030/050 | Train Loss: 0.00003595\n","Adjusting Layer 1, Kernel Nodes: 726, Adptive Nodes:74\n","Iter: 031/050 | Train Loss: 0.00004250\n","Adjusting Layer 1, Kernel Nodes: 714, Adptive Nodes:86\n","Iter: 032/050 | Train Loss: 0.00021770\n","Adjusting Layer 1, Kernel Nodes: 720, Adptive Nodes:80\n","Iter: 033/050 | Train Loss: 0.00825402\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 034/050 | Train Loss: 1.01261055\n","Adjusting Layer 1, Kernel Nodes: 735, Adptive Nodes:65\n","Iter: 035/050 | Train Loss: 10.30556679\n","Adjusting Layer 1, Kernel Nodes: 703, Adptive Nodes:97\n","Iter: 036/050 | Train Loss: 0.01309242\n","Iter: 037/050 | Train Loss: 0.00730502\n","Iter: 038/050 | Train Loss: 0.00363105\n","Iter: 039/050 | Train Loss: 0.00272157\n","Iter: 040/050 | Train Loss: 0.00216461\n","Iter: 041/050 | Train Loss: 0.00182392\n","Iter: 042/050 | Train Loss: 0.00166031\n","Iter: 043/050 | Train Loss: 0.00157627\n","Iter: 044/050 | Train Loss: 0.00154166\n","Iter: 045/050 | Train Loss: 0.00151969\n","Iter: 046/050 | Train Loss: 0.00149909\n","Iter: 047/050 | Train Loss: 0.00146897\n","Iter: 048/050 | Train Loss: 0.00143101\n","Iter: 049/050 | Train Loss: 0.00139822\n","\n","Iter: 049/050 | Test Loss: 0.00825550 | Test acc: 12.0800\n","tensor(0.0014)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3_QjPUMHekrY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597598676159,"user_tz":-120,"elapsed":302769,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"1b880268-d961-4a49-eded-6f0321cb5e32"},"source":["# 1.2 0.53. runstep=2\n","maxstep=1000\n","for scale in range(10):\n","    scale=(scale-2)*0.02+0.9\n","    for therd in range(10):\n","        therd=(therd-2)*0.02+0.8\n","        print('scale:{:03f},therd:{:03f}'.format(scale,therd))\n","        step_list, loss_no_scale_no_bn,train_loss = full_batch_train_(scale,therd,2,initialize = 'LeCun', batchnorm = False, learning_rate = 0.1, ** shared_model_param_dict)\n","        if maxstep>train_loss:\n","            maxstep=train_loss\n","            sb=scale\n","            th=therd\n","print(maxstep,sb,th)"],"execution_count":124,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Iter: 004/050 | Train Loss: 0.00099082\n","Iter: 005/050 | Train Loss: 0.00079656\n","Iter: 006/050 | Train Loss: 0.00067748\n","Iter: 007/050 | Train Loss: 0.00063430\n","Iter: 008/050 | Train Loss: 0.00061610\n","Iter: 009/050 | Train Loss: 0.00058713\n","Iter: 010/050 | Train Loss: 0.00052183\n","Iter: 011/050 | Train Loss: 0.00043607\n","Iter: 012/050 | Train Loss: 0.00037665\n","Iter: 013/050 | Train Loss: 0.00036192\n","Iter: 014/050 | Train Loss: 0.00035000\n","Iter: 015/050 | Train Loss: 0.00030744\n","Iter: 016/050 | Train Loss: 0.00026291\n","Iter: 017/050 | Train Loss: 0.00023913\n","Iter: 018/050 | Train Loss: 0.00022535\n","Iter: 019/050 | Train Loss: 0.00020611\n","Iter: 020/050 | Train Loss: 0.00017521\n","Iter: 021/050 | Train Loss: 0.00014427\n","Iter: 022/050 | Train Loss: 0.00013430\n","Iter: 023/050 | Train Loss: 0.00013495\n","Adjusting Layer 1, Kernel Nodes: 688, Adptive Nodes:112\n","Iter: 024/050 | Train Loss: 0.00011723\n","Iter: 025/050 | Train Loss: 0.00009445\n","Iter: 026/050 | Train Loss: 0.00008748\n","Iter: 027/050 | Train Loss: 0.00008389\n","Iter: 028/050 | Train Loss: 0.00007143\n","Iter: 029/050 | Train Loss: 0.00005866\n","Iter: 030/050 | Train Loss: 0.00005300\n","Iter: 031/050 | Train Loss: 0.00004854\n","Iter: 032/050 | Train Loss: 0.00004342\n","Iter: 033/050 | Train Loss: 0.00004008\n","Iter: 034/050 | Train Loss: 0.00003465\n","Iter: 035/050 | Train Loss: 0.00002846\n","Iter: 036/050 | Train Loss: 0.00002752\n","Iter: 037/050 | Train Loss: 0.00002725\n","Iter: 038/050 | Train Loss: 0.00002195\n","Iter: 039/050 | Train Loss: 0.00001746\n","Iter: 040/050 | Train Loss: 0.00001779\n","Adjusting Layer 1, Kernel Nodes: 602, Adptive Nodes:198\n","Iter: 041/050 | Train Loss: 0.00001753\n","Iter: 042/050 | Train Loss: 0.00001408\n","Iter: 043/050 | Train Loss: 0.00001221\n","Iter: 044/050 | Train Loss: 0.00001209\n","Iter: 045/050 | Train Loss: 0.00001090\n","Iter: 046/050 | Train Loss: 0.00000934\n","Iter: 047/050 | Train Loss: 0.00000843\n","Iter: 048/050 | Train Loss: 0.00000771\n","Iter: 049/050 | Train Loss: 0.00000719\n","\n","Iter: 049/050 | Test Loss: 0.00094260 | Test acc: 64.9900\n","scale:0.900000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00409593\n","Iter: 001/050 | Train Loss: 0.00278209\n","Iter: 002/050 | Train Loss: 0.00134071\n","Iter: 003/050 | Train Loss: 0.00097106\n","Iter: 004/050 | Train Loss: 0.00099230\n","Adjusting Layer 1, Kernel Nodes: 775, Adptive Nodes:25\n","Iter: 005/050 | Train Loss: 0.00076000\n","Iter: 006/050 | Train Loss: 0.00059976\n","Iter: 007/050 | Train Loss: 0.00058521\n","Iter: 008/050 | Train Loss: 0.00059951\n","Adjusting Layer 1, Kernel Nodes: 683, Adptive Nodes:117\n","Iter: 009/050 | Train Loss: 0.00053783\n","Iter: 010/050 | Train Loss: 0.00042767\n","Iter: 011/050 | Train Loss: 0.00035740\n","Iter: 012/050 | Train Loss: 0.00035433\n","Iter: 013/050 | Train Loss: 0.00035081\n","Iter: 014/050 | Train Loss: 0.00029290\n","Iter: 015/050 | Train Loss: 0.00022684\n","Iter: 016/050 | Train Loss: 0.00020700\n","Iter: 017/050 | Train Loss: 0.00020967\n","Adjusting Layer 1, Kernel Nodes: 505, Adptive Nodes:295\n","Iter: 018/050 | Train Loss: 0.00018172\n","Iter: 019/050 | Train Loss: 0.00013769\n","Iter: 020/050 | Train Loss: 0.00011911\n","Iter: 021/050 | Train Loss: 0.00011736\n","Iter: 022/050 | Train Loss: 0.00010181\n","Iter: 023/050 | Train Loss: 0.00007905\n","Iter: 024/050 | Train Loss: 0.00006886\n","Iter: 025/050 | Train Loss: 0.00006657\n","Iter: 026/050 | Train Loss: 0.00005990\n","Iter: 027/050 | Train Loss: 0.00005012\n","Iter: 028/050 | Train Loss: 0.00004341\n","Iter: 029/050 | Train Loss: 0.00003971\n","Iter: 030/050 | Train Loss: 0.00003674\n","Iter: 031/050 | Train Loss: 0.00003318\n","Iter: 032/050 | Train Loss: 0.00002913\n","Iter: 033/050 | Train Loss: 0.00002592\n","Iter: 034/050 | Train Loss: 0.00002350\n","Iter: 035/050 | Train Loss: 0.00002149\n","Iter: 036/050 | Train Loss: 0.00001998\n","Iter: 037/050 | Train Loss: 0.00001770\n","Iter: 038/050 | Train Loss: 0.00001448\n","Iter: 039/050 | Train Loss: 0.00001293\n","Iter: 040/050 | Train Loss: 0.00001322\n","Adjusting Layer 1, Kernel Nodes: 607, Adptive Nodes:193\n","Iter: 041/050 | Train Loss: 0.00001138\n","Iter: 042/050 | Train Loss: 0.00000837\n","Iter: 043/050 | Train Loss: 0.00000811\n","Iter: 044/050 | Train Loss: 0.00000849\n","Adjusting Layer 1, Kernel Nodes: 377, Adptive Nodes:423\n","Iter: 045/050 | Train Loss: 0.00000627\n","Iter: 046/050 | Train Loss: 0.00000566\n","Iter: 047/050 | Train Loss: 0.00000620\n","Adjusting Layer 1, Kernel Nodes: 512, Adptive Nodes:288\n","Iter: 048/050 | Train Loss: 0.00000457\n","Iter: 049/050 | Train Loss: 0.00000432\n","\n","Iter: 049/050 | Test Loss: 0.00100977 | Test acc: 63.4500\n","scale:0.900000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00394758\n","Iter: 001/050 | Train Loss: 0.00247708\n","Iter: 002/050 | Train Loss: 0.00138369\n","Iter: 003/050 | Train Loss: 0.00094210\n","Iter: 004/050 | Train Loss: 0.00099112\n","Adjusting Layer 1, Kernel Nodes: 673, Adptive Nodes:127\n","Iter: 005/050 | Train Loss: 0.00075185\n","Iter: 006/050 | Train Loss: 0.00058481\n","Iter: 007/050 | Train Loss: 0.00057627\n","Iter: 008/050 | Train Loss: 0.00059551\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 009/050 | Train Loss: 0.00052528\n","Iter: 010/050 | Train Loss: 0.00041245\n","Iter: 011/050 | Train Loss: 0.00035542\n","Iter: 012/050 | Train Loss: 0.00036078\n","Adjusting Layer 1, Kernel Nodes: 496, Adptive Nodes:304\n","Iter: 013/050 | Train Loss: 0.00034356\n","Iter: 014/050 | Train Loss: 0.00026656\n","Iter: 015/050 | Train Loss: 0.00021576\n","Iter: 016/050 | Train Loss: 0.00021735\n","Adjusting Layer 1, Kernel Nodes: 547, Adptive Nodes:253\n","Iter: 017/050 | Train Loss: 0.00019723\n","Iter: 018/050 | Train Loss: 0.00014243\n","Iter: 019/050 | Train Loss: 0.00012234\n","Iter: 020/050 | Train Loss: 0.00012617\n","Adjusting Layer 1, Kernel Nodes: 683, Adptive Nodes:117\n","Iter: 021/050 | Train Loss: 0.00009138\n","Iter: 022/050 | Train Loss: 0.00007646\n","Iter: 023/050 | Train Loss: 0.00008111\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 024/050 | Train Loss: 0.00005465\n","Iter: 025/050 | Train Loss: 0.00006208\n","Adjusting Layer 1, Kernel Nodes: 90, Adptive Nodes:710\n","Iter: 026/050 | Train Loss: 0.00004554\n","Iter: 027/050 | Train Loss: 0.00004759\n","Adjusting Layer 1, Kernel Nodes: 219, Adptive Nodes:581\n","Iter: 028/050 | Train Loss: 0.00004017\n","Iter: 029/050 | Train Loss: 0.00003497\n","Iter: 030/050 | Train Loss: 0.00003428\n","Iter: 031/050 | Train Loss: 0.00002572\n","Iter: 032/050 | Train Loss: 0.00002712\n","Adjusting Layer 1, Kernel Nodes: 644, Adptive Nodes:156\n","Iter: 033/050 | Train Loss: 0.00002026\n","Iter: 034/050 | Train Loss: 0.00002063\n","Adjusting Layer 1, Kernel Nodes: 122, Adptive Nodes:678\n","Iter: 035/050 | Train Loss: 0.00001578\n","Iter: 036/050 | Train Loss: 0.00001620\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 037/050 | Train Loss: 0.00001355\n","Iter: 038/050 | Train Loss: 0.00001359\n","Adjusting Layer 1, Kernel Nodes: 134, Adptive Nodes:666\n","Iter: 039/050 | Train Loss: 0.00001143\n","Iter: 040/050 | Train Loss: 0.00001140\n","Iter: 041/050 | Train Loss: 0.00000995\n","Iter: 042/050 | Train Loss: 0.00000872\n","Iter: 043/050 | Train Loss: 0.00000808\n","Iter: 044/050 | Train Loss: 0.00000642\n","Iter: 045/050 | Train Loss: 0.00000649\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 046/050 | Train Loss: 0.00000535\n","Iter: 047/050 | Train Loss: 0.00000506\n","Iter: 048/050 | Train Loss: 0.00000433\n","Iter: 049/050 | Train Loss: 0.00000403\n","\n","Iter: 049/050 | Test Loss: 0.00104848 | Test acc: 62.7000\n","scale:0.900000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00382124\n","Iter: 001/050 | Train Loss: 0.00223580\n","Iter: 002/050 | Train Loss: 0.00142761\n","Iter: 003/050 | Train Loss: 0.00091579\n","Iter: 004/050 | Train Loss: 0.00099081\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00074870\n","Iter: 006/050 | Train Loss: 0.00057430\n","Iter: 007/050 | Train Loss: 0.00056898\n","Iter: 008/050 | Train Loss: 0.00059279\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 009/050 | Train Loss: 0.00052422\n","Iter: 010/050 | Train Loss: 0.00040998\n","Iter: 011/050 | Train Loss: 0.00035408\n","Iter: 012/050 | Train Loss: 0.00036294\n","Adjusting Layer 1, Kernel Nodes: 605, Adptive Nodes:195\n","Iter: 013/050 | Train Loss: 0.00034840\n","Iter: 014/050 | Train Loss: 0.00027098\n","Iter: 015/050 | Train Loss: 0.00021915\n","Iter: 016/050 | Train Loss: 0.00022118\n","Adjusting Layer 1, Kernel Nodes: 475, Adptive Nodes:325\n","Iter: 017/050 | Train Loss: 0.00020594\n","Iter: 018/050 | Train Loss: 0.00015278\n","Iter: 019/050 | Train Loss: 0.00012614\n","Iter: 020/050 | Train Loss: 0.00012985\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 021/050 | Train Loss: 0.00010464\n","Iter: 022/050 | Train Loss: 0.00007728\n","Iter: 023/050 | Train Loss: 0.00008020\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 024/050 | Train Loss: 0.00006474\n","Iter: 025/050 | Train Loss: 0.00005306\n","Iter: 026/050 | Train Loss: 0.00005556\n","Adjusting Layer 1, Kernel Nodes: 278, Adptive Nodes:522\n","Iter: 027/050 | Train Loss: 0.00004146\n","Iter: 028/050 | Train Loss: 0.00004387\n","Adjusting Layer 1, Kernel Nodes: 234, Adptive Nodes:566\n","Iter: 029/050 | Train Loss: 0.00003425\n","Iter: 030/050 | Train Loss: 0.00003418\n","Iter: 031/050 | Train Loss: 0.00002835\n","Iter: 032/050 | Train Loss: 0.00002607\n","Iter: 033/050 | Train Loss: 0.00002284\n","Iter: 034/050 | Train Loss: 0.00001990\n","Iter: 035/050 | Train Loss: 0.00001766\n","Iter: 036/050 | Train Loss: 0.00001548\n","Iter: 037/050 | Train Loss: 0.00001395\n","Iter: 038/050 | Train Loss: 0.00001277\n","Iter: 039/050 | Train Loss: 0.00001214\n","Iter: 040/050 | Train Loss: 0.00001080\n","Iter: 041/050 | Train Loss: 0.00001066\n","Iter: 042/050 | Train Loss: 0.00000878\n","Iter: 043/050 | Train Loss: 0.00000860\n","Iter: 044/050 | Train Loss: 0.00000718\n","Iter: 045/050 | Train Loss: 0.00000646\n","Iter: 046/050 | Train Loss: 0.00000610\n","Iter: 047/050 | Train Loss: 0.00000483\n","Iter: 048/050 | Train Loss: 0.00000501\n","Adjusting Layer 1, Kernel Nodes: 516, Adptive Nodes:284\n","Iter: 049/050 | Train Loss: 0.00000388\n","\n","Iter: 049/050 | Test Loss: 0.00104097 | Test acc: 62.9200\n","scale:0.900000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00371622\n","Iter: 001/050 | Train Loss: 0.00205706\n","Iter: 002/050 | Train Loss: 0.00146380\n","Iter: 003/050 | Train Loss: 0.00089413\n","Iter: 004/050 | Train Loss: 0.00098656\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 005/050 | Train Loss: 0.00074738\n","Iter: 006/050 | Train Loss: 0.00056685\n","Iter: 007/050 | Train Loss: 0.00056266\n","Iter: 008/050 | Train Loss: 0.00058937\n","Adjusting Layer 1, Kernel Nodes: 761, Adptive Nodes:39\n","Iter: 009/050 | Train Loss: 0.00052342\n","Iter: 010/050 | Train Loss: 0.00040921\n","Iter: 011/050 | Train Loss: 0.00035262\n","Iter: 012/050 | Train Loss: 0.00036367\n","Adjusting Layer 1, Kernel Nodes: 762, Adptive Nodes:38\n","Iter: 013/050 | Train Loss: 0.00035277\n","Iter: 014/050 | Train Loss: 0.00027509\n","Iter: 015/050 | Train Loss: 0.00022068\n","Iter: 016/050 | Train Loss: 0.00022290\n","Adjusting Layer 1, Kernel Nodes: 442, Adptive Nodes:358\n","Iter: 017/050 | Train Loss: 0.00021165\n","Iter: 018/050 | Train Loss: 0.00016058\n","Iter: 019/050 | Train Loss: 0.00012926\n","Iter: 020/050 | Train Loss: 0.00013132\n","Adjusting Layer 1, Kernel Nodes: 518, Adptive Nodes:282\n","Iter: 021/050 | Train Loss: 0.00011429\n","Iter: 022/050 | Train Loss: 0.00008386\n","Iter: 023/050 | Train Loss: 0.00007949\n","Iter: 024/050 | Train Loss: 0.00007661\n","Iter: 025/050 | Train Loss: 0.00005960\n","Iter: 026/050 | Train Loss: 0.00005110\n","Iter: 027/050 | Train Loss: 0.00005122\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 028/050 | Train Loss: 0.00004313\n","Iter: 029/050 | Train Loss: 0.00003682\n","Iter: 030/050 | Train Loss: 0.00003558\n","Iter: 031/050 | Train Loss: 0.00003132\n","Iter: 032/050 | Train Loss: 0.00002676\n","Iter: 033/050 | Train Loss: 0.00002500\n","Iter: 034/050 | Train Loss: 0.00002302\n","Iter: 035/050 | Train Loss: 0.00001925\n","Iter: 036/050 | Train Loss: 0.00001737\n","Iter: 037/050 | Train Loss: 0.00001681\n","Iter: 038/050 | Train Loss: 0.00001361\n","Iter: 039/050 | Train Loss: 0.00001169\n","Iter: 040/050 | Train Loss: 0.00001193\n","Adjusting Layer 1, Kernel Nodes: 249, Adptive Nodes:551\n","Iter: 041/050 | Train Loss: 0.00000928\n","Iter: 042/050 | Train Loss: 0.00000852\n","Iter: 043/050 | Train Loss: 0.00000885\n","Adjusting Layer 1, Kernel Nodes: 513, Adptive Nodes:287\n","Iter: 044/050 | Train Loss: 0.00000649\n","Iter: 045/050 | Train Loss: 0.00000723\n","Adjusting Layer 1, Kernel Nodes: 485, Adptive Nodes:315\n","Iter: 046/050 | Train Loss: 0.00000537\n","Iter: 047/050 | Train Loss: 0.00000540\n","Adjusting Layer 1, Kernel Nodes: 244, Adptive Nodes:556\n","Iter: 048/050 | Train Loss: 0.00000463\n","Iter: 049/050 | Train Loss: 0.00000375\n","\n","Iter: 049/050 | Test Loss: 0.00101187 | Test acc: 63.8800\n","scale:0.900000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00363021\n","Iter: 001/050 | Train Loss: 0.00191719\n","Iter: 002/050 | Train Loss: 0.00149229\n","Iter: 003/050 | Train Loss: 0.00087576\n","Iter: 004/050 | Train Loss: 0.00097902\n","Adjusting Layer 1, Kernel Nodes: 662, Adptive Nodes:138\n","Iter: 005/050 | Train Loss: 0.00074681\n","Iter: 006/050 | Train Loss: 0.00056094\n","Iter: 007/050 | Train Loss: 0.00055643\n","Iter: 008/050 | Train Loss: 0.00058463\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 009/050 | Train Loss: 0.00052032\n","Iter: 010/050 | Train Loss: 0.00040784\n","Iter: 011/050 | Train Loss: 0.00035283\n","Iter: 012/050 | Train Loss: 0.00036454\n","Adjusting Layer 1, Kernel Nodes: 700, Adptive Nodes:100\n","Iter: 013/050 | Train Loss: 0.00035401\n","Iter: 014/050 | Train Loss: 0.00027818\n","Iter: 015/050 | Train Loss: 0.00022275\n","Iter: 016/050 | Train Loss: 0.00022353\n","Adjusting Layer 1, Kernel Nodes: 555, Adptive Nodes:245\n","Iter: 017/050 | Train Loss: 0.00021694\n","Iter: 018/050 | Train Loss: 0.00016800\n","Iter: 019/050 | Train Loss: 0.00013232\n","Iter: 020/050 | Train Loss: 0.00013431\n","Adjusting Layer 1, Kernel Nodes: 672, Adptive Nodes:128\n","Iter: 021/050 | Train Loss: 0.00012289\n","Iter: 022/050 | Train Loss: 0.00008977\n","Iter: 023/050 | Train Loss: 0.00008127\n","Iter: 024/050 | Train Loss: 0.00008086\n","Iter: 025/050 | Train Loss: 0.00006558\n","Iter: 026/050 | Train Loss: 0.00005330\n","Iter: 027/050 | Train Loss: 0.00005189\n","Iter: 028/050 | Train Loss: 0.00004806\n","Iter: 029/050 | Train Loss: 0.00003965\n","Iter: 030/050 | Train Loss: 0.00003501\n","Iter: 031/050 | Train Loss: 0.00003324\n","Iter: 032/050 | Train Loss: 0.00003037\n","Iter: 033/050 | Train Loss: 0.00002693\n","Iter: 034/050 | Train Loss: 0.00002275\n","Iter: 035/050 | Train Loss: 0.00002089\n","Iter: 036/050 | Train Loss: 0.00002111\n","Adjusting Layer 1, Kernel Nodes: 569, Adptive Nodes:231\n","Iter: 037/050 | Train Loss: 0.00001655\n","Iter: 038/050 | Train Loss: 0.00001376\n","Iter: 039/050 | Train Loss: 0.00001515\n","Adjusting Layer 1, Kernel Nodes: 651, Adptive Nodes:149\n","Iter: 040/050 | Train Loss: 0.00001131\n","Iter: 041/050 | Train Loss: 0.00000953\n","Iter: 042/050 | Train Loss: 0.00001048\n","Adjusting Layer 1, Kernel Nodes: 369, Adptive Nodes:431\n","Iter: 043/050 | Train Loss: 0.00000715\n","Iter: 044/050 | Train Loss: 0.00000747\n","Adjusting Layer 1, Kernel Nodes: 413, Adptive Nodes:387\n","Iter: 045/050 | Train Loss: 0.00000671\n","Iter: 046/050 | Train Loss: 0.00000540\n","Iter: 047/050 | Train Loss: 0.00000572\n","Adjusting Layer 1, Kernel Nodes: 342, Adptive Nodes:458\n","Iter: 048/050 | Train Loss: 0.00000426\n","Iter: 049/050 | Train Loss: 0.00000432\n","\n","Iter: 049/050 | Test Loss: 0.00098564 | Test acc: 64.5300\n","scale:0.900000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00356083\n","Iter: 001/050 | Train Loss: 0.00181586\n","Iter: 002/050 | Train Loss: 0.00151645\n","Iter: 003/050 | Train Loss: 0.00086206\n","Iter: 004/050 | Train Loss: 0.00097044\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 005/050 | Train Loss: 0.00074977\n","Iter: 006/050 | Train Loss: 0.00055927\n","Iter: 007/050 | Train Loss: 0.00055088\n","Iter: 008/050 | Train Loss: 0.00058225\n","Adjusting Layer 1, Kernel Nodes: 713, Adptive Nodes:87\n","Iter: 009/050 | Train Loss: 0.00052455\n","Iter: 010/050 | Train Loss: 0.00041194\n","Iter: 011/050 | Train Loss: 0.00035311\n","Iter: 012/050 | Train Loss: 0.00036429\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 013/050 | Train Loss: 0.00035885\n","Iter: 014/050 | Train Loss: 0.00028678\n","Iter: 015/050 | Train Loss: 0.00022783\n","Iter: 016/050 | Train Loss: 0.00022430\n","Iter: 017/050 | Train Loss: 0.00022399\n","Iter: 018/050 | Train Loss: 0.00018757\n","Iter: 019/050 | Train Loss: 0.00014478\n","Iter: 020/050 | Train Loss: 0.00013028\n","Iter: 021/050 | Train Loss: 0.00013050\n","Adjusting Layer 1, Kernel Nodes: 741, Adptive Nodes:59\n","Iter: 022/050 | Train Loss: 0.00011530\n","Iter: 023/050 | Train Loss: 0.00009082\n","Iter: 024/050 | Train Loss: 0.00007981\n","Iter: 025/050 | Train Loss: 0.00007746\n","Iter: 026/050 | Train Loss: 0.00006996\n","Iter: 027/050 | Train Loss: 0.00005826\n","Iter: 028/050 | Train Loss: 0.00004974\n","Iter: 029/050 | Train Loss: 0.00004513\n","Iter: 030/050 | Train Loss: 0.00004229\n","Iter: 031/050 | Train Loss: 0.00003917\n","Iter: 032/050 | Train Loss: 0.00003255\n","Iter: 033/050 | Train Loss: 0.00002630\n","Iter: 034/050 | Train Loss: 0.00002681\n","Adjusting Layer 1, Kernel Nodes: 603, Adptive Nodes:197\n","Iter: 035/050 | Train Loss: 0.00002708\n","Adjusting Layer 1, Kernel Nodes: 544, Adptive Nodes:256\n","Iter: 036/050 | Train Loss: 0.00001973\n","Iter: 037/050 | Train Loss: 0.00001713\n","Iter: 038/050 | Train Loss: 0.00001955\n","Adjusting Layer 1, Kernel Nodes: 479, Adptive Nodes:321\n","Iter: 039/050 | Train Loss: 0.00001559\n","Iter: 040/050 | Train Loss: 0.00001177\n","Iter: 041/050 | Train Loss: 0.00001333\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 042/050 | Train Loss: 0.00001109\n","Iter: 043/050 | Train Loss: 0.00000823\n","Iter: 044/050 | Train Loss: 0.00000900\n","Adjusting Layer 1, Kernel Nodes: 489, Adptive Nodes:311\n","Iter: 045/050 | Train Loss: 0.00000741\n","Iter: 046/050 | Train Loss: 0.00000598\n","Iter: 047/050 | Train Loss: 0.00000643\n","Adjusting Layer 1, Kernel Nodes: 480, Adptive Nodes:320\n","Iter: 048/050 | Train Loss: 0.00000498\n","Iter: 049/050 | Train Loss: 0.00000466\n","\n","Iter: 049/050 | Test Loss: 0.00096212 | Test acc: 65.2500\n","scale:0.900000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00350595\n","Iter: 001/050 | Train Loss: 0.00173923\n","Iter: 002/050 | Train Loss: 0.00153407\n","Iter: 003/050 | Train Loss: 0.00085074\n","Iter: 004/050 | Train Loss: 0.00096362\n","Adjusting Layer 1, Kernel Nodes: 669, Adptive Nodes:131\n","Iter: 005/050 | Train Loss: 0.00075408\n","Iter: 006/050 | Train Loss: 0.00055853\n","Iter: 007/050 | Train Loss: 0.00054589\n","Iter: 008/050 | Train Loss: 0.00057917\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 009/050 | Train Loss: 0.00052831\n","Iter: 010/050 | Train Loss: 0.00041849\n","Iter: 011/050 | Train Loss: 0.00035393\n","Iter: 012/050 | Train Loss: 0.00036084\n","Adjusting Layer 1, Kernel Nodes: 689, Adptive Nodes:111\n","Iter: 013/050 | Train Loss: 0.00036150\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 014/050 | Train Loss: 0.00028925\n","Iter: 015/050 | Train Loss: 0.00022726\n","Iter: 016/050 | Train Loss: 0.00022688\n","Iter: 017/050 | Train Loss: 0.00022602\n","Iter: 018/050 | Train Loss: 0.00018307\n","Iter: 019/050 | Train Loss: 0.00014076\n","Iter: 020/050 | Train Loss: 0.00013513\n","Iter: 021/050 | Train Loss: 0.00013497\n","Iter: 022/050 | Train Loss: 0.00011196\n","Iter: 023/050 | Train Loss: 0.00008871\n","Iter: 024/050 | Train Loss: 0.00008234\n","Iter: 025/050 | Train Loss: 0.00008024\n","Iter: 026/050 | Train Loss: 0.00007140\n","Iter: 027/050 | Train Loss: 0.00005969\n","Iter: 028/050 | Train Loss: 0.00005097\n","Iter: 029/050 | Train Loss: 0.00004559\n","Iter: 030/050 | Train Loss: 0.00004368\n","Iter: 031/050 | Train Loss: 0.00004129\n","Iter: 032/050 | Train Loss: 0.00003279\n","Iter: 033/050 | Train Loss: 0.00002606\n","Iter: 034/050 | Train Loss: 0.00002786\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 035/050 | Train Loss: 0.00002808\n","Adjusting Layer 1, Kernel Nodes: 577, Adptive Nodes:223\n","Iter: 036/050 | Train Loss: 0.00002077\n","Iter: 037/050 | Train Loss: 0.00001711\n","Iter: 038/050 | Train Loss: 0.00001876\n","Adjusting Layer 1, Kernel Nodes: 525, Adptive Nodes:275\n","Iter: 039/050 | Train Loss: 0.00001728\n","Iter: 040/050 | Train Loss: 0.00001308\n","Iter: 041/050 | Train Loss: 0.00001175\n","Iter: 042/050 | Train Loss: 0.00001209\n","Adjusting Layer 1, Kernel Nodes: 758, Adptive Nodes:42\n","Iter: 043/050 | Train Loss: 0.00001042\n","Iter: 044/050 | Train Loss: 0.00000814\n","Iter: 045/050 | Train Loss: 0.00000787\n","Iter: 046/050 | Train Loss: 0.00000789\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 047/050 | Train Loss: 0.00000608\n","Iter: 048/050 | Train Loss: 0.00000545\n","Iter: 049/050 | Train Loss: 0.00000583\n","\n","Iter: 049/050 | Test Loss: 0.00095426 | Test acc: 65.4700\n","scale:0.900000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00346386\n","Iter: 001/050 | Train Loss: 0.00168330\n","Iter: 002/050 | Train Loss: 0.00154728\n","Iter: 003/050 | Train Loss: 0.00084145\n","Iter: 004/050 | Train Loss: 0.00095666\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 005/050 | Train Loss: 0.00075909\n","Iter: 006/050 | Train Loss: 0.00055831\n","Iter: 007/050 | Train Loss: 0.00054072\n","Iter: 008/050 | Train Loss: 0.00057511\n","Adjusting Layer 1, Kernel Nodes: 755, Adptive Nodes:45\n","Iter: 009/050 | Train Loss: 0.00053003\n","Iter: 010/050 | Train Loss: 0.00042230\n","Iter: 011/050 | Train Loss: 0.00035400\n","Iter: 012/050 | Train Loss: 0.00035679\n","Adjusting Layer 1, Kernel Nodes: 716, Adptive Nodes:84\n","Iter: 013/050 | Train Loss: 0.00036064\n","Adjusting Layer 1, Kernel Nodes: 634, Adptive Nodes:166\n","Iter: 014/050 | Train Loss: 0.00029583\n","Iter: 015/050 | Train Loss: 0.00023011\n","Iter: 016/050 | Train Loss: 0.00022198\n","Iter: 017/050 | Train Loss: 0.00022565\n","Adjusting Layer 1, Kernel Nodes: 681, Adptive Nodes:119\n","Iter: 018/050 | Train Loss: 0.00018987\n","Iter: 019/050 | Train Loss: 0.00014518\n","Iter: 020/050 | Train Loss: 0.00013346\n","Iter: 021/050 | Train Loss: 0.00013557\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 022/050 | Train Loss: 0.00011596\n","Iter: 023/050 | Train Loss: 0.00008958\n","Iter: 024/050 | Train Loss: 0.00008287\n","Iter: 025/050 | Train Loss: 0.00008244\n","Iter: 026/050 | Train Loss: 0.00007145\n","Iter: 027/050 | Train Loss: 0.00005744\n","Iter: 028/050 | Train Loss: 0.00005058\n","Iter: 029/050 | Train Loss: 0.00004838\n","Iter: 030/050 | Train Loss: 0.00004531\n","Iter: 031/050 | Train Loss: 0.00003809\n","Iter: 032/050 | Train Loss: 0.00002997\n","Iter: 033/050 | Train Loss: 0.00002906\n","Iter: 034/050 | Train Loss: 0.00003077\n","Adjusting Layer 1, Kernel Nodes: 583, Adptive Nodes:217\n","Iter: 035/050 | Train Loss: 0.00002474\n","Iter: 036/050 | Train Loss: 0.00001837\n","Iter: 037/050 | Train Loss: 0.00001952\n","Adjusting Layer 1, Kernel Nodes: 588, Adptive Nodes:212\n","Iter: 038/050 | Train Loss: 0.00001998\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 039/050 | Train Loss: 0.00001486\n","Iter: 040/050 | Train Loss: 0.00001266\n","Iter: 041/050 | Train Loss: 0.00001352\n","Adjusting Layer 1, Kernel Nodes: 586, Adptive Nodes:214\n","Iter: 042/050 | Train Loss: 0.00001174\n","Iter: 043/050 | Train Loss: 0.00000911\n","Iter: 044/050 | Train Loss: 0.00000874\n","Iter: 045/050 | Train Loss: 0.00000868\n","Iter: 046/050 | Train Loss: 0.00000708\n","Iter: 047/050 | Train Loss: 0.00000582\n","Iter: 048/050 | Train Loss: 0.00000612\n","Adjusting Layer 1, Kernel Nodes: 626, Adptive Nodes:174\n","Iter: 049/050 | Train Loss: 0.00000562\n","\n","Iter: 049/050 | Test Loss: 0.00094625 | Test acc: 65.5900\n","scale:0.900000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 000/050 | Train Loss: 0.00343269\n","Iter: 001/050 | Train Loss: 0.00164057\n","Iter: 002/050 | Train Loss: 0.00155698\n","Iter: 003/050 | Train Loss: 0.00083326\n","Iter: 004/050 | Train Loss: 0.00094573\n","Adjusting Layer 1, Kernel Nodes: 670, Adptive Nodes:130\n","Iter: 005/050 | Train Loss: 0.00076336\n","Iter: 006/050 | Train Loss: 0.00055823\n","Iter: 007/050 | Train Loss: 0.00053620\n","Iter: 008/050 | Train Loss: 0.00057146\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 009/050 | Train Loss: 0.00052898\n","Iter: 010/050 | Train Loss: 0.00042204\n","Iter: 011/050 | Train Loss: 0.00035315\n","Iter: 012/050 | Train Loss: 0.00035467\n","Adjusting Layer 1, Kernel Nodes: 760, Adptive Nodes:40\n","Iter: 013/050 | Train Loss: 0.00035808\n","Adjusting Layer 1, Kernel Nodes: 655, Adptive Nodes:145\n","Iter: 014/050 | Train Loss: 0.00029747\n","Iter: 015/050 | Train Loss: 0.00023232\n","Iter: 016/050 | Train Loss: 0.00022053\n","Iter: 017/050 | Train Loss: 0.00022544\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 018/050 | Train Loss: 0.00019666\n","Iter: 019/050 | Train Loss: 0.00015143\n","Iter: 020/050 | Train Loss: 0.00013190\n","Iter: 021/050 | Train Loss: 0.00013552\n","Adjusting Layer 1, Kernel Nodes: 726, Adptive Nodes:74\n","Iter: 022/050 | Train Loss: 0.00012404\n","Iter: 023/050 | Train Loss: 0.00009580\n","Iter: 024/050 | Train Loss: 0.00008255\n","Iter: 025/050 | Train Loss: 0.00008201\n","Iter: 026/050 | Train Loss: 0.00007640\n","Iter: 027/050 | Train Loss: 0.00006459\n","Iter: 028/050 | Train Loss: 0.00005236\n","Iter: 029/050 | Train Loss: 0.00004562\n","Iter: 030/050 | Train Loss: 0.00004648\n","Adjusting Layer 1, Kernel Nodes: 769, Adptive Nodes:31\n","Iter: 031/050 | Train Loss: 0.00004366\n","Iter: 032/050 | Train Loss: 0.00003288\n","Iter: 033/050 | Train Loss: 0.00002766\n","Iter: 034/050 | Train Loss: 0.00003008\n","Adjusting Layer 1, Kernel Nodes: 680, Adptive Nodes:120\n","Iter: 035/050 | Train Loss: 0.00002870\n","Iter: 036/050 | Train Loss: 0.00002187\n","Iter: 037/050 | Train Loss: 0.00001802\n","Iter: 038/050 | Train Loss: 0.00001880\n","Adjusting Layer 1, Kernel Nodes: 774, Adptive Nodes:26\n","Iter: 039/050 | Train Loss: 0.00001854\n","Iter: 040/050 | Train Loss: 0.00001489\n","Iter: 041/050 | Train Loss: 0.00001198\n","Iter: 042/050 | Train Loss: 0.00001212\n","Adjusting Layer 1, Kernel Nodes: 548, Adptive Nodes:252\n","Iter: 043/050 | Train Loss: 0.00001229\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 044/050 | Train Loss: 0.00000976\n","Iter: 045/050 | Train Loss: 0.00000775\n","Iter: 046/050 | Train Loss: 0.00000821\n","Adjusting Layer 1, Kernel Nodes: 489, Adptive Nodes:311\n","Iter: 047/050 | Train Loss: 0.00000787\n","Iter: 048/050 | Train Loss: 0.00000591\n","Iter: 049/050 | Train Loss: 0.00000535\n","\n","Iter: 049/050 | Test Loss: 0.00093916 | Test acc: 65.5900\n","scale:0.920000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00426700\n","Iter: 001/050 | Train Loss: 0.00318132\n","Iter: 002/050 | Train Loss: 0.00129353\n","Iter: 003/050 | Train Loss: 0.00100367\n","Iter: 004/050 | Train Loss: 0.00099057\n","Iter: 005/050 | Train Loss: 0.00079637\n","Iter: 006/050 | Train Loss: 0.00067686\n","Iter: 007/050 | Train Loss: 0.00063376\n","Iter: 008/050 | Train Loss: 0.00061606\n","Iter: 009/050 | Train Loss: 0.00058717\n","Iter: 010/050 | Train Loss: 0.00052169\n","Iter: 011/050 | Train Loss: 0.00043569\n","Iter: 012/050 | Train Loss: 0.00037680\n","Iter: 013/050 | Train Loss: 0.00036212\n","Iter: 014/050 | Train Loss: 0.00034944\n","Iter: 015/050 | Train Loss: 0.00030691\n","Iter: 016/050 | Train Loss: 0.00026298\n","Iter: 017/050 | Train Loss: 0.00023860\n","Iter: 018/050 | Train Loss: 0.00022436\n","Iter: 019/050 | Train Loss: 0.00020618\n","Iter: 020/050 | Train Loss: 0.00017615\n","Iter: 021/050 | Train Loss: 0.00014429\n","Iter: 022/050 | Train Loss: 0.00013380\n","Iter: 023/050 | Train Loss: 0.00013514\n","Adjusting Layer 1, Kernel Nodes: 688, Adptive Nodes:112\n","Iter: 024/050 | Train Loss: 0.00011734\n","Iter: 025/050 | Train Loss: 0.00009405\n","Iter: 026/050 | Train Loss: 0.00008706\n","Iter: 027/050 | Train Loss: 0.00008373\n","Iter: 028/050 | Train Loss: 0.00007123\n","Iter: 029/050 | Train Loss: 0.00005835\n","Iter: 030/050 | Train Loss: 0.00005293\n","Iter: 031/050 | Train Loss: 0.00004849\n","Iter: 032/050 | Train Loss: 0.00004309\n","Iter: 033/050 | Train Loss: 0.00003987\n","Iter: 034/050 | Train Loss: 0.00003466\n","Iter: 035/050 | Train Loss: 0.00002853\n","Iter: 036/050 | Train Loss: 0.00002767\n","Iter: 037/050 | Train Loss: 0.00002726\n","Iter: 038/050 | Train Loss: 0.00002182\n","Iter: 039/050 | Train Loss: 0.00001756\n","Iter: 040/050 | Train Loss: 0.00001795\n","Adjusting Layer 1, Kernel Nodes: 596, Adptive Nodes:204\n","Iter: 041/050 | Train Loss: 0.00001733\n","Iter: 042/050 | Train Loss: 0.00001393\n","Iter: 043/050 | Train Loss: 0.00001240\n","Iter: 044/050 | Train Loss: 0.00001216\n","Iter: 045/050 | Train Loss: 0.00001081\n","Iter: 046/050 | Train Loss: 0.00000945\n","Iter: 047/050 | Train Loss: 0.00000858\n","Iter: 048/050 | Train Loss: 0.00000769\n","Iter: 049/050 | Train Loss: 0.00000720\n","\n","Iter: 049/050 | Test Loss: 0.00094357 | Test acc: 65.0100\n","scale:0.920000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00409051\n","Iter: 001/050 | Train Loss: 0.00277086\n","Iter: 002/050 | Train Loss: 0.00134252\n","Iter: 003/050 | Train Loss: 0.00097016\n","Iter: 004/050 | Train Loss: 0.00099220\n","Adjusting Layer 1, Kernel Nodes: 775, Adptive Nodes:25\n","Iter: 005/050 | Train Loss: 0.00075972\n","Iter: 006/050 | Train Loss: 0.00059948\n","Iter: 007/050 | Train Loss: 0.00058497\n","Iter: 008/050 | Train Loss: 0.00059911\n","Adjusting Layer 1, Kernel Nodes: 674, Adptive Nodes:126\n","Iter: 009/050 | Train Loss: 0.00053708\n","Iter: 010/050 | Train Loss: 0.00042693\n","Iter: 011/050 | Train Loss: 0.00035705\n","Iter: 012/050 | Train Loss: 0.00035420\n","Iter: 013/050 | Train Loss: 0.00035034\n","Iter: 014/050 | Train Loss: 0.00029197\n","Iter: 015/050 | Train Loss: 0.00022615\n","Iter: 016/050 | Train Loss: 0.00020682\n","Iter: 017/050 | Train Loss: 0.00020933\n","Adjusting Layer 1, Kernel Nodes: 551, Adptive Nodes:249\n","Iter: 018/050 | Train Loss: 0.00018097\n","Iter: 019/050 | Train Loss: 0.00013687\n","Iter: 020/050 | Train Loss: 0.00011894\n","Iter: 021/050 | Train Loss: 0.00011769\n","Iter: 022/050 | Train Loss: 0.00010138\n","Iter: 023/050 | Train Loss: 0.00007792\n","Iter: 024/050 | Train Loss: 0.00006863\n","Iter: 025/050 | Train Loss: 0.00006721\n","Iter: 026/050 | Train Loss: 0.00005979\n","Iter: 027/050 | Train Loss: 0.00004930\n","Iter: 028/050 | Train Loss: 0.00004332\n","Iter: 029/050 | Train Loss: 0.00004029\n","Iter: 030/050 | Train Loss: 0.00003679\n","Iter: 031/050 | Train Loss: 0.00003275\n","Iter: 032/050 | Train Loss: 0.00002899\n","Iter: 033/050 | Train Loss: 0.00002601\n","Iter: 034/050 | Train Loss: 0.00002351\n","Iter: 035/050 | Train Loss: 0.00002145\n","Iter: 036/050 | Train Loss: 0.00001983\n","Iter: 037/050 | Train Loss: 0.00001748\n","Iter: 038/050 | Train Loss: 0.00001446\n","Iter: 039/050 | Train Loss: 0.00001309\n","Iter: 040/050 | Train Loss: 0.00001321\n","Adjusting Layer 1, Kernel Nodes: 433, Adptive Nodes:367\n","Iter: 041/050 | Train Loss: 0.00001108\n","Iter: 042/050 | Train Loss: 0.00000831\n","Iter: 043/050 | Train Loss: 0.00000832\n","Adjusting Layer 1, Kernel Nodes: 524, Adptive Nodes:276\n","Iter: 044/050 | Train Loss: 0.00000807\n","Iter: 045/050 | Train Loss: 0.00000602\n","Iter: 046/050 | Train Loss: 0.00000594\n","Iter: 047/050 | Train Loss: 0.00000603\n","Adjusting Layer 1, Kernel Nodes: 370, Adptive Nodes:430\n","Iter: 048/050 | Train Loss: 0.00000451\n","Iter: 049/050 | Train Loss: 0.00000453\n","\n","Iter: 049/050 | Test Loss: 0.00101071 | Test acc: 63.4400\n","scale:0.920000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00394212\n","Iter: 001/050 | Train Loss: 0.00246741\n","Iter: 002/050 | Train Loss: 0.00138510\n","Iter: 003/050 | Train Loss: 0.00094167\n","Iter: 004/050 | Train Loss: 0.00099114\n","Adjusting Layer 1, Kernel Nodes: 672, Adptive Nodes:128\n","Iter: 005/050 | Train Loss: 0.00075177\n","Iter: 006/050 | Train Loss: 0.00058446\n","Iter: 007/050 | Train Loss: 0.00057576\n","Iter: 008/050 | Train Loss: 0.00059497\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 009/050 | Train Loss: 0.00052474\n","Iter: 010/050 | Train Loss: 0.00041214\n","Iter: 011/050 | Train Loss: 0.00035519\n","Iter: 012/050 | Train Loss: 0.00036090\n","Adjusting Layer 1, Kernel Nodes: 491, Adptive Nodes:309\n","Iter: 013/050 | Train Loss: 0.00034327\n","Iter: 014/050 | Train Loss: 0.00026654\n","Iter: 015/050 | Train Loss: 0.00021603\n","Iter: 016/050 | Train Loss: 0.00021709\n","Adjusting Layer 1, Kernel Nodes: 537, Adptive Nodes:263\n","Iter: 017/050 | Train Loss: 0.00019671\n","Iter: 018/050 | Train Loss: 0.00014252\n","Iter: 019/050 | Train Loss: 0.00012200\n","Iter: 020/050 | Train Loss: 0.00012555\n","Adjusting Layer 1, Kernel Nodes: 779, Adptive Nodes:21\n","Iter: 021/050 | Train Loss: 0.00009116\n","Iter: 022/050 | Train Loss: 0.00007637\n","Iter: 023/050 | Train Loss: 0.00008065\n","Adjusting Layer 1, Kernel Nodes: 370, Adptive Nodes:430\n","Iter: 024/050 | Train Loss: 0.00005525\n","Iter: 025/050 | Train Loss: 0.00006155\n","Adjusting Layer 1, Kernel Nodes: 544, Adptive Nodes:256\n","Iter: 026/050 | Train Loss: 0.00004615\n","Iter: 027/050 | Train Loss: 0.00004770\n","Adjusting Layer 1, Kernel Nodes: 186, Adptive Nodes:614\n","Iter: 028/050 | Train Loss: 0.00003918\n","Iter: 029/050 | Train Loss: 0.00003539\n","Iter: 030/050 | Train Loss: 0.00003383\n","Iter: 031/050 | Train Loss: 0.00002606\n","Iter: 032/050 | Train Loss: 0.00002676\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 033/050 | Train Loss: 0.00002079\n","Iter: 034/050 | Train Loss: 0.00002020\n","Iter: 035/050 | Train Loss: 0.00001668\n","Iter: 036/050 | Train Loss: 0.00001623\n","Iter: 037/050 | Train Loss: 0.00001422\n","Iter: 038/050 | Train Loss: 0.00001381\n","Iter: 039/050 | Train Loss: 0.00001236\n","Iter: 040/050 | Train Loss: 0.00001157\n","Iter: 041/050 | Train Loss: 0.00000993\n","Iter: 042/050 | Train Loss: 0.00000907\n","Iter: 043/050 | Train Loss: 0.00000762\n","Iter: 044/050 | Train Loss: 0.00000723\n","Iter: 045/050 | Train Loss: 0.00000597\n","Iter: 046/050 | Train Loss: 0.00000582\n","Iter: 047/050 | Train Loss: 0.00000463\n","Iter: 048/050 | Train Loss: 0.00000470\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 049/050 | Train Loss: 0.00000439\n","\n","Iter: 049/050 | Test Loss: 0.00104727 | Test acc: 62.8100\n","scale:0.920000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00381833\n","Iter: 001/050 | Train Loss: 0.00223178\n","Iter: 002/050 | Train Loss: 0.00142858\n","Iter: 003/050 | Train Loss: 0.00091568\n","Iter: 004/050 | Train Loss: 0.00099081\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00074885\n","Iter: 006/050 | Train Loss: 0.00057400\n","Iter: 007/050 | Train Loss: 0.00056848\n","Iter: 008/050 | Train Loss: 0.00059250\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 009/050 | Train Loss: 0.00052376\n","Iter: 010/050 | Train Loss: 0.00040923\n","Iter: 011/050 | Train Loss: 0.00035363\n","Iter: 012/050 | Train Loss: 0.00036297\n","Adjusting Layer 1, Kernel Nodes: 603, Adptive Nodes:197\n","Iter: 013/050 | Train Loss: 0.00034788\n","Iter: 014/050 | Train Loss: 0.00026998\n","Iter: 015/050 | Train Loss: 0.00021877\n","Iter: 016/050 | Train Loss: 0.00022123\n","Adjusting Layer 1, Kernel Nodes: 516, Adptive Nodes:284\n","Iter: 017/050 | Train Loss: 0.00020540\n","Iter: 018/050 | Train Loss: 0.00015171\n","Iter: 019/050 | Train Loss: 0.00012633\n","Iter: 020/050 | Train Loss: 0.00013054\n","Adjusting Layer 1, Kernel Nodes: 560, Adptive Nodes:240\n","Iter: 021/050 | Train Loss: 0.00010415\n","Iter: 022/050 | Train Loss: 0.00007722\n","Iter: 023/050 | Train Loss: 0.00008083\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 024/050 | Train Loss: 0.00006468\n","Iter: 025/050 | Train Loss: 0.00005303\n","Iter: 026/050 | Train Loss: 0.00005597\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 027/050 | Train Loss: 0.00004126\n","Iter: 028/050 | Train Loss: 0.00004441\n","Adjusting Layer 1, Kernel Nodes: 204, Adptive Nodes:596\n","Iter: 029/050 | Train Loss: 0.00003334\n","Iter: 030/050 | Train Loss: 0.00003497\n","Adjusting Layer 1, Kernel Nodes: 33, Adptive Nodes:767\n","Iter: 031/050 | Train Loss: 0.00002664\n","Iter: 032/050 | Train Loss: 0.00002647\n","Iter: 033/050 | Train Loss: 0.00002271\n","Iter: 034/050 | Train Loss: 0.00001942\n","Iter: 035/050 | Train Loss: 0.00001816\n","Iter: 036/050 | Train Loss: 0.00001516\n","Iter: 037/050 | Train Loss: 0.00001444\n","Iter: 038/050 | Train Loss: 0.00001285\n","Iter: 039/050 | Train Loss: 0.00001238\n","Iter: 040/050 | Train Loss: 0.00001099\n","Iter: 041/050 | Train Loss: 0.00001037\n","Iter: 042/050 | Train Loss: 0.00000915\n","Iter: 043/050 | Train Loss: 0.00000788\n","Iter: 044/050 | Train Loss: 0.00000758\n","Iter: 045/050 | Train Loss: 0.00000613\n","Iter: 046/050 | Train Loss: 0.00000593\n","Iter: 047/050 | Train Loss: 0.00000532\n","Iter: 048/050 | Train Loss: 0.00000436\n","Iter: 049/050 | Train Loss: 0.00000447\n","\n","Iter: 049/050 | Test Loss: 0.00103823 | Test acc: 62.9500\n","scale:0.920000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00371349\n","Iter: 001/050 | Train Loss: 0.00205386\n","Iter: 002/050 | Train Loss: 0.00146480\n","Iter: 003/050 | Train Loss: 0.00089428\n","Iter: 004/050 | Train Loss: 0.00098627\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 005/050 | Train Loss: 0.00074743\n","Iter: 006/050 | Train Loss: 0.00056674\n","Iter: 007/050 | Train Loss: 0.00056254\n","Iter: 008/050 | Train Loss: 0.00058926\n","Adjusting Layer 1, Kernel Nodes: 751, Adptive Nodes:49\n","Iter: 009/050 | Train Loss: 0.00052265\n","Iter: 010/050 | Train Loss: 0.00040829\n","Iter: 011/050 | Train Loss: 0.00035257\n","Iter: 012/050 | Train Loss: 0.00036387\n","Adjusting Layer 1, Kernel Nodes: 693, Adptive Nodes:107\n","Iter: 013/050 | Train Loss: 0.00035184\n","Iter: 014/050 | Train Loss: 0.00027415\n","Iter: 015/050 | Train Loss: 0.00021978\n","Iter: 016/050 | Train Loss: 0.00022209\n","Adjusting Layer 1, Kernel Nodes: 445, Adptive Nodes:355\n","Iter: 017/050 | Train Loss: 0.00021154\n","Iter: 018/050 | Train Loss: 0.00015974\n","Iter: 019/050 | Train Loss: 0.00012826\n","Iter: 020/050 | Train Loss: 0.00013206\n","Adjusting Layer 1, Kernel Nodes: 589, Adptive Nodes:211\n","Iter: 021/050 | Train Loss: 0.00011429\n","Iter: 022/050 | Train Loss: 0.00008265\n","Iter: 023/050 | Train Loss: 0.00008020\n","Iter: 024/050 | Train Loss: 0.00007636\n","Iter: 025/050 | Train Loss: 0.00005823\n","Iter: 026/050 | Train Loss: 0.00005166\n","Iter: 027/050 | Train Loss: 0.00005107\n","Iter: 028/050 | Train Loss: 0.00004379\n","Iter: 029/050 | Train Loss: 0.00003723\n","Iter: 030/050 | Train Loss: 0.00003414\n","Iter: 031/050 | Train Loss: 0.00003219\n","Iter: 032/050 | Train Loss: 0.00002904\n","Iter: 033/050 | Train Loss: 0.00002382\n","Iter: 034/050 | Train Loss: 0.00002170\n","Iter: 035/050 | Train Loss: 0.00002200\n","Adjusting Layer 1, Kernel Nodes: 548, Adptive Nodes:252\n","Iter: 036/050 | Train Loss: 0.00001752\n","Iter: 037/050 | Train Loss: 0.00001479\n","Iter: 038/050 | Train Loss: 0.00001549\n","Adjusting Layer 1, Kernel Nodes: 480, Adptive Nodes:320\n","Iter: 039/050 | Train Loss: 0.00001165\n","Iter: 040/050 | Train Loss: 0.00001024\n","Iter: 041/050 | Train Loss: 0.00001058\n","Adjusting Layer 1, Kernel Nodes: 439, Adptive Nodes:361\n","Iter: 042/050 | Train Loss: 0.00000726\n","Iter: 043/050 | Train Loss: 0.00000831\n","Adjusting Layer 1, Kernel Nodes: 419, Adptive Nodes:381\n","Iter: 044/050 | Train Loss: 0.00000660\n","Iter: 045/050 | Train Loss: 0.00000610\n","Iter: 046/050 | Train Loss: 0.00000621\n","Adjusting Layer 1, Kernel Nodes: 392, Adptive Nodes:408\n","Iter: 047/050 | Train Loss: 0.00000438\n","Iter: 048/050 | Train Loss: 0.00000513\n","Adjusting Layer 1, Kernel Nodes: 412, Adptive Nodes:388\n","Iter: 049/050 | Train Loss: 0.00000346\n","\n","Iter: 049/050 | Test Loss: 0.00100551 | Test acc: 63.9900\n","scale:0.920000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00362721\n","Iter: 001/050 | Train Loss: 0.00191415\n","Iter: 002/050 | Train Loss: 0.00149335\n","Iter: 003/050 | Train Loss: 0.00087588\n","Iter: 004/050 | Train Loss: 0.00097871\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 005/050 | Train Loss: 0.00074688\n","Iter: 006/050 | Train Loss: 0.00056069\n","Iter: 007/050 | Train Loss: 0.00055597\n","Iter: 008/050 | Train Loss: 0.00058438\n","Adjusting Layer 1, Kernel Nodes: 744, Adptive Nodes:56\n","Iter: 009/050 | Train Loss: 0.00051994\n","Iter: 010/050 | Train Loss: 0.00040716\n","Iter: 011/050 | Train Loss: 0.00035245\n","Iter: 012/050 | Train Loss: 0.00036426\n","Adjusting Layer 1, Kernel Nodes: 705, Adptive Nodes:95\n","Iter: 013/050 | Train Loss: 0.00035358\n","Iter: 014/050 | Train Loss: 0.00027748\n","Iter: 015/050 | Train Loss: 0.00022215\n","Iter: 016/050 | Train Loss: 0.00022320\n","Adjusting Layer 1, Kernel Nodes: 521, Adptive Nodes:279\n","Iter: 017/050 | Train Loss: 0.00021647\n","Iter: 018/050 | Train Loss: 0.00016751\n","Iter: 019/050 | Train Loss: 0.00013188\n","Iter: 020/050 | Train Loss: 0.00013386\n","Adjusting Layer 1, Kernel Nodes: 685, Adptive Nodes:115\n","Iter: 021/050 | Train Loss: 0.00012286\n","Iter: 022/050 | Train Loss: 0.00008975\n","Iter: 023/050 | Train Loss: 0.00008101\n","Iter: 024/050 | Train Loss: 0.00008088\n","Iter: 025/050 | Train Loss: 0.00006574\n","Iter: 026/050 | Train Loss: 0.00005331\n","Iter: 027/050 | Train Loss: 0.00005180\n","Iter: 028/050 | Train Loss: 0.00004810\n","Iter: 029/050 | Train Loss: 0.00003979\n","Iter: 030/050 | Train Loss: 0.00003486\n","Iter: 031/050 | Train Loss: 0.00003304\n","Iter: 032/050 | Train Loss: 0.00003030\n","Iter: 033/050 | Train Loss: 0.00002690\n","Iter: 034/050 | Train Loss: 0.00002272\n","Iter: 035/050 | Train Loss: 0.00002074\n","Iter: 036/050 | Train Loss: 0.00002107\n","Adjusting Layer 1, Kernel Nodes: 397, Adptive Nodes:403\n","Iter: 037/050 | Train Loss: 0.00001673\n","Iter: 038/050 | Train Loss: 0.00001352\n","Iter: 039/050 | Train Loss: 0.00001508\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 040/050 | Train Loss: 0.00001156\n","Iter: 041/050 | Train Loss: 0.00000930\n","Iter: 042/050 | Train Loss: 0.00001047\n","Adjusting Layer 1, Kernel Nodes: 317, Adptive Nodes:483\n","Iter: 043/050 | Train Loss: 0.00000742\n","Iter: 044/050 | Train Loss: 0.00000721\n","Iter: 045/050 | Train Loss: 0.00000714\n","Iter: 046/050 | Train Loss: 0.00000545\n","Iter: 047/050 | Train Loss: 0.00000558\n","Adjusting Layer 1, Kernel Nodes: 588, Adptive Nodes:212\n","Iter: 048/050 | Train Loss: 0.00000494\n","Iter: 049/050 | Train Loss: 0.00000398\n","\n","Iter: 049/050 | Test Loss: 0.00098744 | Test acc: 64.3900\n","scale:0.920000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00355759\n","Iter: 001/050 | Train Loss: 0.00181258\n","Iter: 002/050 | Train Loss: 0.00151726\n","Iter: 003/050 | Train Loss: 0.00086195\n","Iter: 004/050 | Train Loss: 0.00096996\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00075004\n","Iter: 006/050 | Train Loss: 0.00055891\n","Iter: 007/050 | Train Loss: 0.00055008\n","Iter: 008/050 | Train Loss: 0.00058157\n","Adjusting Layer 1, Kernel Nodes: 713, Adptive Nodes:87\n","Iter: 009/050 | Train Loss: 0.00052394\n","Iter: 010/050 | Train Loss: 0.00041125\n","Iter: 011/050 | Train Loss: 0.00035288\n","Iter: 012/050 | Train Loss: 0.00036407\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 013/050 | Train Loss: 0.00035869\n","Iter: 014/050 | Train Loss: 0.00028669\n","Iter: 015/050 | Train Loss: 0.00022772\n","Iter: 016/050 | Train Loss: 0.00022447\n","Iter: 017/050 | Train Loss: 0.00022450\n","Adjusting Layer 1, Kernel Nodes: 683, Adptive Nodes:117\n","Iter: 018/050 | Train Loss: 0.00018349\n","Iter: 019/050 | Train Loss: 0.00014085\n","Iter: 020/050 | Train Loss: 0.00013359\n","Iter: 021/050 | Train Loss: 0.00013297\n","Iter: 022/050 | Train Loss: 0.00010983\n","Iter: 023/050 | Train Loss: 0.00008605\n","Iter: 024/050 | Train Loss: 0.00008049\n","Iter: 025/050 | Train Loss: 0.00007885\n","Iter: 026/050 | Train Loss: 0.00006821\n","Iter: 027/050 | Train Loss: 0.00005572\n","Iter: 028/050 | Train Loss: 0.00004921\n","Iter: 029/050 | Train Loss: 0.00004553\n","Iter: 030/050 | Train Loss: 0.00004163\n","Iter: 031/050 | Train Loss: 0.00003765\n","Iter: 032/050 | Train Loss: 0.00003152\n","Iter: 033/050 | Train Loss: 0.00002607\n","Iter: 034/050 | Train Loss: 0.00002664\n","Adjusting Layer 1, Kernel Nodes: 605, Adptive Nodes:195\n","Iter: 035/050 | Train Loss: 0.00002648\n","Iter: 036/050 | Train Loss: 0.00002004\n","Iter: 037/050 | Train Loss: 0.00001636\n","Iter: 038/050 | Train Loss: 0.00001831\n","Adjusting Layer 1, Kernel Nodes: 505, Adptive Nodes:295\n","Iter: 039/050 | Train Loss: 0.00001690\n","Iter: 040/050 | Train Loss: 0.00001216\n","Iter: 041/050 | Train Loss: 0.00001134\n","Iter: 042/050 | Train Loss: 0.00001200\n","Adjusting Layer 1, Kernel Nodes: 518, Adptive Nodes:282\n","Iter: 043/050 | Train Loss: 0.00000964\n","Iter: 044/050 | Train Loss: 0.00000769\n","Iter: 045/050 | Train Loss: 0.00000781\n","Adjusting Layer 1, Kernel Nodes: 591, Adptive Nodes:209\n","Iter: 046/050 | Train Loss: 0.00000722\n","Iter: 047/050 | Train Loss: 0.00000571\n","Iter: 048/050 | Train Loss: 0.00000537\n","Iter: 049/050 | Train Loss: 0.00000539\n","\n","Iter: 049/050 | Test Loss: 0.00096514 | Test acc: 65.1500\n","scale:0.920000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00350289\n","Iter: 001/050 | Train Loss: 0.00173629\n","Iter: 002/050 | Train Loss: 0.00153493\n","Iter: 003/050 | Train Loss: 0.00085066\n","Iter: 004/050 | Train Loss: 0.00096321\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 005/050 | Train Loss: 0.00075402\n","Iter: 006/050 | Train Loss: 0.00055812\n","Iter: 007/050 | Train Loss: 0.00054534\n","Iter: 008/050 | Train Loss: 0.00057867\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 009/050 | Train Loss: 0.00052776\n","Iter: 010/050 | Train Loss: 0.00041780\n","Iter: 011/050 | Train Loss: 0.00035360\n","Iter: 012/050 | Train Loss: 0.00036067\n","Adjusting Layer 1, Kernel Nodes: 681, Adptive Nodes:119\n","Iter: 013/050 | Train Loss: 0.00036115\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 014/050 | Train Loss: 0.00028868\n","Iter: 015/050 | Train Loss: 0.00022694\n","Iter: 016/050 | Train Loss: 0.00022704\n","Adjusting Layer 1, Kernel Nodes: 599, Adptive Nodes:201\n","Iter: 017/050 | Train Loss: 0.00022427\n","Iter: 018/050 | Train Loss: 0.00017666\n","Iter: 019/050 | Train Loss: 0.00013804\n","Iter: 020/050 | Train Loss: 0.00013916\n","Adjusting Layer 1, Kernel Nodes: 776, Adptive Nodes:24\n","Iter: 021/050 | Train Loss: 0.00013206\n","Iter: 022/050 | Train Loss: 0.00009916\n","Iter: 023/050 | Train Loss: 0.00008520\n","Iter: 024/050 | Train Loss: 0.00008711\n","Adjusting Layer 1, Kernel Nodes: 541, Adptive Nodes:259\n","Iter: 025/050 | Train Loss: 0.00007375\n","Iter: 026/050 | Train Loss: 0.00005788\n","Iter: 027/050 | Train Loss: 0.00005592\n","Iter: 028/050 | Train Loss: 0.00005234\n","Iter: 029/050 | Train Loss: 0.00004215\n","Iter: 030/050 | Train Loss: 0.00003694\n","Iter: 031/050 | Train Loss: 0.00003521\n","Iter: 032/050 | Train Loss: 0.00003148\n","Iter: 033/050 | Train Loss: 0.00002716\n","Iter: 034/050 | Train Loss: 0.00002353\n","Iter: 035/050 | Train Loss: 0.00002208\n","Iter: 036/050 | Train Loss: 0.00002125\n","Iter: 037/050 | Train Loss: 0.00001715\n","Iter: 038/050 | Train Loss: 0.00001442\n","Iter: 039/050 | Train Loss: 0.00001553\n","Adjusting Layer 1, Kernel Nodes: 789, Adptive Nodes:11\n","Iter: 040/050 | Train Loss: 0.00001330\n","Iter: 041/050 | Train Loss: 0.00000978\n","Iter: 042/050 | Train Loss: 0.00001049\n","Adjusting Layer 1, Kernel Nodes: 362, Adptive Nodes:438\n","Iter: 043/050 | Train Loss: 0.00000969\n","Iter: 044/050 | Train Loss: 0.00000687\n","Iter: 045/050 | Train Loss: 0.00000729\n","Adjusting Layer 1, Kernel Nodes: 432, Adptive Nodes:368\n","Iter: 046/050 | Train Loss: 0.00000688\n","Iter: 047/050 | Train Loss: 0.00000520\n","Iter: 048/050 | Train Loss: 0.00000534\n","Adjusting Layer 1, Kernel Nodes: 510, Adptive Nodes:290\n","Iter: 049/050 | Train Loss: 0.00000474\n","\n","Iter: 049/050 | Test Loss: 0.00096727 | Test acc: 64.9900\n","scale:0.920000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00346139\n","Iter: 001/050 | Train Loss: 0.00168090\n","Iter: 002/050 | Train Loss: 0.00154745\n","Iter: 003/050 | Train Loss: 0.00084131\n","Iter: 004/050 | Train Loss: 0.00095593\n","Adjusting Layer 1, Kernel Nodes: 667, Adptive Nodes:133\n","Iter: 005/050 | Train Loss: 0.00075884\n","Iter: 006/050 | Train Loss: 0.00055795\n","Iter: 007/050 | Train Loss: 0.00054059\n","Iter: 008/050 | Train Loss: 0.00057509\n","Adjusting Layer 1, Kernel Nodes: 753, Adptive Nodes:47\n","Iter: 009/050 | Train Loss: 0.00052961\n","Iter: 010/050 | Train Loss: 0.00042165\n","Iter: 011/050 | Train Loss: 0.00035378\n","Iter: 012/050 | Train Loss: 0.00035717\n","Adjusting Layer 1, Kernel Nodes: 716, Adptive Nodes:84\n","Iter: 013/050 | Train Loss: 0.00036061\n","Adjusting Layer 1, Kernel Nodes: 634, Adptive Nodes:166\n","Iter: 014/050 | Train Loss: 0.00029521\n","Iter: 015/050 | Train Loss: 0.00022973\n","Iter: 016/050 | Train Loss: 0.00022204\n","Iter: 017/050 | Train Loss: 0.00022554\n","Adjusting Layer 1, Kernel Nodes: 673, Adptive Nodes:127\n","Iter: 018/050 | Train Loss: 0.00018952\n","Iter: 019/050 | Train Loss: 0.00014497\n","Iter: 020/050 | Train Loss: 0.00013363\n","Iter: 021/050 | Train Loss: 0.00013563\n","Adjusting Layer 1, Kernel Nodes: 784, Adptive Nodes:16\n","Iter: 022/050 | Train Loss: 0.00011567\n","Iter: 023/050 | Train Loss: 0.00008952\n","Iter: 024/050 | Train Loss: 0.00008298\n","Iter: 025/050 | Train Loss: 0.00008227\n","Iter: 026/050 | Train Loss: 0.00007124\n","Iter: 027/050 | Train Loss: 0.00005748\n","Iter: 028/050 | Train Loss: 0.00005057\n","Iter: 029/050 | Train Loss: 0.00004821\n","Iter: 030/050 | Train Loss: 0.00004515\n","Iter: 031/050 | Train Loss: 0.00003797\n","Iter: 032/050 | Train Loss: 0.00002984\n","Iter: 033/050 | Train Loss: 0.00002891\n","Iter: 034/050 | Train Loss: 0.00003062\n","Adjusting Layer 1, Kernel Nodes: 593, Adptive Nodes:207\n","Iter: 035/050 | Train Loss: 0.00002466\n","Iter: 036/050 | Train Loss: 0.00001839\n","Iter: 037/050 | Train Loss: 0.00001950\n","Adjusting Layer 1, Kernel Nodes: 559, Adptive Nodes:241\n","Iter: 038/050 | Train Loss: 0.00001991\n","Adjusting Layer 1, Kernel Nodes: 610, Adptive Nodes:190\n","Iter: 039/050 | Train Loss: 0.00001485\n","Iter: 040/050 | Train Loss: 0.00001268\n","Iter: 041/050 | Train Loss: 0.00001350\n","Adjusting Layer 1, Kernel Nodes: 581, Adptive Nodes:219\n","Iter: 042/050 | Train Loss: 0.00001176\n","Iter: 043/050 | Train Loss: 0.00000914\n","Iter: 044/050 | Train Loss: 0.00000872\n","Iter: 045/050 | Train Loss: 0.00000868\n","Iter: 046/050 | Train Loss: 0.00000712\n","Iter: 047/050 | Train Loss: 0.00000581\n","Iter: 048/050 | Train Loss: 0.00000611\n","Adjusting Layer 1, Kernel Nodes: 628, Adptive Nodes:172\n","Iter: 049/050 | Train Loss: 0.00000565\n","\n","Iter: 049/050 | Test Loss: 0.00094611 | Test acc: 65.6100\n","scale:0.920000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 000/050 | Train Loss: 0.00343175\n","Iter: 001/050 | Train Loss: 0.00163972\n","Iter: 002/050 | Train Loss: 0.00155705\n","Iter: 003/050 | Train Loss: 0.00083320\n","Iter: 004/050 | Train Loss: 0.00094548\n","Adjusting Layer 1, Kernel Nodes: 669, Adptive Nodes:131\n","Iter: 005/050 | Train Loss: 0.00076331\n","Iter: 006/050 | Train Loss: 0.00055809\n","Iter: 007/050 | Train Loss: 0.00053589\n","Iter: 008/050 | Train Loss: 0.00057122\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 009/050 | Train Loss: 0.00052886\n","Iter: 010/050 | Train Loss: 0.00042202\n","Iter: 011/050 | Train Loss: 0.00035319\n","Iter: 012/050 | Train Loss: 0.00035456\n","Adjusting Layer 1, Kernel Nodes: 753, Adptive Nodes:47\n","Iter: 013/050 | Train Loss: 0.00035800\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 014/050 | Train Loss: 0.00029757\n","Iter: 015/050 | Train Loss: 0.00023217\n","Iter: 016/050 | Train Loss: 0.00022029\n","Iter: 017/050 | Train Loss: 0.00022552\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 018/050 | Train Loss: 0.00019677\n","Iter: 019/050 | Train Loss: 0.00015155\n","Iter: 020/050 | Train Loss: 0.00013209\n","Iter: 021/050 | Train Loss: 0.00013546\n","Adjusting Layer 1, Kernel Nodes: 728, Adptive Nodes:72\n","Iter: 022/050 | Train Loss: 0.00012410\n","Iter: 023/050 | Train Loss: 0.00009602\n","Iter: 024/050 | Train Loss: 0.00008246\n","Iter: 025/050 | Train Loss: 0.00008178\n","Iter: 026/050 | Train Loss: 0.00007636\n","Iter: 027/050 | Train Loss: 0.00006468\n","Iter: 028/050 | Train Loss: 0.00005238\n","Iter: 029/050 | Train Loss: 0.00004552\n","Iter: 030/050 | Train Loss: 0.00004638\n","Adjusting Layer 1, Kernel Nodes: 769, Adptive Nodes:31\n","Iter: 031/050 | Train Loss: 0.00004374\n","Iter: 032/050 | Train Loss: 0.00003297\n","Iter: 033/050 | Train Loss: 0.00002757\n","Iter: 034/050 | Train Loss: 0.00002991\n","Adjusting Layer 1, Kernel Nodes: 713, Adptive Nodes:87\n","Iter: 035/050 | Train Loss: 0.00002866\n","Iter: 036/050 | Train Loss: 0.00002192\n","Iter: 037/050 | Train Loss: 0.00001797\n","Iter: 038/050 | Train Loss: 0.00001862\n","Adjusting Layer 1, Kernel Nodes: 769, Adptive Nodes:31\n","Iter: 039/050 | Train Loss: 0.00001846\n","Iter: 040/050 | Train Loss: 0.00001490\n","Iter: 041/050 | Train Loss: 0.00001192\n","Iter: 042/050 | Train Loss: 0.00001199\n","Adjusting Layer 1, Kernel Nodes: 578, Adptive Nodes:222\n","Iter: 043/050 | Train Loss: 0.00001223\n","Adjusting Layer 1, Kernel Nodes: 792, Adptive Nodes:8\n","Iter: 044/050 | Train Loss: 0.00000974\n","Iter: 045/050 | Train Loss: 0.00000770\n","Iter: 046/050 | Train Loss: 0.00000815\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 047/050 | Train Loss: 0.00000784\n","Iter: 048/050 | Train Loss: 0.00000591\n","Iter: 049/050 | Train Loss: 0.00000534\n","\n","Iter: 049/050 | Test Loss: 0.00093897 | Test acc: 65.6500\n","scale:0.940000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00426232\n","Iter: 001/050 | Train Loss: 0.00316916\n","Iter: 002/050 | Train Loss: 0.00129525\n","Iter: 003/050 | Train Loss: 0.00100251\n","Iter: 004/050 | Train Loss: 0.00099009\n","Iter: 005/050 | Train Loss: 0.00079563\n","Iter: 006/050 | Train Loss: 0.00067589\n","Iter: 007/050 | Train Loss: 0.00063311\n","Iter: 008/050 | Train Loss: 0.00061548\n","Iter: 009/050 | Train Loss: 0.00058654\n","Iter: 010/050 | Train Loss: 0.00052056\n","Iter: 011/050 | Train Loss: 0.00043392\n","Iter: 012/050 | Train Loss: 0.00037507\n","Iter: 013/050 | Train Loss: 0.00036109\n","Iter: 014/050 | Train Loss: 0.00034922\n","Iter: 015/050 | Train Loss: 0.00030674\n","Iter: 016/050 | Train Loss: 0.00026261\n","Iter: 017/050 | Train Loss: 0.00023899\n","Iter: 018/050 | Train Loss: 0.00022508\n","Iter: 019/050 | Train Loss: 0.00020568\n","Iter: 020/050 | Train Loss: 0.00017476\n","Iter: 021/050 | Train Loss: 0.00014447\n","Iter: 022/050 | Train Loss: 0.00013492\n","Iter: 023/050 | Train Loss: 0.00013503\n","Adjusting Layer 1, Kernel Nodes: 678, Adptive Nodes:122\n","Iter: 024/050 | Train Loss: 0.00011638\n","Iter: 025/050 | Train Loss: 0.00009471\n","Iter: 026/050 | Train Loss: 0.00008767\n","Iter: 027/050 | Train Loss: 0.00008282\n","Iter: 028/050 | Train Loss: 0.00007065\n","Iter: 029/050 | Train Loss: 0.00005869\n","Iter: 030/050 | Train Loss: 0.00005279\n","Iter: 031/050 | Train Loss: 0.00004822\n","Iter: 032/050 | Train Loss: 0.00004337\n","Iter: 033/050 | Train Loss: 0.00003977\n","Iter: 034/050 | Train Loss: 0.00003404\n","Iter: 035/050 | Train Loss: 0.00002827\n","Iter: 036/050 | Train Loss: 0.00002767\n","Iter: 037/050 | Train Loss: 0.00002691\n","Iter: 038/050 | Train Loss: 0.00002139\n","Iter: 039/050 | Train Loss: 0.00001751\n","Iter: 040/050 | Train Loss: 0.00001797\n","Adjusting Layer 1, Kernel Nodes: 614, Adptive Nodes:186\n","Iter: 041/050 | Train Loss: 0.00001718\n","Iter: 042/050 | Train Loss: 0.00001379\n","Iter: 043/050 | Train Loss: 0.00001233\n","Iter: 044/050 | Train Loss: 0.00001208\n","Iter: 045/050 | Train Loss: 0.00001072\n","Iter: 046/050 | Train Loss: 0.00000936\n","Iter: 047/050 | Train Loss: 0.00000853\n","Iter: 048/050 | Train Loss: 0.00000767\n","Iter: 049/050 | Train Loss: 0.00000715\n","\n","Iter: 049/050 | Test Loss: 0.00094187 | Test acc: 65.0000\n","scale:0.940000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00408588\n","Iter: 001/050 | Train Loss: 0.00276041\n","Iter: 002/050 | Train Loss: 0.00134467\n","Iter: 003/050 | Train Loss: 0.00096915\n","Iter: 004/050 | Train Loss: 0.00099189\n","Adjusting Layer 1, Kernel Nodes: 779, Adptive Nodes:21\n","Iter: 005/050 | Train Loss: 0.00075951\n","Iter: 006/050 | Train Loss: 0.00059907\n","Iter: 007/050 | Train Loss: 0.00058448\n","Iter: 008/050 | Train Loss: 0.00059857\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 009/050 | Train Loss: 0.00053574\n","Iter: 010/050 | Train Loss: 0.00042552\n","Iter: 011/050 | Train Loss: 0.00035703\n","Iter: 012/050 | Train Loss: 0.00035516\n","Iter: 013/050 | Train Loss: 0.00035029\n","Iter: 014/050 | Train Loss: 0.00029110\n","Iter: 015/050 | Train Loss: 0.00022644\n","Iter: 016/050 | Train Loss: 0.00020798\n","Iter: 017/050 | Train Loss: 0.00021048\n","Adjusting Layer 1, Kernel Nodes: 464, Adptive Nodes:336\n","Iter: 018/050 | Train Loss: 0.00018121\n","Iter: 019/050 | Train Loss: 0.00013648\n","Iter: 020/050 | Train Loss: 0.00011874\n","Iter: 021/050 | Train Loss: 0.00011781\n","Iter: 022/050 | Train Loss: 0.00010184\n","Iter: 023/050 | Train Loss: 0.00007886\n","Iter: 024/050 | Train Loss: 0.00006948\n","Iter: 025/050 | Train Loss: 0.00006711\n","Iter: 026/050 | Train Loss: 0.00005969\n","Iter: 027/050 | Train Loss: 0.00005014\n","Iter: 028/050 | Train Loss: 0.00004367\n","Iter: 029/050 | Train Loss: 0.00003935\n","Iter: 030/050 | Train Loss: 0.00003618\n","Iter: 031/050 | Train Loss: 0.00003335\n","Iter: 032/050 | Train Loss: 0.00002942\n","Iter: 033/050 | Train Loss: 0.00002571\n","Iter: 034/050 | Train Loss: 0.00002345\n","Iter: 035/050 | Train Loss: 0.00002168\n","Iter: 036/050 | Train Loss: 0.00001991\n","Iter: 037/050 | Train Loss: 0.00001761\n","Iter: 038/050 | Train Loss: 0.00001457\n","Iter: 039/050 | Train Loss: 0.00001270\n","Iter: 040/050 | Train Loss: 0.00001274\n","Adjusting Layer 1, Kernel Nodes: 483, Adptive Nodes:317\n","Iter: 041/050 | Train Loss: 0.00001144\n","Iter: 042/050 | Train Loss: 0.00000864\n","Iter: 043/050 | Train Loss: 0.00000798\n","Iter: 044/050 | Train Loss: 0.00000828\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 045/050 | Train Loss: 0.00000658\n","Iter: 046/050 | Train Loss: 0.00000561\n","Iter: 047/050 | Train Loss: 0.00000589\n","Adjusting Layer 1, Kernel Nodes: 388, Adptive Nodes:412\n","Iter: 048/050 | Train Loss: 0.00000493\n","Iter: 049/050 | Train Loss: 0.00000423\n","\n","Iter: 049/050 | Test Loss: 0.00100989 | Test acc: 63.6000\n","scale:0.940000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00393750\n","Iter: 001/050 | Train Loss: 0.00245831\n","Iter: 002/050 | Train Loss: 0.00138734\n","Iter: 003/050 | Train Loss: 0.00094087\n","Iter: 004/050 | Train Loss: 0.00099088\n","Adjusting Layer 1, Kernel Nodes: 668, Adptive Nodes:132\n","Iter: 005/050 | Train Loss: 0.00075150\n","Iter: 006/050 | Train Loss: 0.00058381\n","Iter: 007/050 | Train Loss: 0.00057520\n","Iter: 008/050 | Train Loss: 0.00059454\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 009/050 | Train Loss: 0.00052363\n","Iter: 010/050 | Train Loss: 0.00041082\n","Iter: 011/050 | Train Loss: 0.00035460\n","Iter: 012/050 | Train Loss: 0.00036052\n","Adjusting Layer 1, Kernel Nodes: 488, Adptive Nodes:312\n","Iter: 013/050 | Train Loss: 0.00034233\n","Iter: 014/050 | Train Loss: 0.00026533\n","Iter: 015/050 | Train Loss: 0.00021541\n","Iter: 016/050 | Train Loss: 0.00021716\n","Adjusting Layer 1, Kernel Nodes: 540, Adptive Nodes:260\n","Iter: 017/050 | Train Loss: 0.00019637\n","Iter: 018/050 | Train Loss: 0.00014204\n","Iter: 019/050 | Train Loss: 0.00012256\n","Iter: 020/050 | Train Loss: 0.00012582\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 021/050 | Train Loss: 0.00009102\n","Iter: 022/050 | Train Loss: 0.00007686\n","Iter: 023/050 | Train Loss: 0.00008094\n","Adjusting Layer 1, Kernel Nodes: 468, Adptive Nodes:332\n","Iter: 024/050 | Train Loss: 0.00005504\n","Iter: 025/050 | Train Loss: 0.00006250\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 026/050 | Train Loss: 0.00004578\n","Iter: 027/050 | Train Loss: 0.00004814\n","Adjusting Layer 1, Kernel Nodes: 19, Adptive Nodes:781\n","Iter: 028/050 | Train Loss: 0.00003649\n","Iter: 029/050 | Train Loss: 0.00003796\n","Adjusting Layer 1, Kernel Nodes: 158, Adptive Nodes:642\n","Iter: 030/050 | Train Loss: 0.00003029\n","Iter: 031/050 | Train Loss: 0.00002863\n","Iter: 032/050 | Train Loss: 0.00002433\n","Iter: 033/050 | Train Loss: 0.00002164\n","Iter: 034/050 | Train Loss: 0.00001946\n","Iter: 035/050 | Train Loss: 0.00001700\n","Iter: 036/050 | Train Loss: 0.00001587\n","Iter: 037/050 | Train Loss: 0.00001427\n","Iter: 038/050 | Train Loss: 0.00001360\n","Iter: 039/050 | Train Loss: 0.00001221\n","Iter: 040/050 | Train Loss: 0.00001149\n","Iter: 041/050 | Train Loss: 0.00000971\n","Iter: 042/050 | Train Loss: 0.00000921\n","Iter: 043/050 | Train Loss: 0.00000749\n","Iter: 044/050 | Train Loss: 0.00000723\n","Iter: 045/050 | Train Loss: 0.00000605\n","Iter: 046/050 | Train Loss: 0.00000554\n","Iter: 047/050 | Train Loss: 0.00000509\n","Iter: 048/050 | Train Loss: 0.00000424\n","Iter: 049/050 | Train Loss: 0.00000433\n","\n","Iter: 049/050 | Test Loss: 0.00105088 | Test acc: 62.6500\n","scale:0.940000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00381375\n","Iter: 001/050 | Train Loss: 0.00222391\n","Iter: 002/050 | Train Loss: 0.00143083\n","Iter: 003/050 | Train Loss: 0.00091472\n","Iter: 004/050 | Train Loss: 0.00099028\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00074883\n","Iter: 006/050 | Train Loss: 0.00057342\n","Iter: 007/050 | Train Loss: 0.00056796\n","Iter: 008/050 | Train Loss: 0.00059220\n","Adjusting Layer 1, Kernel Nodes: 735, Adptive Nodes:65\n","Iter: 009/050 | Train Loss: 0.00052279\n","Iter: 010/050 | Train Loss: 0.00040811\n","Iter: 011/050 | Train Loss: 0.00035370\n","Iter: 012/050 | Train Loss: 0.00036406\n","Adjusting Layer 1, Kernel Nodes: 728, Adptive Nodes:72\n","Iter: 013/050 | Train Loss: 0.00034718\n","Iter: 014/050 | Train Loss: 0.00026754\n","Iter: 015/050 | Train Loss: 0.00021936\n","Iter: 016/050 | Train Loss: 0.00022388\n","Adjusting Layer 1, Kernel Nodes: 422, Adptive Nodes:378\n","Iter: 017/050 | Train Loss: 0.00020413\n","Iter: 018/050 | Train Loss: 0.00015030\n","Iter: 019/050 | Train Loss: 0.00012871\n","Iter: 020/050 | Train Loss: 0.00013198\n","Adjusting Layer 1, Kernel Nodes: 501, Adptive Nodes:299\n","Iter: 021/050 | Train Loss: 0.00010292\n","Iter: 022/050 | Train Loss: 0.00007891\n","Iter: 023/050 | Train Loss: 0.00008230\n","Adjusting Layer 1, Kernel Nodes: 639, Adptive Nodes:161\n","Iter: 024/050 | Train Loss: 0.00006476\n","Iter: 025/050 | Train Loss: 0.00005457\n","Iter: 026/050 | Train Loss: 0.00005641\n","Adjusting Layer 1, Kernel Nodes: 295, Adptive Nodes:505\n","Iter: 027/050 | Train Loss: 0.00004173\n","Iter: 028/050 | Train Loss: 0.00004388\n","Adjusting Layer 1, Kernel Nodes: 541, Adptive Nodes:259\n","Iter: 029/050 | Train Loss: 0.00003464\n","Iter: 030/050 | Train Loss: 0.00003392\n","Iter: 031/050 | Train Loss: 0.00002878\n","Iter: 032/050 | Train Loss: 0.00002564\n","Iter: 033/050 | Train Loss: 0.00002314\n","Iter: 034/050 | Train Loss: 0.00001949\n","Iter: 035/050 | Train Loss: 0.00001791\n","Iter: 036/050 | Train Loss: 0.00001520\n","Iter: 037/050 | Train Loss: 0.00001409\n","Iter: 038/050 | Train Loss: 0.00001253\n","Iter: 039/050 | Train Loss: 0.00001209\n","Iter: 040/050 | Train Loss: 0.00001070\n","Iter: 041/050 | Train Loss: 0.00001053\n","Iter: 042/050 | Train Loss: 0.00000887\n","Iter: 043/050 | Train Loss: 0.00000844\n","Iter: 044/050 | Train Loss: 0.00000728\n","Iter: 045/050 | Train Loss: 0.00000638\n","Iter: 046/050 | Train Loss: 0.00000613\n","Iter: 047/050 | Train Loss: 0.00000490\n","Iter: 048/050 | Train Loss: 0.00000499\n","Adjusting Layer 1, Kernel Nodes: 310, Adptive Nodes:490\n","Iter: 049/050 | Train Loss: 0.00000399\n","\n","Iter: 049/050 | Test Loss: 0.00104223 | Test acc: 63.0000\n","scale:0.940000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00371094\n","Iter: 001/050 | Train Loss: 0.00205083\n","Iter: 002/050 | Train Loss: 0.00146535\n","Iter: 003/050 | Train Loss: 0.00089410\n","Iter: 004/050 | Train Loss: 0.00098557\n","Adjusting Layer 1, Kernel Nodes: 657, Adptive Nodes:143\n","Iter: 005/050 | Train Loss: 0.00074727\n","Iter: 006/050 | Train Loss: 0.00056669\n","Iter: 007/050 | Train Loss: 0.00056218\n","Iter: 008/050 | Train Loss: 0.00058865\n","Adjusting Layer 1, Kernel Nodes: 753, Adptive Nodes:47\n","Iter: 009/050 | Train Loss: 0.00052187\n","Iter: 010/050 | Train Loss: 0.00040776\n","Iter: 011/050 | Train Loss: 0.00035237\n","Iter: 012/050 | Train Loss: 0.00036403\n","Adjusting Layer 1, Kernel Nodes: 651, Adptive Nodes:149\n","Iter: 013/050 | Train Loss: 0.00035135\n","Iter: 014/050 | Train Loss: 0.00027355\n","Iter: 015/050 | Train Loss: 0.00021984\n","Iter: 016/050 | Train Loss: 0.00022213\n","Adjusting Layer 1, Kernel Nodes: 452, Adptive Nodes:348\n","Iter: 017/050 | Train Loss: 0.00021115\n","Iter: 018/050 | Train Loss: 0.00015949\n","Iter: 019/050 | Train Loss: 0.00012807\n","Iter: 020/050 | Train Loss: 0.00013126\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 021/050 | Train Loss: 0.00011373\n","Iter: 022/050 | Train Loss: 0.00008235\n","Iter: 023/050 | Train Loss: 0.00007974\n","Iter: 024/050 | Train Loss: 0.00007614\n","Iter: 025/050 | Train Loss: 0.00005806\n","Iter: 026/050 | Train Loss: 0.00005143\n","Iter: 027/050 | Train Loss: 0.00005120\n","Iter: 028/050 | Train Loss: 0.00004370\n","Iter: 029/050 | Train Loss: 0.00003699\n","Iter: 030/050 | Train Loss: 0.00003429\n","Iter: 031/050 | Train Loss: 0.00003210\n","Iter: 032/050 | Train Loss: 0.00002862\n","Iter: 033/050 | Train Loss: 0.00002383\n","Iter: 034/050 | Train Loss: 0.00002172\n","Iter: 035/050 | Train Loss: 0.00002169\n","Iter: 036/050 | Train Loss: 0.00001827\n","Iter: 037/050 | Train Loss: 0.00001452\n","Iter: 038/050 | Train Loss: 0.00001477\n","Adjusting Layer 1, Kernel Nodes: 524, Adptive Nodes:276\n","Iter: 039/050 | Train Loss: 0.00001358\n","Iter: 040/050 | Train Loss: 0.00000983\n","Iter: 041/050 | Train Loss: 0.00000972\n","Iter: 042/050 | Train Loss: 0.00000960\n","Iter: 043/050 | Train Loss: 0.00000684\n","Iter: 044/050 | Train Loss: 0.00000681\n","Iter: 045/050 | Train Loss: 0.00000723\n","Adjusting Layer 1, Kernel Nodes: 501, Adptive Nodes:299\n","Iter: 046/050 | Train Loss: 0.00000512\n","Iter: 047/050 | Train Loss: 0.00000548\n","Adjusting Layer 1, Kernel Nodes: 454, Adptive Nodes:346\n","Iter: 048/050 | Train Loss: 0.00000480\n","Iter: 049/050 | Train Loss: 0.00000382\n","\n","Iter: 049/050 | Test Loss: 0.00100098 | Test acc: 63.8800\n","scale:0.940000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00362464\n","Iter: 001/050 | Train Loss: 0.00191132\n","Iter: 002/050 | Train Loss: 0.00149394\n","Iter: 003/050 | Train Loss: 0.00087574\n","Iter: 004/050 | Train Loss: 0.00097807\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 005/050 | Train Loss: 0.00074721\n","Iter: 006/050 | Train Loss: 0.00056075\n","Iter: 007/050 | Train Loss: 0.00055564\n","Iter: 008/050 | Train Loss: 0.00058397\n","Adjusting Layer 1, Kernel Nodes: 743, Adptive Nodes:57\n","Iter: 009/050 | Train Loss: 0.00051963\n","Iter: 010/050 | Train Loss: 0.00040670\n","Iter: 011/050 | Train Loss: 0.00035237\n","Iter: 012/050 | Train Loss: 0.00036479\n","Adjusting Layer 1, Kernel Nodes: 708, Adptive Nodes:92\n","Iter: 013/050 | Train Loss: 0.00035312\n","Iter: 014/050 | Train Loss: 0.00027621\n","Iter: 015/050 | Train Loss: 0.00022213\n","Iter: 016/050 | Train Loss: 0.00022403\n","Adjusting Layer 1, Kernel Nodes: 520, Adptive Nodes:280\n","Iter: 017/050 | Train Loss: 0.00021659\n","Iter: 018/050 | Train Loss: 0.00016714\n","Iter: 019/050 | Train Loss: 0.00013233\n","Iter: 020/050 | Train Loss: 0.00013474\n","Adjusting Layer 1, Kernel Nodes: 690, Adptive Nodes:110\n","Iter: 021/050 | Train Loss: 0.00012269\n","Iter: 022/050 | Train Loss: 0.00008943\n","Iter: 023/050 | Train Loss: 0.00008130\n","Iter: 024/050 | Train Loss: 0.00008089\n","Iter: 025/050 | Train Loss: 0.00006535\n","Iter: 026/050 | Train Loss: 0.00005314\n","Iter: 027/050 | Train Loss: 0.00005193\n","Iter: 028/050 | Train Loss: 0.00004826\n","Iter: 029/050 | Train Loss: 0.00003955\n","Iter: 030/050 | Train Loss: 0.00003491\n","Iter: 031/050 | Train Loss: 0.00003351\n","Iter: 032/050 | Train Loss: 0.00003032\n","Iter: 033/050 | Train Loss: 0.00002652\n","Iter: 034/050 | Train Loss: 0.00002295\n","Iter: 035/050 | Train Loss: 0.00002084\n","Iter: 036/050 | Train Loss: 0.00002054\n","Iter: 037/050 | Train Loss: 0.00001744\n","Iter: 038/050 | Train Loss: 0.00001349\n","Iter: 039/050 | Train Loss: 0.00001394\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 040/050 | Train Loss: 0.00001333\n","Iter: 041/050 | Train Loss: 0.00000940\n","Iter: 042/050 | Train Loss: 0.00000903\n","Iter: 043/050 | Train Loss: 0.00000938\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 044/050 | Train Loss: 0.00000670\n","Iter: 045/050 | Train Loss: 0.00000644\n","Iter: 046/050 | Train Loss: 0.00000660\n","Adjusting Layer 1, Kernel Nodes: 222, Adptive Nodes:578\n","Iter: 047/050 | Train Loss: 0.00000503\n","Iter: 048/050 | Train Loss: 0.00000496\n","Iter: 049/050 | Train Loss: 0.00000462\n","\n","Iter: 049/050 | Test Loss: 0.00098601 | Test acc: 64.5500\n","scale:0.940000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00355462\n","Iter: 001/050 | Train Loss: 0.00180904\n","Iter: 002/050 | Train Loss: 0.00151790\n","Iter: 003/050 | Train Loss: 0.00086148\n","Iter: 004/050 | Train Loss: 0.00096954\n","Adjusting Layer 1, Kernel Nodes: 661, Adptive Nodes:139\n","Iter: 005/050 | Train Loss: 0.00074983\n","Iter: 006/050 | Train Loss: 0.00055864\n","Iter: 007/050 | Train Loss: 0.00054973\n","Iter: 008/050 | Train Loss: 0.00058113\n","Adjusting Layer 1, Kernel Nodes: 710, Adptive Nodes:90\n","Iter: 009/050 | Train Loss: 0.00052338\n","Iter: 010/050 | Train Loss: 0.00041062\n","Iter: 011/050 | Train Loss: 0.00035261\n","Iter: 012/050 | Train Loss: 0.00036429\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 013/050 | Train Loss: 0.00035865\n","Iter: 014/050 | Train Loss: 0.00028576\n","Iter: 015/050 | Train Loss: 0.00022690\n","Iter: 016/050 | Train Loss: 0.00022440\n","Iter: 017/050 | Train Loss: 0.00022419\n","Iter: 018/050 | Train Loss: 0.00018694\n","Iter: 019/050 | Train Loss: 0.00014397\n","Iter: 020/050 | Train Loss: 0.00013024\n","Iter: 021/050 | Train Loss: 0.00013086\n","Adjusting Layer 1, Kernel Nodes: 705, Adptive Nodes:95\n","Iter: 022/050 | Train Loss: 0.00011498\n","Iter: 023/050 | Train Loss: 0.00009034\n","Iter: 024/050 | Train Loss: 0.00007966\n","Iter: 025/050 | Train Loss: 0.00007733\n","Iter: 026/050 | Train Loss: 0.00006952\n","Iter: 027/050 | Train Loss: 0.00005769\n","Iter: 028/050 | Train Loss: 0.00004928\n","Iter: 029/050 | Train Loss: 0.00004475\n","Iter: 030/050 | Train Loss: 0.00004213\n","Iter: 031/050 | Train Loss: 0.00003911\n","Iter: 032/050 | Train Loss: 0.00003220\n","Iter: 033/050 | Train Loss: 0.00002597\n","Iter: 034/050 | Train Loss: 0.00002687\n","Adjusting Layer 1, Kernel Nodes: 510, Adptive Nodes:290\n","Iter: 035/050 | Train Loss: 0.00002699\n","Adjusting Layer 1, Kernel Nodes: 508, Adptive Nodes:292\n","Iter: 036/050 | Train Loss: 0.00001920\n","Iter: 037/050 | Train Loss: 0.00001701\n","Iter: 038/050 | Train Loss: 0.00001951\n","Adjusting Layer 1, Kernel Nodes: 560, Adptive Nodes:240\n","Iter: 039/050 | Train Loss: 0.00001520\n","Iter: 040/050 | Train Loss: 0.00001172\n","Iter: 041/050 | Train Loss: 0.00001328\n","Adjusting Layer 1, Kernel Nodes: 483, Adptive Nodes:317\n","Iter: 042/050 | Train Loss: 0.00001099\n","Iter: 043/050 | Train Loss: 0.00000835\n","Iter: 044/050 | Train Loss: 0.00000893\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 045/050 | Train Loss: 0.00000740\n","Iter: 046/050 | Train Loss: 0.00000605\n","Iter: 047/050 | Train Loss: 0.00000631\n","Adjusting Layer 1, Kernel Nodes: 151, Adptive Nodes:649\n","Iter: 048/050 | Train Loss: 0.00000507\n","Iter: 049/050 | Train Loss: 0.00000459\n","\n","Iter: 049/050 | Test Loss: 0.00096058 | Test acc: 65.2300\n","scale:0.940000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00349976\n","Iter: 001/050 | Train Loss: 0.00173287\n","Iter: 002/050 | Train Loss: 0.00153549\n","Iter: 003/050 | Train Loss: 0.00085034\n","Iter: 004/050 | Train Loss: 0.00096255\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 005/050 | Train Loss: 0.00075406\n","Iter: 006/050 | Train Loss: 0.00055793\n","Iter: 007/050 | Train Loss: 0.00054508\n","Iter: 008/050 | Train Loss: 0.00057836\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 009/050 | Train Loss: 0.00052735\n","Iter: 010/050 | Train Loss: 0.00041741\n","Iter: 011/050 | Train Loss: 0.00035344\n","Iter: 012/050 | Train Loss: 0.00036075\n","Adjusting Layer 1, Kernel Nodes: 677, Adptive Nodes:123\n","Iter: 013/050 | Train Loss: 0.00036101\n","Adjusting Layer 1, Kernel Nodes: 732, Adptive Nodes:68\n","Iter: 014/050 | Train Loss: 0.00028824\n","Iter: 015/050 | Train Loss: 0.00022667\n","Iter: 016/050 | Train Loss: 0.00022695\n","Adjusting Layer 1, Kernel Nodes: 620, Adptive Nodes:180\n","Iter: 017/050 | Train Loss: 0.00022399\n","Iter: 018/050 | Train Loss: 0.00017604\n","Iter: 019/050 | Train Loss: 0.00013783\n","Iter: 020/050 | Train Loss: 0.00013907\n","Adjusting Layer 1, Kernel Nodes: 774, Adptive Nodes:26\n","Iter: 021/050 | Train Loss: 0.00013163\n","Iter: 022/050 | Train Loss: 0.00009883\n","Iter: 023/050 | Train Loss: 0.00008515\n","Iter: 024/050 | Train Loss: 0.00008699\n","Adjusting Layer 1, Kernel Nodes: 536, Adptive Nodes:264\n","Iter: 025/050 | Train Loss: 0.00007360\n","Iter: 026/050 | Train Loss: 0.00005778\n","Iter: 027/050 | Train Loss: 0.00005585\n","Iter: 028/050 | Train Loss: 0.00005229\n","Iter: 029/050 | Train Loss: 0.00004208\n","Iter: 030/050 | Train Loss: 0.00003689\n","Iter: 031/050 | Train Loss: 0.00003523\n","Iter: 032/050 | Train Loss: 0.00003150\n","Iter: 033/050 | Train Loss: 0.00002716\n","Iter: 034/050 | Train Loss: 0.00002352\n","Iter: 035/050 | Train Loss: 0.00002211\n","Iter: 036/050 | Train Loss: 0.00002132\n","Iter: 037/050 | Train Loss: 0.00001723\n","Iter: 038/050 | Train Loss: 0.00001444\n","Iter: 039/050 | Train Loss: 0.00001553\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 040/050 | Train Loss: 0.00001337\n","Iter: 041/050 | Train Loss: 0.00000977\n","Iter: 042/050 | Train Loss: 0.00001040\n","Adjusting Layer 1, Kernel Nodes: 430, Adptive Nodes:370\n","Iter: 043/050 | Train Loss: 0.00000972\n","Iter: 044/050 | Train Loss: 0.00000687\n","Iter: 045/050 | Train Loss: 0.00000727\n","Adjusting Layer 1, Kernel Nodes: 470, Adptive Nodes:330\n","Iter: 046/050 | Train Loss: 0.00000691\n","Iter: 047/050 | Train Loss: 0.00000517\n","Iter: 048/050 | Train Loss: 0.00000538\n","Adjusting Layer 1, Kernel Nodes: 539, Adptive Nodes:261\n","Iter: 049/050 | Train Loss: 0.00000473\n","\n","Iter: 049/050 | Test Loss: 0.00096695 | Test acc: 65.0300\n","scale:0.940000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00345873\n","Iter: 001/050 | Train Loss: 0.00167836\n","Iter: 002/050 | Train Loss: 0.00154788\n","Iter: 003/050 | Train Loss: 0.00084096\n","Iter: 004/050 | Train Loss: 0.00095499\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 005/050 | Train Loss: 0.00075887\n","Iter: 006/050 | Train Loss: 0.00055777\n","Iter: 007/050 | Train Loss: 0.00054001\n","Iter: 008/050 | Train Loss: 0.00057394\n","Adjusting Layer 1, Kernel Nodes: 755, Adptive Nodes:45\n","Iter: 009/050 | Train Loss: 0.00052830\n","Iter: 010/050 | Train Loss: 0.00042065\n","Iter: 011/050 | Train Loss: 0.00035337\n","Iter: 012/050 | Train Loss: 0.00035750\n","Adjusting Layer 1, Kernel Nodes: 719, Adptive Nodes:81\n","Iter: 013/050 | Train Loss: 0.00036053\n","Adjusting Layer 1, Kernel Nodes: 636, Adptive Nodes:164\n","Iter: 014/050 | Train Loss: 0.00029430\n","Iter: 015/050 | Train Loss: 0.00022934\n","Iter: 016/050 | Train Loss: 0.00022258\n","Iter: 017/050 | Train Loss: 0.00022595\n","Adjusting Layer 1, Kernel Nodes: 680, Adptive Nodes:120\n","Iter: 018/050 | Train Loss: 0.00018913\n","Iter: 019/050 | Train Loss: 0.00014465\n","Iter: 020/050 | Train Loss: 0.00013420\n","Iter: 021/050 | Train Loss: 0.00013595\n","Adjusting Layer 1, Kernel Nodes: 784, Adptive Nodes:16\n","Iter: 022/050 | Train Loss: 0.00011530\n","Iter: 023/050 | Train Loss: 0.00008936\n","Iter: 024/050 | Train Loss: 0.00008304\n","Iter: 025/050 | Train Loss: 0.00008211\n","Iter: 026/050 | Train Loss: 0.00007118\n","Iter: 027/050 | Train Loss: 0.00005754\n","Iter: 028/050 | Train Loss: 0.00005051\n","Iter: 029/050 | Train Loss: 0.00004825\n","Iter: 030/050 | Train Loss: 0.00004537\n","Iter: 031/050 | Train Loss: 0.00003806\n","Iter: 032/050 | Train Loss: 0.00002982\n","Iter: 033/050 | Train Loss: 0.00002890\n","Iter: 034/050 | Train Loss: 0.00003062\n","Adjusting Layer 1, Kernel Nodes: 766, Adptive Nodes:34\n","Iter: 035/050 | Train Loss: 0.00002480\n","Iter: 036/050 | Train Loss: 0.00001844\n","Iter: 037/050 | Train Loss: 0.00001942\n","Adjusting Layer 1, Kernel Nodes: 505, Adptive Nodes:295\n","Iter: 038/050 | Train Loss: 0.00002010\n","Adjusting Layer 1, Kernel Nodes: 508, Adptive Nodes:292\n","Iter: 039/050 | Train Loss: 0.00001508\n","Iter: 040/050 | Train Loss: 0.00001275\n","Iter: 041/050 | Train Loss: 0.00001364\n","Adjusting Layer 1, Kernel Nodes: 777, Adptive Nodes:23\n","Iter: 042/050 | Train Loss: 0.00001191\n","Iter: 043/050 | Train Loss: 0.00000919\n","Iter: 044/050 | Train Loss: 0.00000882\n","Iter: 045/050 | Train Loss: 0.00000873\n","Iter: 046/050 | Train Loss: 0.00000712\n","Iter: 047/050 | Train Loss: 0.00000590\n","Iter: 048/050 | Train Loss: 0.00000619\n","Adjusting Layer 1, Kernel Nodes: 408, Adptive Nodes:392\n","Iter: 049/050 | Train Loss: 0.00000567\n","\n","Iter: 049/050 | Test Loss: 0.00094663 | Test acc: 65.5500\n","scale:0.940000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 000/050 | Train Loss: 0.00342956\n","Iter: 001/050 | Train Loss: 0.00163789\n","Iter: 002/050 | Train Loss: 0.00155753\n","Iter: 003/050 | Train Loss: 0.00083304\n","Iter: 004/050 | Train Loss: 0.00094471\n","Adjusting Layer 1, Kernel Nodes: 670, Adptive Nodes:130\n","Iter: 005/050 | Train Loss: 0.00076346\n","Iter: 006/050 | Train Loss: 0.00055797\n","Iter: 007/050 | Train Loss: 0.00053547\n","Iter: 008/050 | Train Loss: 0.00057050\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 009/050 | Train Loss: 0.00052828\n","Iter: 010/050 | Train Loss: 0.00042163\n","Iter: 011/050 | Train Loss: 0.00035307\n","Iter: 012/050 | Train Loss: 0.00035474\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 013/050 | Train Loss: 0.00035802\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 014/050 | Train Loss: 0.00029701\n","Iter: 015/050 | Train Loss: 0.00023190\n","Iter: 016/050 | Train Loss: 0.00022068\n","Iter: 017/050 | Train Loss: 0.00022589\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 018/050 | Train Loss: 0.00019639\n","Iter: 019/050 | Train Loss: 0.00015103\n","Iter: 020/050 | Train Loss: 0.00013239\n","Iter: 021/050 | Train Loss: 0.00013571\n","Adjusting Layer 1, Kernel Nodes: 735, Adptive Nodes:65\n","Iter: 022/050 | Train Loss: 0.00012326\n","Iter: 023/050 | Train Loss: 0.00009534\n","Iter: 024/050 | Train Loss: 0.00008245\n","Iter: 025/050 | Train Loss: 0.00008141\n","Iter: 026/050 | Train Loss: 0.00007571\n","Iter: 027/050 | Train Loss: 0.00006450\n","Iter: 028/050 | Train Loss: 0.00005232\n","Iter: 029/050 | Train Loss: 0.00004516\n","Iter: 030/050 | Train Loss: 0.00004604\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 031/050 | Train Loss: 0.00004362\n","Iter: 032/050 | Train Loss: 0.00003278\n","Iter: 033/050 | Train Loss: 0.00002728\n","Iter: 034/050 | Train Loss: 0.00002962\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 035/050 | Train Loss: 0.00002845\n","Iter: 036/050 | Train Loss: 0.00002191\n","Iter: 037/050 | Train Loss: 0.00001795\n","Iter: 038/050 | Train Loss: 0.00001842\n","Adjusting Layer 1, Kernel Nodes: 778, Adptive Nodes:22\n","Iter: 039/050 | Train Loss: 0.00001838\n","Iter: 040/050 | Train Loss: 0.00001503\n","Iter: 041/050 | Train Loss: 0.00001189\n","Iter: 042/050 | Train Loss: 0.00001179\n","Iter: 043/050 | Train Loss: 0.00001220\n","Adjusting Layer 1, Kernel Nodes: 543, Adptive Nodes:257\n","Iter: 044/050 | Train Loss: 0.00001006\n","Iter: 045/050 | Train Loss: 0.00000769\n","Iter: 046/050 | Train Loss: 0.00000774\n","Adjusting Layer 1, Kernel Nodes: 764, Adptive Nodes:36\n","Iter: 047/050 | Train Loss: 0.00000797\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 048/050 | Train Loss: 0.00000626\n","Iter: 049/050 | Train Loss: 0.00000520\n","\n","Iter: 049/050 | Test Loss: 0.00093977 | Test acc: 65.7400\n","scale:0.960000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00425838\n","Iter: 001/050 | Train Loss: 0.00315973\n","Iter: 002/050 | Train Loss: 0.00129634\n","Iter: 003/050 | Train Loss: 0.00100174\n","Iter: 004/050 | Train Loss: 0.00099006\n","Iter: 005/050 | Train Loss: 0.00079536\n","Iter: 006/050 | Train Loss: 0.00067520\n","Iter: 007/050 | Train Loss: 0.00063231\n","Iter: 008/050 | Train Loss: 0.00061493\n","Iter: 009/050 | Train Loss: 0.00058632\n","Iter: 010/050 | Train Loss: 0.00052034\n","Iter: 011/050 | Train Loss: 0.00043358\n","Iter: 012/050 | Train Loss: 0.00037438\n","Iter: 013/050 | Train Loss: 0.00036039\n","Iter: 014/050 | Train Loss: 0.00034892\n","Iter: 015/050 | Train Loss: 0.00030656\n","Iter: 016/050 | Train Loss: 0.00026187\n","Iter: 017/050 | Train Loss: 0.00023819\n","Iter: 018/050 | Train Loss: 0.00022455\n","Iter: 019/050 | Train Loss: 0.00020557\n","Iter: 020/050 | Train Loss: 0.00017459\n","Iter: 021/050 | Train Loss: 0.00014414\n","Iter: 022/050 | Train Loss: 0.00013474\n","Iter: 023/050 | Train Loss: 0.00013516\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 024/050 | Train Loss: 0.00011630\n","Iter: 025/050 | Train Loss: 0.00009444\n","Iter: 026/050 | Train Loss: 0.00008763\n","Iter: 027/050 | Train Loss: 0.00008297\n","Iter: 028/050 | Train Loss: 0.00007052\n","Iter: 029/050 | Train Loss: 0.00005846\n","Iter: 030/050 | Train Loss: 0.00005295\n","Iter: 031/050 | Train Loss: 0.00004838\n","Iter: 032/050 | Train Loss: 0.00004329\n","Iter: 033/050 | Train Loss: 0.00003989\n","Iter: 034/050 | Train Loss: 0.00003438\n","Iter: 035/050 | Train Loss: 0.00002857\n","Iter: 036/050 | Train Loss: 0.00002792\n","Iter: 037/050 | Train Loss: 0.00002707\n","Iter: 038/050 | Train Loss: 0.00002149\n","Iter: 039/050 | Train Loss: 0.00001771\n","Iter: 040/050 | Train Loss: 0.00001812\n","Adjusting Layer 1, Kernel Nodes: 614, Adptive Nodes:186\n","Iter: 041/050 | Train Loss: 0.00001721\n","Iter: 042/050 | Train Loss: 0.00001390\n","Iter: 043/050 | Train Loss: 0.00001245\n","Iter: 044/050 | Train Loss: 0.00001213\n","Iter: 045/050 | Train Loss: 0.00001082\n","Iter: 046/050 | Train Loss: 0.00000944\n","Iter: 047/050 | Train Loss: 0.00000847\n","Iter: 048/050 | Train Loss: 0.00000768\n","Iter: 049/050 | Train Loss: 0.00000723\n","\n","Iter: 049/050 | Test Loss: 0.00094130 | Test acc: 64.9900\n","scale:0.960000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00408200\n","Iter: 001/050 | Train Loss: 0.00275222\n","Iter: 002/050 | Train Loss: 0.00134543\n","Iter: 003/050 | Train Loss: 0.00096819\n","Iter: 004/050 | Train Loss: 0.00099120\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 005/050 | Train Loss: 0.00075881\n","Iter: 006/050 | Train Loss: 0.00059843\n","Iter: 007/050 | Train Loss: 0.00058426\n","Iter: 008/050 | Train Loss: 0.00059856\n","Adjusting Layer 1, Kernel Nodes: 631, Adptive Nodes:169\n","Iter: 009/050 | Train Loss: 0.00053621\n","Iter: 010/050 | Train Loss: 0.00042626\n","Iter: 011/050 | Train Loss: 0.00035689\n","Iter: 012/050 | Train Loss: 0.00035399\n","Iter: 013/050 | Train Loss: 0.00034946\n","Iter: 014/050 | Train Loss: 0.00029105\n","Iter: 015/050 | Train Loss: 0.00022603\n","Iter: 016/050 | Train Loss: 0.00020754\n","Iter: 017/050 | Train Loss: 0.00020976\n","Adjusting Layer 1, Kernel Nodes: 543, Adptive Nodes:257\n","Iter: 018/050 | Train Loss: 0.00017997\n","Iter: 019/050 | Train Loss: 0.00013564\n","Iter: 020/050 | Train Loss: 0.00011854\n","Iter: 021/050 | Train Loss: 0.00011751\n","Iter: 022/050 | Train Loss: 0.00010122\n","Iter: 023/050 | Train Loss: 0.00007807\n","Iter: 024/050 | Train Loss: 0.00006894\n","Iter: 025/050 | Train Loss: 0.00006750\n","Iter: 026/050 | Train Loss: 0.00005993\n","Iter: 027/050 | Train Loss: 0.00004911\n","Iter: 028/050 | Train Loss: 0.00004288\n","Iter: 029/050 | Train Loss: 0.00004002\n","Iter: 030/050 | Train Loss: 0.00003674\n","Iter: 031/050 | Train Loss: 0.00003257\n","Iter: 032/050 | Train Loss: 0.00002876\n","Iter: 033/050 | Train Loss: 0.00002592\n","Iter: 034/050 | Train Loss: 0.00002352\n","Iter: 035/050 | Train Loss: 0.00002142\n","Iter: 036/050 | Train Loss: 0.00001969\n","Iter: 037/050 | Train Loss: 0.00001732\n","Iter: 038/050 | Train Loss: 0.00001445\n","Iter: 039/050 | Train Loss: 0.00001312\n","Iter: 040/050 | Train Loss: 0.00001305\n","Iter: 041/050 | Train Loss: 0.00001159\n","Iter: 042/050 | Train Loss: 0.00000898\n","Iter: 043/050 | Train Loss: 0.00000789\n","Iter: 044/050 | Train Loss: 0.00000825\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 045/050 | Train Loss: 0.00000746\n","Iter: 046/050 | Train Loss: 0.00000595\n","Iter: 047/050 | Train Loss: 0.00000556\n","Iter: 048/050 | Train Loss: 0.00000554\n","Iter: 049/050 | Train Loss: 0.00000492\n","\n","Iter: 049/050 | Test Loss: 0.00100429 | Test acc: 63.7300\n","scale:0.960000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00393372\n","Iter: 001/050 | Train Loss: 0.00245120\n","Iter: 002/050 | Train Loss: 0.00138810\n","Iter: 003/050 | Train Loss: 0.00094044\n","Iter: 004/050 | Train Loss: 0.00099065\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 005/050 | Train Loss: 0.00075052\n","Iter: 006/050 | Train Loss: 0.00058253\n","Iter: 007/050 | Train Loss: 0.00057421\n","Iter: 008/050 | Train Loss: 0.00059325\n","Adjusting Layer 1, Kernel Nodes: 739, Adptive Nodes:61\n","Iter: 009/050 | Train Loss: 0.00052128\n","Iter: 010/050 | Train Loss: 0.00040920\n","Iter: 011/050 | Train Loss: 0.00035414\n","Iter: 012/050 | Train Loss: 0.00036067\n","Adjusting Layer 1, Kernel Nodes: 487, Adptive Nodes:313\n","Iter: 013/050 | Train Loss: 0.00034138\n","Iter: 014/050 | Train Loss: 0.00026383\n","Iter: 015/050 | Train Loss: 0.00021501\n","Iter: 016/050 | Train Loss: 0.00021706\n","Adjusting Layer 1, Kernel Nodes: 507, Adptive Nodes:293\n","Iter: 017/050 | Train Loss: 0.00019540\n","Iter: 018/050 | Train Loss: 0.00014116\n","Iter: 019/050 | Train Loss: 0.00012305\n","Iter: 020/050 | Train Loss: 0.00012611\n","Adjusting Layer 1, Kernel Nodes: 743, Adptive Nodes:57\n","Iter: 021/050 | Train Loss: 0.00009024\n","Iter: 022/050 | Train Loss: 0.00007703\n","Iter: 023/050 | Train Loss: 0.00008063\n","Adjusting Layer 1, Kernel Nodes: 311, Adptive Nodes:489\n","Iter: 024/050 | Train Loss: 0.00005491\n","Iter: 025/050 | Train Loss: 0.00006117\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 026/050 | Train Loss: 0.00004586\n","Iter: 027/050 | Train Loss: 0.00004811\n","Adjusting Layer 1, Kernel Nodes: 25, Adptive Nodes:775\n","Iter: 028/050 | Train Loss: 0.00003867\n","Iter: 029/050 | Train Loss: 0.00003606\n","Iter: 030/050 | Train Loss: 0.00003386\n","Iter: 031/050 | Train Loss: 0.00002624\n","Iter: 032/050 | Train Loss: 0.00002686\n","Adjusting Layer 1, Kernel Nodes: 446, Adptive Nodes:354\n","Iter: 033/050 | Train Loss: 0.00002064\n","Iter: 034/050 | Train Loss: 0.00002087\n","Adjusting Layer 1, Kernel Nodes: 353, Adptive Nodes:447\n","Iter: 035/050 | Train Loss: 0.00001655\n","Iter: 036/050 | Train Loss: 0.00001647\n","Iter: 037/050 | Train Loss: 0.00001427\n","Iter: 038/050 | Train Loss: 0.00001337\n","Iter: 039/050 | Train Loss: 0.00001252\n","Iter: 040/050 | Train Loss: 0.00001092\n","Iter: 041/050 | Train Loss: 0.00001059\n","Iter: 042/050 | Train Loss: 0.00000851\n","Iter: 043/050 | Train Loss: 0.00000829\n","Iter: 044/050 | Train Loss: 0.00000677\n","Iter: 045/050 | Train Loss: 0.00000618\n","Iter: 046/050 | Train Loss: 0.00000579\n","Iter: 047/050 | Train Loss: 0.00000466\n","Iter: 048/050 | Train Loss: 0.00000476\n","Adjusting Layer 1, Kernel Nodes: 479, Adptive Nodes:321\n","Iter: 049/050 | Train Loss: 0.00000377\n","\n","Iter: 049/050 | Test Loss: 0.00105695 | Test acc: 62.4100\n","scale:0.960000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00380957\n","Iter: 001/050 | Train Loss: 0.00221747\n","Iter: 002/050 | Train Loss: 0.00143184\n","Iter: 003/050 | Train Loss: 0.00091412\n","Iter: 004/050 | Train Loss: 0.00098983\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 005/050 | Train Loss: 0.00074858\n","Iter: 006/050 | Train Loss: 0.00057323\n","Iter: 007/050 | Train Loss: 0.00056792\n","Iter: 008/050 | Train Loss: 0.00059230\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 009/050 | Train Loss: 0.00052265\n","Iter: 010/050 | Train Loss: 0.00040776\n","Iter: 011/050 | Train Loss: 0.00035352\n","Iter: 012/050 | Train Loss: 0.00036431\n","Adjusting Layer 1, Kernel Nodes: 739, Adptive Nodes:61\n","Iter: 013/050 | Train Loss: 0.00034740\n","Iter: 014/050 | Train Loss: 0.00026731\n","Iter: 015/050 | Train Loss: 0.00021933\n","Iter: 016/050 | Train Loss: 0.00022418\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 017/050 | Train Loss: 0.00020296\n","Iter: 018/050 | Train Loss: 0.00014832\n","Iter: 019/050 | Train Loss: 0.00012925\n","Iter: 020/050 | Train Loss: 0.00013214\n","Adjusting Layer 1, Kernel Nodes: 282, Adptive Nodes:518\n","Iter: 021/050 | Train Loss: 0.00010054\n","Iter: 022/050 | Train Loss: 0.00007857\n","Iter: 023/050 | Train Loss: 0.00008297\n","Adjusting Layer 1, Kernel Nodes: 602, Adptive Nodes:198\n","Iter: 024/050 | Train Loss: 0.00006439\n","Iter: 025/050 | Train Loss: 0.00005443\n","Iter: 026/050 | Train Loss: 0.00005710\n","Adjusting Layer 1, Kernel Nodes: 359, Adptive Nodes:441\n","Iter: 027/050 | Train Loss: 0.00004118\n","Iter: 028/050 | Train Loss: 0.00004397\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 029/050 | Train Loss: 0.00003445\n","Iter: 030/050 | Train Loss: 0.00003326\n","Iter: 031/050 | Train Loss: 0.00002958\n","Iter: 032/050 | Train Loss: 0.00002466\n","Iter: 033/050 | Train Loss: 0.00002377\n","Iter: 034/050 | Train Loss: 0.00001888\n","Iter: 035/050 | Train Loss: 0.00001793\n","Iter: 036/050 | Train Loss: 0.00001523\n","Iter: 037/050 | Train Loss: 0.00001382\n","Iter: 038/050 | Train Loss: 0.00001277\n","Iter: 039/050 | Train Loss: 0.00001178\n","Iter: 040/050 | Train Loss: 0.00001083\n","Iter: 041/050 | Train Loss: 0.00001020\n","Iter: 042/050 | Train Loss: 0.00000904\n","Iter: 043/050 | Train Loss: 0.00000817\n","Iter: 044/050 | Train Loss: 0.00000746\n","Iter: 045/050 | Train Loss: 0.00000637\n","Iter: 046/050 | Train Loss: 0.00000603\n","Iter: 047/050 | Train Loss: 0.00000518\n","Iter: 048/050 | Train Loss: 0.00000465\n","Iter: 049/050 | Train Loss: 0.00000424\n","\n","Iter: 049/050 | Test Loss: 0.00103813 | Test acc: 62.9600\n","scale:0.960000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00370691\n","Iter: 001/050 | Train Loss: 0.00204528\n","Iter: 002/050 | Train Loss: 0.00146635\n","Iter: 003/050 | Train Loss: 0.00089366\n","Iter: 004/050 | Train Loss: 0.00098531\n","Adjusting Layer 1, Kernel Nodes: 655, Adptive Nodes:145\n","Iter: 005/050 | Train Loss: 0.00074707\n","Iter: 006/050 | Train Loss: 0.00056640\n","Iter: 007/050 | Train Loss: 0.00056232\n","Iter: 008/050 | Train Loss: 0.00058869\n","Adjusting Layer 1, Kernel Nodes: 750, Adptive Nodes:50\n","Iter: 009/050 | Train Loss: 0.00052121\n","Iter: 010/050 | Train Loss: 0.00040678\n","Iter: 011/050 | Train Loss: 0.00035254\n","Iter: 012/050 | Train Loss: 0.00036484\n","Adjusting Layer 1, Kernel Nodes: 643, Adptive Nodes:157\n","Iter: 013/050 | Train Loss: 0.00035120\n","Iter: 014/050 | Train Loss: 0.00027299\n","Iter: 015/050 | Train Loss: 0.00022009\n","Iter: 016/050 | Train Loss: 0.00022264\n","Adjusting Layer 1, Kernel Nodes: 459, Adptive Nodes:341\n","Iter: 017/050 | Train Loss: 0.00021122\n","Iter: 018/050 | Train Loss: 0.00015937\n","Iter: 019/050 | Train Loss: 0.00012833\n","Iter: 020/050 | Train Loss: 0.00013165\n","Adjusting Layer 1, Kernel Nodes: 722, Adptive Nodes:78\n","Iter: 021/050 | Train Loss: 0.00011373\n","Iter: 022/050 | Train Loss: 0.00008217\n","Iter: 023/050 | Train Loss: 0.00008004\n","Iter: 024/050 | Train Loss: 0.00007622\n","Iter: 025/050 | Train Loss: 0.00005759\n","Iter: 026/050 | Train Loss: 0.00005148\n","Iter: 027/050 | Train Loss: 0.00005157\n","Adjusting Layer 1, Kernel Nodes: 789, Adptive Nodes:11\n","Iter: 028/050 | Train Loss: 0.00004152\n","Iter: 029/050 | Train Loss: 0.00003716\n","Iter: 030/050 | Train Loss: 0.00003533\n","Iter: 031/050 | Train Loss: 0.00003017\n","Iter: 032/050 | Train Loss: 0.00002664\n","Iter: 033/050 | Train Loss: 0.00002462\n","Iter: 034/050 | Train Loss: 0.00002223\n","Iter: 035/050 | Train Loss: 0.00001875\n","Iter: 036/050 | Train Loss: 0.00001740\n","Iter: 037/050 | Train Loss: 0.00001629\n","Iter: 038/050 | Train Loss: 0.00001292\n","Iter: 039/050 | Train Loss: 0.00001190\n","Iter: 040/050 | Train Loss: 0.00001153\n","Iter: 041/050 | Train Loss: 0.00000901\n","Iter: 042/050 | Train Loss: 0.00000850\n","Iter: 043/050 | Train Loss: 0.00000879\n","Adjusting Layer 1, Kernel Nodes: 231, Adptive Nodes:569\n","Iter: 044/050 | Train Loss: 0.00000676\n","Iter: 045/050 | Train Loss: 0.00000671\n","Iter: 046/050 | Train Loss: 0.00000639\n","Iter: 047/050 | Train Loss: 0.00000477\n","Iter: 048/050 | Train Loss: 0.00000495\n","Adjusting Layer 1, Kernel Nodes: 337, Adptive Nodes:463\n","Iter: 049/050 | Train Loss: 0.00000422\n","\n","Iter: 049/050 | Test Loss: 0.00101044 | Test acc: 63.8000\n","scale:0.960000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00362207\n","Iter: 001/050 | Train Loss: 0.00190813\n","Iter: 002/050 | Train Loss: 0.00149448\n","Iter: 003/050 | Train Loss: 0.00087558\n","Iter: 004/050 | Train Loss: 0.00097785\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 005/050 | Train Loss: 0.00074709\n","Iter: 006/050 | Train Loss: 0.00056062\n","Iter: 007/050 | Train Loss: 0.00055534\n","Iter: 008/050 | Train Loss: 0.00058360\n","Adjusting Layer 1, Kernel Nodes: 743, Adptive Nodes:57\n","Iter: 009/050 | Train Loss: 0.00051923\n","Iter: 010/050 | Train Loss: 0.00040655\n","Iter: 011/050 | Train Loss: 0.00035224\n","Iter: 012/050 | Train Loss: 0.00036494\n","Adjusting Layer 1, Kernel Nodes: 739, Adptive Nodes:61\n","Iter: 013/050 | Train Loss: 0.00035324\n","Iter: 014/050 | Train Loss: 0.00027613\n","Iter: 015/050 | Train Loss: 0.00022264\n","Iter: 016/050 | Train Loss: 0.00022486\n","Adjusting Layer 1, Kernel Nodes: 518, Adptive Nodes:282\n","Iter: 017/050 | Train Loss: 0.00021673\n","Iter: 018/050 | Train Loss: 0.00016714\n","Iter: 019/050 | Train Loss: 0.00013290\n","Iter: 020/050 | Train Loss: 0.00013517\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 021/050 | Train Loss: 0.00012284\n","Iter: 022/050 | Train Loss: 0.00008981\n","Iter: 023/050 | Train Loss: 0.00008156\n","Iter: 024/050 | Train Loss: 0.00008119\n","Iter: 025/050 | Train Loss: 0.00006569\n","Iter: 026/050 | Train Loss: 0.00005325\n","Iter: 027/050 | Train Loss: 0.00005212\n","Iter: 028/050 | Train Loss: 0.00004845\n","Iter: 029/050 | Train Loss: 0.00003949\n","Iter: 030/050 | Train Loss: 0.00003486\n","Iter: 031/050 | Train Loss: 0.00003345\n","Iter: 032/050 | Train Loss: 0.00003024\n","Iter: 033/050 | Train Loss: 0.00002660\n","Iter: 034/050 | Train Loss: 0.00002297\n","Iter: 035/050 | Train Loss: 0.00002087\n","Iter: 036/050 | Train Loss: 0.00002074\n","Iter: 037/050 | Train Loss: 0.00001754\n","Iter: 038/050 | Train Loss: 0.00001353\n","Iter: 039/050 | Train Loss: 0.00001414\n","Adjusting Layer 1, Kernel Nodes: 520, Adptive Nodes:280\n","Iter: 040/050 | Train Loss: 0.00001339\n","Iter: 041/050 | Train Loss: 0.00000942\n","Iter: 042/050 | Train Loss: 0.00000923\n","Iter: 043/050 | Train Loss: 0.00000942\n","Adjusting Layer 1, Kernel Nodes: 424, Adptive Nodes:376\n","Iter: 044/050 | Train Loss: 0.00000679\n","Iter: 045/050 | Train Loss: 0.00000652\n","Iter: 046/050 | Train Loss: 0.00000664\n","Adjusting Layer 1, Kernel Nodes: 534, Adptive Nodes:266\n","Iter: 047/050 | Train Loss: 0.00000520\n","Iter: 048/050 | Train Loss: 0.00000501\n","Iter: 049/050 | Train Loss: 0.00000463\n","\n","Iter: 049/050 | Test Loss: 0.00098685 | Test acc: 64.5800\n","scale:0.960000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00355211\n","Iter: 001/050 | Train Loss: 0.00180620\n","Iter: 002/050 | Train Loss: 0.00151852\n","Iter: 003/050 | Train Loss: 0.00086109\n","Iter: 004/050 | Train Loss: 0.00096893\n","Adjusting Layer 1, Kernel Nodes: 662, Adptive Nodes:138\n","Iter: 005/050 | Train Loss: 0.00075001\n","Iter: 006/050 | Train Loss: 0.00055867\n","Iter: 007/050 | Train Loss: 0.00054957\n","Iter: 008/050 | Train Loss: 0.00058116\n","Adjusting Layer 1, Kernel Nodes: 708, Adptive Nodes:92\n","Iter: 009/050 | Train Loss: 0.00052339\n","Iter: 010/050 | Train Loss: 0.00041041\n","Iter: 011/050 | Train Loss: 0.00035265\n","Iter: 012/050 | Train Loss: 0.00036487\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 013/050 | Train Loss: 0.00035875\n","Iter: 014/050 | Train Loss: 0.00028485\n","Iter: 015/050 | Train Loss: 0.00022686\n","Iter: 016/050 | Train Loss: 0.00022544\n","Iter: 017/050 | Train Loss: 0.00022469\n","Iter: 018/050 | Train Loss: 0.00018637\n","Iter: 019/050 | Train Loss: 0.00014364\n","Iter: 020/050 | Train Loss: 0.00013072\n","Iter: 021/050 | Train Loss: 0.00013111\n","Adjusting Layer 1, Kernel Nodes: 697, Adptive Nodes:103\n","Iter: 022/050 | Train Loss: 0.00011473\n","Iter: 023/050 | Train Loss: 0.00009015\n","Iter: 024/050 | Train Loss: 0.00007969\n","Iter: 025/050 | Train Loss: 0.00007736\n","Iter: 026/050 | Train Loss: 0.00006944\n","Iter: 027/050 | Train Loss: 0.00005759\n","Iter: 028/050 | Train Loss: 0.00004926\n","Iter: 029/050 | Train Loss: 0.00004467\n","Iter: 030/050 | Train Loss: 0.00004199\n","Iter: 031/050 | Train Loss: 0.00003904\n","Iter: 032/050 | Train Loss: 0.00003218\n","Iter: 033/050 | Train Loss: 0.00002585\n","Iter: 034/050 | Train Loss: 0.00002670\n","Adjusting Layer 1, Kernel Nodes: 507, Adptive Nodes:293\n","Iter: 035/050 | Train Loss: 0.00002702\n","Adjusting Layer 1, Kernel Nodes: 512, Adptive Nodes:288\n","Iter: 036/050 | Train Loss: 0.00001925\n","Iter: 037/050 | Train Loss: 0.00001692\n","Iter: 038/050 | Train Loss: 0.00001949\n","Adjusting Layer 1, Kernel Nodes: 546, Adptive Nodes:254\n","Iter: 039/050 | Train Loss: 0.00001530\n","Iter: 040/050 | Train Loss: 0.00001169\n","Iter: 041/050 | Train Loss: 0.00001322\n","Adjusting Layer 1, Kernel Nodes: 494, Adptive Nodes:306\n","Iter: 042/050 | Train Loss: 0.00001104\n","Iter: 043/050 | Train Loss: 0.00000833\n","Iter: 044/050 | Train Loss: 0.00000890\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 045/050 | Train Loss: 0.00000745\n","Iter: 046/050 | Train Loss: 0.00000601\n","Iter: 047/050 | Train Loss: 0.00000628\n","Adjusting Layer 1, Kernel Nodes: 190, Adptive Nodes:610\n","Iter: 048/050 | Train Loss: 0.00000509\n","Iter: 049/050 | Train Loss: 0.00000452\n","\n","Iter: 049/050 | Test Loss: 0.00096055 | Test acc: 65.2100\n","scale:0.960000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00349720\n","Iter: 001/050 | Train Loss: 0.00173005\n","Iter: 002/050 | Train Loss: 0.00153590\n","Iter: 003/050 | Train Loss: 0.00085009\n","Iter: 004/050 | Train Loss: 0.00096226\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 005/050 | Train Loss: 0.00075402\n","Iter: 006/050 | Train Loss: 0.00055754\n","Iter: 007/050 | Train Loss: 0.00054460\n","Iter: 008/050 | Train Loss: 0.00057786\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 009/050 | Train Loss: 0.00052677\n","Iter: 010/050 | Train Loss: 0.00041682\n","Iter: 011/050 | Train Loss: 0.00035293\n","Iter: 012/050 | Train Loss: 0.00036090\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 013/050 | Train Loss: 0.00036115\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 014/050 | Train Loss: 0.00028744\n","Iter: 015/050 | Train Loss: 0.00022614\n","Iter: 016/050 | Train Loss: 0.00022679\n","Adjusting Layer 1, Kernel Nodes: 657, Adptive Nodes:143\n","Iter: 017/050 | Train Loss: 0.00022342\n","Iter: 018/050 | Train Loss: 0.00017524\n","Iter: 019/050 | Train Loss: 0.00013744\n","Iter: 020/050 | Train Loss: 0.00013880\n","Adjusting Layer 1, Kernel Nodes: 772, Adptive Nodes:28\n","Iter: 021/050 | Train Loss: 0.00013106\n","Iter: 022/050 | Train Loss: 0.00009834\n","Iter: 023/050 | Train Loss: 0.00008491\n","Iter: 024/050 | Train Loss: 0.00008644\n","Adjusting Layer 1, Kernel Nodes: 514, Adptive Nodes:286\n","Iter: 025/050 | Train Loss: 0.00007308\n","Iter: 026/050 | Train Loss: 0.00005763\n","Iter: 027/050 | Train Loss: 0.00005544\n","Iter: 028/050 | Train Loss: 0.00005182\n","Iter: 029/050 | Train Loss: 0.00004216\n","Iter: 030/050 | Train Loss: 0.00003689\n","Iter: 031/050 | Train Loss: 0.00003493\n","Iter: 032/050 | Train Loss: 0.00003166\n","Iter: 033/050 | Train Loss: 0.00002737\n","Iter: 034/050 | Train Loss: 0.00002328\n","Iter: 035/050 | Train Loss: 0.00002215\n","Iter: 036/050 | Train Loss: 0.00002148\n","Iter: 037/050 | Train Loss: 0.00001706\n","Iter: 038/050 | Train Loss: 0.00001444\n","Iter: 039/050 | Train Loss: 0.00001567\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 040/050 | Train Loss: 0.00001325\n","Iter: 041/050 | Train Loss: 0.00000981\n","Iter: 042/050 | Train Loss: 0.00001050\n","Adjusting Layer 1, Kernel Nodes: 436, Adptive Nodes:364\n","Iter: 043/050 | Train Loss: 0.00000961\n","Iter: 044/050 | Train Loss: 0.00000694\n","Iter: 045/050 | Train Loss: 0.00000733\n","Adjusting Layer 1, Kernel Nodes: 482, Adptive Nodes:318\n","Iter: 046/050 | Train Loss: 0.00000681\n","Iter: 047/050 | Train Loss: 0.00000525\n","Iter: 048/050 | Train Loss: 0.00000534\n","Adjusting Layer 1, Kernel Nodes: 649, Adptive Nodes:151\n","Iter: 049/050 | Train Loss: 0.00000468\n","\n","Iter: 049/050 | Test Loss: 0.00096787 | Test acc: 64.9400\n","scale:0.960000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00345600\n","Iter: 001/050 | Train Loss: 0.00167545\n","Iter: 002/050 | Train Loss: 0.00154815\n","Iter: 003/050 | Train Loss: 0.00084072\n","Iter: 004/050 | Train Loss: 0.00095449\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 005/050 | Train Loss: 0.00075868\n","Iter: 006/050 | Train Loss: 0.00055729\n","Iter: 007/050 | Train Loss: 0.00053944\n","Iter: 008/050 | Train Loss: 0.00057334\n","Adjusting Layer 1, Kernel Nodes: 754, Adptive Nodes:46\n","Iter: 009/050 | Train Loss: 0.00052768\n","Iter: 010/050 | Train Loss: 0.00042008\n","Iter: 011/050 | Train Loss: 0.00035287\n","Iter: 012/050 | Train Loss: 0.00035758\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 013/050 | Train Loss: 0.00036041\n","Adjusting Layer 1, Kernel Nodes: 637, Adptive Nodes:163\n","Iter: 014/050 | Train Loss: 0.00029411\n","Iter: 015/050 | Train Loss: 0.00022988\n","Iter: 016/050 | Train Loss: 0.00022394\n","Iter: 017/050 | Train Loss: 0.00022685\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 018/050 | Train Loss: 0.00018917\n","Iter: 019/050 | Train Loss: 0.00014467\n","Iter: 020/050 | Train Loss: 0.00013476\n","Iter: 021/050 | Train Loss: 0.00013631\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 022/050 | Train Loss: 0.00011521\n","Iter: 023/050 | Train Loss: 0.00008937\n","Iter: 024/050 | Train Loss: 0.00008331\n","Iter: 025/050 | Train Loss: 0.00008241\n","Iter: 026/050 | Train Loss: 0.00007132\n","Iter: 027/050 | Train Loss: 0.00005755\n","Iter: 028/050 | Train Loss: 0.00005068\n","Iter: 029/050 | Train Loss: 0.00004846\n","Iter: 030/050 | Train Loss: 0.00004535\n","Iter: 031/050 | Train Loss: 0.00003802\n","Iter: 032/050 | Train Loss: 0.00002994\n","Iter: 033/050 | Train Loss: 0.00002899\n","Iter: 034/050 | Train Loss: 0.00003066\n","Adjusting Layer 1, Kernel Nodes: 765, Adptive Nodes:35\n","Iter: 035/050 | Train Loss: 0.00002486\n","Iter: 036/050 | Train Loss: 0.00001843\n","Iter: 037/050 | Train Loss: 0.00001937\n","Adjusting Layer 1, Kernel Nodes: 637, Adptive Nodes:163\n","Iter: 038/050 | Train Loss: 0.00002015\n","Adjusting Layer 1, Kernel Nodes: 772, Adptive Nodes:28\n","Iter: 039/050 | Train Loss: 0.00001500\n","Iter: 040/050 | Train Loss: 0.00001263\n","Iter: 041/050 | Train Loss: 0.00001377\n","Adjusting Layer 1, Kernel Nodes: 431, Adptive Nodes:369\n","Iter: 042/050 | Train Loss: 0.00001185\n","Iter: 043/050 | Train Loss: 0.00000898\n","Iter: 044/050 | Train Loss: 0.00000888\n","Iter: 045/050 | Train Loss: 0.00000872\n","Iter: 046/050 | Train Loss: 0.00000693\n","Iter: 047/050 | Train Loss: 0.00000594\n","Iter: 048/050 | Train Loss: 0.00000624\n","Adjusting Layer 1, Kernel Nodes: 403, Adptive Nodes:397\n","Iter: 049/050 | Train Loss: 0.00000556\n","\n","Iter: 049/050 | Test Loss: 0.00094612 | Test acc: 65.5800\n","scale:0.960000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 000/050 | Train Loss: 0.00342768\n","Iter: 001/050 | Train Loss: 0.00163608\n","Iter: 002/050 | Train Loss: 0.00155770\n","Iter: 003/050 | Train Loss: 0.00083287\n","Iter: 004/050 | Train Loss: 0.00094437\n","Adjusting Layer 1, Kernel Nodes: 667, Adptive Nodes:133\n","Iter: 005/050 | Train Loss: 0.00076349\n","Iter: 006/050 | Train Loss: 0.00055790\n","Iter: 007/050 | Train Loss: 0.00053523\n","Iter: 008/050 | Train Loss: 0.00057035\n","Adjusting Layer 1, Kernel Nodes: 731, Adptive Nodes:69\n","Iter: 009/050 | Train Loss: 0.00052818\n","Iter: 010/050 | Train Loss: 0.00042143\n","Iter: 011/050 | Train Loss: 0.00035279\n","Iter: 012/050 | Train Loss: 0.00035451\n","Adjusting Layer 1, Kernel Nodes: 741, Adptive Nodes:59\n","Iter: 013/050 | Train Loss: 0.00035782\n","Adjusting Layer 1, Kernel Nodes: 651, Adptive Nodes:149\n","Iter: 014/050 | Train Loss: 0.00029673\n","Iter: 015/050 | Train Loss: 0.00023159\n","Iter: 016/050 | Train Loss: 0.00022033\n","Iter: 017/050 | Train Loss: 0.00022549\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 018/050 | Train Loss: 0.00019612\n","Iter: 019/050 | Train Loss: 0.00015095\n","Iter: 020/050 | Train Loss: 0.00013242\n","Iter: 021/050 | Train Loss: 0.00013574\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 022/050 | Train Loss: 0.00012320\n","Iter: 023/050 | Train Loss: 0.00009529\n","Iter: 024/050 | Train Loss: 0.00008253\n","Iter: 025/050 | Train Loss: 0.00008143\n","Iter: 026/050 | Train Loss: 0.00007559\n","Iter: 027/050 | Train Loss: 0.00006447\n","Iter: 028/050 | Train Loss: 0.00005228\n","Iter: 029/050 | Train Loss: 0.00004514\n","Iter: 030/050 | Train Loss: 0.00004616\n","Adjusting Layer 1, Kernel Nodes: 786, Adptive Nodes:14\n","Iter: 031/050 | Train Loss: 0.00004360\n","Iter: 032/050 | Train Loss: 0.00003268\n","Iter: 033/050 | Train Loss: 0.00002744\n","Iter: 034/050 | Train Loss: 0.00002977\n","Adjusting Layer 1, Kernel Nodes: 759, Adptive Nodes:41\n","Iter: 035/050 | Train Loss: 0.00002842\n","Iter: 036/050 | Train Loss: 0.00002192\n","Iter: 037/050 | Train Loss: 0.00001802\n","Iter: 038/050 | Train Loss: 0.00001847\n","Adjusting Layer 1, Kernel Nodes: 730, Adptive Nodes:70\n","Iter: 039/050 | Train Loss: 0.00001842\n","Iter: 040/050 | Train Loss: 0.00001504\n","Iter: 041/050 | Train Loss: 0.00001189\n","Iter: 042/050 | Train Loss: 0.00001186\n","Iter: 043/050 | Train Loss: 0.00001226\n","Adjusting Layer 1, Kernel Nodes: 691, Adptive Nodes:109\n","Iter: 044/050 | Train Loss: 0.00001003\n","Iter: 045/050 | Train Loss: 0.00000769\n","Iter: 046/050 | Train Loss: 0.00000783\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 047/050 | Train Loss: 0.00000800\n","Adjusting Layer 1, Kernel Nodes: 553, Adptive Nodes:247\n","Iter: 048/050 | Train Loss: 0.00000621\n","Iter: 049/050 | Train Loss: 0.00000522\n","\n","Iter: 049/050 | Test Loss: 0.00093949 | Test acc: 65.7100\n","scale:0.980000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00425325\n","Iter: 001/050 | Train Loss: 0.00314798\n","Iter: 002/050 | Train Loss: 0.00129778\n","Iter: 003/050 | Train Loss: 0.00100091\n","Iter: 004/050 | Train Loss: 0.00099020\n","Iter: 005/050 | Train Loss: 0.00079528\n","Iter: 006/050 | Train Loss: 0.00067473\n","Iter: 007/050 | Train Loss: 0.00063169\n","Iter: 008/050 | Train Loss: 0.00061454\n","Iter: 009/050 | Train Loss: 0.00058624\n","Iter: 010/050 | Train Loss: 0.00052005\n","Iter: 011/050 | Train Loss: 0.00043332\n","Iter: 012/050 | Train Loss: 0.00037472\n","Iter: 013/050 | Train Loss: 0.00036090\n","Iter: 014/050 | Train Loss: 0.00034855\n","Iter: 015/050 | Train Loss: 0.00030692\n","Iter: 016/050 | Train Loss: 0.00026259\n","Iter: 017/050 | Train Loss: 0.00023772\n","Iter: 018/050 | Train Loss: 0.00022391\n","Iter: 019/050 | Train Loss: 0.00020592\n","Iter: 020/050 | Train Loss: 0.00017536\n","Iter: 021/050 | Train Loss: 0.00014424\n","Iter: 022/050 | Train Loss: 0.00013367\n","Iter: 023/050 | Train Loss: 0.00013468\n","Adjusting Layer 1, Kernel Nodes: 673, Adptive Nodes:127\n","Iter: 024/050 | Train Loss: 0.00011719\n","Iter: 025/050 | Train Loss: 0.00009422\n","Iter: 026/050 | Train Loss: 0.00008652\n","Iter: 027/050 | Train Loss: 0.00008300\n","Iter: 028/050 | Train Loss: 0.00007117\n","Iter: 029/050 | Train Loss: 0.00005823\n","Iter: 030/050 | Train Loss: 0.00005258\n","Iter: 031/050 | Train Loss: 0.00004859\n","Iter: 032/050 | Train Loss: 0.00004324\n","Iter: 033/050 | Train Loss: 0.00003963\n","Iter: 034/050 | Train Loss: 0.00003451\n","Iter: 035/050 | Train Loss: 0.00002864\n","Iter: 036/050 | Train Loss: 0.00002771\n","Iter: 037/050 | Train Loss: 0.00002724\n","Iter: 038/050 | Train Loss: 0.00002190\n","Iter: 039/050 | Train Loss: 0.00001773\n","Iter: 040/050 | Train Loss: 0.00001803\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 041/050 | Train Loss: 0.00001736\n","Iter: 042/050 | Train Loss: 0.00001390\n","Iter: 043/050 | Train Loss: 0.00001224\n","Iter: 044/050 | Train Loss: 0.00001201\n","Iter: 045/050 | Train Loss: 0.00001073\n","Iter: 046/050 | Train Loss: 0.00000927\n","Iter: 047/050 | Train Loss: 0.00000837\n","Iter: 048/050 | Train Loss: 0.00000764\n","Iter: 049/050 | Train Loss: 0.00000716\n","\n","Iter: 049/050 | Test Loss: 0.00094100 | Test acc: 65.0000\n","scale:0.980000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00407886\n","Iter: 001/050 | Train Loss: 0.00274590\n","Iter: 002/050 | Train Loss: 0.00134622\n","Iter: 003/050 | Train Loss: 0.00096767\n","Iter: 004/050 | Train Loss: 0.00099093\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 005/050 | Train Loss: 0.00075864\n","Iter: 006/050 | Train Loss: 0.00059809\n","Iter: 007/050 | Train Loss: 0.00058401\n","Iter: 008/050 | Train Loss: 0.00059839\n","Adjusting Layer 1, Kernel Nodes: 616, Adptive Nodes:184\n","Iter: 009/050 | Train Loss: 0.00053584\n","Iter: 010/050 | Train Loss: 0.00042575\n","Iter: 011/050 | Train Loss: 0.00035660\n","Iter: 012/050 | Train Loss: 0.00035429\n","Iter: 013/050 | Train Loss: 0.00034952\n","Iter: 014/050 | Train Loss: 0.00029046\n","Iter: 015/050 | Train Loss: 0.00022591\n","Iter: 016/050 | Train Loss: 0.00020812\n","Iter: 017/050 | Train Loss: 0.00021034\n","Adjusting Layer 1, Kernel Nodes: 548, Adptive Nodes:252\n","Iter: 018/050 | Train Loss: 0.00017937\n","Iter: 019/050 | Train Loss: 0.00013515\n","Iter: 020/050 | Train Loss: 0.00011888\n","Iter: 021/050 | Train Loss: 0.00011759\n","Iter: 022/050 | Train Loss: 0.00010086\n","Iter: 023/050 | Train Loss: 0.00007801\n","Iter: 024/050 | Train Loss: 0.00006903\n","Iter: 025/050 | Train Loss: 0.00006735\n","Iter: 026/050 | Train Loss: 0.00005967\n","Iter: 027/050 | Train Loss: 0.00004886\n","Iter: 028/050 | Train Loss: 0.00004249\n","Iter: 029/050 | Train Loss: 0.00003973\n","Iter: 030/050 | Train Loss: 0.00003681\n","Iter: 031/050 | Train Loss: 0.00003255\n","Iter: 032/050 | Train Loss: 0.00002855\n","Iter: 033/050 | Train Loss: 0.00002595\n","Iter: 034/050 | Train Loss: 0.00002369\n","Iter: 035/050 | Train Loss: 0.00002145\n","Iter: 036/050 | Train Loss: 0.00001967\n","Iter: 037/050 | Train Loss: 0.00001730\n","Iter: 038/050 | Train Loss: 0.00001442\n","Iter: 039/050 | Train Loss: 0.00001319\n","Iter: 040/050 | Train Loss: 0.00001312\n","Iter: 041/050 | Train Loss: 0.00001153\n","Iter: 042/050 | Train Loss: 0.00000893\n","Iter: 043/050 | Train Loss: 0.00000797\n","Iter: 044/050 | Train Loss: 0.00000830\n","Adjusting Layer 1, Kernel Nodes: 499, Adptive Nodes:301\n","Iter: 045/050 | Train Loss: 0.00000739\n","Iter: 046/050 | Train Loss: 0.00000593\n","Iter: 047/050 | Train Loss: 0.00000559\n","Iter: 048/050 | Train Loss: 0.00000550\n","Iter: 049/050 | Train Loss: 0.00000489\n","\n","Iter: 049/050 | Test Loss: 0.00100396 | Test acc: 63.6000\n","scale:0.980000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00393034\n","Iter: 001/050 | Train Loss: 0.00244555\n","Iter: 002/050 | Train Loss: 0.00138881\n","Iter: 003/050 | Train Loss: 0.00094009\n","Iter: 004/050 | Train Loss: 0.00099047\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 005/050 | Train Loss: 0.00075075\n","Iter: 006/050 | Train Loss: 0.00058237\n","Iter: 007/050 | Train Loss: 0.00057406\n","Iter: 008/050 | Train Loss: 0.00059333\n","Adjusting Layer 1, Kernel Nodes: 740, Adptive Nodes:60\n","Iter: 009/050 | Train Loss: 0.00052165\n","Iter: 010/050 | Train Loss: 0.00040919\n","Iter: 011/050 | Train Loss: 0.00035386\n","Iter: 012/050 | Train Loss: 0.00036015\n","Adjusting Layer 1, Kernel Nodes: 485, Adptive Nodes:315\n","Iter: 013/050 | Train Loss: 0.00034143\n","Iter: 014/050 | Train Loss: 0.00026424\n","Iter: 015/050 | Train Loss: 0.00021505\n","Iter: 016/050 | Train Loss: 0.00021704\n","Adjusting Layer 1, Kernel Nodes: 499, Adptive Nodes:301\n","Iter: 017/050 | Train Loss: 0.00019585\n","Iter: 018/050 | Train Loss: 0.00014200\n","Iter: 019/050 | Train Loss: 0.00012310\n","Iter: 020/050 | Train Loss: 0.00012557\n","Adjusting Layer 1, Kernel Nodes: 784, Adptive Nodes:16\n","Iter: 021/050 | Train Loss: 0.00009107\n","Iter: 022/050 | Train Loss: 0.00007724\n","Iter: 023/050 | Train Loss: 0.00008032\n","Adjusting Layer 1, Kernel Nodes: 245, Adptive Nodes:555\n","Iter: 024/050 | Train Loss: 0.00005608\n","Iter: 025/050 | Train Loss: 0.00006028\n","Adjusting Layer 1, Kernel Nodes: 698, Adptive Nodes:102\n","Iter: 026/050 | Train Loss: 0.00004810\n","Iter: 027/050 | Train Loss: 0.00004652\n","Iter: 028/050 | Train Loss: 0.00004008\n","Iter: 029/050 | Train Loss: 0.00003602\n","Iter: 030/050 | Train Loss: 0.00003274\n","Iter: 031/050 | Train Loss: 0.00002754\n","Iter: 032/050 | Train Loss: 0.00002602\n","Iter: 033/050 | Train Loss: 0.00002128\n","Iter: 034/050 | Train Loss: 0.00002009\n","Iter: 035/050 | Train Loss: 0.00001697\n","Iter: 036/050 | Train Loss: 0.00001602\n","Iter: 037/050 | Train Loss: 0.00001469\n","Iter: 038/050 | Train Loss: 0.00001368\n","Iter: 039/050 | Train Loss: 0.00001262\n","Iter: 040/050 | Train Loss: 0.00001137\n","Iter: 041/050 | Train Loss: 0.00000986\n","Iter: 042/050 | Train Loss: 0.00000910\n","Iter: 043/050 | Train Loss: 0.00000760\n","Iter: 044/050 | Train Loss: 0.00000738\n","Iter: 045/050 | Train Loss: 0.00000612\n","Iter: 046/050 | Train Loss: 0.00000582\n","Iter: 047/050 | Train Loss: 0.00000498\n","Iter: 048/050 | Train Loss: 0.00000447\n","Iter: 049/050 | Train Loss: 0.00000412\n","\n","Iter: 049/050 | Test Loss: 0.00105229 | Test acc: 62.5500\n","scale:0.980000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00380622\n","Iter: 001/050 | Train Loss: 0.00221256\n","Iter: 002/050 | Train Loss: 0.00143255\n","Iter: 003/050 | Train Loss: 0.00091381\n","Iter: 004/050 | Train Loss: 0.00098973\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 005/050 | Train Loss: 0.00074841\n","Iter: 006/050 | Train Loss: 0.00057276\n","Iter: 007/050 | Train Loss: 0.00056763\n","Iter: 008/050 | Train Loss: 0.00059191\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 009/050 | Train Loss: 0.00052158\n","Iter: 010/050 | Train Loss: 0.00040689\n","Iter: 011/050 | Train Loss: 0.00035296\n","Iter: 012/050 | Train Loss: 0.00036429\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 013/050 | Train Loss: 0.00034663\n","Iter: 014/050 | Train Loss: 0.00026605\n","Iter: 015/050 | Train Loss: 0.00021931\n","Iter: 016/050 | Train Loss: 0.00022504\n","Adjusting Layer 1, Kernel Nodes: 444, Adptive Nodes:356\n","Iter: 017/050 | Train Loss: 0.00020341\n","Iter: 018/050 | Train Loss: 0.00014914\n","Iter: 019/050 | Train Loss: 0.00012981\n","Iter: 020/050 | Train Loss: 0.00013271\n","Adjusting Layer 1, Kernel Nodes: 432, Adptive Nodes:368\n","Iter: 021/050 | Train Loss: 0.00010184\n","Iter: 022/050 | Train Loss: 0.00007924\n","Iter: 023/050 | Train Loss: 0.00008283\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 024/050 | Train Loss: 0.00006390\n","Iter: 025/050 | Train Loss: 0.00005502\n","Iter: 026/050 | Train Loss: 0.00005595\n","Adjusting Layer 1, Kernel Nodes: 319, Adptive Nodes:481\n","Iter: 027/050 | Train Loss: 0.00004159\n","Iter: 028/050 | Train Loss: 0.00004457\n","Adjusting Layer 1, Kernel Nodes: 175, Adptive Nodes:625\n","Iter: 029/050 | Train Loss: 0.00003438\n","Iter: 030/050 | Train Loss: 0.00003372\n","Iter: 031/050 | Train Loss: 0.00002983\n","Iter: 032/050 | Train Loss: 0.00002483\n","Iter: 033/050 | Train Loss: 0.00002388\n","Iter: 034/050 | Train Loss: 0.00001941\n","Iter: 035/050 | Train Loss: 0.00001769\n","Iter: 036/050 | Train Loss: 0.00001572\n","Iter: 037/050 | Train Loss: 0.00001376\n","Iter: 038/050 | Train Loss: 0.00001263\n","Iter: 039/050 | Train Loss: 0.00001206\n","Iter: 040/050 | Train Loss: 0.00001074\n","Iter: 041/050 | Train Loss: 0.00001038\n","Iter: 042/050 | Train Loss: 0.00000957\n","Iter: 043/050 | Train Loss: 0.00000813\n","Iter: 044/050 | Train Loss: 0.00000789\n","Iter: 045/050 | Train Loss: 0.00000651\n","Iter: 046/050 | Train Loss: 0.00000587\n","Iter: 047/050 | Train Loss: 0.00000555\n","Iter: 048/050 | Train Loss: 0.00000446\n","Iter: 049/050 | Train Loss: 0.00000436\n","\n","Iter: 049/050 | Test Loss: 0.00103658 | Test acc: 63.0900\n","scale:0.980000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00370275\n","Iter: 001/050 | Train Loss: 0.00203934\n","Iter: 002/050 | Train Loss: 0.00146725\n","Iter: 003/050 | Train Loss: 0.00089291\n","Iter: 004/050 | Train Loss: 0.00098502\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 005/050 | Train Loss: 0.00074685\n","Iter: 006/050 | Train Loss: 0.00056584\n","Iter: 007/050 | Train Loss: 0.00056199\n","Iter: 008/050 | Train Loss: 0.00058824\n","Adjusting Layer 1, Kernel Nodes: 750, Adptive Nodes:50\n","Iter: 009/050 | Train Loss: 0.00052033\n","Iter: 010/050 | Train Loss: 0.00040601\n","Iter: 011/050 | Train Loss: 0.00035225\n","Iter: 012/050 | Train Loss: 0.00036504\n","Adjusting Layer 1, Kernel Nodes: 757, Adptive Nodes:43\n","Iter: 013/050 | Train Loss: 0.00035061\n","Iter: 014/050 | Train Loss: 0.00027137\n","Iter: 015/050 | Train Loss: 0.00022096\n","Iter: 016/050 | Train Loss: 0.00022524\n","Adjusting Layer 1, Kernel Nodes: 433, Adptive Nodes:367\n","Iter: 017/050 | Train Loss: 0.00021102\n","Iter: 018/050 | Train Loss: 0.00015891\n","Iter: 019/050 | Train Loss: 0.00013058\n","Iter: 020/050 | Train Loss: 0.00013326\n","Adjusting Layer 1, Kernel Nodes: 521, Adptive Nodes:279\n","Iter: 021/050 | Train Loss: 0.00011326\n","Iter: 022/050 | Train Loss: 0.00008350\n","Iter: 023/050 | Train Loss: 0.00008069\n","Iter: 024/050 | Train Loss: 0.00007641\n","Iter: 025/050 | Train Loss: 0.00005862\n","Iter: 026/050 | Train Loss: 0.00005147\n","Iter: 027/050 | Train Loss: 0.00005178\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 028/050 | Train Loss: 0.00004221\n","Iter: 029/050 | Train Loss: 0.00003694\n","Iter: 030/050 | Train Loss: 0.00003585\n","Iter: 031/050 | Train Loss: 0.00003052\n","Iter: 032/050 | Train Loss: 0.00002681\n","Iter: 033/050 | Train Loss: 0.00002507\n","Iter: 034/050 | Train Loss: 0.00002252\n","Iter: 035/050 | Train Loss: 0.00001924\n","Iter: 036/050 | Train Loss: 0.00001752\n","Iter: 037/050 | Train Loss: 0.00001656\n","Iter: 038/050 | Train Loss: 0.00001342\n","Iter: 039/050 | Train Loss: 0.00001170\n","Iter: 040/050 | Train Loss: 0.00001170\n","Adjusting Layer 1, Kernel Nodes: 253, Adptive Nodes:547\n","Iter: 041/050 | Train Loss: 0.00000905\n","Iter: 042/050 | Train Loss: 0.00000862\n","Iter: 043/050 | Train Loss: 0.00000867\n","Adjusting Layer 1, Kernel Nodes: 585, Adptive Nodes:215\n","Iter: 044/050 | Train Loss: 0.00000650\n","Iter: 045/050 | Train Loss: 0.00000727\n","Adjusting Layer 1, Kernel Nodes: 489, Adptive Nodes:311\n","Iter: 046/050 | Train Loss: 0.00000517\n","Iter: 047/050 | Train Loss: 0.00000556\n","Adjusting Layer 1, Kernel Nodes: 338, Adptive Nodes:462\n","Iter: 048/050 | Train Loss: 0.00000422\n","Iter: 049/050 | Train Loss: 0.00000422\n","\n","Iter: 049/050 | Test Loss: 0.00101116 | Test acc: 64.0300\n","scale:0.980000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00361835\n","Iter: 001/050 | Train Loss: 0.00190340\n","Iter: 002/050 | Train Loss: 0.00149531\n","Iter: 003/050 | Train Loss: 0.00087510\n","Iter: 004/050 | Train Loss: 0.00097727\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 005/050 | Train Loss: 0.00074713\n","Iter: 006/050 | Train Loss: 0.00056044\n","Iter: 007/050 | Train Loss: 0.00055492\n","Iter: 008/050 | Train Loss: 0.00058305\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 009/050 | Train Loss: 0.00051928\n","Iter: 010/050 | Train Loss: 0.00040695\n","Iter: 011/050 | Train Loss: 0.00035221\n","Iter: 012/050 | Train Loss: 0.00036563\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 013/050 | Train Loss: 0.00035441\n","Iter: 014/050 | Train Loss: 0.00027628\n","Iter: 015/050 | Train Loss: 0.00022229\n","Iter: 016/050 | Train Loss: 0.00022578\n","Adjusting Layer 1, Kernel Nodes: 584, Adptive Nodes:216\n","Iter: 017/050 | Train Loss: 0.00021857\n","Iter: 018/050 | Train Loss: 0.00016783\n","Iter: 019/050 | Train Loss: 0.00013218\n","Iter: 020/050 | Train Loss: 0.00013513\n","Adjusting Layer 1, Kernel Nodes: 753, Adptive Nodes:47\n","Iter: 021/050 | Train Loss: 0.00012342\n","Iter: 022/050 | Train Loss: 0.00008886\n","Iter: 023/050 | Train Loss: 0.00008117\n","Iter: 024/050 | Train Loss: 0.00008211\n","Adjusting Layer 1, Kernel Nodes: 784, Adptive Nodes:16\n","Iter: 025/050 | Train Loss: 0.00006226\n","Iter: 026/050 | Train Loss: 0.00005369\n","Iter: 027/050 | Train Loss: 0.00005449\n","Adjusting Layer 1, Kernel Nodes: 591, Adptive Nodes:209\n","Iter: 028/050 | Train Loss: 0.00004185\n","Iter: 029/050 | Train Loss: 0.00003868\n","Iter: 030/050 | Train Loss: 0.00003648\n","Iter: 031/050 | Train Loss: 0.00002953\n","Iter: 032/050 | Train Loss: 0.00002833\n","Iter: 033/050 | Train Loss: 0.00002541\n","Iter: 034/050 | Train Loss: 0.00002166\n","Iter: 035/050 | Train Loss: 0.00002001\n","Iter: 036/050 | Train Loss: 0.00001790\n","Iter: 037/050 | Train Loss: 0.00001556\n","Iter: 038/050 | Train Loss: 0.00001371\n","Iter: 039/050 | Train Loss: 0.00001260\n","Iter: 040/050 | Train Loss: 0.00001089\n","Iter: 041/050 | Train Loss: 0.00000961\n","Iter: 042/050 | Train Loss: 0.00000942\n","Iter: 043/050 | Train Loss: 0.00000827\n","Iter: 044/050 | Train Loss: 0.00000736\n","Iter: 045/050 | Train Loss: 0.00000728\n","Iter: 046/050 | Train Loss: 0.00000619\n","Iter: 047/050 | Train Loss: 0.00000530\n","Iter: 048/050 | Train Loss: 0.00000523\n","Iter: 049/050 | Train Loss: 0.00000440\n","\n","Iter: 049/050 | Test Loss: 0.00100029 | Test acc: 64.2600\n","scale:0.980000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00355000\n","Iter: 001/050 | Train Loss: 0.00180407\n","Iter: 002/050 | Train Loss: 0.00151894\n","Iter: 003/050 | Train Loss: 0.00086096\n","Iter: 004/050 | Train Loss: 0.00096848\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 005/050 | Train Loss: 0.00075017\n","Iter: 006/050 | Train Loss: 0.00055862\n","Iter: 007/050 | Train Loss: 0.00054906\n","Iter: 008/050 | Train Loss: 0.00058071\n","Adjusting Layer 1, Kernel Nodes: 707, Adptive Nodes:93\n","Iter: 009/050 | Train Loss: 0.00052314\n","Iter: 010/050 | Train Loss: 0.00041049\n","Iter: 011/050 | Train Loss: 0.00035247\n","Iter: 012/050 | Train Loss: 0.00036464\n","Adjusting Layer 1, Kernel Nodes: 744, Adptive Nodes:56\n","Iter: 013/050 | Train Loss: 0.00035869\n","Iter: 014/050 | Train Loss: 0.00028477\n","Iter: 015/050 | Train Loss: 0.00022679\n","Iter: 016/050 | Train Loss: 0.00022551\n","Iter: 017/050 | Train Loss: 0.00022500\n","Iter: 018/050 | Train Loss: 0.00018668\n","Iter: 019/050 | Train Loss: 0.00014372\n","Iter: 020/050 | Train Loss: 0.00013105\n","Iter: 021/050 | Train Loss: 0.00013168\n","Adjusting Layer 1, Kernel Nodes: 698, Adptive Nodes:102\n","Iter: 022/050 | Train Loss: 0.00011496\n","Iter: 023/050 | Train Loss: 0.00009018\n","Iter: 024/050 | Train Loss: 0.00007990\n","Iter: 025/050 | Train Loss: 0.00007758\n","Iter: 026/050 | Train Loss: 0.00006954\n","Iter: 027/050 | Train Loss: 0.00005775\n","Iter: 028/050 | Train Loss: 0.00004939\n","Iter: 029/050 | Train Loss: 0.00004474\n","Iter: 030/050 | Train Loss: 0.00004207\n","Iter: 031/050 | Train Loss: 0.00003916\n","Iter: 032/050 | Train Loss: 0.00003220\n","Iter: 033/050 | Train Loss: 0.00002584\n","Iter: 034/050 | Train Loss: 0.00002675\n","Adjusting Layer 1, Kernel Nodes: 512, Adptive Nodes:288\n","Iter: 035/050 | Train Loss: 0.00002703\n","Adjusting Layer 1, Kernel Nodes: 517, Adptive Nodes:283\n","Iter: 036/050 | Train Loss: 0.00001924\n","Iter: 037/050 | Train Loss: 0.00001690\n","Iter: 038/050 | Train Loss: 0.00001943\n","Adjusting Layer 1, Kernel Nodes: 547, Adptive Nodes:253\n","Iter: 039/050 | Train Loss: 0.00001527\n","Iter: 040/050 | Train Loss: 0.00001165\n","Iter: 041/050 | Train Loss: 0.00001311\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 042/050 | Train Loss: 0.00001100\n","Iter: 043/050 | Train Loss: 0.00000831\n","Iter: 044/050 | Train Loss: 0.00000876\n","Adjusting Layer 1, Kernel Nodes: 786, Adptive Nodes:14\n","Iter: 045/050 | Train Loss: 0.00000739\n","Iter: 046/050 | Train Loss: 0.00000596\n","Iter: 047/050 | Train Loss: 0.00000618\n","Adjusting Layer 1, Kernel Nodes: 154, Adptive Nodes:646\n","Iter: 048/050 | Train Loss: 0.00000504\n","Iter: 049/050 | Train Loss: 0.00000446\n","\n","Iter: 049/050 | Test Loss: 0.00096124 | Test acc: 65.1800\n","scale:0.980000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00349512\n","Iter: 001/050 | Train Loss: 0.00172813\n","Iter: 002/050 | Train Loss: 0.00153651\n","Iter: 003/050 | Train Loss: 0.00085002\n","Iter: 004/050 | Train Loss: 0.00096172\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00075474\n","Iter: 006/050 | Train Loss: 0.00055791\n","Iter: 007/050 | Train Loss: 0.00054405\n","Iter: 008/050 | Train Loss: 0.00057767\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 009/050 | Train Loss: 0.00052747\n","Iter: 010/050 | Train Loss: 0.00041745\n","Iter: 011/050 | Train Loss: 0.00035270\n","Iter: 012/050 | Train Loss: 0.00036024\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 013/050 | Train Loss: 0.00036120\n","Adjusting Layer 1, Kernel Nodes: 701, Adptive Nodes:99\n","Iter: 014/050 | Train Loss: 0.00028815\n","Iter: 015/050 | Train Loss: 0.00022633\n","Iter: 016/050 | Train Loss: 0.00022641\n","Adjusting Layer 1, Kernel Nodes: 662, Adptive Nodes:138\n","Iter: 017/050 | Train Loss: 0.00022386\n","Iter: 018/050 | Train Loss: 0.00017648\n","Iter: 019/050 | Train Loss: 0.00013758\n","Iter: 020/050 | Train Loss: 0.00013820\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 021/050 | Train Loss: 0.00013161\n","Iter: 022/050 | Train Loss: 0.00009897\n","Iter: 023/050 | Train Loss: 0.00008445\n","Iter: 024/050 | Train Loss: 0.00008632\n","Adjusting Layer 1, Kernel Nodes: 500, Adptive Nodes:300\n","Iter: 025/050 | Train Loss: 0.00007356\n","Iter: 026/050 | Train Loss: 0.00005757\n","Iter: 027/050 | Train Loss: 0.00005509\n","Iter: 028/050 | Train Loss: 0.00005190\n","Iter: 029/050 | Train Loss: 0.00004235\n","Iter: 030/050 | Train Loss: 0.00003679\n","Iter: 031/050 | Train Loss: 0.00003470\n","Iter: 032/050 | Train Loss: 0.00003165\n","Iter: 033/050 | Train Loss: 0.00002757\n","Iter: 034/050 | Train Loss: 0.00002319\n","Iter: 035/050 | Train Loss: 0.00002190\n","Iter: 036/050 | Train Loss: 0.00002165\n","Iter: 037/050 | Train Loss: 0.00001726\n","Iter: 038/050 | Train Loss: 0.00001420\n","Iter: 039/050 | Train Loss: 0.00001550\n","Adjusting Layer 1, Kernel Nodes: 777, Adptive Nodes:23\n","Iter: 040/050 | Train Loss: 0.00001345\n","Iter: 041/050 | Train Loss: 0.00000973\n","Iter: 042/050 | Train Loss: 0.00001027\n","Adjusting Layer 1, Kernel Nodes: 436, Adptive Nodes:364\n","Iter: 043/050 | Train Loss: 0.00000975\n","Iter: 044/050 | Train Loss: 0.00000702\n","Iter: 045/050 | Train Loss: 0.00000718\n","Adjusting Layer 1, Kernel Nodes: 509, Adptive Nodes:291\n","Iter: 046/050 | Train Loss: 0.00000690\n","Iter: 047/050 | Train Loss: 0.00000535\n","Iter: 048/050 | Train Loss: 0.00000523\n","Iter: 049/050 | Train Loss: 0.00000484\n","\n","Iter: 049/050 | Test Loss: 0.00096822 | Test acc: 64.9200\n","scale:0.980000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00345419\n","Iter: 001/050 | Train Loss: 0.00167366\n","Iter: 002/050 | Train Loss: 0.00154863\n","Iter: 003/050 | Train Loss: 0.00084048\n","Iter: 004/050 | Train Loss: 0.00095380\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00075855\n","Iter: 006/050 | Train Loss: 0.00055703\n","Iter: 007/050 | Train Loss: 0.00053884\n","Iter: 008/050 | Train Loss: 0.00057277\n","Adjusting Layer 1, Kernel Nodes: 754, Adptive Nodes:46\n","Iter: 009/050 | Train Loss: 0.00052699\n","Iter: 010/050 | Train Loss: 0.00041964\n","Iter: 011/050 | Train Loss: 0.00035274\n","Iter: 012/050 | Train Loss: 0.00035741\n","Adjusting Layer 1, Kernel Nodes: 747, Adptive Nodes:53\n","Iter: 013/050 | Train Loss: 0.00036020\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 014/050 | Train Loss: 0.00029387\n","Iter: 015/050 | Train Loss: 0.00022975\n","Iter: 016/050 | Train Loss: 0.00022403\n","Iter: 017/050 | Train Loss: 0.00022674\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 018/050 | Train Loss: 0.00018872\n","Iter: 019/050 | Train Loss: 0.00014429\n","Iter: 020/050 | Train Loss: 0.00013478\n","Iter: 021/050 | Train Loss: 0.00013624\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 022/050 | Train Loss: 0.00011470\n","Iter: 023/050 | Train Loss: 0.00008917\n","Iter: 024/050 | Train Loss: 0.00008331\n","Iter: 025/050 | Train Loss: 0.00008220\n","Iter: 026/050 | Train Loss: 0.00007094\n","Iter: 027/050 | Train Loss: 0.00005730\n","Iter: 028/050 | Train Loss: 0.00005064\n","Iter: 029/050 | Train Loss: 0.00004854\n","Iter: 030/050 | Train Loss: 0.00004531\n","Iter: 031/050 | Train Loss: 0.00003782\n","Iter: 032/050 | Train Loss: 0.00002996\n","Iter: 033/050 | Train Loss: 0.00002926\n","Iter: 034/050 | Train Loss: 0.00003074\n","Adjusting Layer 1, Kernel Nodes: 705, Adptive Nodes:95\n","Iter: 035/050 | Train Loss: 0.00002475\n","Iter: 036/050 | Train Loss: 0.00001850\n","Iter: 037/050 | Train Loss: 0.00001959\n","Adjusting Layer 1, Kernel Nodes: 646, Adptive Nodes:154\n","Iter: 038/050 | Train Loss: 0.00002019\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 039/050 | Train Loss: 0.00001492\n","Iter: 040/050 | Train Loss: 0.00001272\n","Iter: 041/050 | Train Loss: 0.00001390\n","Adjusting Layer 1, Kernel Nodes: 383, Adptive Nodes:417\n","Iter: 042/050 | Train Loss: 0.00001184\n","Iter: 043/050 | Train Loss: 0.00000902\n","Iter: 044/050 | Train Loss: 0.00000899\n","Iter: 045/050 | Train Loss: 0.00000880\n","Iter: 046/050 | Train Loss: 0.00000699\n","Iter: 047/050 | Train Loss: 0.00000601\n","Iter: 048/050 | Train Loss: 0.00000631\n","Adjusting Layer 1, Kernel Nodes: 434, Adptive Nodes:366\n","Iter: 049/050 | Train Loss: 0.00000562\n","\n","Iter: 049/050 | Test Loss: 0.00094621 | Test acc: 65.5700\n","scale:0.980000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 000/050 | Train Loss: 0.00342604\n","Iter: 001/050 | Train Loss: 0.00163420\n","Iter: 002/050 | Train Loss: 0.00155775\n","Iter: 003/050 | Train Loss: 0.00083261\n","Iter: 004/050 | Train Loss: 0.00094431\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 005/050 | Train Loss: 0.00076350\n","Iter: 006/050 | Train Loss: 0.00055775\n","Iter: 007/050 | Train Loss: 0.00053488\n","Iter: 008/050 | Train Loss: 0.00057026\n","Adjusting Layer 1, Kernel Nodes: 730, Adptive Nodes:70\n","Iter: 009/050 | Train Loss: 0.00052812\n","Iter: 010/050 | Train Loss: 0.00042117\n","Iter: 011/050 | Train Loss: 0.00035267\n","Iter: 012/050 | Train Loss: 0.00035464\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 013/050 | Train Loss: 0.00035793\n","Adjusting Layer 1, Kernel Nodes: 649, Adptive Nodes:151\n","Iter: 014/050 | Train Loss: 0.00029652\n","Iter: 015/050 | Train Loss: 0.00023131\n","Iter: 016/050 | Train Loss: 0.00022012\n","Iter: 017/050 | Train Loss: 0.00022539\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 018/050 | Train Loss: 0.00019613\n","Iter: 019/050 | Train Loss: 0.00015092\n","Iter: 020/050 | Train Loss: 0.00013227\n","Iter: 021/050 | Train Loss: 0.00013556\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 022/050 | Train Loss: 0.00012316\n","Iter: 023/050 | Train Loss: 0.00009538\n","Iter: 024/050 | Train Loss: 0.00008252\n","Iter: 025/050 | Train Loss: 0.00008121\n","Iter: 026/050 | Train Loss: 0.00007551\n","Iter: 027/050 | Train Loss: 0.00006462\n","Iter: 028/050 | Train Loss: 0.00005242\n","Iter: 029/050 | Train Loss: 0.00004500\n","Iter: 030/050 | Train Loss: 0.00004591\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 031/050 | Train Loss: 0.00004375\n","Iter: 032/050 | Train Loss: 0.00003304\n","Iter: 033/050 | Train Loss: 0.00002738\n","Iter: 034/050 | Train Loss: 0.00002957\n","Adjusting Layer 1, Kernel Nodes: 728, Adptive Nodes:72\n","Iter: 035/050 | Train Loss: 0.00002856\n","Iter: 036/050 | Train Loss: 0.00002222\n","Iter: 037/050 | Train Loss: 0.00001804\n","Iter: 038/050 | Train Loss: 0.00001828\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 039/050 | Train Loss: 0.00001842\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 040/050 | Train Loss: 0.00001506\n","Iter: 041/050 | Train Loss: 0.00001190\n","Iter: 042/050 | Train Loss: 0.00001202\n","Adjusting Layer 1, Kernel Nodes: 700, Adptive Nodes:100\n","Iter: 043/050 | Train Loss: 0.00001214\n","Adjusting Layer 1, Kernel Nodes: 794, Adptive Nodes:6\n","Iter: 044/050 | Train Loss: 0.00000934\n","Iter: 045/050 | Train Loss: 0.00000769\n","Iter: 046/050 | Train Loss: 0.00000839\n","Adjusting Layer 1, Kernel Nodes: 482, Adptive Nodes:318\n","Iter: 047/050 | Train Loss: 0.00000742\n","Iter: 048/050 | Train Loss: 0.00000552\n","Iter: 049/050 | Train Loss: 0.00000566\n","\n","Iter: 049/050 | Test Loss: 0.00093946 | Test acc: 65.6400\n","scale:1.000000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00424803\n","Iter: 001/050 | Train Loss: 0.00313578\n","Iter: 002/050 | Train Loss: 0.00129946\n","Iter: 003/050 | Train Loss: 0.00100047\n","Iter: 004/050 | Train Loss: 0.00098996\n","Iter: 005/050 | Train Loss: 0.00079533\n","Iter: 006/050 | Train Loss: 0.00067421\n","Iter: 007/050 | Train Loss: 0.00063137\n","Iter: 008/050 | Train Loss: 0.00061436\n","Iter: 009/050 | Train Loss: 0.00058619\n","Iter: 010/050 | Train Loss: 0.00052029\n","Iter: 011/050 | Train Loss: 0.00043405\n","Iter: 012/050 | Train Loss: 0.00037552\n","Iter: 013/050 | Train Loss: 0.00036146\n","Iter: 014/050 | Train Loss: 0.00034870\n","Iter: 015/050 | Train Loss: 0.00030711\n","Iter: 016/050 | Train Loss: 0.00026272\n","Iter: 017/050 | Train Loss: 0.00023791\n","Iter: 018/050 | Train Loss: 0.00022435\n","Iter: 019/050 | Train Loss: 0.00020663\n","Iter: 020/050 | Train Loss: 0.00017577\n","Iter: 021/050 | Train Loss: 0.00014436\n","Iter: 022/050 | Train Loss: 0.00013388\n","Iter: 023/050 | Train Loss: 0.00013464\n","Adjusting Layer 1, Kernel Nodes: 680, Adptive Nodes:120\n","Iter: 024/050 | Train Loss: 0.00011741\n","Iter: 025/050 | Train Loss: 0.00009416\n","Iter: 026/050 | Train Loss: 0.00008587\n","Iter: 027/050 | Train Loss: 0.00008232\n","Iter: 028/050 | Train Loss: 0.00007078\n","Iter: 029/050 | Train Loss: 0.00005813\n","Iter: 030/050 | Train Loss: 0.00005223\n","Iter: 031/050 | Train Loss: 0.00004820\n","Iter: 032/050 | Train Loss: 0.00004330\n","Iter: 033/050 | Train Loss: 0.00003965\n","Iter: 034/050 | Train Loss: 0.00003435\n","Iter: 035/050 | Train Loss: 0.00002838\n","Iter: 036/050 | Train Loss: 0.00002719\n","Iter: 037/050 | Train Loss: 0.00002691\n","Iter: 038/050 | Train Loss: 0.00002212\n","Iter: 039/050 | Train Loss: 0.00001776\n","Iter: 040/050 | Train Loss: 0.00001773\n","Iter: 041/050 | Train Loss: 0.00001747\n","Iter: 042/050 | Train Loss: 0.00001489\n","Iter: 043/050 | Train Loss: 0.00001272\n","Iter: 044/050 | Train Loss: 0.00001152\n","Iter: 045/050 | Train Loss: 0.00001050\n","Iter: 046/050 | Train Loss: 0.00000996\n","Iter: 047/050 | Train Loss: 0.00000929\n","Iter: 048/050 | Train Loss: 0.00000784\n","Iter: 049/050 | Train Loss: 0.00000680\n","\n","Iter: 049/050 | Test Loss: 0.00093766 | Test acc: 64.9300\n","scale:1.000000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00407516\n","Iter: 001/050 | Train Loss: 0.00273870\n","Iter: 002/050 | Train Loss: 0.00134669\n","Iter: 003/050 | Train Loss: 0.00096747\n","Iter: 004/050 | Train Loss: 0.00099108\n","Adjusting Layer 1, Kernel Nodes: 789, Adptive Nodes:11\n","Iter: 005/050 | Train Loss: 0.00075736\n","Iter: 006/050 | Train Loss: 0.00059766\n","Iter: 007/050 | Train Loss: 0.00058623\n","Iter: 008/050 | Train Loss: 0.00060162\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 009/050 | Train Loss: 0.00053574\n","Iter: 010/050 | Train Loss: 0.00042352\n","Iter: 011/050 | Train Loss: 0.00035750\n","Iter: 012/050 | Train Loss: 0.00035736\n","Iter: 013/050 | Train Loss: 0.00035032\n","Iter: 014/050 | Train Loss: 0.00028901\n","Iter: 015/050 | Train Loss: 0.00022636\n","Iter: 016/050 | Train Loss: 0.00021069\n","Iter: 017/050 | Train Loss: 0.00021245\n","Adjusting Layer 1, Kernel Nodes: 554, Adptive Nodes:246\n","Iter: 018/050 | Train Loss: 0.00018029\n","Iter: 019/050 | Train Loss: 0.00013525\n","Iter: 020/050 | Train Loss: 0.00011893\n","Iter: 021/050 | Train Loss: 0.00011769\n","Iter: 022/050 | Train Loss: 0.00010006\n","Iter: 023/050 | Train Loss: 0.00007654\n","Iter: 024/050 | Train Loss: 0.00006869\n","Iter: 025/050 | Train Loss: 0.00006806\n","Iter: 026/050 | Train Loss: 0.00005984\n","Iter: 027/050 | Train Loss: 0.00004860\n","Iter: 028/050 | Train Loss: 0.00004306\n","Iter: 029/050 | Train Loss: 0.00004093\n","Iter: 030/050 | Train Loss: 0.00003705\n","Iter: 031/050 | Train Loss: 0.00003187\n","Iter: 032/050 | Train Loss: 0.00002837\n","Iter: 033/050 | Train Loss: 0.00002614\n","Iter: 034/050 | Train Loss: 0.00002322\n","Iter: 035/050 | Train Loss: 0.00002059\n","Iter: 036/050 | Train Loss: 0.00001925\n","Iter: 037/050 | Train Loss: 0.00001734\n","Iter: 038/050 | Train Loss: 0.00001445\n","Iter: 039/050 | Train Loss: 0.00001295\n","Iter: 040/050 | Train Loss: 0.00001289\n","Iter: 041/050 | Train Loss: 0.00001159\n","Iter: 042/050 | Train Loss: 0.00000896\n","Iter: 043/050 | Train Loss: 0.00000768\n","Iter: 044/050 | Train Loss: 0.00000795\n","Adjusting Layer 1, Kernel Nodes: 485, Adptive Nodes:315\n","Iter: 045/050 | Train Loss: 0.00000723\n","Iter: 046/050 | Train Loss: 0.00000586\n","Iter: 047/050 | Train Loss: 0.00000541\n","Iter: 048/050 | Train Loss: 0.00000527\n","Iter: 049/050 | Train Loss: 0.00000477\n","\n","Iter: 049/050 | Test Loss: 0.00100428 | Test acc: 63.6300\n","scale:1.000000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00392757\n","Iter: 001/050 | Train Loss: 0.00244109\n","Iter: 002/050 | Train Loss: 0.00138912\n","Iter: 003/050 | Train Loss: 0.00093965\n","Iter: 004/050 | Train Loss: 0.00098968\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 005/050 | Train Loss: 0.00075056\n","Iter: 006/050 | Train Loss: 0.00058214\n","Iter: 007/050 | Train Loss: 0.00057413\n","Iter: 008/050 | Train Loss: 0.00059349\n","Adjusting Layer 1, Kernel Nodes: 740, Adptive Nodes:60\n","Iter: 009/050 | Train Loss: 0.00052155\n","Iter: 010/050 | Train Loss: 0.00040881\n","Iter: 011/050 | Train Loss: 0.00035370\n","Iter: 012/050 | Train Loss: 0.00036023\n","Adjusting Layer 1, Kernel Nodes: 489, Adptive Nodes:311\n","Iter: 013/050 | Train Loss: 0.00034141\n","Iter: 014/050 | Train Loss: 0.00026435\n","Iter: 015/050 | Train Loss: 0.00021489\n","Iter: 016/050 | Train Loss: 0.00021697\n","Adjusting Layer 1, Kernel Nodes: 497, Adptive Nodes:303\n","Iter: 017/050 | Train Loss: 0.00019618\n","Iter: 018/050 | Train Loss: 0.00014209\n","Iter: 019/050 | Train Loss: 0.00012293\n","Iter: 020/050 | Train Loss: 0.00012578\n","Adjusting Layer 1, Kernel Nodes: 724, Adptive Nodes:76\n","Iter: 021/050 | Train Loss: 0.00009126\n","Iter: 022/050 | Train Loss: 0.00007669\n","Iter: 023/050 | Train Loss: 0.00008053\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 024/050 | Train Loss: 0.00005492\n","Iter: 025/050 | Train Loss: 0.00006189\n","Adjusting Layer 1, Kernel Nodes: 257, Adptive Nodes:543\n","Iter: 026/050 | Train Loss: 0.00004455\n","Iter: 027/050 | Train Loss: 0.00004912\n","Adjusting Layer 1, Kernel Nodes: 14, Adptive Nodes:786\n","Iter: 028/050 | Train Loss: 0.00003804\n","Iter: 029/050 | Train Loss: 0.00003664\n","Iter: 030/050 | Train Loss: 0.00003412\n","Iter: 031/050 | Train Loss: 0.00002610\n","Iter: 032/050 | Train Loss: 0.00002741\n","Adjusting Layer 1, Kernel Nodes: 517, Adptive Nodes:283\n","Iter: 033/050 | Train Loss: 0.00002023\n","Iter: 034/050 | Train Loss: 0.00002083\n","Adjusting Layer 1, Kernel Nodes: 288, Adptive Nodes:512\n","Iter: 035/050 | Train Loss: 0.00001619\n","Iter: 036/050 | Train Loss: 0.00001574\n","Iter: 037/050 | Train Loss: 0.00001440\n","Iter: 038/050 | Train Loss: 0.00001255\n","Iter: 039/050 | Train Loss: 0.00001286\n","Adjusting Layer 1, Kernel Nodes: 587, Adptive Nodes:213\n","Iter: 040/050 | Train Loss: 0.00001080\n","Iter: 041/050 | Train Loss: 0.00001060\n","Iter: 042/050 | Train Loss: 0.00000838\n","Iter: 043/050 | Train Loss: 0.00000830\n","Iter: 044/050 | Train Loss: 0.00000651\n","Iter: 045/050 | Train Loss: 0.00000651\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 046/050 | Train Loss: 0.00000533\n","Iter: 047/050 | Train Loss: 0.00000513\n","Iter: 048/050 | Train Loss: 0.00000421\n","Iter: 049/050 | Train Loss: 0.00000417\n","\n","Iter: 049/050 | Test Loss: 0.00105288 | Test acc: 62.3900\n","scale:1.000000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00380271\n","Iter: 001/050 | Train Loss: 0.00220686\n","Iter: 002/050 | Train Loss: 0.00143318\n","Iter: 003/050 | Train Loss: 0.00091301\n","Iter: 004/050 | Train Loss: 0.00098914\n","Adjusting Layer 1, Kernel Nodes: 655, Adptive Nodes:145\n","Iter: 005/050 | Train Loss: 0.00074816\n","Iter: 006/050 | Train Loss: 0.00057224\n","Iter: 007/050 | Train Loss: 0.00056748\n","Iter: 008/050 | Train Loss: 0.00059201\n","Adjusting Layer 1, Kernel Nodes: 746, Adptive Nodes:54\n","Iter: 009/050 | Train Loss: 0.00052122\n","Iter: 010/050 | Train Loss: 0.00040655\n","Iter: 011/050 | Train Loss: 0.00035289\n","Iter: 012/050 | Train Loss: 0.00036444\n","Adjusting Layer 1, Kernel Nodes: 755, Adptive Nodes:45\n","Iter: 013/050 | Train Loss: 0.00034630\n","Iter: 014/050 | Train Loss: 0.00026567\n","Iter: 015/050 | Train Loss: 0.00021925\n","Iter: 016/050 | Train Loss: 0.00022522\n","Adjusting Layer 1, Kernel Nodes: 461, Adptive Nodes:339\n","Iter: 017/050 | Train Loss: 0.00020333\n","Iter: 018/050 | Train Loss: 0.00014884\n","Iter: 019/050 | Train Loss: 0.00013014\n","Iter: 020/050 | Train Loss: 0.00013280\n","Adjusting Layer 1, Kernel Nodes: 364, Adptive Nodes:436\n","Iter: 021/050 | Train Loss: 0.00010133\n","Iter: 022/050 | Train Loss: 0.00007932\n","Iter: 023/050 | Train Loss: 0.00008279\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 024/050 | Train Loss: 0.00006370\n","Iter: 025/050 | Train Loss: 0.00005507\n","Iter: 026/050 | Train Loss: 0.00005578\n","Adjusting Layer 1, Kernel Nodes: 197, Adptive Nodes:603\n","Iter: 027/050 | Train Loss: 0.00004136\n","Iter: 028/050 | Train Loss: 0.00004401\n","Adjusting Layer 1, Kernel Nodes: 476, Adptive Nodes:324\n","Iter: 029/050 | Train Loss: 0.00003414\n","Iter: 030/050 | Train Loss: 0.00003384\n","Iter: 031/050 | Train Loss: 0.00002899\n","Iter: 032/050 | Train Loss: 0.00002533\n","Iter: 033/050 | Train Loss: 0.00002353\n","Iter: 034/050 | Train Loss: 0.00001924\n","Iter: 035/050 | Train Loss: 0.00001794\n","Iter: 036/050 | Train Loss: 0.00001520\n","Iter: 037/050 | Train Loss: 0.00001383\n","Iter: 038/050 | Train Loss: 0.00001266\n","Iter: 039/050 | Train Loss: 0.00001181\n","Iter: 040/050 | Train Loss: 0.00001083\n","Iter: 041/050 | Train Loss: 0.00001034\n","Iter: 042/050 | Train Loss: 0.00000904\n","Iter: 043/050 | Train Loss: 0.00000833\n","Iter: 044/050 | Train Loss: 0.00000742\n","Iter: 045/050 | Train Loss: 0.00000641\n","Iter: 046/050 | Train Loss: 0.00000603\n","Iter: 047/050 | Train Loss: 0.00000515\n","Iter: 048/050 | Train Loss: 0.00000471\n","Iter: 049/050 | Train Loss: 0.00000424\n","\n","Iter: 049/050 | Test Loss: 0.00104286 | Test acc: 63.0000\n","scale:1.000000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00369930\n","Iter: 001/050 | Train Loss: 0.00203422\n","Iter: 002/050 | Train Loss: 0.00146807\n","Iter: 003/050 | Train Loss: 0.00089220\n","Iter: 004/050 | Train Loss: 0.00098460\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 005/050 | Train Loss: 0.00074647\n","Iter: 006/050 | Train Loss: 0.00056527\n","Iter: 007/050 | Train Loss: 0.00056189\n","Iter: 008/050 | Train Loss: 0.00058840\n","Adjusting Layer 1, Kernel Nodes: 752, Adptive Nodes:48\n","Iter: 009/050 | Train Loss: 0.00051996\n","Iter: 010/050 | Train Loss: 0.00040564\n","Iter: 011/050 | Train Loss: 0.00035225\n","Iter: 012/050 | Train Loss: 0.00036512\n","Adjusting Layer 1, Kernel Nodes: 701, Adptive Nodes:99\n","Iter: 013/050 | Train Loss: 0.00035062\n","Iter: 014/050 | Train Loss: 0.00027138\n","Iter: 015/050 | Train Loss: 0.00021988\n","Iter: 016/050 | Train Loss: 0.00022395\n","Adjusting Layer 1, Kernel Nodes: 435, Adptive Nodes:365\n","Iter: 017/050 | Train Loss: 0.00021151\n","Iter: 018/050 | Train Loss: 0.00015861\n","Iter: 019/050 | Train Loss: 0.00012932\n","Iter: 020/050 | Train Loss: 0.00013371\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 021/050 | Train Loss: 0.00011360\n","Iter: 022/050 | Train Loss: 0.00008233\n","Iter: 023/050 | Train Loss: 0.00008162\n","Iter: 024/050 | Train Loss: 0.00007632\n","Iter: 025/050 | Train Loss: 0.00005734\n","Iter: 026/050 | Train Loss: 0.00005250\n","Iter: 027/050 | Train Loss: 0.00005184\n","Iter: 028/050 | Train Loss: 0.00004321\n","Iter: 029/050 | Train Loss: 0.00003714\n","Iter: 030/050 | Train Loss: 0.00003488\n","Iter: 031/050 | Train Loss: 0.00003236\n","Iter: 032/050 | Train Loss: 0.00002849\n","Iter: 033/050 | Train Loss: 0.00002386\n","Iter: 034/050 | Train Loss: 0.00002236\n","Iter: 035/050 | Train Loss: 0.00002174\n","Iter: 036/050 | Train Loss: 0.00001779\n","Iter: 037/050 | Train Loss: 0.00001481\n","Iter: 038/050 | Train Loss: 0.00001517\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 039/050 | Train Loss: 0.00001314\n","Iter: 040/050 | Train Loss: 0.00000988\n","Iter: 041/050 | Train Loss: 0.00001014\n","Adjusting Layer 1, Kernel Nodes: 385, Adptive Nodes:415\n","Iter: 042/050 | Train Loss: 0.00000887\n","Iter: 043/050 | Train Loss: 0.00000673\n","Iter: 044/050 | Train Loss: 0.00000758\n","Adjusting Layer 1, Kernel Nodes: 636, Adptive Nodes:164\n","Iter: 045/050 | Train Loss: 0.00000581\n","Iter: 046/050 | Train Loss: 0.00000571\n","Iter: 047/050 | Train Loss: 0.00000526\n","Iter: 048/050 | Train Loss: 0.00000402\n","Iter: 049/050 | Train Loss: 0.00000440\n","\n","Iter: 049/050 | Test Loss: 0.00100667 | Test acc: 63.9200\n","scale:1.000000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00361477\n","Iter: 001/050 | Train Loss: 0.00189879\n","Iter: 002/050 | Train Loss: 0.00149584\n","Iter: 003/050 | Train Loss: 0.00087459\n","Iter: 004/050 | Train Loss: 0.00097681\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 005/050 | Train Loss: 0.00074679\n","Iter: 006/050 | Train Loss: 0.00056019\n","Iter: 007/050 | Train Loss: 0.00055510\n","Iter: 008/050 | Train Loss: 0.00058324\n","Adjusting Layer 1, Kernel Nodes: 732, Adptive Nodes:68\n","Iter: 009/050 | Train Loss: 0.00051855\n","Iter: 010/050 | Train Loss: 0.00040530\n","Iter: 011/050 | Train Loss: 0.00035203\n","Iter: 012/050 | Train Loss: 0.00036571\n","Adjusting Layer 1, Kernel Nodes: 792, Adptive Nodes:8\n","Iter: 013/050 | Train Loss: 0.00035301\n","Iter: 014/050 | Train Loss: 0.00027559\n","Iter: 015/050 | Train Loss: 0.00022326\n","Iter: 016/050 | Train Loss: 0.00022575\n","Adjusting Layer 1, Kernel Nodes: 508, Adptive Nodes:292\n","Iter: 017/050 | Train Loss: 0.00021651\n","Iter: 018/050 | Train Loss: 0.00016743\n","Iter: 019/050 | Train Loss: 0.00013358\n","Iter: 020/050 | Train Loss: 0.00013496\n","Adjusting Layer 1, Kernel Nodes: 724, Adptive Nodes:76\n","Iter: 021/050 | Train Loss: 0.00012285\n","Iter: 022/050 | Train Loss: 0.00009050\n","Iter: 023/050 | Train Loss: 0.00008176\n","Iter: 024/050 | Train Loss: 0.00008158\n","Iter: 025/050 | Train Loss: 0.00006636\n","Iter: 026/050 | Train Loss: 0.00005350\n","Iter: 027/050 | Train Loss: 0.00005238\n","Iter: 028/050 | Train Loss: 0.00004905\n","Iter: 029/050 | Train Loss: 0.00003960\n","Iter: 030/050 | Train Loss: 0.00003487\n","Iter: 031/050 | Train Loss: 0.00003419\n","Iter: 032/050 | Train Loss: 0.00003036\n","Iter: 033/050 | Train Loss: 0.00002616\n","Iter: 034/050 | Train Loss: 0.00002353\n","Iter: 035/050 | Train Loss: 0.00002123\n","Iter: 036/050 | Train Loss: 0.00002024\n","Iter: 037/050 | Train Loss: 0.00001783\n","Iter: 038/050 | Train Loss: 0.00001403\n","Iter: 039/050 | Train Loss: 0.00001379\n","Iter: 040/050 | Train Loss: 0.00001390\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 041/050 | Train Loss: 0.00000999\n","Iter: 042/050 | Train Loss: 0.00000902\n","Iter: 043/050 | Train Loss: 0.00000979\n","Adjusting Layer 1, Kernel Nodes: 314, Adptive Nodes:486\n","Iter: 044/050 | Train Loss: 0.00000719\n","Iter: 045/050 | Train Loss: 0.00000639\n","Iter: 046/050 | Train Loss: 0.00000694\n","Adjusting Layer 1, Kernel Nodes: 347, Adptive Nodes:453\n","Iter: 047/050 | Train Loss: 0.00000531\n","Iter: 048/050 | Train Loss: 0.00000496\n","Iter: 049/050 | Train Loss: 0.00000492\n","\n","Iter: 049/050 | Test Loss: 0.00098899 | Test acc: 64.5100\n","scale:1.000000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00354703\n","Iter: 001/050 | Train Loss: 0.00180065\n","Iter: 002/050 | Train Loss: 0.00151933\n","Iter: 003/050 | Train Loss: 0.00086059\n","Iter: 004/050 | Train Loss: 0.00096810\n","Adjusting Layer 1, Kernel Nodes: 655, Adptive Nodes:145\n","Iter: 005/050 | Train Loss: 0.00075004\n","Iter: 006/050 | Train Loss: 0.00055822\n","Iter: 007/050 | Train Loss: 0.00054880\n","Iter: 008/050 | Train Loss: 0.00058043\n","Adjusting Layer 1, Kernel Nodes: 706, Adptive Nodes:94\n","Iter: 009/050 | Train Loss: 0.00052268\n","Iter: 010/050 | Train Loss: 0.00041019\n","Iter: 011/050 | Train Loss: 0.00035245\n","Iter: 012/050 | Train Loss: 0.00036461\n","Adjusting Layer 1, Kernel Nodes: 757, Adptive Nodes:43\n","Iter: 013/050 | Train Loss: 0.00035842\n","Iter: 014/050 | Train Loss: 0.00028527\n","Iter: 015/050 | Train Loss: 0.00022733\n","Iter: 016/050 | Train Loss: 0.00022521\n","Iter: 017/050 | Train Loss: 0.00022485\n","Iter: 018/050 | Train Loss: 0.00018715\n","Iter: 019/050 | Train Loss: 0.00014413\n","Iter: 020/050 | Train Loss: 0.00013070\n","Iter: 021/050 | Train Loss: 0.00013124\n","Adjusting Layer 1, Kernel Nodes: 763, Adptive Nodes:37\n","Iter: 022/050 | Train Loss: 0.00011508\n","Iter: 023/050 | Train Loss: 0.00009045\n","Iter: 024/050 | Train Loss: 0.00008000\n","Iter: 025/050 | Train Loss: 0.00007748\n","Iter: 026/050 | Train Loss: 0.00006963\n","Iter: 027/050 | Train Loss: 0.00005802\n","Iter: 028/050 | Train Loss: 0.00004946\n","Iter: 029/050 | Train Loss: 0.00004469\n","Iter: 030/050 | Train Loss: 0.00004223\n","Iter: 031/050 | Train Loss: 0.00003927\n","Iter: 032/050 | Train Loss: 0.00003222\n","Iter: 033/050 | Train Loss: 0.00002598\n","Iter: 034/050 | Train Loss: 0.00002692\n","Adjusting Layer 1, Kernel Nodes: 512, Adptive Nodes:288\n","Iter: 035/050 | Train Loss: 0.00002721\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 036/050 | Train Loss: 0.00001944\n","Iter: 037/050 | Train Loss: 0.00001696\n","Iter: 038/050 | Train Loss: 0.00001958\n","Adjusting Layer 1, Kernel Nodes: 504, Adptive Nodes:296\n","Iter: 039/050 | Train Loss: 0.00001546\n","Iter: 040/050 | Train Loss: 0.00001166\n","Iter: 041/050 | Train Loss: 0.00001325\n","Adjusting Layer 1, Kernel Nodes: 486, Adptive Nodes:314\n","Iter: 042/050 | Train Loss: 0.00001115\n","Iter: 043/050 | Train Loss: 0.00000831\n","Iter: 044/050 | Train Loss: 0.00000889\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 045/050 | Train Loss: 0.00000748\n","Iter: 046/050 | Train Loss: 0.00000598\n","Iter: 047/050 | Train Loss: 0.00000629\n","Adjusting Layer 1, Kernel Nodes: 148, Adptive Nodes:652\n","Iter: 048/050 | Train Loss: 0.00000507\n","Iter: 049/050 | Train Loss: 0.00000450\n","\n","Iter: 049/050 | Test Loss: 0.00096133 | Test acc: 65.1700\n","scale:1.000000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00349370\n","Iter: 001/050 | Train Loss: 0.00172680\n","Iter: 002/050 | Train Loss: 0.00153680\n","Iter: 003/050 | Train Loss: 0.00085000\n","Iter: 004/050 | Train Loss: 0.00096155\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 005/050 | Train Loss: 0.00075473\n","Iter: 006/050 | Train Loss: 0.00055787\n","Iter: 007/050 | Train Loss: 0.00054392\n","Iter: 008/050 | Train Loss: 0.00057760\n","Adjusting Layer 1, Kernel Nodes: 735, Adptive Nodes:65\n","Iter: 009/050 | Train Loss: 0.00052754\n","Iter: 010/050 | Train Loss: 0.00041770\n","Iter: 011/050 | Train Loss: 0.00035274\n","Iter: 012/050 | Train Loss: 0.00036017\n","Adjusting Layer 1, Kernel Nodes: 673, Adptive Nodes:127\n","Iter: 013/050 | Train Loss: 0.00036142\n","Adjusting Layer 1, Kernel Nodes: 697, Adptive Nodes:103\n","Iter: 014/050 | Train Loss: 0.00028867\n","Iter: 015/050 | Train Loss: 0.00022649\n","Iter: 016/050 | Train Loss: 0.00022626\n","Iter: 017/050 | Train Loss: 0.00022587\n","Iter: 018/050 | Train Loss: 0.00018330\n","Iter: 019/050 | Train Loss: 0.00014028\n","Iter: 020/050 | Train Loss: 0.00013409\n","Iter: 021/050 | Train Loss: 0.00013446\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 022/050 | Train Loss: 0.00010993\n","Iter: 023/050 | Train Loss: 0.00008680\n","Iter: 024/050 | Train Loss: 0.00008316\n","Iter: 025/050 | Train Loss: 0.00007989\n","Iter: 026/050 | Train Loss: 0.00006766\n","Iter: 027/050 | Train Loss: 0.00005624\n","Iter: 028/050 | Train Loss: 0.00005070\n","Iter: 029/050 | Train Loss: 0.00004731\n","Iter: 030/050 | Train Loss: 0.00004352\n","Iter: 031/050 | Train Loss: 0.00003759\n","Iter: 032/050 | Train Loss: 0.00003035\n","Iter: 033/050 | Train Loss: 0.00002828\n","Iter: 034/050 | Train Loss: 0.00002972\n","Adjusting Layer 1, Kernel Nodes: 568, Adptive Nodes:232\n","Iter: 035/050 | Train Loss: 0.00002454\n","Iter: 036/050 | Train Loss: 0.00001814\n","Iter: 037/050 | Train Loss: 0.00001922\n","Adjusting Layer 1, Kernel Nodes: 558, Adptive Nodes:242\n","Iter: 038/050 | Train Loss: 0.00001965\n","Adjusting Layer 1, Kernel Nodes: 603, Adptive Nodes:197\n","Iter: 039/050 | Train Loss: 0.00001390\n","Iter: 040/050 | Train Loss: 0.00001281\n","Iter: 041/050 | Train Loss: 0.00001374\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 042/050 | Train Loss: 0.00001028\n","Iter: 043/050 | Train Loss: 0.00000890\n","Iter: 044/050 | Train Loss: 0.00000932\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 045/050 | Train Loss: 0.00000723\n","Iter: 046/050 | Train Loss: 0.00000646\n","Iter: 047/050 | Train Loss: 0.00000655\n","Adjusting Layer 1, Kernel Nodes: 638, Adptive Nodes:162\n","Iter: 048/050 | Train Loss: 0.00000511\n","Iter: 049/050 | Train Loss: 0.00000496\n","\n","Iter: 049/050 | Test Loss: 0.00095886 | Test acc: 65.2000\n","scale:1.000000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00345315\n","Iter: 001/050 | Train Loss: 0.00167263\n","Iter: 002/050 | Train Loss: 0.00154876\n","Iter: 003/050 | Train Loss: 0.00084033\n","Iter: 004/050 | Train Loss: 0.00095368\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00075834\n","Iter: 006/050 | Train Loss: 0.00055704\n","Iter: 007/050 | Train Loss: 0.00053898\n","Iter: 008/050 | Train Loss: 0.00057304\n","Adjusting Layer 1, Kernel Nodes: 754, Adptive Nodes:46\n","Iter: 009/050 | Train Loss: 0.00052717\n","Iter: 010/050 | Train Loss: 0.00041971\n","Iter: 011/050 | Train Loss: 0.00035289\n","Iter: 012/050 | Train Loss: 0.00035726\n","Adjusting Layer 1, Kernel Nodes: 760, Adptive Nodes:40\n","Iter: 013/050 | Train Loss: 0.00036025\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 014/050 | Train Loss: 0.00029436\n","Iter: 015/050 | Train Loss: 0.00023001\n","Iter: 016/050 | Train Loss: 0.00022360\n","Iter: 017/050 | Train Loss: 0.00022660\n","Adjusting Layer 1, Kernel Nodes: 643, Adptive Nodes:157\n","Iter: 018/050 | Train Loss: 0.00018935\n","Iter: 019/050 | Train Loss: 0.00014459\n","Iter: 020/050 | Train Loss: 0.00013450\n","Iter: 021/050 | Train Loss: 0.00013648\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 022/050 | Train Loss: 0.00011517\n","Iter: 023/050 | Train Loss: 0.00008890\n","Iter: 024/050 | Train Loss: 0.00008311\n","Iter: 025/050 | Train Loss: 0.00008250\n","Iter: 026/050 | Train Loss: 0.00007088\n","Iter: 027/050 | Train Loss: 0.00005700\n","Iter: 028/050 | Train Loss: 0.00005082\n","Iter: 029/050 | Train Loss: 0.00004883\n","Iter: 030/050 | Train Loss: 0.00004522\n","Iter: 031/050 | Train Loss: 0.00003779\n","Iter: 032/050 | Train Loss: 0.00003007\n","Iter: 033/050 | Train Loss: 0.00002923\n","Iter: 034/050 | Train Loss: 0.00003069\n","Adjusting Layer 1, Kernel Nodes: 760, Adptive Nodes:40\n","Iter: 035/050 | Train Loss: 0.00002467\n","Iter: 036/050 | Train Loss: 0.00001840\n","Iter: 037/050 | Train Loss: 0.00001964\n","Adjusting Layer 1, Kernel Nodes: 605, Adptive Nodes:195\n","Iter: 038/050 | Train Loss: 0.00002023\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 039/050 | Train Loss: 0.00001482\n","Iter: 040/050 | Train Loss: 0.00001274\n","Iter: 041/050 | Train Loss: 0.00001402\n","Adjusting Layer 1, Kernel Nodes: 391, Adptive Nodes:409\n","Iter: 042/050 | Train Loss: 0.00001179\n","Iter: 043/050 | Train Loss: 0.00000897\n","Iter: 044/050 | Train Loss: 0.00000910\n","Adjusting Layer 1, Kernel Nodes: 413, Adptive Nodes:387\n","Iter: 045/050 | Train Loss: 0.00000870\n","Iter: 046/050 | Train Loss: 0.00000670\n","Iter: 047/050 | Train Loss: 0.00000615\n","Iter: 048/050 | Train Loss: 0.00000637\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 049/050 | Train Loss: 0.00000512\n","\n","Iter: 049/050 | Test Loss: 0.00094622 | Test acc: 65.5200\n","scale:1.000000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 000/050 | Train Loss: 0.00342483\n","Iter: 001/050 | Train Loss: 0.00163286\n","Iter: 002/050 | Train Loss: 0.00155780\n","Iter: 003/050 | Train Loss: 0.00083255\n","Iter: 004/050 | Train Loss: 0.00094437\n","Adjusting Layer 1, Kernel Nodes: 665, Adptive Nodes:135\n","Iter: 005/050 | Train Loss: 0.00076346\n","Iter: 006/050 | Train Loss: 0.00055762\n","Iter: 007/050 | Train Loss: 0.00053489\n","Iter: 008/050 | Train Loss: 0.00057042\n","Adjusting Layer 1, Kernel Nodes: 730, Adptive Nodes:70\n","Iter: 009/050 | Train Loss: 0.00052812\n","Iter: 010/050 | Train Loss: 0.00042098\n","Iter: 011/050 | Train Loss: 0.00035251\n","Iter: 012/050 | Train Loss: 0.00035404\n","Adjusting Layer 1, Kernel Nodes: 733, Adptive Nodes:67\n","Iter: 013/050 | Train Loss: 0.00035752\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 014/050 | Train Loss: 0.00029661\n","Iter: 015/050 | Train Loss: 0.00023133\n","Iter: 016/050 | Train Loss: 0.00021983\n","Iter: 017/050 | Train Loss: 0.00022518\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 018/050 | Train Loss: 0.00019640\n","Iter: 019/050 | Train Loss: 0.00015106\n","Iter: 020/050 | Train Loss: 0.00013224\n","Iter: 021/050 | Train Loss: 0.00013574\n","Adjusting Layer 1, Kernel Nodes: 743, Adptive Nodes:57\n","Iter: 022/050 | Train Loss: 0.00012331\n","Iter: 023/050 | Train Loss: 0.00009535\n","Iter: 024/050 | Train Loss: 0.00008258\n","Iter: 025/050 | Train Loss: 0.00008131\n","Iter: 026/050 | Train Loss: 0.00007545\n","Iter: 027/050 | Train Loss: 0.00006461\n","Iter: 028/050 | Train Loss: 0.00005252\n","Iter: 029/050 | Train Loss: 0.00004515\n","Iter: 030/050 | Train Loss: 0.00004613\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 031/050 | Train Loss: 0.00004384\n","Iter: 032/050 | Train Loss: 0.00003300\n","Iter: 033/050 | Train Loss: 0.00002749\n","Iter: 034/050 | Train Loss: 0.00002977\n","Adjusting Layer 1, Kernel Nodes: 764, Adptive Nodes:36\n","Iter: 035/050 | Train Loss: 0.00002859\n","Iter: 036/050 | Train Loss: 0.00002212\n","Iter: 037/050 | Train Loss: 0.00001808\n","Iter: 038/050 | Train Loss: 0.00001843\n","Adjusting Layer 1, Kernel Nodes: 689, Adptive Nodes:111\n","Iter: 039/050 | Train Loss: 0.00001846\n","Adjusting Layer 1, Kernel Nodes: 572, Adptive Nodes:228\n","Iter: 040/050 | Train Loss: 0.00001498\n","Iter: 041/050 | Train Loss: 0.00001194\n","Iter: 042/050 | Train Loss: 0.00001216\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 043/050 | Train Loss: 0.00001217\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 044/050 | Train Loss: 0.00000932\n","Iter: 045/050 | Train Loss: 0.00000778\n","Iter: 046/050 | Train Loss: 0.00000846\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 047/050 | Train Loss: 0.00000739\n","Iter: 048/050 | Train Loss: 0.00000552\n","Iter: 049/050 | Train Loss: 0.00000573\n","\n","Iter: 049/050 | Test Loss: 0.00093905 | Test acc: 65.5900\n","scale:1.020000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00424351\n","Iter: 001/050 | Train Loss: 0.00312514\n","Iter: 002/050 | Train Loss: 0.00130131\n","Iter: 003/050 | Train Loss: 0.00099945\n","Iter: 004/050 | Train Loss: 0.00098991\n","Iter: 005/050 | Train Loss: 0.00079560\n","Iter: 006/050 | Train Loss: 0.00067397\n","Iter: 007/050 | Train Loss: 0.00063062\n","Iter: 008/050 | Train Loss: 0.00061371\n","Iter: 009/050 | Train Loss: 0.00058584\n","Iter: 010/050 | Train Loss: 0.00052021\n","Iter: 011/050 | Train Loss: 0.00043423\n","Iter: 012/050 | Train Loss: 0.00037583\n","Iter: 013/050 | Train Loss: 0.00036198\n","Iter: 014/050 | Train Loss: 0.00034939\n","Iter: 015/050 | Train Loss: 0.00030798\n","Iter: 016/050 | Train Loss: 0.00026320\n","Iter: 017/050 | Train Loss: 0.00023818\n","Iter: 018/050 | Train Loss: 0.00022479\n","Iter: 019/050 | Train Loss: 0.00020714\n","Iter: 020/050 | Train Loss: 0.00017573\n","Iter: 021/050 | Train Loss: 0.00014442\n","Iter: 022/050 | Train Loss: 0.00013398\n","Iter: 023/050 | Train Loss: 0.00013418\n","Adjusting Layer 1, Kernel Nodes: 668, Adptive Nodes:132\n","Iter: 024/050 | Train Loss: 0.00011685\n","Iter: 025/050 | Train Loss: 0.00009467\n","Iter: 026/050 | Train Loss: 0.00008620\n","Iter: 027/050 | Train Loss: 0.00008186\n","Iter: 028/050 | Train Loss: 0.00007067\n","Iter: 029/050 | Train Loss: 0.00005860\n","Iter: 030/050 | Train Loss: 0.00005228\n","Iter: 031/050 | Train Loss: 0.00004799\n","Iter: 032/050 | Train Loss: 0.00004335\n","Iter: 033/050 | Train Loss: 0.00003994\n","Iter: 034/050 | Train Loss: 0.00003464\n","Iter: 035/050 | Train Loss: 0.00002851\n","Iter: 036/050 | Train Loss: 0.00002746\n","Iter: 037/050 | Train Loss: 0.00002713\n","Iter: 038/050 | Train Loss: 0.00002199\n","Iter: 039/050 | Train Loss: 0.00001774\n","Iter: 040/050 | Train Loss: 0.00001798\n","Adjusting Layer 1, Kernel Nodes: 622, Adptive Nodes:178\n","Iter: 041/050 | Train Loss: 0.00001747\n","Iter: 042/050 | Train Loss: 0.00001412\n","Iter: 043/050 | Train Loss: 0.00001233\n","Iter: 044/050 | Train Loss: 0.00001199\n","Iter: 045/050 | Train Loss: 0.00001078\n","Iter: 046/050 | Train Loss: 0.00000932\n","Iter: 047/050 | Train Loss: 0.00000830\n","Iter: 048/050 | Train Loss: 0.00000761\n","Iter: 049/050 | Train Loss: 0.00000723\n","\n","Iter: 049/050 | Test Loss: 0.00094281 | Test acc: 64.8200\n","scale:1.020000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00407050\n","Iter: 001/050 | Train Loss: 0.00272940\n","Iter: 002/050 | Train Loss: 0.00134827\n","Iter: 003/050 | Train Loss: 0.00096656\n","Iter: 004/050 | Train Loss: 0.00099064\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 005/050 | Train Loss: 0.00075744\n","Iter: 006/050 | Train Loss: 0.00059793\n","Iter: 007/050 | Train Loss: 0.00058703\n","Iter: 008/050 | Train Loss: 0.00060256\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 009/050 | Train Loss: 0.00053584\n","Iter: 010/050 | Train Loss: 0.00042331\n","Iter: 011/050 | Train Loss: 0.00035787\n","Iter: 012/050 | Train Loss: 0.00035778\n","Iter: 013/050 | Train Loss: 0.00035011\n","Iter: 014/050 | Train Loss: 0.00028924\n","Iter: 015/050 | Train Loss: 0.00022717\n","Iter: 016/050 | Train Loss: 0.00021114\n","Iter: 017/050 | Train Loss: 0.00021263\n","Adjusting Layer 1, Kernel Nodes: 544, Adptive Nodes:256\n","Iter: 018/050 | Train Loss: 0.00018063\n","Iter: 019/050 | Train Loss: 0.00013539\n","Iter: 020/050 | Train Loss: 0.00011854\n","Iter: 021/050 | Train Loss: 0.00011738\n","Iter: 022/050 | Train Loss: 0.00010038\n","Iter: 023/050 | Train Loss: 0.00007678\n","Iter: 024/050 | Train Loss: 0.00006852\n","Iter: 025/050 | Train Loss: 0.00006835\n","Iter: 026/050 | Train Loss: 0.00006045\n","Iter: 027/050 | Train Loss: 0.00004893\n","Iter: 028/050 | Train Loss: 0.00004326\n","Iter: 029/050 | Train Loss: 0.00004131\n","Iter: 030/050 | Train Loss: 0.00003743\n","Iter: 031/050 | Train Loss: 0.00003220\n","Iter: 032/050 | Train Loss: 0.00002882\n","Iter: 033/050 | Train Loss: 0.00002657\n","Iter: 034/050 | Train Loss: 0.00002348\n","Iter: 035/050 | Train Loss: 0.00002089\n","Iter: 036/050 | Train Loss: 0.00001965\n","Iter: 037/050 | Train Loss: 0.00001756\n","Iter: 038/050 | Train Loss: 0.00001439\n","Iter: 039/050 | Train Loss: 0.00001297\n","Iter: 040/050 | Train Loss: 0.00001310\n","Adjusting Layer 1, Kernel Nodes: 526, Adptive Nodes:274\n","Iter: 041/050 | Train Loss: 0.00001103\n","Iter: 042/050 | Train Loss: 0.00000830\n","Iter: 043/050 | Train Loss: 0.00000824\n","Iter: 044/050 | Train Loss: 0.00000822\n","Iter: 045/050 | Train Loss: 0.00000647\n","Iter: 046/050 | Train Loss: 0.00000557\n","Iter: 047/050 | Train Loss: 0.00000590\n","Adjusting Layer 1, Kernel Nodes: 460, Adptive Nodes:340\n","Iter: 048/050 | Train Loss: 0.00000522\n","Iter: 049/050 | Train Loss: 0.00000428\n","\n","Iter: 049/050 | Test Loss: 0.00100580 | Test acc: 63.4800\n","scale:1.020000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00392430\n","Iter: 001/050 | Train Loss: 0.00243470\n","Iter: 002/050 | Train Loss: 0.00139034\n","Iter: 003/050 | Train Loss: 0.00093881\n","Iter: 004/050 | Train Loss: 0.00098980\n","Adjusting Layer 1, Kernel Nodes: 662, Adptive Nodes:138\n","Iter: 005/050 | Train Loss: 0.00075052\n","Iter: 006/050 | Train Loss: 0.00058168\n","Iter: 007/050 | Train Loss: 0.00057422\n","Iter: 008/050 | Train Loss: 0.00059393\n","Adjusting Layer 1, Kernel Nodes: 741, Adptive Nodes:59\n","Iter: 009/050 | Train Loss: 0.00052180\n","Iter: 010/050 | Train Loss: 0.00040885\n","Iter: 011/050 | Train Loss: 0.00035368\n","Iter: 012/050 | Train Loss: 0.00036039\n","Adjusting Layer 1, Kernel Nodes: 485, Adptive Nodes:315\n","Iter: 013/050 | Train Loss: 0.00034173\n","Iter: 014/050 | Train Loss: 0.00026451\n","Iter: 015/050 | Train Loss: 0.00021469\n","Iter: 016/050 | Train Loss: 0.00021677\n","Adjusting Layer 1, Kernel Nodes: 527, Adptive Nodes:273\n","Iter: 017/050 | Train Loss: 0.00019638\n","Iter: 018/050 | Train Loss: 0.00014212\n","Iter: 019/050 | Train Loss: 0.00012292\n","Iter: 020/050 | Train Loss: 0.00012606\n","Adjusting Layer 1, Kernel Nodes: 708, Adptive Nodes:92\n","Iter: 021/050 | Train Loss: 0.00009105\n","Iter: 022/050 | Train Loss: 0.00007678\n","Iter: 023/050 | Train Loss: 0.00008067\n","Adjusting Layer 1, Kernel Nodes: 251, Adptive Nodes:549\n","Iter: 024/050 | Train Loss: 0.00005579\n","Iter: 025/050 | Train Loss: 0.00006005\n","Adjusting Layer 1, Kernel Nodes: 635, Adptive Nodes:165\n","Iter: 026/050 | Train Loss: 0.00004897\n","Iter: 027/050 | Train Loss: 0.00004474\n","Iter: 028/050 | Train Loss: 0.00004249\n","Iter: 029/050 | Train Loss: 0.00003379\n","Iter: 030/050 | Train Loss: 0.00003472\n","Adjusting Layer 1, Kernel Nodes: 157, Adptive Nodes:643\n","Iter: 031/050 | Train Loss: 0.00002658\n","Iter: 032/050 | Train Loss: 0.00002619\n","Iter: 033/050 | Train Loss: 0.00002196\n","Iter: 034/050 | Train Loss: 0.00001945\n","Iter: 035/050 | Train Loss: 0.00001783\n","Iter: 036/050 | Train Loss: 0.00001539\n","Iter: 037/050 | Train Loss: 0.00001446\n","Iter: 038/050 | Train Loss: 0.00001294\n","Iter: 039/050 | Train Loss: 0.00001244\n","Iter: 040/050 | Train Loss: 0.00001075\n","Iter: 041/050 | Train Loss: 0.00001031\n","Iter: 042/050 | Train Loss: 0.00000878\n","Iter: 043/050 | Train Loss: 0.00000768\n","Iter: 044/050 | Train Loss: 0.00000733\n","Iter: 045/050 | Train Loss: 0.00000586\n","Iter: 046/050 | Train Loss: 0.00000583\n","Iter: 047/050 | Train Loss: 0.00000507\n","Iter: 048/050 | Train Loss: 0.00000431\n","Iter: 049/050 | Train Loss: 0.00000429\n","\n","Iter: 049/050 | Test Loss: 0.00104810 | Test acc: 62.5600\n","scale:1.020000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00379972\n","Iter: 001/050 | Train Loss: 0.00220146\n","Iter: 002/050 | Train Loss: 0.00143402\n","Iter: 003/050 | Train Loss: 0.00091212\n","Iter: 004/050 | Train Loss: 0.00098912\n","Adjusting Layer 1, Kernel Nodes: 651, Adptive Nodes:149\n","Iter: 005/050 | Train Loss: 0.00074812\n","Iter: 006/050 | Train Loss: 0.00057171\n","Iter: 007/050 | Train Loss: 0.00056731\n","Iter: 008/050 | Train Loss: 0.00059181\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 009/050 | Train Loss: 0.00052067\n","Iter: 010/050 | Train Loss: 0.00040620\n","Iter: 011/050 | Train Loss: 0.00035295\n","Iter: 012/050 | Train Loss: 0.00036466\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 013/050 | Train Loss: 0.00034605\n","Iter: 014/050 | Train Loss: 0.00026545\n","Iter: 015/050 | Train Loss: 0.00021938\n","Iter: 016/050 | Train Loss: 0.00022542\n","Adjusting Layer 1, Kernel Nodes: 456, Adptive Nodes:344\n","Iter: 017/050 | Train Loss: 0.00020315\n","Iter: 018/050 | Train Loss: 0.00014862\n","Iter: 019/050 | Train Loss: 0.00013054\n","Iter: 020/050 | Train Loss: 0.00013305\n","Adjusting Layer 1, Kernel Nodes: 368, Adptive Nodes:432\n","Iter: 021/050 | Train Loss: 0.00010092\n","Iter: 022/050 | Train Loss: 0.00007974\n","Iter: 023/050 | Train Loss: 0.00008318\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 024/050 | Train Loss: 0.00006344\n","Iter: 025/050 | Train Loss: 0.00005559\n","Iter: 026/050 | Train Loss: 0.00005572\n","Adjusting Layer 1, Kernel Nodes: 223, Adptive Nodes:577\n","Iter: 027/050 | Train Loss: 0.00004146\n","Iter: 028/050 | Train Loss: 0.00004439\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 029/050 | Train Loss: 0.00003430\n","Iter: 030/050 | Train Loss: 0.00003383\n","Iter: 031/050 | Train Loss: 0.00002952\n","Iter: 032/050 | Train Loss: 0.00002507\n","Iter: 033/050 | Train Loss: 0.00002389\n","Iter: 034/050 | Train Loss: 0.00001932\n","Iter: 035/050 | Train Loss: 0.00001797\n","Iter: 036/050 | Train Loss: 0.00001544\n","Iter: 037/050 | Train Loss: 0.00001379\n","Iter: 038/050 | Train Loss: 0.00001259\n","Iter: 039/050 | Train Loss: 0.00001186\n","Iter: 040/050 | Train Loss: 0.00001069\n","Iter: 041/050 | Train Loss: 0.00001027\n","Iter: 042/050 | Train Loss: 0.00000924\n","Iter: 043/050 | Train Loss: 0.00000813\n","Iter: 044/050 | Train Loss: 0.00000768\n","Iter: 045/050 | Train Loss: 0.00000638\n","Iter: 046/050 | Train Loss: 0.00000597\n","Iter: 047/050 | Train Loss: 0.00000535\n","Iter: 048/050 | Train Loss: 0.00000458\n","Iter: 049/050 | Train Loss: 0.00000437\n","\n","Iter: 049/050 | Test Loss: 0.00103868 | Test acc: 63.0400\n","scale:1.020000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00369637\n","Iter: 001/050 | Train Loss: 0.00202936\n","Iter: 002/050 | Train Loss: 0.00146886\n","Iter: 003/050 | Train Loss: 0.00089121\n","Iter: 004/050 | Train Loss: 0.00098451\n","Adjusting Layer 1, Kernel Nodes: 648, Adptive Nodes:152\n","Iter: 005/050 | Train Loss: 0.00074635\n","Iter: 006/050 | Train Loss: 0.00056452\n","Iter: 007/050 | Train Loss: 0.00056159\n","Iter: 008/050 | Train Loss: 0.00058800\n","Adjusting Layer 1, Kernel Nodes: 751, Adptive Nodes:49\n","Iter: 009/050 | Train Loss: 0.00051898\n","Iter: 010/050 | Train Loss: 0.00040502\n","Iter: 011/050 | Train Loss: 0.00035211\n","Iter: 012/050 | Train Loss: 0.00036519\n","Adjusting Layer 1, Kernel Nodes: 623, Adptive Nodes:177\n","Iter: 013/050 | Train Loss: 0.00034992\n","Iter: 014/050 | Train Loss: 0.00027094\n","Iter: 015/050 | Train Loss: 0.00021998\n","Iter: 016/050 | Train Loss: 0.00022380\n","Adjusting Layer 1, Kernel Nodes: 474, Adptive Nodes:326\n","Iter: 017/050 | Train Loss: 0.00021108\n","Iter: 018/050 | Train Loss: 0.00015863\n","Iter: 019/050 | Train Loss: 0.00012894\n","Iter: 020/050 | Train Loss: 0.00013260\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 021/050 | Train Loss: 0.00011303\n","Iter: 022/050 | Train Loss: 0.00008204\n","Iter: 023/050 | Train Loss: 0.00008090\n","Iter: 024/050 | Train Loss: 0.00007600\n","Iter: 025/050 | Train Loss: 0.00005718\n","Iter: 026/050 | Train Loss: 0.00005202\n","Iter: 027/050 | Train Loss: 0.00005189\n","Iter: 028/050 | Train Loss: 0.00004311\n","Iter: 029/050 | Train Loss: 0.00003660\n","Iter: 030/050 | Train Loss: 0.00003514\n","Iter: 031/050 | Train Loss: 0.00003247\n","Iter: 032/050 | Train Loss: 0.00002791\n","Iter: 033/050 | Train Loss: 0.00002405\n","Iter: 034/050 | Train Loss: 0.00002277\n","Iter: 035/050 | Train Loss: 0.00002140\n","Iter: 036/050 | Train Loss: 0.00001768\n","Iter: 037/050 | Train Loss: 0.00001508\n","Iter: 038/050 | Train Loss: 0.00001508\n","Iter: 039/050 | Train Loss: 0.00001366\n","Iter: 040/050 | Train Loss: 0.00001039\n","Iter: 041/050 | Train Loss: 0.00000941\n","Iter: 042/050 | Train Loss: 0.00000991\n","Adjusting Layer 1, Kernel Nodes: 461, Adptive Nodes:339\n","Iter: 043/050 | Train Loss: 0.00000770\n","Iter: 044/050 | Train Loss: 0.00000637\n","Iter: 045/050 | Train Loss: 0.00000722\n","Adjusting Layer 1, Kernel Nodes: 537, Adptive Nodes:263\n","Iter: 046/050 | Train Loss: 0.00000563\n","Iter: 047/050 | Train Loss: 0.00000499\n","Iter: 048/050 | Train Loss: 0.00000529\n","Adjusting Layer 1, Kernel Nodes: 444, Adptive Nodes:356\n","Iter: 049/050 | Train Loss: 0.00000374\n","\n","Iter: 049/050 | Test Loss: 0.00100181 | Test acc: 64.0500\n","scale:1.020000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00361187\n","Iter: 001/050 | Train Loss: 0.00189439\n","Iter: 002/050 | Train Loss: 0.00149691\n","Iter: 003/050 | Train Loss: 0.00087365\n","Iter: 004/050 | Train Loss: 0.00097662\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 005/050 | Train Loss: 0.00074701\n","Iter: 006/050 | Train Loss: 0.00055967\n","Iter: 007/050 | Train Loss: 0.00055454\n","Iter: 008/050 | Train Loss: 0.00058301\n","Adjusting Layer 1, Kernel Nodes: 731, Adptive Nodes:69\n","Iter: 009/050 | Train Loss: 0.00051790\n","Iter: 010/050 | Train Loss: 0.00040447\n","Iter: 011/050 | Train Loss: 0.00035182\n","Iter: 012/050 | Train Loss: 0.00036602\n","Adjusting Layer 1, Kernel Nodes: 758, Adptive Nodes:42\n","Iter: 013/050 | Train Loss: 0.00035276\n","Iter: 014/050 | Train Loss: 0.00027497\n","Iter: 015/050 | Train Loss: 0.00022273\n","Iter: 016/050 | Train Loss: 0.00022548\n","Adjusting Layer 1, Kernel Nodes: 525, Adptive Nodes:275\n","Iter: 017/050 | Train Loss: 0.00021640\n","Iter: 018/050 | Train Loss: 0.00016648\n","Iter: 019/050 | Train Loss: 0.00013315\n","Iter: 020/050 | Train Loss: 0.00013565\n","Adjusting Layer 1, Kernel Nodes: 714, Adptive Nodes:86\n","Iter: 021/050 | Train Loss: 0.00012245\n","Iter: 022/050 | Train Loss: 0.00008987\n","Iter: 023/050 | Train Loss: 0.00008227\n","Iter: 024/050 | Train Loss: 0.00008146\n","Iter: 025/050 | Train Loss: 0.00006579\n","Iter: 026/050 | Train Loss: 0.00005355\n","Iter: 027/050 | Train Loss: 0.00005254\n","Iter: 028/050 | Train Loss: 0.00004887\n","Iter: 029/050 | Train Loss: 0.00003946\n","Iter: 030/050 | Train Loss: 0.00003492\n","Iter: 031/050 | Train Loss: 0.00003414\n","Iter: 032/050 | Train Loss: 0.00003031\n","Iter: 033/050 | Train Loss: 0.00002621\n","Iter: 034/050 | Train Loss: 0.00002333\n","Iter: 035/050 | Train Loss: 0.00002113\n","Iter: 036/050 | Train Loss: 0.00002033\n","Iter: 037/050 | Train Loss: 0.00001742\n","Iter: 038/050 | Train Loss: 0.00001371\n","Iter: 039/050 | Train Loss: 0.00001400\n","Adjusting Layer 1, Kernel Nodes: 525, Adptive Nodes:275\n","Iter: 040/050 | Train Loss: 0.00001323\n","Iter: 041/050 | Train Loss: 0.00000947\n","Iter: 042/050 | Train Loss: 0.00000923\n","Iter: 043/050 | Train Loss: 0.00000943\n","Adjusting Layer 1, Kernel Nodes: 455, Adptive Nodes:345\n","Iter: 044/050 | Train Loss: 0.00000675\n","Iter: 045/050 | Train Loss: 0.00000655\n","Iter: 046/050 | Train Loss: 0.00000670\n","Adjusting Layer 1, Kernel Nodes: 363, Adptive Nodes:437\n","Iter: 047/050 | Train Loss: 0.00000510\n","Iter: 048/050 | Train Loss: 0.00000503\n","Iter: 049/050 | Train Loss: 0.00000468\n","\n","Iter: 049/050 | Test Loss: 0.00098884 | Test acc: 64.3600\n","scale:1.020000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00354415\n","Iter: 001/050 | Train Loss: 0.00179670\n","Iter: 002/050 | Train Loss: 0.00151997\n","Iter: 003/050 | Train Loss: 0.00085993\n","Iter: 004/050 | Train Loss: 0.00096769\n","Adjusting Layer 1, Kernel Nodes: 654, Adptive Nodes:146\n","Iter: 005/050 | Train Loss: 0.00075032\n","Iter: 006/050 | Train Loss: 0.00055813\n","Iter: 007/050 | Train Loss: 0.00054876\n","Iter: 008/050 | Train Loss: 0.00058060\n","Adjusting Layer 1, Kernel Nodes: 707, Adptive Nodes:93\n","Iter: 009/050 | Train Loss: 0.00052235\n","Iter: 010/050 | Train Loss: 0.00040932\n","Iter: 011/050 | Train Loss: 0.00035214\n","Iter: 012/050 | Train Loss: 0.00036497\n","Adjusting Layer 1, Kernel Nodes: 763, Adptive Nodes:37\n","Iter: 013/050 | Train Loss: 0.00035811\n","Iter: 014/050 | Train Loss: 0.00028441\n","Iter: 015/050 | Train Loss: 0.00022722\n","Iter: 016/050 | Train Loss: 0.00022561\n","Iter: 017/050 | Train Loss: 0.00022465\n","Iter: 018/050 | Train Loss: 0.00018643\n","Iter: 019/050 | Train Loss: 0.00014366\n","Iter: 020/050 | Train Loss: 0.00013051\n","Iter: 021/050 | Train Loss: 0.00013101\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 022/050 | Train Loss: 0.00011467\n","Iter: 023/050 | Train Loss: 0.00008988\n","Iter: 024/050 | Train Loss: 0.00007959\n","Iter: 025/050 | Train Loss: 0.00007732\n","Iter: 026/050 | Train Loss: 0.00006928\n","Iter: 027/050 | Train Loss: 0.00005783\n","Iter: 028/050 | Train Loss: 0.00004942\n","Iter: 029/050 | Train Loss: 0.00004417\n","Iter: 030/050 | Train Loss: 0.00004172\n","Iter: 031/050 | Train Loss: 0.00003926\n","Iter: 032/050 | Train Loss: 0.00003187\n","Iter: 033/050 | Train Loss: 0.00002546\n","Iter: 034/050 | Train Loss: 0.00002683\n","Adjusting Layer 1, Kernel Nodes: 524, Adptive Nodes:276\n","Iter: 035/050 | Train Loss: 0.00002706\n","Adjusting Layer 1, Kernel Nodes: 557, Adptive Nodes:243\n","Iter: 036/050 | Train Loss: 0.00001909\n","Iter: 037/050 | Train Loss: 0.00001691\n","Iter: 038/050 | Train Loss: 0.00001939\n","Adjusting Layer 1, Kernel Nodes: 522, Adptive Nodes:278\n","Iter: 039/050 | Train Loss: 0.00001514\n","Iter: 040/050 | Train Loss: 0.00001153\n","Iter: 041/050 | Train Loss: 0.00001301\n","Adjusting Layer 1, Kernel Nodes: 510, Adptive Nodes:290\n","Iter: 042/050 | Train Loss: 0.00001092\n","Iter: 043/050 | Train Loss: 0.00000822\n","Iter: 044/050 | Train Loss: 0.00000873\n","Adjusting Layer 1, Kernel Nodes: 563, Adptive Nodes:237\n","Iter: 045/050 | Train Loss: 0.00000742\n","Iter: 046/050 | Train Loss: 0.00000591\n","Iter: 047/050 | Train Loss: 0.00000614\n","Adjusting Layer 1, Kernel Nodes: 307, Adptive Nodes:493\n","Iter: 048/050 | Train Loss: 0.00000503\n","Iter: 049/050 | Train Loss: 0.00000435\n","\n","Iter: 049/050 | Test Loss: 0.00096344 | Test acc: 65.0300\n","scale:1.020000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00349198\n","Iter: 001/050 | Train Loss: 0.00172428\n","Iter: 002/050 | Train Loss: 0.00153705\n","Iter: 003/050 | Train Loss: 0.00084935\n","Iter: 004/050 | Train Loss: 0.00096144\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 005/050 | Train Loss: 0.00075496\n","Iter: 006/050 | Train Loss: 0.00055745\n","Iter: 007/050 | Train Loss: 0.00054357\n","Iter: 008/050 | Train Loss: 0.00057737\n","Adjusting Layer 1, Kernel Nodes: 732, Adptive Nodes:68\n","Iter: 009/050 | Train Loss: 0.00052700\n","Iter: 010/050 | Train Loss: 0.00041680\n","Iter: 011/050 | Train Loss: 0.00035225\n","Iter: 012/050 | Train Loss: 0.00036025\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 013/050 | Train Loss: 0.00036103\n","Adjusting Layer 1, Kernel Nodes: 695, Adptive Nodes:105\n","Iter: 014/050 | Train Loss: 0.00028759\n","Iter: 015/050 | Train Loss: 0.00022578\n","Iter: 016/050 | Train Loss: 0.00022619\n","Adjusting Layer 1, Kernel Nodes: 682, Adptive Nodes:118\n","Iter: 017/050 | Train Loss: 0.00022362\n","Iter: 018/050 | Train Loss: 0.00017591\n","Iter: 019/050 | Train Loss: 0.00013738\n","Iter: 020/050 | Train Loss: 0.00013855\n","Adjusting Layer 1, Kernel Nodes: 779, Adptive Nodes:21\n","Iter: 021/050 | Train Loss: 0.00013155\n","Iter: 022/050 | Train Loss: 0.00009858\n","Iter: 023/050 | Train Loss: 0.00008452\n","Iter: 024/050 | Train Loss: 0.00008657\n","Adjusting Layer 1, Kernel Nodes: 494, Adptive Nodes:306\n","Iter: 025/050 | Train Loss: 0.00007326\n","Iter: 026/050 | Train Loss: 0.00005730\n","Iter: 027/050 | Train Loss: 0.00005535\n","Iter: 028/050 | Train Loss: 0.00005200\n","Iter: 029/050 | Train Loss: 0.00004218\n","Iter: 030/050 | Train Loss: 0.00003691\n","Iter: 031/050 | Train Loss: 0.00003486\n","Iter: 032/050 | Train Loss: 0.00003154\n","Iter: 033/050 | Train Loss: 0.00002742\n","Iter: 034/050 | Train Loss: 0.00002315\n","Iter: 035/050 | Train Loss: 0.00002187\n","Iter: 036/050 | Train Loss: 0.00002155\n","Iter: 037/050 | Train Loss: 0.00001718\n","Iter: 038/050 | Train Loss: 0.00001423\n","Iter: 039/050 | Train Loss: 0.00001557\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 040/050 | Train Loss: 0.00001345\n","Iter: 041/050 | Train Loss: 0.00000974\n","Iter: 042/050 | Train Loss: 0.00001033\n","Adjusting Layer 1, Kernel Nodes: 384, Adptive Nodes:416\n","Iter: 043/050 | Train Loss: 0.00000978\n","Iter: 044/050 | Train Loss: 0.00000699\n","Iter: 045/050 | Train Loss: 0.00000716\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 046/050 | Train Loss: 0.00000692\n","Iter: 047/050 | Train Loss: 0.00000530\n","Iter: 048/050 | Train Loss: 0.00000521\n","Iter: 049/050 | Train Loss: 0.00000487\n","\n","Iter: 049/050 | Test Loss: 0.00096917 | Test acc: 64.9200\n","scale:1.020000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00345235\n","Iter: 001/050 | Train Loss: 0.00167138\n","Iter: 002/050 | Train Loss: 0.00154874\n","Iter: 003/050 | Train Loss: 0.00083995\n","Iter: 004/050 | Train Loss: 0.00095363\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 005/050 | Train Loss: 0.00075843\n","Iter: 006/050 | Train Loss: 0.00055678\n","Iter: 007/050 | Train Loss: 0.00053865\n","Iter: 008/050 | Train Loss: 0.00057267\n","Adjusting Layer 1, Kernel Nodes: 754, Adptive Nodes:46\n","Iter: 009/050 | Train Loss: 0.00052683\n","Iter: 010/050 | Train Loss: 0.00041945\n","Iter: 011/050 | Train Loss: 0.00035266\n","Iter: 012/050 | Train Loss: 0.00035744\n","Adjusting Layer 1, Kernel Nodes: 758, Adptive Nodes:42\n","Iter: 013/050 | Train Loss: 0.00036045\n","Adjusting Layer 1, Kernel Nodes: 634, Adptive Nodes:166\n","Iter: 014/050 | Train Loss: 0.00029441\n","Iter: 015/050 | Train Loss: 0.00023018\n","Iter: 016/050 | Train Loss: 0.00022391\n","Iter: 017/050 | Train Loss: 0.00022678\n","Adjusting Layer 1, Kernel Nodes: 646, Adptive Nodes:154\n","Iter: 018/050 | Train Loss: 0.00018965\n","Iter: 019/050 | Train Loss: 0.00014487\n","Iter: 020/050 | Train Loss: 0.00013451\n","Iter: 021/050 | Train Loss: 0.00013656\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 022/050 | Train Loss: 0.00011569\n","Iter: 023/050 | Train Loss: 0.00008910\n","Iter: 024/050 | Train Loss: 0.00008300\n","Iter: 025/050 | Train Loss: 0.00008251\n","Iter: 026/050 | Train Loss: 0.00007112\n","Iter: 027/050 | Train Loss: 0.00005717\n","Iter: 028/050 | Train Loss: 0.00005074\n","Iter: 029/050 | Train Loss: 0.00004867\n","Iter: 030/050 | Train Loss: 0.00004530\n","Iter: 031/050 | Train Loss: 0.00003799\n","Iter: 032/050 | Train Loss: 0.00003004\n","Iter: 033/050 | Train Loss: 0.00002909\n","Iter: 034/050 | Train Loss: 0.00003077\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 035/050 | Train Loss: 0.00002491\n","Iter: 036/050 | Train Loss: 0.00001854\n","Iter: 037/050 | Train Loss: 0.00001957\n","Adjusting Layer 1, Kernel Nodes: 528, Adptive Nodes:272\n","Iter: 038/050 | Train Loss: 0.00002023\n","Adjusting Layer 1, Kernel Nodes: 593, Adptive Nodes:207\n","Iter: 039/050 | Train Loss: 0.00001509\n","Iter: 040/050 | Train Loss: 0.00001270\n","Iter: 041/050 | Train Loss: 0.00001374\n","Adjusting Layer 1, Kernel Nodes: 511, Adptive Nodes:289\n","Iter: 042/050 | Train Loss: 0.00001194\n","Iter: 043/050 | Train Loss: 0.00000906\n","Iter: 044/050 | Train Loss: 0.00000877\n","Iter: 045/050 | Train Loss: 0.00000877\n","Adjusting Layer 1, Kernel Nodes: 628, Adptive Nodes:172\n","Iter: 046/050 | Train Loss: 0.00000689\n","Iter: 047/050 | Train Loss: 0.00000592\n","Iter: 048/050 | Train Loss: 0.00000627\n","Adjusting Layer 1, Kernel Nodes: 590, Adptive Nodes:210\n","Iter: 049/050 | Train Loss: 0.00000524\n","\n","Iter: 049/050 | Test Loss: 0.00094580 | Test acc: 65.5600\n","scale:1.020000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 000/050 | Train Loss: 0.00342400\n","Iter: 001/050 | Train Loss: 0.00163167\n","Iter: 002/050 | Train Loss: 0.00155802\n","Iter: 003/050 | Train Loss: 0.00083210\n","Iter: 004/050 | Train Loss: 0.00094413\n","Adjusting Layer 1, Kernel Nodes: 661, Adptive Nodes:139\n","Iter: 005/050 | Train Loss: 0.00076390\n","Iter: 006/050 | Train Loss: 0.00055762\n","Iter: 007/050 | Train Loss: 0.00053460\n","Iter: 008/050 | Train Loss: 0.00057023\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 009/050 | Train Loss: 0.00052808\n","Iter: 010/050 | Train Loss: 0.00042097\n","Iter: 011/050 | Train Loss: 0.00035235\n","Iter: 012/050 | Train Loss: 0.00035393\n","Adjusting Layer 1, Kernel Nodes: 733, Adptive Nodes:67\n","Iter: 013/050 | Train Loss: 0.00035737\n","Adjusting Layer 1, Kernel Nodes: 646, Adptive Nodes:154\n","Iter: 014/050 | Train Loss: 0.00029645\n","Iter: 015/050 | Train Loss: 0.00023135\n","Iter: 016/050 | Train Loss: 0.00021992\n","Iter: 017/050 | Train Loss: 0.00022505\n","Adjusting Layer 1, Kernel Nodes: 643, Adptive Nodes:157\n","Iter: 018/050 | Train Loss: 0.00019611\n","Iter: 019/050 | Train Loss: 0.00015080\n","Iter: 020/050 | Train Loss: 0.00013213\n","Iter: 021/050 | Train Loss: 0.00013565\n","Adjusting Layer 1, Kernel Nodes: 740, Adptive Nodes:60\n","Iter: 022/050 | Train Loss: 0.00012299\n","Iter: 023/050 | Train Loss: 0.00009502\n","Iter: 024/050 | Train Loss: 0.00008245\n","Iter: 025/050 | Train Loss: 0.00008125\n","Iter: 026/050 | Train Loss: 0.00007529\n","Iter: 027/050 | Train Loss: 0.00006438\n","Iter: 028/050 | Train Loss: 0.00005239\n","Iter: 029/050 | Train Loss: 0.00004524\n","Iter: 030/050 | Train Loss: 0.00004617\n","Adjusting Layer 1, Kernel Nodes: 750, Adptive Nodes:50\n","Iter: 031/050 | Train Loss: 0.00004370\n","Iter: 032/050 | Train Loss: 0.00003284\n","Iter: 033/050 | Train Loss: 0.00002745\n","Iter: 034/050 | Train Loss: 0.00002980\n","Adjusting Layer 1, Kernel Nodes: 776, Adptive Nodes:24\n","Iter: 035/050 | Train Loss: 0.00002850\n","Iter: 036/050 | Train Loss: 0.00002192\n","Iter: 037/050 | Train Loss: 0.00001802\n","Iter: 038/050 | Train Loss: 0.00001849\n","Adjusting Layer 1, Kernel Nodes: 623, Adptive Nodes:177\n","Iter: 039/050 | Train Loss: 0.00001837\n","Iter: 040/050 | Train Loss: 0.00001506\n","Iter: 041/050 | Train Loss: 0.00001198\n","Iter: 042/050 | Train Loss: 0.00001184\n","Iter: 043/050 | Train Loss: 0.00001222\n","Adjusting Layer 1, Kernel Nodes: 769, Adptive Nodes:31\n","Iter: 044/050 | Train Loss: 0.00001014\n","Iter: 045/050 | Train Loss: 0.00000778\n","Iter: 046/050 | Train Loss: 0.00000778\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 047/050 | Train Loss: 0.00000805\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 048/050 | Train Loss: 0.00000634\n","Iter: 049/050 | Train Loss: 0.00000522\n","\n","Iter: 049/050 | Test Loss: 0.00093858 | Test acc: 65.6200\n","scale:1.040000,therd:0.760000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00423946\n","Iter: 001/050 | Train Loss: 0.00311571\n","Iter: 002/050 | Train Loss: 0.00130255\n","Iter: 003/050 | Train Loss: 0.00099875\n","Iter: 004/050 | Train Loss: 0.00098973\n","Iter: 005/050 | Train Loss: 0.00079559\n","Iter: 006/050 | Train Loss: 0.00067381\n","Iter: 007/050 | Train Loss: 0.00063027\n","Iter: 008/050 | Train Loss: 0.00061347\n","Iter: 009/050 | Train Loss: 0.00058609\n","Iter: 010/050 | Train Loss: 0.00052061\n","Iter: 011/050 | Train Loss: 0.00043472\n","Iter: 012/050 | Train Loss: 0.00037595\n","Iter: 013/050 | Train Loss: 0.00036158\n","Iter: 014/050 | Train Loss: 0.00034959\n","Iter: 015/050 | Train Loss: 0.00030833\n","Iter: 016/050 | Train Loss: 0.00026320\n","Iter: 017/050 | Train Loss: 0.00023788\n","Iter: 018/050 | Train Loss: 0.00022470\n","Iter: 019/050 | Train Loss: 0.00020740\n","Iter: 020/050 | Train Loss: 0.00017600\n","Iter: 021/050 | Train Loss: 0.00014423\n","Iter: 022/050 | Train Loss: 0.00013311\n","Iter: 023/050 | Train Loss: 0.00013329\n","Adjusting Layer 1, Kernel Nodes: 674, Adptive Nodes:126\n","Iter: 024/050 | Train Loss: 0.00011719\n","Iter: 025/050 | Train Loss: 0.00009442\n","Iter: 026/050 | Train Loss: 0.00008515\n","Iter: 027/050 | Train Loss: 0.00008151\n","Iter: 028/050 | Train Loss: 0.00007097\n","Iter: 029/050 | Train Loss: 0.00005856\n","Iter: 030/050 | Train Loss: 0.00005197\n","Iter: 031/050 | Train Loss: 0.00004790\n","Iter: 032/050 | Train Loss: 0.00004314\n","Iter: 033/050 | Train Loss: 0.00003963\n","Iter: 034/050 | Train Loss: 0.00003459\n","Iter: 035/050 | Train Loss: 0.00002839\n","Iter: 036/050 | Train Loss: 0.00002702\n","Iter: 037/050 | Train Loss: 0.00002702\n","Iter: 038/050 | Train Loss: 0.00002225\n","Iter: 039/050 | Train Loss: 0.00001765\n","Iter: 040/050 | Train Loss: 0.00001754\n","Iter: 041/050 | Train Loss: 0.00001732\n","Iter: 042/050 | Train Loss: 0.00001477\n","Iter: 043/050 | Train Loss: 0.00001263\n","Iter: 044/050 | Train Loss: 0.00001138\n","Iter: 045/050 | Train Loss: 0.00001035\n","Iter: 046/050 | Train Loss: 0.00000985\n","Iter: 047/050 | Train Loss: 0.00000915\n","Iter: 048/050 | Train Loss: 0.00000764\n","Iter: 049/050 | Train Loss: 0.00000658\n","\n","Iter: 049/050 | Test Loss: 0.00094116 | Test acc: 64.8500\n","scale:1.040000,therd:0.780000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00406608\n","Iter: 001/050 | Train Loss: 0.00272010\n","Iter: 002/050 | Train Loss: 0.00135013\n","Iter: 003/050 | Train Loss: 0.00096536\n","Iter: 004/050 | Train Loss: 0.00099052\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 005/050 | Train Loss: 0.00075852\n","Iter: 006/050 | Train Loss: 0.00059765\n","Iter: 007/050 | Train Loss: 0.00058441\n","Iter: 008/050 | Train Loss: 0.00059912\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 009/050 | Train Loss: 0.00053535\n","Iter: 010/050 | Train Loss: 0.00042477\n","Iter: 011/050 | Train Loss: 0.00035751\n","Iter: 012/050 | Train Loss: 0.00035619\n","Iter: 013/050 | Train Loss: 0.00034914\n","Iter: 014/050 | Train Loss: 0.00028872\n","Iter: 015/050 | Train Loss: 0.00022645\n","Iter: 016/050 | Train Loss: 0.00021024\n","Iter: 017/050 | Train Loss: 0.00021097\n","Adjusting Layer 1, Kernel Nodes: 579, Adptive Nodes:221\n","Iter: 018/050 | Train Loss: 0.00017833\n","Iter: 019/050 | Train Loss: 0.00013456\n","Iter: 020/050 | Train Loss: 0.00011860\n","Iter: 021/050 | Train Loss: 0.00011725\n","Iter: 022/050 | Train Loss: 0.00010072\n","Iter: 023/050 | Train Loss: 0.00007747\n","Iter: 024/050 | Train Loss: 0.00006828\n","Iter: 025/050 | Train Loss: 0.00006766\n","Iter: 026/050 | Train Loss: 0.00006027\n","Iter: 027/050 | Train Loss: 0.00004850\n","Iter: 028/050 | Train Loss: 0.00004196\n","Iter: 029/050 | Train Loss: 0.00004010\n","Iter: 030/050 | Train Loss: 0.00003743\n","Iter: 031/050 | Train Loss: 0.00003249\n","Iter: 032/050 | Train Loss: 0.00002820\n","Iter: 033/050 | Train Loss: 0.00002590\n","Iter: 034/050 | Train Loss: 0.00002387\n","Iter: 035/050 | Train Loss: 0.00002166\n","Iter: 036/050 | Train Loss: 0.00001952\n","Iter: 037/050 | Train Loss: 0.00001680\n","Iter: 038/050 | Train Loss: 0.00001429\n","Iter: 039/050 | Train Loss: 0.00001353\n","Iter: 040/050 | Train Loss: 0.00001300\n","Iter: 041/050 | Train Loss: 0.00001085\n","Iter: 042/050 | Train Loss: 0.00000870\n","Iter: 043/050 | Train Loss: 0.00000827\n","Iter: 044/050 | Train Loss: 0.00000828\n","Adjusting Layer 1, Kernel Nodes: 478, Adptive Nodes:322\n","Iter: 045/050 | Train Loss: 0.00000705\n","Iter: 046/050 | Train Loss: 0.00000603\n","Iter: 047/050 | Train Loss: 0.00000568\n","Iter: 048/050 | Train Loss: 0.00000529\n","Iter: 049/050 | Train Loss: 0.00000484\n","\n","Iter: 049/050 | Test Loss: 0.00100464 | Test acc: 63.5000\n","scale:1.040000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00391963\n","Iter: 001/050 | Train Loss: 0.00242557\n","Iter: 002/050 | Train Loss: 0.00139205\n","Iter: 003/050 | Train Loss: 0.00093763\n","Iter: 004/050 | Train Loss: 0.00098987\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 005/050 | Train Loss: 0.00075065\n","Iter: 006/050 | Train Loss: 0.00058131\n","Iter: 007/050 | Train Loss: 0.00057397\n","Iter: 008/050 | Train Loss: 0.00059384\n","Adjusting Layer 1, Kernel Nodes: 741, Adptive Nodes:59\n","Iter: 009/050 | Train Loss: 0.00052153\n","Iter: 010/050 | Train Loss: 0.00040838\n","Iter: 011/050 | Train Loss: 0.00035325\n","Iter: 012/050 | Train Loss: 0.00035997\n","Adjusting Layer 1, Kernel Nodes: 485, Adptive Nodes:315\n","Iter: 013/050 | Train Loss: 0.00034114\n","Iter: 014/050 | Train Loss: 0.00026427\n","Iter: 015/050 | Train Loss: 0.00021496\n","Iter: 016/050 | Train Loss: 0.00021629\n","Adjusting Layer 1, Kernel Nodes: 558, Adptive Nodes:242\n","Iter: 017/050 | Train Loss: 0.00019565\n","Iter: 018/050 | Train Loss: 0.00014176\n","Iter: 019/050 | Train Loss: 0.00012287\n","Iter: 020/050 | Train Loss: 0.00012600\n","Adjusting Layer 1, Kernel Nodes: 750, Adptive Nodes:50\n","Iter: 021/050 | Train Loss: 0.00009071\n","Iter: 022/050 | Train Loss: 0.00007721\n","Iter: 023/050 | Train Loss: 0.00008068\n","Adjusting Layer 1, Kernel Nodes: 277, Adptive Nodes:523\n","Iter: 024/050 | Train Loss: 0.00005513\n","Iter: 025/050 | Train Loss: 0.00006108\n","Adjusting Layer 1, Kernel Nodes: 556, Adptive Nodes:244\n","Iter: 026/050 | Train Loss: 0.00004749\n","Iter: 027/050 | Train Loss: 0.00004574\n","Iter: 028/050 | Train Loss: 0.00004194\n","Iter: 029/050 | Train Loss: 0.00003415\n","Iter: 030/050 | Train Loss: 0.00003500\n","Adjusting Layer 1, Kernel Nodes: 184, Adptive Nodes:616\n","Iter: 031/050 | Train Loss: 0.00002646\n","Iter: 032/050 | Train Loss: 0.00002663\n","Adjusting Layer 1, Kernel Nodes: 579, Adptive Nodes:221\n","Iter: 033/050 | Train Loss: 0.00002156\n","Iter: 034/050 | Train Loss: 0.00001986\n","Iter: 035/050 | Train Loss: 0.00001736\n","Iter: 036/050 | Train Loss: 0.00001561\n","Iter: 037/050 | Train Loss: 0.00001441\n","Iter: 038/050 | Train Loss: 0.00001326\n","Iter: 039/050 | Train Loss: 0.00001221\n","Iter: 040/050 | Train Loss: 0.00001116\n","Iter: 041/050 | Train Loss: 0.00001018\n","Iter: 042/050 | Train Loss: 0.00000885\n","Iter: 043/050 | Train Loss: 0.00000832\n","Iter: 044/050 | Train Loss: 0.00000689\n","Iter: 045/050 | Train Loss: 0.00000659\n","Iter: 046/050 | Train Loss: 0.00000558\n","Iter: 047/050 | Train Loss: 0.00000505\n","Iter: 048/050 | Train Loss: 0.00000466\n","Iter: 049/050 | Train Loss: 0.00000391\n","\n","Iter: 049/050 | Test Loss: 0.00105345 | Test acc: 62.5800\n","scale:1.040000,therd:0.820000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00379719\n","Iter: 001/050 | Train Loss: 0.00219660\n","Iter: 002/050 | Train Loss: 0.00143493\n","Iter: 003/050 | Train Loss: 0.00091118\n","Iter: 004/050 | Train Loss: 0.00098885\n","Adjusting Layer 1, Kernel Nodes: 649, Adptive Nodes:151\n","Iter: 005/050 | Train Loss: 0.00074824\n","Iter: 006/050 | Train Loss: 0.00057146\n","Iter: 007/050 | Train Loss: 0.00056711\n","Iter: 008/050 | Train Loss: 0.00059148\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 009/050 | Train Loss: 0.00052024\n","Iter: 010/050 | Train Loss: 0.00040579\n","Iter: 011/050 | Train Loss: 0.00035292\n","Iter: 012/050 | Train Loss: 0.00036455\n","Adjusting Layer 1, Kernel Nodes: 769, Adptive Nodes:31\n","Iter: 013/050 | Train Loss: 0.00034554\n","Iter: 014/050 | Train Loss: 0.00026497\n","Iter: 015/050 | Train Loss: 0.00021944\n","Iter: 016/050 | Train Loss: 0.00022558\n","Adjusting Layer 1, Kernel Nodes: 426, Adptive Nodes:374\n","Iter: 017/050 | Train Loss: 0.00020299\n","Iter: 018/050 | Train Loss: 0.00014860\n","Iter: 019/050 | Train Loss: 0.00013077\n","Iter: 020/050 | Train Loss: 0.00013325\n","Adjusting Layer 1, Kernel Nodes: 399, Adptive Nodes:401\n","Iter: 021/050 | Train Loss: 0.00010108\n","Iter: 022/050 | Train Loss: 0.00008006\n","Iter: 023/050 | Train Loss: 0.00008329\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 024/050 | Train Loss: 0.00006352\n","Iter: 025/050 | Train Loss: 0.00005593\n","Iter: 026/050 | Train Loss: 0.00005574\n","Iter: 027/050 | Train Loss: 0.00004195\n","Iter: 028/050 | Train Loss: 0.00004229\n","Adjusting Layer 1, Kernel Nodes: 151, Adptive Nodes:649\n","Iter: 029/050 | Train Loss: 0.00003706\n","Iter: 030/050 | Train Loss: 0.00003139\n","Iter: 031/050 | Train Loss: 0.00003067\n","Iter: 032/050 | Train Loss: 0.00002531\n","Iter: 033/050 | Train Loss: 0.00002314\n","Iter: 034/050 | Train Loss: 0.00002086\n","Iter: 035/050 | Train Loss: 0.00001759\n","Iter: 036/050 | Train Loss: 0.00001569\n","Iter: 037/050 | Train Loss: 0.00001419\n","Iter: 038/050 | Train Loss: 0.00001215\n","Iter: 039/050 | Train Loss: 0.00001152\n","Iter: 040/050 | Train Loss: 0.00001109\n","Iter: 041/050 | Train Loss: 0.00000958\n","Iter: 042/050 | Train Loss: 0.00000943\n","Iter: 043/050 | Train Loss: 0.00000834\n","Iter: 044/050 | Train Loss: 0.00000693\n","Iter: 045/050 | Train Loss: 0.00000682\n","Iter: 046/050 | Train Loss: 0.00000568\n","Iter: 047/050 | Train Loss: 0.00000504\n","Iter: 048/050 | Train Loss: 0.00000503\n","Iter: 049/050 | Train Loss: 0.00000403\n","\n","Iter: 049/050 | Test Loss: 0.00103305 | Test acc: 63.1500\n","scale:1.040000,therd:0.840000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00369389\n","Iter: 001/050 | Train Loss: 0.00202500\n","Iter: 002/050 | Train Loss: 0.00146948\n","Iter: 003/050 | Train Loss: 0.00089034\n","Iter: 004/050 | Train Loss: 0.00098413\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 005/050 | Train Loss: 0.00074635\n","Iter: 006/050 | Train Loss: 0.00056424\n","Iter: 007/050 | Train Loss: 0.00056165\n","Iter: 008/050 | Train Loss: 0.00058837\n","Adjusting Layer 1, Kernel Nodes: 759, Adptive Nodes:41\n","Iter: 009/050 | Train Loss: 0.00051918\n","Iter: 010/050 | Train Loss: 0.00040479\n","Iter: 011/050 | Train Loss: 0.00035178\n","Iter: 012/050 | Train Loss: 0.00036519\n","Adjusting Layer 1, Kernel Nodes: 619, Adptive Nodes:181\n","Iter: 013/050 | Train Loss: 0.00035015\n","Iter: 014/050 | Train Loss: 0.00027115\n","Iter: 015/050 | Train Loss: 0.00021992\n","Iter: 016/050 | Train Loss: 0.00022353\n","Adjusting Layer 1, Kernel Nodes: 466, Adptive Nodes:334\n","Iter: 017/050 | Train Loss: 0.00021123\n","Iter: 018/050 | Train Loss: 0.00015885\n","Iter: 019/050 | Train Loss: 0.00012855\n","Iter: 020/050 | Train Loss: 0.00013207\n","Adjusting Layer 1, Kernel Nodes: 778, Adptive Nodes:22\n","Iter: 021/050 | Train Loss: 0.00011309\n","Iter: 022/050 | Train Loss: 0.00008200\n","Iter: 023/050 | Train Loss: 0.00008066\n","Iter: 024/050 | Train Loss: 0.00007600\n","Iter: 025/050 | Train Loss: 0.00005723\n","Iter: 026/050 | Train Loss: 0.00005182\n","Iter: 027/050 | Train Loss: 0.00005169\n","Iter: 028/050 | Train Loss: 0.00004315\n","Iter: 029/050 | Train Loss: 0.00003664\n","Iter: 030/050 | Train Loss: 0.00003507\n","Iter: 031/050 | Train Loss: 0.00003237\n","Iter: 032/050 | Train Loss: 0.00002806\n","Iter: 033/050 | Train Loss: 0.00002404\n","Iter: 034/050 | Train Loss: 0.00002248\n","Iter: 035/050 | Train Loss: 0.00002138\n","Iter: 036/050 | Train Loss: 0.00001770\n","Iter: 037/050 | Train Loss: 0.00001487\n","Iter: 038/050 | Train Loss: 0.00001498\n","Adjusting Layer 1, Kernel Nodes: 496, Adptive Nodes:304\n","Iter: 039/050 | Train Loss: 0.00001301\n","Iter: 040/050 | Train Loss: 0.00000992\n","Iter: 041/050 | Train Loss: 0.00001009\n","Adjusting Layer 1, Kernel Nodes: 439, Adptive Nodes:361\n","Iter: 042/050 | Train Loss: 0.00000879\n","Iter: 043/050 | Train Loss: 0.00000678\n","Iter: 044/050 | Train Loss: 0.00000753\n","Adjusting Layer 1, Kernel Nodes: 535, Adptive Nodes:265\n","Iter: 045/050 | Train Loss: 0.00000582\n","Iter: 046/050 | Train Loss: 0.00000572\n","Iter: 047/050 | Train Loss: 0.00000531\n","Iter: 048/050 | Train Loss: 0.00000408\n","Iter: 049/050 | Train Loss: 0.00000444\n","\n","Iter: 049/050 | Test Loss: 0.00100758 | Test acc: 63.8600\n","scale:1.040000,therd:0.860000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00360950\n","Iter: 001/050 | Train Loss: 0.00189046\n","Iter: 002/050 | Train Loss: 0.00149768\n","Iter: 003/050 | Train Loss: 0.00087273\n","Iter: 004/050 | Train Loss: 0.00097640\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 005/050 | Train Loss: 0.00074682\n","Iter: 006/050 | Train Loss: 0.00055898\n","Iter: 007/050 | Train Loss: 0.00055400\n","Iter: 008/050 | Train Loss: 0.00058269\n","Adjusting Layer 1, Kernel Nodes: 731, Adptive Nodes:69\n","Iter: 009/050 | Train Loss: 0.00051758\n","Iter: 010/050 | Train Loss: 0.00040411\n","Iter: 011/050 | Train Loss: 0.00035139\n","Iter: 012/050 | Train Loss: 0.00036591\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 013/050 | Train Loss: 0.00035261\n","Iter: 014/050 | Train Loss: 0.00027420\n","Iter: 015/050 | Train Loss: 0.00022201\n","Iter: 016/050 | Train Loss: 0.00022552\n","Adjusting Layer 1, Kernel Nodes: 523, Adptive Nodes:277\n","Iter: 017/050 | Train Loss: 0.00021642\n","Iter: 018/050 | Train Loss: 0.00016586\n","Iter: 019/050 | Train Loss: 0.00013264\n","Iter: 020/050 | Train Loss: 0.00013591\n","Adjusting Layer 1, Kernel Nodes: 750, Adptive Nodes:50\n","Iter: 021/050 | Train Loss: 0.00012219\n","Iter: 022/050 | Train Loss: 0.00008926\n","Iter: 023/050 | Train Loss: 0.00008243\n","Iter: 024/050 | Train Loss: 0.00008139\n","Iter: 025/050 | Train Loss: 0.00006528\n","Iter: 026/050 | Train Loss: 0.00005360\n","Iter: 027/050 | Train Loss: 0.00005255\n","Iter: 028/050 | Train Loss: 0.00004837\n","Iter: 029/050 | Train Loss: 0.00003945\n","Iter: 030/050 | Train Loss: 0.00003504\n","Iter: 031/050 | Train Loss: 0.00003370\n","Iter: 032/050 | Train Loss: 0.00003024\n","Iter: 033/050 | Train Loss: 0.00002644\n","Iter: 034/050 | Train Loss: 0.00002302\n","Iter: 035/050 | Train Loss: 0.00002103\n","Iter: 036/050 | Train Loss: 0.00002061\n","Iter: 037/050 | Train Loss: 0.00001730\n","Iter: 038/050 | Train Loss: 0.00001358\n","Iter: 039/050 | Train Loss: 0.00001415\n","Adjusting Layer 1, Kernel Nodes: 380, Adptive Nodes:420\n","Iter: 040/050 | Train Loss: 0.00001321\n","Iter: 041/050 | Train Loss: 0.00000939\n","Iter: 042/050 | Train Loss: 0.00000927\n","Iter: 043/050 | Train Loss: 0.00000940\n","Adjusting Layer 1, Kernel Nodes: 446, Adptive Nodes:354\n","Iter: 044/050 | Train Loss: 0.00000677\n","Iter: 045/050 | Train Loss: 0.00000649\n","Iter: 046/050 | Train Loss: 0.00000655\n","Adjusting Layer 1, Kernel Nodes: 496, Adptive Nodes:304\n","Iter: 047/050 | Train Loss: 0.00000512\n","Iter: 048/050 | Train Loss: 0.00000488\n","Iter: 049/050 | Train Loss: 0.00000464\n","\n","Iter: 049/050 | Test Loss: 0.00098872 | Test acc: 64.4500\n","scale:1.040000,therd:0.880000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00354225\n","Iter: 001/050 | Train Loss: 0.00179335\n","Iter: 002/050 | Train Loss: 0.00152061\n","Iter: 003/050 | Train Loss: 0.00085902\n","Iter: 004/050 | Train Loss: 0.00096769\n","Adjusting Layer 1, Kernel Nodes: 651, Adptive Nodes:149\n","Iter: 005/050 | Train Loss: 0.00075023\n","Iter: 006/050 | Train Loss: 0.00055765\n","Iter: 007/050 | Train Loss: 0.00054855\n","Iter: 008/050 | Train Loss: 0.00058078\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 009/050 | Train Loss: 0.00052237\n","Iter: 010/050 | Train Loss: 0.00040888\n","Iter: 011/050 | Train Loss: 0.00035170\n","Iter: 012/050 | Train Loss: 0.00036474\n","Adjusting Layer 1, Kernel Nodes: 761, Adptive Nodes:39\n","Iter: 013/050 | Train Loss: 0.00035805\n","Iter: 014/050 | Train Loss: 0.00028436\n","Iter: 015/050 | Train Loss: 0.00022693\n","Iter: 016/050 | Train Loss: 0.00022500\n","Iter: 017/050 | Train Loss: 0.00022421\n","Iter: 018/050 | Train Loss: 0.00018648\n","Iter: 019/050 | Train Loss: 0.00014368\n","Iter: 020/050 | Train Loss: 0.00012999\n","Iter: 021/050 | Train Loss: 0.00013048\n","Adjusting Layer 1, Kernel Nodes: 689, Adptive Nodes:111\n","Iter: 022/050 | Train Loss: 0.00011466\n","Iter: 023/050 | Train Loss: 0.00008998\n","Iter: 024/050 | Train Loss: 0.00007945\n","Iter: 025/050 | Train Loss: 0.00007703\n","Iter: 026/050 | Train Loss: 0.00006911\n","Iter: 027/050 | Train Loss: 0.00005769\n","Iter: 028/050 | Train Loss: 0.00004911\n","Iter: 029/050 | Train Loss: 0.00004393\n","Iter: 030/050 | Train Loss: 0.00004163\n","Iter: 031/050 | Train Loss: 0.00003916\n","Iter: 032/050 | Train Loss: 0.00003177\n","Iter: 033/050 | Train Loss: 0.00002544\n","Iter: 034/050 | Train Loss: 0.00002688\n","Adjusting Layer 1, Kernel Nodes: 514, Adptive Nodes:286\n","Iter: 035/050 | Train Loss: 0.00002704\n","Adjusting Layer 1, Kernel Nodes: 524, Adptive Nodes:276\n","Iter: 036/050 | Train Loss: 0.00001907\n","Iter: 037/050 | Train Loss: 0.00001704\n","Iter: 038/050 | Train Loss: 0.00001954\n","Adjusting Layer 1, Kernel Nodes: 539, Adptive Nodes:261\n","Iter: 039/050 | Train Loss: 0.00001519\n","Iter: 040/050 | Train Loss: 0.00001167\n","Iter: 041/050 | Train Loss: 0.00001314\n","Adjusting Layer 1, Kernel Nodes: 482, Adptive Nodes:318\n","Iter: 042/050 | Train Loss: 0.00001091\n","Iter: 043/050 | Train Loss: 0.00000828\n","Iter: 044/050 | Train Loss: 0.00000876\n","Adjusting Layer 1, Kernel Nodes: 786, Adptive Nodes:14\n","Iter: 045/050 | Train Loss: 0.00000733\n","Iter: 046/050 | Train Loss: 0.00000593\n","Iter: 047/050 | Train Loss: 0.00000616\n","Adjusting Layer 1, Kernel Nodes: 145, Adptive Nodes:655\n","Iter: 048/050 | Train Loss: 0.00000498\n","Iter: 049/050 | Train Loss: 0.00000444\n","\n","Iter: 049/050 | Test Loss: 0.00096304 | Test acc: 65.0700\n","scale:1.040000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00348989\n","Iter: 001/050 | Train Loss: 0.00172112\n","Iter: 002/050 | Train Loss: 0.00153753\n","Iter: 003/050 | Train Loss: 0.00084847\n","Iter: 004/050 | Train Loss: 0.00096137\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 005/050 | Train Loss: 0.00075496\n","Iter: 006/050 | Train Loss: 0.00055694\n","Iter: 007/050 | Train Loss: 0.00054334\n","Iter: 008/050 | Train Loss: 0.00057759\n","Adjusting Layer 1, Kernel Nodes: 732, Adptive Nodes:68\n","Iter: 009/050 | Train Loss: 0.00052697\n","Iter: 010/050 | Train Loss: 0.00041616\n","Iter: 011/050 | Train Loss: 0.00035186\n","Iter: 012/050 | Train Loss: 0.00036063\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 013/050 | Train Loss: 0.00036109\n","Adjusting Layer 1, Kernel Nodes: 694, Adptive Nodes:106\n","Iter: 014/050 | Train Loss: 0.00028680\n","Iter: 015/050 | Train Loss: 0.00022537\n","Iter: 016/050 | Train Loss: 0.00022634\n","Adjusting Layer 1, Kernel Nodes: 697, Adptive Nodes:103\n","Iter: 017/050 | Train Loss: 0.00022346\n","Iter: 018/050 | Train Loss: 0.00017528\n","Iter: 019/050 | Train Loss: 0.00013720\n","Iter: 020/050 | Train Loss: 0.00013879\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 021/050 | Train Loss: 0.00013129\n","Iter: 022/050 | Train Loss: 0.00009821\n","Iter: 023/050 | Train Loss: 0.00008460\n","Iter: 024/050 | Train Loss: 0.00008651\n","Adjusting Layer 1, Kernel Nodes: 500, Adptive Nodes:300\n","Iter: 025/050 | Train Loss: 0.00007300\n","Iter: 026/050 | Train Loss: 0.00005719\n","Iter: 027/050 | Train Loss: 0.00005528\n","Iter: 028/050 | Train Loss: 0.00005191\n","Iter: 029/050 | Train Loss: 0.00004207\n","Iter: 030/050 | Train Loss: 0.00003679\n","Iter: 031/050 | Train Loss: 0.00003485\n","Iter: 032/050 | Train Loss: 0.00003149\n","Iter: 033/050 | Train Loss: 0.00002730\n","Iter: 034/050 | Train Loss: 0.00002317\n","Iter: 035/050 | Train Loss: 0.00002183\n","Iter: 036/050 | Train Loss: 0.00002141\n","Iter: 037/050 | Train Loss: 0.00001717\n","Iter: 038/050 | Train Loss: 0.00001420\n","Iter: 039/050 | Train Loss: 0.00001545\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 040/050 | Train Loss: 0.00001344\n","Iter: 041/050 | Train Loss: 0.00000975\n","Iter: 042/050 | Train Loss: 0.00001026\n","Adjusting Layer 1, Kernel Nodes: 411, Adptive Nodes:389\n","Iter: 043/050 | Train Loss: 0.00000978\n","Iter: 044/050 | Train Loss: 0.00000699\n","Iter: 045/050 | Train Loss: 0.00000713\n","Adjusting Layer 1, Kernel Nodes: 448, Adptive Nodes:352\n","Iter: 046/050 | Train Loss: 0.00000692\n","Iter: 047/050 | Train Loss: 0.00000527\n","Iter: 048/050 | Train Loss: 0.00000518\n","Iter: 049/050 | Train Loss: 0.00000484\n","\n","Iter: 049/050 | Test Loss: 0.00096922 | Test acc: 64.9400\n","scale:1.040000,therd:0.920000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00345107\n","Iter: 001/050 | Train Loss: 0.00166944\n","Iter: 002/050 | Train Loss: 0.00154920\n","Iter: 003/050 | Train Loss: 0.00083947\n","Iter: 004/050 | Train Loss: 0.00095368\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 005/050 | Train Loss: 0.00075891\n","Iter: 006/050 | Train Loss: 0.00055674\n","Iter: 007/050 | Train Loss: 0.00053843\n","Iter: 008/050 | Train Loss: 0.00057246\n","Adjusting Layer 1, Kernel Nodes: 754, Adptive Nodes:46\n","Iter: 009/050 | Train Loss: 0.00052656\n","Iter: 010/050 | Train Loss: 0.00041908\n","Iter: 011/050 | Train Loss: 0.00035247\n","Iter: 012/050 | Train Loss: 0.00035728\n","Adjusting Layer 1, Kernel Nodes: 756, Adptive Nodes:44\n","Iter: 013/050 | Train Loss: 0.00036030\n","Adjusting Layer 1, Kernel Nodes: 633, Adptive Nodes:167\n","Iter: 014/050 | Train Loss: 0.00029387\n","Iter: 015/050 | Train Loss: 0.00022971\n","Iter: 016/050 | Train Loss: 0.00022356\n","Iter: 017/050 | Train Loss: 0.00022624\n","Adjusting Layer 1, Kernel Nodes: 644, Adptive Nodes:156\n","Iter: 018/050 | Train Loss: 0.00018917\n","Iter: 019/050 | Train Loss: 0.00014458\n","Iter: 020/050 | Train Loss: 0.00013411\n","Iter: 021/050 | Train Loss: 0.00013628\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 022/050 | Train Loss: 0.00011546\n","Iter: 023/050 | Train Loss: 0.00008887\n","Iter: 024/050 | Train Loss: 0.00008279\n","Iter: 025/050 | Train Loss: 0.00008237\n","Iter: 026/050 | Train Loss: 0.00007100\n","Iter: 027/050 | Train Loss: 0.00005701\n","Iter: 028/050 | Train Loss: 0.00005062\n","Iter: 029/050 | Train Loss: 0.00004852\n","Iter: 030/050 | Train Loss: 0.00004512\n","Iter: 031/050 | Train Loss: 0.00003788\n","Iter: 032/050 | Train Loss: 0.00002991\n","Iter: 033/050 | Train Loss: 0.00002892\n","Iter: 034/050 | Train Loss: 0.00003067\n","Adjusting Layer 1, Kernel Nodes: 599, Adptive Nodes:201\n","Iter: 035/050 | Train Loss: 0.00002485\n","Iter: 036/050 | Train Loss: 0.00001843\n","Iter: 037/050 | Train Loss: 0.00001949\n","Adjusting Layer 1, Kernel Nodes: 479, Adptive Nodes:321\n","Iter: 038/050 | Train Loss: 0.00002020\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 039/050 | Train Loss: 0.00001503\n","Iter: 040/050 | Train Loss: 0.00001261\n","Iter: 041/050 | Train Loss: 0.00001363\n","Adjusting Layer 1, Kernel Nodes: 504, Adptive Nodes:296\n","Iter: 042/050 | Train Loss: 0.00001187\n","Iter: 043/050 | Train Loss: 0.00000904\n","Iter: 044/050 | Train Loss: 0.00000867\n","Iter: 045/050 | Train Loss: 0.00000870\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 046/050 | Train Loss: 0.00000693\n","Iter: 047/050 | Train Loss: 0.00000583\n","Iter: 048/050 | Train Loss: 0.00000618\n","Adjusting Layer 1, Kernel Nodes: 556, Adptive Nodes:244\n","Iter: 049/050 | Train Loss: 0.00000526\n","\n","Iter: 049/050 | Test Loss: 0.00094561 | Test acc: 65.5600\n","scale:1.040000,therd:0.940000\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 000/050 | Train Loss: 0.00342335\n","Iter: 001/050 | Train Loss: 0.00163057\n","Iter: 002/050 | Train Loss: 0.00155795\n","Iter: 003/050 | Train Loss: 0.00083173\n","Iter: 004/050 | Train Loss: 0.00094370\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 005/050 | Train Loss: 0.00076405\n","Iter: 006/050 | Train Loss: 0.00055749\n","Iter: 007/050 | Train Loss: 0.00053439\n","Iter: 008/050 | Train Loss: 0.00056998\n","Adjusting Layer 1, Kernel Nodes: 727, Adptive Nodes:73\n","Iter: 009/050 | Train Loss: 0.00052792\n","Iter: 010/050 | Train Loss: 0.00042097\n","Iter: 011/050 | Train Loss: 0.00035231\n","Iter: 012/050 | Train Loss: 0.00035397\n","Adjusting Layer 1, Kernel Nodes: 734, Adptive Nodes:66\n","Iter: 013/050 | Train Loss: 0.00035746\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 014/050 | Train Loss: 0.00029667\n","Iter: 015/050 | Train Loss: 0.00023148\n","Iter: 016/050 | Train Loss: 0.00021981\n","Iter: 017/050 | Train Loss: 0.00022511\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 018/050 | Train Loss: 0.00019628\n","Iter: 019/050 | Train Loss: 0.00015083\n","Iter: 020/050 | Train Loss: 0.00013207\n","Iter: 021/050 | Train Loss: 0.00013571\n","Adjusting Layer 1, Kernel Nodes: 739, Adptive Nodes:61\n","Iter: 022/050 | Train Loss: 0.00012316\n","Iter: 023/050 | Train Loss: 0.00009509\n","Iter: 024/050 | Train Loss: 0.00008245\n","Iter: 025/050 | Train Loss: 0.00008134\n","Iter: 026/050 | Train Loss: 0.00007544\n","Iter: 027/050 | Train Loss: 0.00006450\n","Iter: 028/050 | Train Loss: 0.00005245\n","Iter: 029/050 | Train Loss: 0.00004522\n","Iter: 030/050 | Train Loss: 0.00004618\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 031/050 | Train Loss: 0.00004379\n","Iter: 032/050 | Train Loss: 0.00003290\n","Iter: 033/050 | Train Loss: 0.00002743\n","Iter: 034/050 | Train Loss: 0.00002977\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 035/050 | Train Loss: 0.00002853\n","Iter: 036/050 | Train Loss: 0.00002201\n","Iter: 037/050 | Train Loss: 0.00001804\n","Iter: 038/050 | Train Loss: 0.00001843\n","Adjusting Layer 1, Kernel Nodes: 685, Adptive Nodes:115\n","Iter: 039/050 | Train Loss: 0.00001841\n","Iter: 040/050 | Train Loss: 0.00001516\n","Iter: 041/050 | Train Loss: 0.00001202\n","Iter: 042/050 | Train Loss: 0.00001182\n","Iter: 043/050 | Train Loss: 0.00001225\n","Adjusting Layer 1, Kernel Nodes: 774, Adptive Nodes:26\n","Iter: 044/050 | Train Loss: 0.00001024\n","Iter: 045/050 | Train Loss: 0.00000784\n","Iter: 046/050 | Train Loss: 0.00000778\n","Iter: 047/050 | Train Loss: 0.00000810\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 048/050 | Train Loss: 0.00000660\n","Iter: 049/050 | Train Loss: 0.00000529\n","\n","Iter: 049/050 | Test Loss: 0.00093804 | Test acc: 65.5700\n","tensor(3.4226e-06) 0.88 0.8400000000000001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2qB3P29ZN5FG","colab_type":"code","colab":{}},"source":["tensor(3.5891e-06) 0.92 0.8200000000000001. det2.\n","tensor(3.5231e-06) 0.9400000000000001 0.8400000000000001 det1\n","tensor(3.7707e-06) 0.86 0.8600000000000001 det0\n","tensor(3.4226e-06) 0.88 0.8400000000000001. d det1\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"raeHw-BchWAx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597600881803,"user_tz":-120,"elapsed":131019,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"2ff7d6e3-8d7f-44af-8015-b3239240f7de"},"source":["step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(0.88,0.84,2,initialize = 'LeCun', batchnorm = False, learning_rate = 0.1, ** shared_model_param_dict)"],"execution_count":134,"outputs":[{"output_type":"stream","text":["Adjusting Layer 1, Kernel Nodes: 668, Adptive Nodes:132\n","Iter: 000/100 | Train Loss: 0.00371951\n","Iter: 001/100 | Train Loss: 0.00206158\n","Iter: 002/100 | Train Loss: 0.00146285\n","Iter: 003/100 | Train Loss: 0.00089450\n","Iter: 004/100 | Train Loss: 0.00098715\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 005/100 | Train Loss: 0.00074759\n","Iter: 006/100 | Train Loss: 0.00056731\n","Iter: 007/100 | Train Loss: 0.00056348\n","Iter: 008/100 | Train Loss: 0.00059008\n","Adjusting Layer 1, Kernel Nodes: 765, Adptive Nodes:35\n","Iter: 009/100 | Train Loss: 0.00052425\n","Iter: 010/100 | Train Loss: 0.00041020\n","Iter: 011/100 | Train Loss: 0.00035297\n","Iter: 012/100 | Train Loss: 0.00036342\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 013/100 | Train Loss: 0.00035318\n","Iter: 014/100 | Train Loss: 0.00027621\n","Iter: 015/100 | Train Loss: 0.00021974\n","Iter: 016/100 | Train Loss: 0.00022089\n","Adjusting Layer 1, Kernel Nodes: 450, Adptive Nodes:350\n","Iter: 017/100 | Train Loss: 0.00021216\n","Iter: 018/100 | Train Loss: 0.00016118\n","Iter: 019/100 | Train Loss: 0.00012762\n","Iter: 020/100 | Train Loss: 0.00013062\n","Adjusting Layer 1, Kernel Nodes: 601, Adptive Nodes:199\n","Iter: 021/100 | Train Loss: 0.00011519\n","Iter: 022/100 | Train Loss: 0.00008294\n","Iter: 023/100 | Train Loss: 0.00007926\n","Iter: 024/100 | Train Loss: 0.00007686\n","Iter: 025/100 | Train Loss: 0.00005909\n","Iter: 026/100 | Train Loss: 0.00005157\n","Iter: 027/100 | Train Loss: 0.00005107\n","Iter: 028/100 | Train Loss: 0.00004426\n","Iter: 029/100 | Train Loss: 0.00003750\n","Iter: 030/100 | Train Loss: 0.00003398\n","Iter: 031/100 | Train Loss: 0.00003210\n","Iter: 032/100 | Train Loss: 0.00002940\n","Iter: 033/100 | Train Loss: 0.00002411\n","Iter: 034/100 | Train Loss: 0.00002135\n","Iter: 035/100 | Train Loss: 0.00002188\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 036/100 | Train Loss: 0.00001788\n","Iter: 037/100 | Train Loss: 0.00001448\n","Iter: 038/100 | Train Loss: 0.00001539\n","Adjusting Layer 1, Kernel Nodes: 447, Adptive Nodes:353\n","Iter: 039/100 | Train Loss: 0.00001196\n","Iter: 040/100 | Train Loss: 0.00001002\n","Iter: 041/100 | Train Loss: 0.00001068\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 042/100 | Train Loss: 0.00000733\n","Iter: 043/100 | Train Loss: 0.00000817\n","Adjusting Layer 1, Kernel Nodes: 392, Adptive Nodes:408\n","Iter: 044/100 | Train Loss: 0.00000671\n","Iter: 045/100 | Train Loss: 0.00000606\n","Iter: 046/100 | Train Loss: 0.00000616\n","Adjusting Layer 1, Kernel Nodes: 423, Adptive Nodes:377\n","Iter: 047/100 | Train Loss: 0.00000441\n","Iter: 048/100 | Train Loss: 0.00000507\n","Adjusting Layer 1, Kernel Nodes: 385, Adptive Nodes:415\n","Iter: 049/100 | Train Loss: 0.00000342\n","Iter: 050/100 | Train Loss: 0.00000384\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 051/100 | Train Loss: 0.00000290\n","Iter: 052/100 | Train Loss: 0.00000296\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 053/100 | Train Loss: 0.00000254\n","Iter: 054/100 | Train Loss: 0.00000243\n","Iter: 055/100 | Train Loss: 0.00000200\n","Iter: 056/100 | Train Loss: 0.00000195\n","Iter: 057/100 | Train Loss: 0.00000160\n","Iter: 058/100 | Train Loss: 0.00000164\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 059/100 | Train Loss: 0.00000135\n","Iter: 060/100 | Train Loss: 0.00000136\n","Adjusting Layer 1, Kernel Nodes: 306, Adptive Nodes:494\n","Iter: 061/100 | Train Loss: 0.00000125\n","Iter: 062/100 | Train Loss: 0.00000107\n","Iter: 063/100 | Train Loss: 0.00000112\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 064/100 | Train Loss: 0.00000086\n","Iter: 065/100 | Train Loss: 0.00000092\n","Adjusting Layer 1, Kernel Nodes: 308, Adptive Nodes:492\n","Iter: 066/100 | Train Loss: 0.00000072\n","Iter: 067/100 | Train Loss: 0.00000070\n","Iter: 068/100 | Train Loss: 0.00000064\n","Iter: 069/100 | Train Loss: 0.00000053\n","Iter: 070/100 | Train Loss: 0.00000057\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 071/100 | Train Loss: 0.00000044\n","Iter: 072/100 | Train Loss: 0.00000046\n","Adjusting Layer 1, Kernel Nodes: 310, Adptive Nodes:490\n","Iter: 073/100 | Train Loss: 0.00000038\n","Iter: 074/100 | Train Loss: 0.00000034\n","Iter: 075/100 | Train Loss: 0.00000034\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 076/100 | Train Loss: 0.00000026\n","Iter: 077/100 | Train Loss: 0.00000031\n","Adjusting Layer 1, Kernel Nodes: 310, Adptive Nodes:490\n","Iter: 078/100 | Train Loss: 0.00000023\n","Iter: 079/100 | Train Loss: 0.00000026\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 080/100 | Train Loss: 0.00000022\n","Iter: 081/100 | Train Loss: 0.00000021\n","Iter: 082/100 | Train Loss: 0.00000020\n","Iter: 083/100 | Train Loss: 0.00000016\n","Iter: 084/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 310, Adptive Nodes:490\n","Iter: 085/100 | Train Loss: 0.00000014\n","Iter: 086/100 | Train Loss: 0.00000015\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 087/100 | Train Loss: 0.00000013\n","Iter: 088/100 | Train Loss: 0.00000012\n","Iter: 089/100 | Train Loss: 0.00000012\n","Iter: 090/100 | Train Loss: 0.00000010\n","Iter: 091/100 | Train Loss: 0.00000011\n","Adjusting Layer 1, Kernel Nodes: 310, Adptive Nodes:490\n","Iter: 092/100 | Train Loss: 0.00000009\n","Iter: 093/100 | Train Loss: 0.00000009\n","Adjusting Layer 1, Kernel Nodes: 490, Adptive Nodes:310\n","Iter: 094/100 | Train Loss: 0.00000008\n","Iter: 095/100 | Train Loss: 0.00000007\n","Iter: 096/100 | Train Loss: 0.00000007\n","Iter: 097/100 | Train Loss: 0.00000006\n","Iter: 098/100 | Train Loss: 0.00000007\n","Adjusting Layer 1, Kernel Nodes: 310, Adptive Nodes:490\n","Iter: 099/100 | Train Loss: 0.00000006\n","\n","Iter: 099/100 | Test Loss: 0.00100217 | Test acc: 64.0400\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TQlWw-DTcT1E","colab_type":"text"},"source":["# No scaling, no batch norm"]},{"cell_type":"code","metadata":{"id":"TzjNEnGGcT1E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597601006278,"user_tz":-120,"elapsed":124468,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"547f6cb3-0942-46e9-99f3-d16f0457f6de"},"source":["step_list, loss_no_scale_no_bn,train_loss = full_batch_train(initialize = 'LeCun', batchnorm = False, learning_rate = 0.1, ** shared_model_param_dict)"],"execution_count":135,"outputs":[{"output_type":"stream","text":["Iter: 000/100 | Train Loss: 0.00340580\n","Iter: 001/100 | Train Loss: 0.00159687\n","Iter: 002/100 | Train Loss: 0.00157065\n","Iter: 003/100 | Train Loss: 0.00082164\n","Iter: 004/100 | Train Loss: 0.00092534\n","Iter: 005/100 | Train Loss: 0.00077363\n","Iter: 006/100 | Train Loss: 0.00056668\n","Iter: 007/100 | Train Loss: 0.00053111\n","Iter: 008/100 | Train Loss: 0.00055974\n","Iter: 009/100 | Train Loss: 0.00052726\n","Iter: 010/100 | Train Loss: 0.00043010\n","Iter: 011/100 | Train Loss: 0.00035524\n","Iter: 012/100 | Train Loss: 0.00034616\n","Iter: 013/100 | Train Loss: 0.00035400\n","Iter: 014/100 | Train Loss: 0.00030334\n","Iter: 015/100 | Train Loss: 0.00023320\n","Iter: 016/100 | Train Loss: 0.00021664\n","Iter: 017/100 | Train Loss: 0.00022575\n","Iter: 018/100 | Train Loss: 0.00020093\n","Iter: 019/100 | Train Loss: 0.00015484\n","Iter: 020/100 | Train Loss: 0.00013146\n","Iter: 021/100 | Train Loss: 0.00012997\n","Iter: 022/100 | Train Loss: 0.00012480\n","Iter: 023/100 | Train Loss: 0.00010707\n","Iter: 024/100 | Train Loss: 0.00008404\n","Iter: 025/100 | Train Loss: 0.00007350\n","Iter: 026/100 | Train Loss: 0.00007421\n","Iter: 027/100 | Train Loss: 0.00006921\n","Iter: 028/100 | Train Loss: 0.00005400\n","Iter: 029/100 | Train Loss: 0.00004178\n","Iter: 030/100 | Train Loss: 0.00004146\n","Iter: 031/100 | Train Loss: 0.00004256\n","Iter: 032/100 | Train Loss: 0.00003745\n","Iter: 033/100 | Train Loss: 0.00002922\n","Iter: 034/100 | Train Loss: 0.00002321\n","Iter: 035/100 | Train Loss: 0.00002292\n","Iter: 036/100 | Train Loss: 0.00002390\n","Iter: 037/100 | Train Loss: 0.00002085\n","Iter: 038/100 | Train Loss: 0.00001594\n","Iter: 039/100 | Train Loss: 0.00001391\n","Iter: 040/100 | Train Loss: 0.00001400\n","Iter: 041/100 | Train Loss: 0.00001410\n","Iter: 042/100 | Train Loss: 0.00001268\n","Iter: 043/100 | Train Loss: 0.00000962\n","Iter: 044/100 | Train Loss: 0.00000817\n","Iter: 045/100 | Train Loss: 0.00000852\n","Iter: 046/100 | Train Loss: 0.00000847\n","Iter: 047/100 | Train Loss: 0.00000757\n","Iter: 048/100 | Train Loss: 0.00000605\n","Iter: 049/100 | Train Loss: 0.00000493\n","Iter: 050/100 | Train Loss: 0.00000500\n","Iter: 051/100 | Train Loss: 0.00000519\n","Iter: 052/100 | Train Loss: 0.00000453\n","Iter: 053/100 | Train Loss: 0.00000363\n","Iter: 054/100 | Train Loss: 0.00000316\n","Iter: 055/100 | Train Loss: 0.00000306\n","Iter: 056/100 | Train Loss: 0.00000311\n","Iter: 057/100 | Train Loss: 0.00000285\n","Iter: 058/100 | Train Loss: 0.00000223\n","Iter: 059/100 | Train Loss: 0.00000185\n","Iter: 060/100 | Train Loss: 0.00000186\n","Iter: 061/100 | Train Loss: 0.00000186\n","Iter: 062/100 | Train Loss: 0.00000169\n","Iter: 063/100 | Train Loss: 0.00000141\n","Iter: 064/100 | Train Loss: 0.00000114\n","Iter: 065/100 | Train Loss: 0.00000109\n","Iter: 066/100 | Train Loss: 0.00000116\n","Iter: 067/100 | Train Loss: 0.00000105\n","Iter: 068/100 | Train Loss: 0.00000087\n","Iter: 069/100 | Train Loss: 0.00000076\n","Iter: 070/100 | Train Loss: 0.00000070\n","Iter: 071/100 | Train Loss: 0.00000072\n","Iter: 072/100 | Train Loss: 0.00000071\n","Iter: 073/100 | Train Loss: 0.00000059\n","Iter: 074/100 | Train Loss: 0.00000050\n","Iter: 075/100 | Train Loss: 0.00000049\n","Iter: 076/100 | Train Loss: 0.00000049\n","Iter: 077/100 | Train Loss: 0.00000047\n","Iter: 078/100 | Train Loss: 0.00000042\n","Iter: 079/100 | Train Loss: 0.00000036\n","Iter: 080/100 | Train Loss: 0.00000034\n","Iter: 081/100 | Train Loss: 0.00000035\n","Iter: 082/100 | Train Loss: 0.00000034\n","Iter: 083/100 | Train Loss: 0.00000030\n","Iter: 084/100 | Train Loss: 0.00000027\n","Iter: 085/100 | Train Loss: 0.00000025\n","Iter: 086/100 | Train Loss: 0.00000025\n","Iter: 087/100 | Train Loss: 0.00000025\n","Iter: 088/100 | Train Loss: 0.00000023\n","Iter: 089/100 | Train Loss: 0.00000020\n","Iter: 090/100 | Train Loss: 0.00000019\n","Iter: 091/100 | Train Loss: 0.00000019\n","Iter: 092/100 | Train Loss: 0.00000019\n","Iter: 093/100 | Train Loss: 0.00000018\n","Iter: 094/100 | Train Loss: 0.00000016\n","Iter: 095/100 | Train Loss: 0.00000015\n","Iter: 096/100 | Train Loss: 0.00000015\n","Iter: 097/100 | Train Loss: 0.00000014\n","Iter: 098/100 | Train Loss: 0.00000014\n","Iter: 099/100 | Train Loss: 0.00000013\n","\n","Iter: 099/100 | Test Loss: 0.00091641 | Test acc: 66.0200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-BHkAbB4cT1G","colab_type":"code","colab":{},"outputId":"33f721c3-e1ad-4218-f8bb-ac0bb81491bb"},"source":["title = '784-800-10; no scaling; no bn; full batch; train data size = 64'\n","# fig = plot_loss_acc(step_list, loss_no_scale_no_bn, aclist_no_scale_no_bn, title, fig_save_path)\n","fig = plot_loss(step_list, loss_no_scale_no_bn, title, fig_save_path)\n","\n","del title"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAhsAAADdCAYAAAASCbA9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVdrA8d+TTiqEEoqEoFIEFAs2QNeGBXVt61pRdNVV322uu6/iKmJZddl13X3XXhB7XVx111UpgmJBsaAgiNJ7J4GEkPa8f5w7yWQyk0xCJpOZeb6fTz6Ze+655z5n6nPPbaKqGGOMMcZESlK0AzDGGGNMfLNkwxhjjDERZcmGMcYYYyLKkg1jjDHGRJQlG8YYY4yJKEs2jDHGGBNRlmyYmCQiM0VkZkCZisiE6EQUXSIywev/XlGO45ci8r2IVIrI9hYsX+81FJFjvLJjmljOV+/i5kcdGe0xpnCE+5y30rrGeusqivS6Wlsif9+0RJPJhohM9p7UUH8XBdTvLyIvisgqESkTkcUi8icRyW9iPbd47f3QnA40Z30i0l1EnhWRLSKyU0RmiMghjbT7hogUi0iJiLwuIvs0I64BIvJXEflAREqb+vDu6fqMiTbv/f1/wJfAFcBVUQ2oFXhJ3I+jHYc/ESny4jog2rFEk4gc6D0PhdGOJZaJSKaI3O79dpaLyAYReauxBFBEOonIpuYk0ylh1HkEmBakfDzQ13+eiPQFPgOKgYeATcAhwPXACSJyqKrWBAm8CBgHlIYTdEvWJyJZwHtAAXCvt8z/AO+JyGGqusiv3Z7AB8Bu4DZAgOuA90XkQFXdFEZ4RwK/ARYB3wCHN9KP1lifgQ5AVbSDSGAneP+vUtVmj2q0U7cCTwBvRDsQP0W4uH4Avo5A++/jPksVEWi7NR2Iex6mASujsP6Y/74RkWxgBtAPeAz3e9UJOAzIB5aHWPQuXP/D1mSyoaofAx8HBNgNeBJ4V1U3+M26DMgFRqrqN17ZYyJSAvweGIrb6gn0d2A2kAY0Zxi4Oeu7GhgIHKeq73n9eAlYDNwJ/MSv3XG4J3yIqi726v4HmA/cAPwujNjeADqqaomX+YVMNlppfQlPVcujHUOC6wYQR4lGXBCRLFUNe0PO20Czz1IT4uT75o9Af2CYqoa1V8HbG3AlLtG7M9wVtfSYjQtxicrTAeW53v91AeW+6bLAhkRkNDAa+HUL4mjO+n4KLPAlGgDeiMHLwGkikulX91xcIrXYr+4iYDpwXjiBqepWVS0JqxfNWJ+IdBGRgQHxBiUiy0VkmogcKiIfisgub3fTb4PUzRCRu7xlKrz/d4lIehjryfJ2XS3xhuG2iMgnIvKTgHoFIvKAiKwUkd0islpEnheRXt78NBG5TUQ+FZGtXrxficjYpmLwlg/c3+/bH3ysiNwtIuu9Nqd6o2KBy58pIt94fVgsIld6w7QaUK/dvQZ+OorIUyKyzdsd94K3ceC/npki8oOI7Csi74jbzbdRRO4RkaSAumH11XuOrvQ99n8tvH5MDrLMZBFZ3oy+hSNJRG72nuNdIjJbAnaVikgfEblfRBZ6fS/xXqPhfnWK/F73n/n1abJfnUbfz/VXKdeLyArvvfWJiBwcGLj3PDe6S8D7LPi+w57xi2usN9/32g7x3uc7gOe8eUeJ2+W83It3o7jdynsFrKPBMRvea1Xl9fll7znbJiKPi0hYW7gicoSIfOQ9BytF5EbcKG5gvR+L26W82u95fUhEOvrVmYDb4AX4wO95OCbcNpqI9RwRmSNut3ap95w+FFAn8PtmuYQ+3GCsX70cEZkoIsvEfc5Xisifw30eW4uI5OI+s4+p6g8iktpUDCIiwAPAi8CHzVlfOLtRgrkEKAFeDyifiUsanhSRW4GNwDDc1vmrqvqdf2URycDt471fVRe6fjRLWOvzvkAPAF4I0sanuH3Lg4HPvC+KAq88WN0TRaRra+3aaMH6foHLKI/F9b8phcCbwDPAs7jk5V4RWaCq73gxCDAFOMWr8zEwHDfisj9wehPreBC4wPs/H5cEHogbzXnVW0cBMAfoCTwOzAO6AqcC+wJrvOWuBl4CngJSgTNxr2+qqj4WRn+D+TNuS+0uoAtupOg5r4948Z0M/BM30nUzbojwDmB9kPba42vg8yywBbgFNzR6LbCfuF2F/sPiObjh5/8CrwEn4T43y3C7Tpvb1zHAz4BjvMcQmSH+pvwWyMR9r2Tg4p8hIsNU9XuvzqG4/kwBVuDeEz/zqzcft0t2DO41m4nblQKwBMJ+P/v8Gkj3YkrBjbq+JiL7qmqlX72FwCzccxjK+7j38U24XccfeeUf+dXJBabiRldfAXZ55T8FOnvxbsBt0V4FHC4iB6jqLhonwNvAt7j3yqG4520T7n0aekGRQbj32w7c1nCFt+6dQapfDlQD9+Peywd669kfGOnVmQL08Nq4A/e5BfcchttGqFiPxz1vM4E/AJXA3rjPZmN+A2QHlPk+Exu9tjOo223xKG5X2AHeskNEZLQ2csMycRseOU3E4VMc8P4KdBTue+47EXkROAdIEZGvgN/6b5j7uQIY4tXtF2Ycjqo26w/3o6zA4yHmT8Ade6F+f48AyUHq3op70+d50zOBH5oZT5Prw32ZKPDHIMuP9uad4U0P86avDFL3Wm/e0GbGeLG33DFB5jVrfV5/g7YVZPnlXt2T/crSvef8Fb+y07x6dwcs/2evfHQT69kGPNBEnUleW8cGmSfe/2QgPcj8qcD3AWUzgZkBZQpM8Jse65V9GPB++I1XPtivbB7uC6GTX9kA3BeNBnnPtbfXwBfTjIC+XumV/zzguVPg6oA2vgI+24O+Ph74XPk9B5ODlE8GljfxGh4Tzvr96m0BOvuV74fbr/6CX1lmkOXzvdf/sSDxNPiuC/P97ItpKdDBb/6ZXvmpQdY1s7F+BrR7cZB5vtf2t0HmBev3SK/+RY09595rpcA9Acv/C9gURsz/xH2W+vuVdQW2e+0WNRGn7zt0hF/ZWK9sZJh9bdBGiFjvwx3T1+A3q7H3apD5J+MSnkf9ysbhNnz2D6h7ldfeqCbW6etzOH9NfWZ834ObgLne83M5LgGqAA4J8hnZDIxr6n0Y7K8lu1Eu8f4H7kLxWYEb/r8SOBs35PIz3JdmLXHD2DcCN6lqcQviaM76fENDu4MsXx5Qpzl1W0Oz1qeqE1RVVHVmmO0vV9W3/ZbfDXyCy9R9TvP+/yVg2YkB80PZjts66h1spjeydDYwVYNky+r75KpWe/HhDenli0gX3A/oviKS10QcoTyiqtV+07O8/3t76+qJ27p4VlW3+cX1HW5LLjDe9vga+PwjoK+Tca9P4PKVuOTA36yAmFrS12h7VlW3+CZUdSHwDjDaGz1CVWt3r4pIBxHpjNul/CnuAPNGhft+9jNZ648a1Hv/+S0nqnpMU+sPQw3wcJC4/Pud4322FuHeH0322/NgwPQsoIuIhNzaFpFk3A/vW1p/V/EmvF08weIUJ9eL0zdkH1ace9jGdiALOMX3nmkuEemP29XwCW50zec83KjlOnG7KLt4sflOtDiuiabfAUaF+TevibZ8ozCCS5qfVdVJuFE/xY3q+Lsb99z8tYl2g2rWbhTvQ3Yhbqj1gyDzf4Ubreinqlu94tfEnW//BxF5WlW/8sr/Dzfk/mQT68wmYGhKVdc3c32+D3qwfd8Z3v9dAf+brOv9+PknHtXa/N0rzYmtJZYHKduG+3H1KcJtnWzxr6Sqm0RkM+6so8ZcjxtuXiEiX+NGIl5U1c+9+V2BPNxZOY0SkUu99gbT8JiiPNwWR3OtCJj2JRS+06P7eP+/p6FgZc21PEhZa78GPvV2VapqpYgs89r3t0ZVA4+k30bdcxKrvgtRNhrXty0ikob73hgDBCbIy8JYR9jvZ0+995+qbvN+wyL1XK/3Tyx8vKR6Ii7xDEzcwzmWoQZYHVDm/1naEWK5rrhdW6Fem8A4BwJ/wp3dFHisULjHXOxJGw/idhO8CWwQkfdwu6Re1cZ3S/jWnYc7xGAncI7W333ZH/ebEep3oluIcgBUdR0Nj1FsKd/vyhuqWvvaqeoqEXkfv91NInIobhfKmb4NwuZq7jEbx+HOFrkjSPYO7nTNWX4//D5TcFnSSOArETkW94a/ECj0Sx4zcPuMioCdqroZt3/91oD2fAuEtT5gK27koGeQmHt4/9d6/30vZDh1/w5c6jd/BQ2/1JvSnPW1RHWI8nAzdsFluSGp6hQRmY07ruAE3FDc9SLyB1W9229djbYjIufhtsT/g8ueN+C2wEfjXuuWHtC8J89Bi7ZsWnH9vnqNPnd+gtULtp5QMUVCqNiT22hdgf3/O27Y+gHc1u423A/pOCCca9uE9X72s6evf3M12DjxNhSnAt1xo77f4n4MFbcFHs5nSzXIpQt8q2hkucaer3rLeQctzsKN6o7HJftluPfK2+HEuadteAn+wbjfu5OBE4Hzgd+LyMhgiZzfupOA53G/A0f7Noz9JHmxhTqLo9Hveu8AznBHeLcGJDqh1rUhyLwNuDMkfe7F7Wr5Ruquv9Hd+9/FK1sdZAOmVnOTDd+BX6F2ofQk+EFhKQH/fVuSz4doZxnuAMGx3rpm78n6VLVGRObhDmgKdDguEfnWq7tGRDY2UneN3+jFRNwBeT7NHoFo5voiZTlwkoh09t+y9ob3OhP6XOtaqroRdxDdE+LOXPgPcJuI/AW3L7yY+lvywZyPe+1P909mRaSpocU95dvyDHbAU/MOgmq55ezha+AZSN1BcohIKu6LL9RnqC1sI/jWZFEE1jUwSFl/3AHtvo2S84GnVfVX/pVE5PYw1xHu+zlSwk1y/O0PDALGqupTvkLvx6tTyKVax0bcj32o18bfsbit+2NU1be7ybdbIlCo56E5bQTl/Wi+6/0hItfgRjzOxf02hXI3buNorKp+FmT+D0Cuqga7dlU4zqOJvQF+mjqo2zfyHOxyE3tRf/SlEPe7HWzk7z7vry+NfE+FvaUo7qJYZwMfa+jzcb8DjpWGp375rjDm69wM4KwgfwtwR/+fhdvNgqouVdVp/n8tWB+4syIGS/3Tubri3jxvaf3z0F/FnQXS36/uQFym+7KvTFW/DYitWacCNXd9EfSm9z/wdMzfB8xvQESSA4+l8DL/73Bnk2R5W0NTgFHeqFZgG76tG99WU5LfvM64kZKIUdW1uKT1YhGp/eIVkQG4szTaQotfgwC/9PaR+4zF/dD/p+Wh7bEfgCPF7xReb8txeOhFWuxi7z3jW89+uNfwv34JbA0B330ichRwRJD2SglIlJrxfo4U33dVWLsUPA0+W57rg5S1Ku8YIt9xM/7fcV1xo9v+QsX5exoK9Tw0p40G/N8/fnzXawr5nIvIBcD/An/zT+gCvAgcJCJnB1k+o7FjXzytdsyGd/zMF8AZ4nd6vPfbM9Jbl89VNPy9vsWb9zdvemNj62vOyMZZuGMnQo1qgLtAyIvAHHHnJG8BjsddMGu6qn4AoKorCXLFNxH5DZChqv8KM6aw1ud5CHcQ6RRva9t3BdFU3KmO/u7CJSHTROQ+3FDfb3FP5p/CCcz7Af6lNznU+z9GREYCqKr/MFrY6xN3XvetuAN6ZoYTSxjewg0v3iTunPs5uC/eMcC/VfW/jSybA6wRkddwb+6twEG4/Xv/1boLPN2E+wC8IyK+UwXzcVsBN+OGFl/HJbT/FpF/4bZOrsIN9xW0Ul9DGQf8G/hIRJ6g7rTJ+bjT5mq1w9fAXz7wrvd67It7jzd5bFQordTXR/CuJSPuFLteuNd1PuEPCYdrFfCxiDyGOw7ql7gRx/F+dV4HLhWRnbjdrPvh3q8LaHha4ee4DYHrcLs8l6nqHMJ7PzeLuOt6zArjINGFXp+uFZFduB/dOara2PEmC3Gnh94r7loeG4AfASNw35uRNh6X9M0Skftxu0evwo0q+v+Af4g74+FpEfkHbkTkNIIfy/AFbnRjnJcg7MZtyDanjWAe9358p+N+p7rgTskvJcSVZL31P+Gt9ytpeAnvj1R1Ke4A8NOAV0TkWdxByam4EZ6f4n67ZoYKrJWP2QB3Rsp03Pfew7gLa/4K9/t4m9963w1cUOruffR5WL/ZGsYpK94GwTu4F7NTE/WOwQ09rcOdPrMEuIcgpyIFWXYmzT/1Nez14Xa7PI/7QSzFXRxnWIh2B+B+fEq8vzeAfZsRVxGNnJbU0vXh3qw1wMAwYlgOTAtSPpmGpxx2wA0BrvCexxW4JCijiXWk4RKiz3HD5WW4I9xvB7ID6vbAnVu+1lvHKtxuqJ5+da7DbQmX474cf03d6V5FAe+VmQHt1zsVjRCnxvm9NmMDys/G/QDu9tZ9mfd872rPr4G37ASvT0NwGwTbcAfsvQR0D+dz5mtjD/oa9NRXb94vvf6Ue++V40M8B4Gv4TE079TXS3BbXKu9dX0IHBpQNwc3JL7Oe7/Owe2XDxbP/rhdUGVe+5PDfT/T+Cmqgf3M9speaKyffvXPxSUQlf7v5VCvrTevH26Eazvux+QN3DEqywP61eA5956bqiBtjiXgs9lIzMNxZ2KU437Eb8R9xgI/28NwydoOXCL0LC5RqPeceXV/gRvar/KPuTltBInzHFzyvw73XbAGN/o8NKBebVs08X2P33cN7oDVCbjvyd1efHO9svxwXv/W/AOOxl2/pdR7X/wLGBDGciHf38H+fOeDmxghIp8CK1T13GjHkghE5HVgkKr28ytLmNcgkfoaTeKupPxv3A9auGe5GBMzWnoFURMF3lHWQ6l/BoxpBSLiO5i4yq9sIG5YfJJfWcK8BonU13bgWNzp4pZomLhkIxvGAN5xEu/jhltX4S64dDXudLmDVHVJFMMzxpiYZiMbxjjFuKv9XYbbt7sLt6/+D5ZoGGPMnrGRDWOMMcZEVETPrzbGGGOMsWTDGGOMMRFlx2yYduHkk0/Wt99ucINVY0zkRfqKp8bYyIZpHzZv3hztEIwxxkSIJRvGGGOMiShLNowxxhgTUZZsmNhRtRvWfN50PWOMMe2KJRum/aupgW9ehfuHwdNnQmlb3KTSGGNMa7Fkw7R/WgMz74HtK2F3Cbw/MdoRGWOMaQZLNkz7l5wCo26rm/7scdhiVxA3xphYYcmGiQ0DRrO71xHucU0VTL+t8frGGGPaDUs2TEyY/cMWLlh+al3Bt6/Dqk+jF5AxxpiwWbJhYsIhfTqxNH0g/64+oq7w3ZvBbiRojDHtniUbJiZ0SEvmgsMK+VPVeVRositcNQcW/Tu6gRljjGmSJRsJQkTyReQ1ESkVkRUicmEjda8TkfUiUiwik0QkPZx2RGSQiMwVkW3e3zQRGdRafbjkyD6sle48U31iXeHUW6G6srVWYUy7dfXVV3PHHXdEOwxjWsSSjcTxAFABFAAXAQ+JyODASiJyEnAjcDxQBOwN+B+N2Vg7a4GfAPlAF+AN4MXW6kCPvA6M3r8H/6g6kxLNdIVbl8Dnk1trFcZETFFREdOmTWvx8g8//DC33HJLK0ZkTNuxZCMBiEgWcA5wi6ruVNXZuERgTJDqlwJPqOoCVd0G3AGMDacdVd2uqstVVXF3kqwG9m3Nvlw2oojt5HB/1Rl1hTPvgfKS1lyNMW2qqqoq2iEYE1GWbCSG/kC1qi72K5sHNBjZ8MrmBdQrEJHO4bYjItuBcuAfwF2hghKRq7zdLnM3bdoUVkcOLuzEgb078lT1SazWLq6wbDN8+LewljcmGsaMGcPKlSs5/fTTyc7OZuLEiYgITzzxBIWFhRx33HEAnHvuuXTv3p28vDyOPvpoFixYUNvG2LFjufnmmwGYOXMme+21F/feey/dunWjR48ePPnkk1HpmzHhSIl2AKZNZAPFAWXFQE4YdX2Pc8JtR1U7eqMglwIrQgWlqo8CjwIMGzYs7NNKLh/Zl1+9sJ2/VP6Uv6U96Ao/fgCG/QzyeoXbjEkA901dzN+nfx9W3QsO683dZx9Qr2zclK954dNVIZf59fH9uG5U/ybbfuaZZ/jggw94/PHHOeGEE1i+fDk33HADs2bNYuHChSQlue2+U045hUmTJpGWlsYNN9zARRddxFdffRW0zfXr11NcXMyaNWuYOnUqP/nJTzjzzDPp1KlTWP01pi3ZyEZi2AnkBpTlAjvCqOt7vKM57ahqKfAw8LSIdGtBzCGdMqQ73XMzeL1mON/UFLnCqnJ4L+QgijHt0oQJE8jKyqJDhw4AXH755eTk5JCens6ECROYN28excWB+b2TmprK+PHjSU1NZfTo0WRnZ/Pdd9+1ZfjGhM2SjcSwGEgRkX5+ZUOBBUHqLvDm+dfboKpbmtkOuPdXJtCqww2pyUmMObIPShJPZf+sbsZXz8H6+a25KmMiqnfv3rWPq6urufHGG9lnn33Izc2lqKgIgM2bNwddtnPnzqSk1A1OZ2ZmsnPnzojGa0xL2W6UBKCqpSIyBbhdRK4ADgTOAIYHqf40MFlEngPWATcDk8NpR0RGAZuBr4Es4E5gG7Cwtft04WGF7N8rj6P6jYbnP4Dv3wUUpo6HMVNae3UmRl03qn9YuzlCufvsAxrsWmkpEWm07Pnnn+f1119n2rRpFBUVUVxcTKdOnVC7cJ2JAzaykTiuBToAG4EXgGtUdYGIFIrIThEpBFDVt4GJwHu44y1WALc21Y43r6NXVgwswZ2JcrKqlrd2ZzplpXF0/67uy/qE20C8t/KS6bBkRmuvzpg9VlBQwNKlS0PO37FjB+np6XTu3JmysjJuuummNozOmMiyZCNBqOpWVT1TVbNUtVBVn/fKV6pqtqqu9Kv7V1UtUNVcVb1MVXc31Y437xVVHei111VVR6vq1xHvXMEgOPCiuul3x0NNdcRXa0xzjBs3jjvvvJOOHTvy6quvNph/ySWX0KdPH3r16sWgQYM44ogjgrRiTGwSG6Iz7cGwYcN07ty5LV5eS9ai/3cwSVW7XMGZD8OBF7RSdMbEtYb7d4xpZTayYWJadY0y5YvVnP7UEh6tGl03Y8adULkreoEZY4ypZcmGiWkC/GPGD8xfU8I/ykezKy3fzShZDZ88FNXYjDHGOJZsmJiWlCSMHV4EQCkdeFh+Wjdz9n1QGvy0QWOMMW3Hkg0T835yyF7kZLizuB8oHk5pzt5uxu4SmDUxipEZY4wBSzZMHMhKT+H8Q93FkapI4dH0S+pmzn0CtiyJUmTGGGPAkg0TJy45sogk75j6v6/uR1mPw91ETRVMvy16gRljjLFkw8SH3vmZnDiouzclPJnldxnzb1+HVZ9GJS5jjDGWbJg4ctmIotrH/7col4qBZ9bNfPdmsGvKGGNMVFiyYeLGYX3zGdzT3ZR2d1UNL+ddBkmpbuaqObDwzShGZ4wxicuSDRM3RITLR/StnX7wq2r00CvqKkybANWVbR+YMUBRURHTpk3bozYmT57MyJEjWykiY9qOJRsmrpw2tAdFnTMZO7yI5688AvnR/0J6npu5dQnMfTK6ARpjTAKyZMPElfSUZKZffwwTfjyYoi5ZkJkPR19fV2HWPVBeEr0ATUIaM2YMK1eu5PTTTyc7O5uJEyfyySefMHz4cDp27MjQoUOZOXNmbf3Jkyez9957k5OTQ9++fXnuuedYuHAhV199NR9//DHZ2dl07Ngxeh0yppnsRmymXdjTG7E1qrIc7j8Uir0b2x51PRw/PjLrMu3HhLw2XFdxk1WKiop4/PHHOeGEE1izZg0HHHAAzzzzDCeffDLTp0/n/PPPZ9GiRWRmZtKjRw8+++wzBgwYwLp169i6dSuDBw9m8uTJPP7448yePbs1o7cbsZmIs5ENE/9SM+D4W+qmP34AitdELx6T8J599llGjx7N6NGjSUpKYtSoUQwbNoy33noLgKSkJObPn8+uXbvo0aMHgwcPjnLExuwZSzZMXNtYUs5f3/2OdYWnQo+hrrCqHN77Y3QDMwltxYoVvPLKK3Ts2LH2b/bs2axbt46srCxeeuklHn74YXr06MGpp57KokWLoh2yMXskJdoBGBMpD81cwl+nfkdltVJVo/zvqDvg6R+7mV89D0dcA933j26QJnLC2LXRlkTq9lb07t2bMWPG8NhjjwWte9JJJ3HSSSexa9cubr75Zq688ko++OCDem0YE0tsZMPErb5dsqisdsckvfDpSnbtNRL6nejNVZh6a/SCMwmnoKCApUuXAnDxxRfz5ptv8s4771BdXU15eTkzZ85k9erVbNiwgTfeeIPS0lLS09PJzs4mOTm5to3Vq1dTUVERza4Y02yWbJi4NWpQAXt16gDAtrJK/vXVGhh1O4j3tl8yHZbMiGKEJpGMGzeOO++8k44dO/LSSy/x+uuvc9ddd9G1a1d69+7Nn//8Z2pqaqipqeHee++lZ8+e5OfnM2vWLB588EEAjjvuOAYPHkz37t3p0qVLlHtkTPjsbBTTLkTqbJTHP1jKnf9ZCED/gmze+c3RyJu/gi+edhUK9oefz4Kk5FZftzExwvbNmIizkQ0T1356aG+y0lwisXjDTj78YQsccxOkZroKG76Br1+KYoTGGBP/LNkwcS03I5Vzh/WunZ704TLI7QFH/qKu0ow7oXJXFKIzxpjEYMmGiXuXDi/CdxD/jEUbWbppJ4z4FWR1dYUla+CTB6MXoDHGxDlLNkzc69sli+MGdKudfuqj5ZCeA8eMq6v0wX1QurntgzPGmARgyYZJCJePrLsb7Cufr6Z4VyUcfCl06e8KK3bArIlRis4YY+KbJRsmIQzfpzMDCnIA6JaTzqqtZZCcAifcVldp7hOwZUmUIjTGmPhlVxA1CUFEuHH0QGpqlGMHdCMpyTuIY8Ap0GcErPgQaqpg2gQ475moxmqMMfHGRjZMwjh2QDeO36+gLtEAEIFRd9RNL3wDVs5p++CMMSaOWbJhzF6HwOCz66an3gJ2sTtjjGk1lmyYhFZeWe0eHD8eklLd41VzYOGb0QvKGGPijCUbJuFUVtfw5ry1nPXgh9z02jeuML8vHHZVXaVpE6C6MirxGWNMvLFkwyScb9eW8MsXvuTLldv597x1bNxR7mYc/TvIyHOPty6BuU9GL0hjjIkjlmwkCBHJF5HXRJvOhGwAABz/SURBVKRURFaIyIWN1L1ORNaLSLGITBKR9HDaEZEjRGSqiGwVkU0i8oqI9Ih035praO+OHFzYEYCK6hqe+2Slm5GZD0f9rq7irHugvDgKERpjTHyxZCNxPABUAAXARcBDIjI4sJKInATcCBwPFAF7A34Xo2i0nU7Ao95yfYAdQLscHrhsRN1Fvp6bs4LdVd6xG4ddBXmF7nHZFpj9tyhEZ4wx8cWSjQQgIlnAOcAtqrpTVWcDbwBjglS/FHhCVReo6jbgDmBsOO2o6n9V9RVVLVHVMuB+YESEu9ciJw/pTo+8DAA276zgzXnr3IzUDDj+lrqKnzwIxWuiEKExxsQPSzYSQ3+gWlUX+5XNAxqMbHhl8wLqFYhI52a2A3A0sCBUUCJylYjMFZG5mzZtCqMbrSc1OYlLjiyqnZ40exnqO911yE+gx1D3uKoc3vtjm8ZmjDHxxpKNxJANBB58UAzkhFHX9zinOe2IyAHAeOD3oYJS1UdVdZiqDuvatWujHYiECw7rTUaq+wh8u66ET5dtdTOSkuDEO+sqfvU8rP+mzeMzxph4YclGYtgJ5AaU5eKOqWiqru/xjnDbEZF9gf8Cv1bVD1oYc8R1zEzj7IP3qp2e9OGyupl9j4Z+J3kTClPHt21wxhgTRyzZSAyLgRQR6edXNpTguzgWePP8621Q1S3htCMifYBpwB2q2u5vMnLZ8KLax+9+u8HdoM1n1G0g3kdkyQz4YXrbBmeMMXHCko0YJSIDReRMEenZVF1VLQWmALeLSJaIjADOAIIlA08DPxORQSLSCbgZmBxOOyLSC5gBPKCqD+9xJ9tAv4IcjurXBXBXKH/qo+V1M7vtBwddXDc9dTzUVLdtgMYYEwcs2YgBIvKIiDzsN30e8A3uh3+RiAwPo5lrgQ7ARuAF4BpVXSAihSKyU0QKAVT1bWAi8B6wwvu7tal2vHlX4E6VvdVrc6eI7Gxxx9vI5SP7kpGaxEWHF3L+Yb3rzzz2D5Ca6R5vmA9fv9T2ARpjTIwTtRtOtXsisgIYp6rPe9OLgU+A/wX+AeSr6vFRDHGPDRs2TOfOnRuVddfUKCXllXTMTAte4b27YNaf3OPcXvDLzyG1Q9sFaExkSdNVjNkzNrIRG7oBqwC84yX2BSaq6nrcRbQOimJsMS8pSUInGgDDfwVZ3dzjkjXu2hvGGGPCZslGbNiKu2InwAnAelWd700LkByVqBJFejYcO65u+oP7oHRz9OIxxpgYY8lGbPgv7qDM/8FdSvxlv3lDgOXRCCpebdm5mylfrK5feNAl0KW/e1yxo263ijHGmCZZshEbrscdo3E18D7uYlk+ZwFvRyOoeKOqjJvyDUfeM4PfvjyP79b7XT4kOQVO8LtFzNxJsGVJ2wdpjDExyJKNGKCqxap6uarur6pjVLXEb95RqnpDNOOLFyLC9rIKKqpqAJj80bL6FQacAn28W73UVMG0CW0boDHGxChLNmKAiKT43+bdKztRRH4jInZwaCu6fGTd3WCnfLGGraUVdTNF4MQ76qYXvgEr57RhdMYYE5ss2YgNLwEP+SZE5Fe4XSd3A3NE5LRoBRZvhvXpxP698gDYXVXDC5+urF+h1yEw5Jy66XdvdlcDM8YYE5IlG7HhCOAtv+nfA/eqagfgceAPUYkqDokIl40oqp1++uPlVFbX1K90/HhISnWPV3/qRjiMMcaEZMlGbOgMrAcQkf2BnoDviqKvAIOiFFdcOvWAHnTNcXutNpTs5q1v1tWv0KkIDruqbnraBKiqwBhjTHCWbMSGDUCR9/hkYIWq+k6F6ADUBFvItEx6SjJjjuhTOz1p9jIaXGn36N9Bhtvdwtal8PnktgvQGGNijCUbseEV4E8i8mfgBtzN0nwOAr6PSlRx7MLDC0lLdh+PeauL+WLl9voVMvPhqN/VTc+6B8qL2zBCY4yJHZZsxIYbgUeAgbgDRe/2m3cI7gBS04q6ZKdzxoF1N9R98sNlDSsddhXkFbrHZVtg9t/aKDpjjIktlmzEAFWtUtXbVfV0Vb1FVXf7zTtbVe+NZnzx6rIRdafBzl9TzO6qgNvLp2a4g0V9PnkQigOuPGqMMYaUaAdgwicihwMjgXzc/VJmq6pd6CFCBvXM5YqRfRlW1IkT9isgJTlIbj7kHPj4flj3FVSVw4w/wlkPNaxnjDEJzG4xHwNEJAt33MbJQBWwBXeGSjLuehvnqmpZ9CLcc9G8xfweW/Y+PHW6NyFw9QfQff+ohmRMM9gt5k3E2W6U2DAROBI4D8hQ1R5ABnC+V253BYumvkdDv5O8CYWp4xutbowxicaSjdhwDnCDqr6iqjUAqlqjqq/gDh49N6rRGRh1O4j3cVoyA36YHt14jDGmHbFkIzbkAatCzFsF5LZhLAmpukZ5e/56znvkY95fvKlhhW4D4aAxddNTx0NNdcN6xhiTgCzZiA3zgGtEpN6+VW/6Gm++iaC/T1vM1c9+zpxlW4OfBgtw7E2Qmukeb5gP815suwCNMaYds2QjNtwEnAQsEpF7ROQ6EbkbWAic6M03EXT2wXvhS/Xe+24TSzbtbFgppzsM/2Xd9Iw7oSKmj9s1xphWYclGDFDVGbgrhX6JOz7jj8BPgS9wyYaN10dYUZcsjh/YrXZ68ofLg1cc/ivI8urtWAtz7DRYY4yxZCNGqOq3qnq+qu6jqpne/wuBrsB70Y4vEVzud5GvVz9fTXFZZcNK6dlw7Li66Q/ug9LNbRCdMca0X5ZsGBOmI/fpzMDuOQDsqqzmpbkrg1c86BLoMsA9rtgBs+zMZGNMYrNkw5gwiQiXjSiqnX7qoxVUVQe54W5yCoy6rW567iTY/EPkAzTGmHbKkg1jmuGMA3uRn5UGwJrtu5j67YbgFfufDH1Gusc1VTB9QtsEaIwx7ZAlG8Y0Q0ZqMhcdXlg7PSnUabAicOLtddML34SVn0Q4OmOMaZ8s2WinRGSTiGxs6g+YFO1YE83FR/QhNdmdB/vZ8m18s7o4eMVeh7gbtfm8ewvYvYiMMQnI7vrafj0A2C9TO1SQm8Gp+/fgrfnrOWNoT3IyGvkYHT/ejWpUV8DqT2HhGzDojLYL1hhj2gG766tpF2Ltrq9rtu8iIyWJztnpTVd+5w/uNvQA+XvDtXMgJS2yARoTPrvrq4k4241iTAv06tghvEQD4KjrISPPPd66FD5/MnKBGWNMO2TJhjGRlpkPR/++bnrmPVAe4jgPY4yJQ5ZsGNMKissqWbS+JHSFQ6+EPO8sll1bYfbf2iYwY4xpByzZSBAiki8ir4lIqYisEJELG6l7nYisF5FiEZkkIunhtCMiaSLyqogsFxEVkWMi3K2o27RjNzf/6xuOuHs61700j5DHQKVmuINFfT55EIpXt02QxhgTZXY2SuJ4AKgACoADgf+IyDxVXeBfSUROAm4EjgPWAq8Bt3ll4bQzG/gb8Epku9M+pCYLr36+mvLKGhauK+GTpVs5cp/OwSsPOccdKLruK6gqh+fPg64DIDkNklLc/+Q0SE71/tLq/if5l6W5q5T6Hjd72VRqb2FrjDFtwM5GSQAikgVsA4ao6mKv7BlgjareGFD3eWC5qt7kTR8PPKeq3ZvZzmrgYlWdGU6MsXY2ir8/vPYNz81x90k5cVABj14yLHTlZR/AU6e1UWSNqE1QUr1kpAVJTu2yqSBJkJQMkhzwP6nuf715gdPef5Em2kkOaC9EO77yYMuFaj9xE7CE7bhpOzaykRj6A9W+BMEzD/hRkLqDgdcD6hWISGegsBntNElErgKuAigsLGyidvt12Yii2mRj6sINrNxSRmHnzOCV+x4Fg8+GBVPaMMIgaqrcX5Ab1yYuL9FB/JIPv8eS5E3j97ixetJwXqu27e0Fb6zeUddD4eFt+iwaE4wlG4khGwg8/aEYyAmjru9xTjPbaZKqPgo8Cm5koyVttAf7dsvh6P5deX/xJlThqY+Xc8tpg0IvcNYjcMhY2LUNqiuhptJd9Ku60vurqPtf419WAdVVfo8Dlw1oI9SyNZZhBKcuAYsnB4+JdgTGAJZsJIqdQG5AWS6wI4y6vsc7mtlOQrl8RBHvL94EwMufreK6Uf3JTg/x8UpJg71bNBjUOlRbnqgETXIqQauhptr7XxMwXQ1a4/7qlQWpW1snWN2A+a25Tg1y9964YHtITPtgyUZiWAykiEg/Vf3eKxsKLAhSd4E372W/ehtUdYuIlDejnYRydL+u7N01i6WbStmxu4pX565i7Ii+0Q4rOBHvCqZpQFa0o2kfVOsSE7RuusHjmrr72/geN1rP73HtdFP1fPNovF44y/Q8uE2fRmNCsWQjAahqqYhMAW4XkStwZ5GcAQwPUv1pYLKIPAesA24GJofbjnearG9zKk1EMoDdGudHIiclCZeN6Mst/5oPwOSPlnPJkUUkJdmWZUwQqTtY1BjT6uw6G4njWqADsBF4AbhGVReISKGI7BSRQgBVfRuYCLwHrPD+bm2qHb/53wG7gF7AO97jPpHsWHtxzsG9yPVuyrZ8SxnvfbcxyhEZY0z7YKe+mnYhlk999Xf3Wwt55P2lAJx2QA/uv9CGsU27Z8NvJuJsN4oxreiS4UXMXbGNS4cXccqQ7tEOxxhj2gVLNoxpRb06duCf1wQ7FMYYYxKXHbNhjDHGmIiyZMMYY4wxEWXJhjERUlOjzFi0gV88/wUVVfF60ShjjGmaHbNhTIRc8NgnzFm2FYAT9ivgzIN6RTkiY4yJDhvZMCZCRu7bpfbxpA+XYaeZG2MSlSUbxkTIhYcXkpbiPmJfry7mi5XbohyRMcZEhyUbxkRI5+x0zjywZ+30pA+XRy8YY4yJIks2jImgy/xuxvb2/PWs3b4ritEYY0x0WLJhTATt1yOXI/fuDEB1jfL0xyuiHJExxrQ9SzaMibDLR9aNbrzw6UrKKqqiGI0xxrQ9SzaMibDjBnajMD8TgOJdlbz25ZooR2SMMW3Lkg1jIiw5SRg7vKh2+skPl9tpsMaYhGLJhjFt4Nxhe5GdnkJqsrB/rzx27rZdKcaYxGFXEDWmDeRkpPKPCw5icM9cuuVmRDscY4xpU5ZsGNNGjh3YLdohGGNMVNhuFGOiZO7yrWwsKY92GMYYE3E2smFMFFTXKBc/MYfyyhq65qQzpGcug3vmMaSX+79Xpw6ISLTDNMaYVmHJhjFRsHTTTsor3W3nN+3YzXvfbeK97zbVzs/rkMrgnrkM6ZXH4J65jN6/B6nJNhBpjIlNlmwYEwVlFdUM69OJb9eVUFZR3WB+8a5KPlqyhY+WbCE7PYXTD+hZb/72sgrWbi+nX0G2JSHGmHbPkg1jomBo7468es1wqmuUZZtLWbC2mAVrS5i/ppj5a4opKa87NXZQj1ySkurvUpm1eBO/fvEr0pKTGNA9hyG9chnUM48hPXPZr0cuGanJbd0lY4wJyZINY6IoOUnYt1s2+3bL5owDewGgqqzetosFa4uZv6aEnh07NFhu/ppiACqqa/hmTTHfrCkGVtW2uU/XLIb0zGNwrzyO2DufwT3z2qxPxhgTyJINY9oZEaF3fia98zM5eUiPoHU6pKXQq2MH1gS5i2x1jbJ4w04Wb9jJlC/XMHZ4EYN/XD/ZWLa5lLwOqeRnpUWkD8YY48+SDWNi0G9H9ee3o/qzvayidvfLgrUlzF9bzLLNpfhfDX1Ir4ajGjdN+YaPl26hZ14Gg72DUIf0zGNIrzwKctPtTBhjTKuyZMOYGNYxM40R+3ZhxL5dastKd1excF1dAnJQYcd6y6gqC9a63TBri8tZW1zO1G831M7vnJVWLwEZ2a8LeR1S26ZDxpi4ZMmGMXEmKz2FYUX5DCvKDzq/ZFcVe3fNZuG6EnZX1TSYv6W0gvcXb+L9xe5U3P/++qh6yYaq8v3GnezdJYsUOxPGGBMGSzaMSTB5man8639GUFVdw5JNpfV2wXy7tqTeTeLSUpLYt1t2veVXb9vFife9T0ZqEvv1yGVg91y65aSTn5VGp6w08jPT6JTljgfplJlmZ8YYYyzZMCZRpXinzQ7onsM5h7iymhplxday2jNhyiqqGlzHw7cLpryyhi9XbufLldtDrmNAQQ7vXHd0vbLPlm/lrW/W0SmzYXKSn5lGx8w00lJsxMSYeGLJhjGmVlKS0LdLFn27ZHFawIXEfHZVVtM9N4P1YdzXpVNWw2M95q3azpMfLm90uZz0FDplpXHS4AL+cOqgevPmrylm9bZdLjnJSqWTl6AkJ9lBrca0V5ZsGGOa5ayD9uKsg/Zi887dLFhbwpKNO9lWVuH+SivZWuoeby2toFtORoPlt5VVNLmOHbur2LG7iu1llQ3mvfr5aiZ/tLxemYi7xLsbGanbhTNqUAEnDu5er+6mHbtJTRZyM1IbXCzNGBMZlmwYY1qkS3Y6P+rflR/179qs5Y4bWECnzLR6Scm20kq2llWwzSur8U7dDXYdkGDJiipsL6tskJz0zs9skGzc+M+vmb5oI0lCg105ORmppKUkkZacRGqycNZBezGoZ2695d+ct5byymq/ekmkpfj+C2nJyaSmCGnJSfTs2KHBMSuqaqcWm4RjyUaCEJF84AngRGAzME5Vnw9R9zrgBqAD8E/gGlXdHU47InI88ABQCMwBxqrqikj1y8SeQ/p04pA+nULOr6lRSsrdCElmWsOvqP175VG6u9obSalga1lF0BEQgE5BkpWtXrJSo+7Mmy2loUdaDi7s1CDZ+NPbi1i9reHF1IJ59eojG5wVNOCWt1FVUr1EJTU5ifQUl9zUS1ySk/jHhQdRkFs3OlRWUcVtb3zrJTN1SU1achKpKXWJT35mGqceEPyCcMZEgyUbieMBoAIoAA4E/iMi81R1gX8lETkJuBE4DlgLvAbc5pU12o6IdAGmAFcAbwJ3AC8BR0S4byaOJCUJHb3jMIK54qi9ueKoveuVVVXXULyr0hspqduVc2hRw6SmQ2oyORkp7PC7/0wowW5yVxHkdOFQAg90VVUqq2tQhcrqaqDhTfj8VddovenS3dW8NHdVk+vt1y3bkg3TrliykQBEJAs4BxiiqjuB2SLyBjCGuiTC51LgCV8SIiJ3AM8BN4bRztnAAlV9xVt2ArBZRAaq6qJI99MkrpTkJDpnp9M5O73Jus9f6XLfiqoatu+qf5zJzvIqKqprqKyuoaKqhn4F2Q2WP31oT7aVVVBZrVRW1dSrX1ntTVe5pCIzrf4ulOoarXd116YEJiuV1eElOnYnYNPeWLKRGPoD1aq62K9sHvCjIHUHA68H1CsQkc64XSONtTPYmwZAVUtFZIlX3iDZEJGrgKsACgsLm9snY/ZIWkoS3XIygh7E2phbThvUdKUQUpKTWHb3aCqr1UtKXIKy2/tfWa31pgOv3JrbIZW7z96/NrnxT2wq/Mp65DavT8ZEmiUbiSEbKA4oKwZywqjre5wTRjvZwKYw14OqPgo8CjBs2LBmbO8ZE7tExB1ImpIETQ/E1JOdnsIFh1libmKPjbUlhp1AbkBZLrAjjLq+xzvCaKc56zHGGJMgLNlIDIuBFBHp51c2FFgQpO4Cb55/vQ2quiWMduot6x3jsU+I9RhjjEkQlmwkAFUtxZ0lcruIZInICOAM4Jkg1Z8GfiYig0SkE3AzMDnMdl4DhojIOSKSAYwHvraDQ40xJrFZspE4rsVdN2Mj8ALu2hkLRKRQRHaKSCGAqr4NTATeA1Z4f7c21Y637Cbc2Sp/BLYBhwPnt0HfjDHGtGOizTkPy5gIEZFNuMSmKV1wFxOLB/HSl3jpByRmXzar6smRDsYkNks2TEwRkbmqOizacbSGeOlLvPQDrC/GRIrtRjHGGGNMRFmyYYwxxpiIsmTDxJpHox1AK4qXvsRLP8D6YkxE2DEbxhhjjIkoG9kwxhhjTERZsmGMMcaYiLJkwxhjjDERZcmGiQkiki8ir4lIqYisEJELox1TOETkFyIyV0R2i8jkgHnHi8giESkTkfdEpE+UwgyLiKSLyBPe879DRL4UkVP85sdMf0TkWRFZJyIlIrJYRK7wmxcz/fAnIv1EpFxEnvUri8m+mPhjyYaJFQ8AFUABcBHwkIgMjm5IYVkL3AlM8i8UkS64+8zcAuQDc4GX2jy65kkBVgE/AvJwsb8sIkUx2J+7gSJVzQV+DNwpIofEYD/8PQB85puI8b6YOGNno5h2z7t77DZgiKou9sqeAdao6o1RDS5MInInsJeqjvWmrwLGqupwbzoLd2npg2LpxnUi8jVwG9CZGO2PiAwAZgK/BjoSg/0QkfOBs4FvgX1V9eJ4eY+Z+GAjGyYW9AeqfYmGZx4QCyMboQzG9QGovaPuEmKoTyJSgHttFhCD/RGRB0WkDFgErAPeIjb7kQvcDlwfMCvm+mLilyUbJhZkA8UBZcVAThRiaS0x3ScRSQWeA57ytpJjrj+qei0uvqNwuxt2E4P9AO4AnlDVVQHlsdgXE6cs2TCxYCeQG1CWC+yIQiytJWb7JCJJwDO4Y2h+4RXHZH9UtVpVZwN7AdcQY/0QkQOBE4D7gsyOqb6Y+GbJhokFi4EUEennVzYUN3wfqxbg+gDU7k/fh3beJxER4AncgbrnqGqlNysm++Mnhbp4Y6kfxwBFwEoRWQ/8DjhHRL4g9vpi4pglG6bd8/Y1TwFuF5EsERkBnIHbum7XRCRFRDKAZCBZRDJEJAV4DRgiIud488cDX8fAgXsPAfsBp6vqLr/ymOmPiHQTkfNFJFtEkkXkJOACYAYx1A/Po7gE4kDv72HgP8BJxF5fTByzZMPEimuBDsBG4AXgGlWNhS20m4FdwI3Axd7jm1V1E3AO8EfcmTaHA+dHK8hweNdo+DnuR229iOz0/i6Ksf4obpfJalysfwF+o6qvx1g/UNUyVV3v+8PtOilX1U2x1hcT3+zUV2OMMcZElI1sGGOMMSaiLNkwxhhjTERZsmGMMcaYiLJkwxhjjDERZcmGMcYYYyLKkg1jjDHGRJQlG8bEARGZICKb/ab7e2UdoxmXMcaAJRvGxKv+wK24W6YbY0xUWbJhjGmSiHSIdgzGmNhlyYYxcUZEjgHe9CaXiYiKyHK/+YUi8qKIbBWRMhF5R0QG+M0v8pa5SESeFpHtfu0FrstX96ci8oiIFIvIahG5zbs7rK/eZBGZG2LZ0/zKVESuE5F7RWSLiGwWkd958y4VkaUisl1EJnn3+zDGxICUaAdgjGl1X+Du/vkX4GxgHbAbQETygdnAFuBqoAx335ZpItI/4OZqf8HdAO9coLqJdU4E/gn8BDged9OvBcDLLYj/etzNxC4ATgP+LCLdgEOBXwGFuFuqLwbuaUH7xpg2ZsmGMXFGVUtE5Dtv8ktVXe43+zogCzhQVbcCiMiHwHLgcuABv7qfqOr/hLna91X1eu/xVBE5GZfotCTZ+F5Vf+7FNg2X7FwJ9FHVEq/8GOAsLNkwJiZYsmFMYjkBmAqUeLe6B9gBfA4MC6j7n2a0+27A9Le4EYiWmO57oKo1IrIMKPMlGp4fgOEtbN8Y08bsmA1jEksX4DygMuDvWKB3QN0NzWh3e8B0BdDSYyqCtdWa7Rtj2piNbBiTWLYCbwB3BJm3I2BaW3G95UBaQFl+K7ZvjGnHLNkwJj5VeP8Dt/6nAz8FFgQcDBppq4EiEclQ1XKvbFQbrt8YE0W2G8WY+OQ7QPTnInK4iOzvTf8VN8IwQ0QuFJEfeaetPiAiF0Qwnn8B2cDjInKCiPweuCyC6zPGtCOWbBgTh1R1Be7017OBD/Guk6Gqm4EjgEW400ffxZ22mgd8HcF45uPOdjkStxvnR960MSYBiGpr7pY1xhhjjKnPRjaMMcYYE1GWbBhjjDEmoizZMMYYY0xEWbJhjDHGmIiyZMMYY4wxEWXJhjHGGGMiypINY4wxxkSUJRvGGGOMiaj/B4AwsNsxBBfwAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 288x216 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"eN9uuKlNoL7W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1597601071730,"user_tz":-120,"elapsed":1375,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"52ec24e3-a3f7-410e-f6e7-a432ea8771f9"},"source":["plt.plot(step_list,loss_no_scale_no_bn['train'],'r-',label='Original_Train',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['train'],'b-',label='Modification_Train',linewidth = 2)\n","plt.plot(step_list,loss_no_scale_no_bn['test'],'r--',label='Original_Test',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['test'],'b--',label='Modification_Test',linewidth = 2)\n","plt.title('784-800-10; no scaling; no bn; full batch; train data size = 64')\n","plt.xlabel('Iter num')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.legend()\n","plt.savefig('/content/drive/My Drive/LCNN/newplots/attack1.pdf')"],"execution_count":137,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"pVPGeVxOcT1J","colab_type":"text"},"source":["# No scaling + batch norm"]},{"cell_type":"code","metadata":{"id":"gzyh4q_17vZ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597610414257,"user_tz":-120,"elapsed":626882,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"f21612cf-11b0-4a96-9117-d18c4b05f1df"},"source":["# 1.2 0.53. runstep=2\n","maxstep=1000\n","for scale in range(10):\n","    scale=(scale-2)*0.05+0.9\n","    for therd in range(10):\n","        therd=(therd-2)*0.05+0.7\n","        print('scale:{:03f},therd:{:03f}'.format(scale,therd))\n","        step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(scale,therd,2,False,initialize = 'LeCun', batchnorm = True, learning_rate = 0.1, ** shared_model_param_dict)\n","        if maxstep>train_loss:\n","            maxstep=train_loss\n","            sb=scale\n","            th=therd\n","print(maxstep,sb,th)"],"execution_count":191,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Iter: 076/100 | Train Loss: 0.00000391\n","Iter: 077/100 | Train Loss: 0.00000368\n","Iter: 078/100 | Train Loss: 0.00000346\n","Iter: 079/100 | Train Loss: 0.00000324\n","Iter: 080/100 | Train Loss: 0.00000304\n","Iter: 081/100 | Train Loss: 0.00000289\n","Iter: 082/100 | Train Loss: 0.00000270\n","Iter: 083/100 | Train Loss: 0.00000255\n","Iter: 084/100 | Train Loss: 0.00000244\n","Iter: 085/100 | Train Loss: 0.00000230\n","Iter: 086/100 | Train Loss: 0.00000217\n","Iter: 087/100 | Train Loss: 0.00000206\n","Iter: 088/100 | Train Loss: 0.00000195\n","Iter: 089/100 | Train Loss: 0.00000186\n","Iter: 090/100 | Train Loss: 0.00000177\n","Iter: 091/100 | Train Loss: 0.00000169\n","Iter: 092/100 | Train Loss: 0.00000161\n","Iter: 093/100 | Train Loss: 0.00000153\n","Iter: 094/100 | Train Loss: 0.00000147\n","Iter: 095/100 | Train Loss: 0.00000140\n","Iter: 096/100 | Train Loss: 0.00000134\n","Iter: 097/100 | Train Loss: 0.00000128\n","Iter: 098/100 | Train Loss: 0.00000123\n","Iter: 099/100 | Train Loss: 0.00000117\n","\n","Iter: 099/100 | Test Loss: 0.00094558 | Test acc: 64.3100\n","scale:1.100000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00149275\n","Iter: 001/100 | Train Loss: 0.00203371\n","Adjusting Layer 1, Kernel Nodes: 556, Adptive Nodes:244\n","Iter: 002/100 | Train Loss: 0.00622090\n","Adjusting Layer 1, Kernel Nodes: 559, Adptive Nodes:241\n","Iter: 003/100 | Train Loss: 0.04753676\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 004/100 | Train Loss: 0.55590540\n","Adjusting Layer 1, Kernel Nodes: 275, Adptive Nodes:525\n","Iter: 005/100 | Train Loss: 0.00274462\n","Iter: 006/100 | Train Loss: 0.00150881\n","Iter: 007/100 | Train Loss: 0.00145254\n","Iter: 008/100 | Train Loss: 0.00136407\n","Iter: 009/100 | Train Loss: 0.00129652\n","Iter: 010/100 | Train Loss: 0.00122535\n","Iter: 011/100 | Train Loss: 0.00115702\n","Iter: 012/100 | Train Loss: 0.00109413\n","Iter: 013/100 | Train Loss: 0.00104882\n","Iter: 014/100 | Train Loss: 0.00099767\n","Iter: 015/100 | Train Loss: 0.00092402\n","Iter: 016/100 | Train Loss: 0.00087664\n","Iter: 017/100 | Train Loss: 0.00083668\n","Iter: 018/100 | Train Loss: 0.00078912\n","Iter: 019/100 | Train Loss: 0.00076073\n","Iter: 020/100 | Train Loss: 0.00071698\n","Iter: 021/100 | Train Loss: 0.00068624\n","Iter: 022/100 | Train Loss: 0.00065153\n","Iter: 023/100 | Train Loss: 0.00060752\n","Iter: 024/100 | Train Loss: 0.00057743\n","Iter: 025/100 | Train Loss: 0.00053924\n","Iter: 026/100 | Train Loss: 0.00050472\n","Iter: 027/100 | Train Loss: 0.00046953\n","Iter: 028/100 | Train Loss: 0.00043724\n","Iter: 029/100 | Train Loss: 0.00040967\n","Iter: 030/100 | Train Loss: 0.00038372\n","Iter: 031/100 | Train Loss: 0.00035568\n","Iter: 032/100 | Train Loss: 0.00033483\n","Iter: 033/100 | Train Loss: 0.00031079\n","Iter: 034/100 | Train Loss: 0.00028565\n","Iter: 035/100 | Train Loss: 0.00026322\n","Iter: 036/100 | Train Loss: 0.00031536\n","Adjusting Layer 1, Kernel Nodes: 774, Adptive Nodes:26\n","Iter: 037/100 | Train Loss: 0.00128820\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 038/100 | Train Loss: 0.00028018\n","Iter: 039/100 | Train Loss: 0.00079205\n","Adjusting Layer 1, Kernel Nodes: 454, Adptive Nodes:346\n","Iter: 040/100 | Train Loss: 0.00026578\n","Iter: 041/100 | Train Loss: 0.00047737\n","Adjusting Layer 1, Kernel Nodes: 523, Adptive Nodes:277\n","Iter: 042/100 | Train Loss: 0.00215353\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 043/100 | Train Loss: 0.09760435\n","Adjusting Layer 1, Kernel Nodes: 216, Adptive Nodes:584\n","Iter: 044/100 | Train Loss: 5.51455784\n","Adjusting Layer 1, Kernel Nodes: 121, Adptive Nodes:679\n","Iter: 045/100 | Train Loss: 10521620.00000000\n","Adjusting Layer 1, Kernel Nodes: 583, Adptive Nodes:217\n","Iter: 046/100 | Train Loss: inf\n","Adjusting Layer 1, Kernel Nodes: 279, Adptive Nodes:521\n","Iter: 047/100 | Train Loss: inf\n","Iter: 048/100 | Train Loss: nan\n","Iter: 049/100 | Train Loss: nan\n","Iter: 050/100 | Train Loss: nan\n","Iter: 051/100 | Train Loss: nan\n","Iter: 052/100 | Train Loss: nan\n","Iter: 053/100 | Train Loss: nan\n","Iter: 054/100 | Train Loss: nan\n","Iter: 055/100 | Train Loss: nan\n","Iter: 056/100 | Train Loss: nan\n","Iter: 057/100 | Train Loss: nan\n","Iter: 058/100 | Train Loss: nan\n","Iter: 059/100 | Train Loss: nan\n","Iter: 060/100 | Train Loss: nan\n","Iter: 061/100 | Train Loss: nan\n","Iter: 062/100 | Train Loss: nan\n","Iter: 063/100 | Train Loss: nan\n","Iter: 064/100 | Train Loss: nan\n","Iter: 065/100 | Train Loss: nan\n","Iter: 066/100 | Train Loss: nan\n","Iter: 067/100 | Train Loss: nan\n","Iter: 068/100 | Train Loss: nan\n","Iter: 069/100 | Train Loss: nan\n","Iter: 070/100 | Train Loss: nan\n","Iter: 071/100 | Train Loss: nan\n","Iter: 072/100 | Train Loss: nan\n","Iter: 073/100 | Train Loss: nan\n","Iter: 074/100 | Train Loss: nan\n","Iter: 075/100 | Train Loss: nan\n","Iter: 076/100 | Train Loss: nan\n","Iter: 077/100 | Train Loss: nan\n","Iter: 078/100 | Train Loss: nan\n","Iter: 079/100 | Train Loss: nan\n","Iter: 080/100 | Train Loss: nan\n","Iter: 081/100 | Train Loss: nan\n","Iter: 082/100 | Train Loss: nan\n","Iter: 083/100 | Train Loss: nan\n","Iter: 084/100 | Train Loss: nan\n","Iter: 085/100 | Train Loss: nan\n","Iter: 086/100 | Train Loss: nan\n","Iter: 087/100 | Train Loss: nan\n","Iter: 088/100 | Train Loss: nan\n","Iter: 089/100 | Train Loss: nan\n","Iter: 090/100 | Train Loss: nan\n","Iter: 091/100 | Train Loss: nan\n","Iter: 092/100 | Train Loss: nan\n","Iter: 093/100 | Train Loss: nan\n","Iter: 094/100 | Train Loss: nan\n","Iter: 095/100 | Train Loss: nan\n","Iter: 096/100 | Train Loss: nan\n","Iter: 097/100 | Train Loss: nan\n","Iter: 098/100 | Train Loss: nan\n","Iter: 099/100 | Train Loss: nan\n","\n","Iter: 099/100 | Test Loss: nan | Test acc: 9.8000\n","scale:1.100000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00142410\n","Iter: 001/100 | Train Loss: 0.00163593\n","Adjusting Layer 1, Kernel Nodes: 578, Adptive Nodes:222\n","Iter: 002/100 | Train Loss: 0.00291736\n","Adjusting Layer 1, Kernel Nodes: 573, Adptive Nodes:227\n","Iter: 003/100 | Train Loss: 0.01295491\n","Adjusting Layer 1, Kernel Nodes: 792, Adptive Nodes:8\n","Iter: 004/100 | Train Loss: 0.26328269\n","Adjusting Layer 1, Kernel Nodes: 282, Adptive Nodes:518\n","Iter: 005/100 | Train Loss: 0.02230727\n","Iter: 006/100 | Train Loss: 0.00156468\n","Iter: 007/100 | Train Loss: 0.00133603\n","Iter: 008/100 | Train Loss: 0.00127706\n","Iter: 009/100 | Train Loss: 0.00121748\n","Iter: 010/100 | Train Loss: 0.00114132\n","Iter: 011/100 | Train Loss: 0.00107732\n","Iter: 012/100 | Train Loss: 0.00103237\n","Iter: 013/100 | Train Loss: 0.00096348\n","Iter: 014/100 | Train Loss: 0.00090659\n","Iter: 015/100 | Train Loss: 0.00083694\n","Iter: 016/100 | Train Loss: 0.00078161\n","Iter: 017/100 | Train Loss: 0.00074470\n","Iter: 018/100 | Train Loss: 0.00069142\n","Iter: 019/100 | Train Loss: 0.00062977\n","Iter: 020/100 | Train Loss: 0.00058983\n","Iter: 021/100 | Train Loss: 0.00056774\n","Iter: 022/100 | Train Loss: 0.00052878\n","Iter: 023/100 | Train Loss: 0.00048448\n","Iter: 024/100 | Train Loss: 0.00046019\n","Iter: 025/100 | Train Loss: 0.00043564\n","Iter: 026/100 | Train Loss: 0.00039613\n","Iter: 027/100 | Train Loss: 0.00036431\n","Iter: 028/100 | Train Loss: 0.00034251\n","Iter: 029/100 | Train Loss: 0.00031517\n","Iter: 030/100 | Train Loss: 0.00028773\n","Iter: 031/100 | Train Loss: 0.00026605\n","Iter: 032/100 | Train Loss: 0.00024641\n","Iter: 033/100 | Train Loss: 0.00022724\n","Iter: 034/100 | Train Loss: 0.00020759\n","Iter: 035/100 | Train Loss: 0.00018963\n","Iter: 036/100 | Train Loss: 0.00017240\n","Iter: 037/100 | Train Loss: 0.00015661\n","Iter: 038/100 | Train Loss: 0.00014341\n","Iter: 039/100 | Train Loss: 0.00013072\n","Iter: 040/100 | Train Loss: 0.00011979\n","Iter: 041/100 | Train Loss: 0.00011004\n","Iter: 042/100 | Train Loss: 0.00010107\n","Iter: 043/100 | Train Loss: 0.00009281\n","Iter: 044/100 | Train Loss: 0.00008451\n","Iter: 045/100 | Train Loss: 0.00007770\n","Iter: 046/100 | Train Loss: 0.00007091\n","Iter: 047/100 | Train Loss: 0.00006484\n","Iter: 048/100 | Train Loss: 0.00005982\n","Iter: 049/100 | Train Loss: 0.00005502\n","Iter: 050/100 | Train Loss: 0.00005088\n","Iter: 051/100 | Train Loss: 0.00004737\n","Iter: 052/100 | Train Loss: 0.00004374\n","Iter: 053/100 | Train Loss: 0.00004032\n","Iter: 054/100 | Train Loss: 0.00003738\n","Iter: 055/100 | Train Loss: 0.00003439\n","Iter: 056/100 | Train Loss: 0.00003187\n","Iter: 057/100 | Train Loss: 0.00002958\n","Iter: 058/100 | Train Loss: 0.00002743\n","Iter: 059/100 | Train Loss: 0.00002564\n","Iter: 060/100 | Train Loss: 0.00002380\n","Iter: 061/100 | Train Loss: 0.00002228\n","Iter: 062/100 | Train Loss: 0.00002063\n","Iter: 063/100 | Train Loss: 0.00001923\n","Iter: 064/100 | Train Loss: 0.00001797\n","Iter: 065/100 | Train Loss: 0.00001677\n","Iter: 066/100 | Train Loss: 0.00001572\n","Iter: 067/100 | Train Loss: 0.00001463\n","Iter: 068/100 | Train Loss: 0.00001370\n","Iter: 069/100 | Train Loss: 0.00001282\n","Iter: 070/100 | Train Loss: 0.00001197\n","Iter: 071/100 | Train Loss: 0.00001120\n","Iter: 072/100 | Train Loss: 0.00001047\n","Iter: 073/100 | Train Loss: 0.00000985\n","Iter: 074/100 | Train Loss: 0.00000931\n","Iter: 075/100 | Train Loss: 0.00000875\n","Iter: 076/100 | Train Loss: 0.00000829\n","Iter: 077/100 | Train Loss: 0.00000784\n","Iter: 078/100 | Train Loss: 0.00000740\n","Iter: 079/100 | Train Loss: 0.00000699\n","Iter: 080/100 | Train Loss: 0.00000658\n","Iter: 081/100 | Train Loss: 0.00000624\n","Iter: 082/100 | Train Loss: 0.00000589\n","Iter: 083/100 | Train Loss: 0.00000557\n","Iter: 084/100 | Train Loss: 0.00000528\n","Iter: 085/100 | Train Loss: 0.00000502\n","Iter: 086/100 | Train Loss: 0.00000479\n","Iter: 087/100 | Train Loss: 0.00000456\n","Iter: 088/100 | Train Loss: 0.00000434\n","Iter: 089/100 | Train Loss: 0.00000414\n","Iter: 090/100 | Train Loss: 0.00000395\n","Iter: 091/100 | Train Loss: 0.00000377\n","Iter: 092/100 | Train Loss: 0.00000360\n","Iter: 093/100 | Train Loss: 0.00000345\n","Iter: 094/100 | Train Loss: 0.00000330\n","Iter: 095/100 | Train Loss: 0.00000316\n","Iter: 096/100 | Train Loss: 0.00000303\n","Iter: 097/100 | Train Loss: 0.00000291\n","Iter: 098/100 | Train Loss: 0.00000280\n","Iter: 099/100 | Train Loss: 0.00000269\n","\n","Iter: 099/100 | Test Loss: 0.00094951 | Test acc: 64.0300\n","scale:1.100000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00137958\n","Iter: 001/100 | Train Loss: 0.00146921\n","Adjusting Layer 1, Kernel Nodes: 628, Adptive Nodes:172\n","Iter: 002/100 | Train Loss: 0.00183213\n","Adjusting Layer 1, Kernel Nodes: 607, Adptive Nodes:193\n","Iter: 003/100 | Train Loss: 0.00511643\n","Adjusting Layer 1, Kernel Nodes: 338, Adptive Nodes:462\n","Iter: 004/100 | Train Loss: 0.00717495\n","Adjusting Layer 1, Kernel Nodes: 519, Adptive Nodes:281\n","Iter: 005/100 | Train Loss: 0.00324569\n","Iter: 006/100 | Train Loss: 0.00139362\n","Iter: 007/100 | Train Loss: 0.00058869\n","Iter: 008/100 | Train Loss: 0.00079236\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 009/100 | Train Loss: 0.00090622\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 010/100 | Train Loss: 0.00070700\n","Iter: 011/100 | Train Loss: 0.00048151\n","Iter: 012/100 | Train Loss: 0.00043862\n","Iter: 013/100 | Train Loss: 0.00048242\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 014/100 | Train Loss: 0.00041029\n","Iter: 015/100 | Train Loss: 0.00027633\n","Iter: 016/100 | Train Loss: 0.00024402\n","Iter: 017/100 | Train Loss: 0.00027108\n","Adjusting Layer 1, Kernel Nodes: 176, Adptive Nodes:624\n","Iter: 018/100 | Train Loss: 0.00022320\n","Iter: 019/100 | Train Loss: 0.00015013\n","Iter: 020/100 | Train Loss: 0.00015069\n","Adjusting Layer 1, Kernel Nodes: 221, Adptive Nodes:579\n","Iter: 021/100 | Train Loss: 0.00015498\n","Adjusting Layer 1, Kernel Nodes: 451, Adptive Nodes:349\n","Iter: 022/100 | Train Loss: 0.00012379\n","Iter: 023/100 | Train Loss: 0.00010376\n","Iter: 024/100 | Train Loss: 0.00010278\n","Iter: 025/100 | Train Loss: 0.00009185\n","Iter: 026/100 | Train Loss: 0.00006934\n","Iter: 027/100 | Train Loss: 0.00006141\n","Iter: 028/100 | Train Loss: 0.00006755\n","Adjusting Layer 1, Kernel Nodes: 246, Adptive Nodes:554\n","Iter: 029/100 | Train Loss: 0.00006552\n","Iter: 030/100 | Train Loss: 0.00005253\n","Iter: 031/100 | Train Loss: 0.00004201\n","Iter: 032/100 | Train Loss: 0.00003815\n","Iter: 033/100 | Train Loss: 0.00003710\n","Iter: 034/100 | Train Loss: 0.00003380\n","Iter: 035/100 | Train Loss: 0.00002825\n","Iter: 036/100 | Train Loss: 0.00002495\n","Iter: 037/100 | Train Loss: 0.00002506\n","Adjusting Layer 1, Kernel Nodes: 539, Adptive Nodes:261\n","Iter: 038/100 | Train Loss: 0.00002317\n","Iter: 039/100 | Train Loss: 0.00001795\n","Iter: 040/100 | Train Loss: 0.00001445\n","Iter: 041/100 | Train Loss: 0.00001503\n","Adjusting Layer 1, Kernel Nodes: 328, Adptive Nodes:472\n","Iter: 042/100 | Train Loss: 0.00001507\n","Adjusting Layer 1, Kernel Nodes: 513, Adptive Nodes:287\n","Iter: 043/100 | Train Loss: 0.00001187\n","Iter: 044/100 | Train Loss: 0.00000931\n","Iter: 045/100 | Train Loss: 0.00000919\n","Iter: 046/100 | Train Loss: 0.00000912\n","Iter: 047/100 | Train Loss: 0.00000768\n","Iter: 048/100 | Train Loss: 0.00000653\n","Iter: 049/100 | Train Loss: 0.00000657\n","Adjusting Layer 1, Kernel Nodes: 285, Adptive Nodes:515\n","Iter: 050/100 | Train Loss: 0.00000642\n","Iter: 051/100 | Train Loss: 0.00000540\n","Iter: 052/100 | Train Loss: 0.00000459\n","Iter: 053/100 | Train Loss: 0.00000434\n","Iter: 054/100 | Train Loss: 0.00000421\n","Iter: 055/100 | Train Loss: 0.00000379\n","Iter: 056/100 | Train Loss: 0.00000318\n","Iter: 057/100 | Train Loss: 0.00000270\n","Iter: 058/100 | Train Loss: 0.00000252\n","Iter: 059/100 | Train Loss: 0.00000237\n","Iter: 060/100 | Train Loss: 0.00000205\n","Iter: 061/100 | Train Loss: 0.00000181\n","Iter: 062/100 | Train Loss: 0.00000182\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 063/100 | Train Loss: 0.00000179\n","Iter: 064/100 | Train Loss: 0.00000174\n","Iter: 065/100 | Train Loss: 0.00000132\n","Iter: 066/100 | Train Loss: 0.00000123\n","Iter: 067/100 | Train Loss: 0.00000142\n","Adjusting Layer 1, Kernel Nodes: 307, Adptive Nodes:493\n","Iter: 068/100 | Train Loss: 0.00000127\n","Iter: 069/100 | Train Loss: 0.00000101\n","Iter: 070/100 | Train Loss: 0.00000090\n","Iter: 071/100 | Train Loss: 0.00000086\n","Iter: 072/100 | Train Loss: 0.00000074\n","Iter: 073/100 | Train Loss: 0.00000070\n","Iter: 074/100 | Train Loss: 0.00000071\n","Adjusting Layer 1, Kernel Nodes: 94, Adptive Nodes:706\n","Iter: 075/100 | Train Loss: 0.00000063\n","Iter: 076/100 | Train Loss: 0.00000059\n","Iter: 077/100 | Train Loss: 0.00000052\n","Iter: 078/100 | Train Loss: 0.00000046\n","Iter: 079/100 | Train Loss: 0.00000043\n","Iter: 080/100 | Train Loss: 0.00000042\n","Iter: 081/100 | Train Loss: 0.00000040\n","Iter: 082/100 | Train Loss: 0.00000036\n","Iter: 083/100 | Train Loss: 0.00000032\n","Iter: 084/100 | Train Loss: 0.00000030\n","Iter: 085/100 | Train Loss: 0.00000028\n","Iter: 086/100 | Train Loss: 0.00000027\n","Iter: 087/100 | Train Loss: 0.00000025\n","Iter: 088/100 | Train Loss: 0.00000023\n","Iter: 089/100 | Train Loss: 0.00000022\n","Iter: 090/100 | Train Loss: 0.00000021\n","Iter: 091/100 | Train Loss: 0.00000020\n","Iter: 092/100 | Train Loss: 0.00000018\n","Iter: 093/100 | Train Loss: 0.00000017\n","Iter: 094/100 | Train Loss: 0.00000016\n","Iter: 095/100 | Train Loss: 0.00000015\n","Iter: 096/100 | Train Loss: 0.00000014\n","Iter: 097/100 | Train Loss: 0.00000013\n","Iter: 098/100 | Train Loss: 0.00000013\n","Iter: 099/100 | Train Loss: 0.00000012\n","\n","Iter: 099/100 | Test Loss: 0.00133907 | Test acc: 55.9500\n","scale:1.100000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00135373\n","Iter: 001/100 | Train Loss: 0.00140747\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 002/100 | Train Loss: 0.00181510\n","Adjusting Layer 1, Kernel Nodes: 480, Adptive Nodes:320\n","Iter: 003/100 | Train Loss: 0.00251992\n","Adjusting Layer 1, Kernel Nodes: 463, Adptive Nodes:337\n","Iter: 004/100 | Train Loss: 0.00264512\n","Adjusting Layer 1, Kernel Nodes: 639, Adptive Nodes:161\n","Iter: 005/100 | Train Loss: 0.00341161\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 006/100 | Train Loss: 0.00674265\n","Adjusting Layer 1, Kernel Nodes: 54, Adptive Nodes:746\n","Iter: 007/100 | Train Loss: 0.00092562\n","Iter: 008/100 | Train Loss: 0.00167096\n","Adjusting Layer 1, Kernel Nodes: 392, Adptive Nodes:408\n","Iter: 009/100 | Train Loss: 0.00092556\n","Iter: 010/100 | Train Loss: 0.00056064\n","Iter: 011/100 | Train Loss: 0.00055734\n","Iter: 012/100 | Train Loss: 0.00061609\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 013/100 | Train Loss: 0.00062397\n","Adjusting Layer 1, Kernel Nodes: 445, Adptive Nodes:355\n","Iter: 014/100 | Train Loss: 0.00055256\n","Iter: 015/100 | Train Loss: 0.00043199\n","Iter: 016/100 | Train Loss: 0.00032808\n","Iter: 017/100 | Train Loss: 0.00027913\n","Iter: 018/100 | Train Loss: 0.00027741\n","Iter: 019/100 | Train Loss: 0.00027449\n","Iter: 020/100 | Train Loss: 0.00023762\n","Iter: 021/100 | Train Loss: 0.00018416\n","Iter: 022/100 | Train Loss: 0.00014708\n","Iter: 023/100 | Train Loss: 0.00013581\n","Iter: 024/100 | Train Loss: 0.00013643\n","Adjusting Layer 1, Kernel Nodes: 452, Adptive Nodes:348\n","Iter: 025/100 | Train Loss: 0.00012792\n","Iter: 026/100 | Train Loss: 0.00010772\n","Iter: 027/100 | Train Loss: 0.00008766\n","Iter: 028/100 | Train Loss: 0.00007395\n","Iter: 029/100 | Train Loss: 0.00006692\n","Iter: 030/100 | Train Loss: 0.00006311\n","Iter: 031/100 | Train Loss: 0.00005834\n","Iter: 032/100 | Train Loss: 0.00005143\n","Iter: 033/100 | Train Loss: 0.00004396\n","Iter: 034/100 | Train Loss: 0.00003882\n","Iter: 035/100 | Train Loss: 0.00003739\n","Iter: 036/100 | Train Loss: 0.00003732\n","Iter: 037/100 | Train Loss: 0.00003460\n","Iter: 038/100 | Train Loss: 0.00002854\n","Iter: 039/100 | Train Loss: 0.00002244\n","Iter: 040/100 | Train Loss: 0.00001951\n","Iter: 041/100 | Train Loss: 0.00001940\n","Iter: 042/100 | Train Loss: 0.00001929\n","Iter: 043/100 | Train Loss: 0.00001733\n","Iter: 044/100 | Train Loss: 0.00001418\n","Iter: 045/100 | Train Loss: 0.00001168\n","Iter: 046/100 | Train Loss: 0.00001074\n","Iter: 047/100 | Train Loss: 0.00001059\n","Iter: 048/100 | Train Loss: 0.00001009\n","Iter: 049/100 | Train Loss: 0.00000894\n","Iter: 050/100 | Train Loss: 0.00000763\n","Iter: 051/100 | Train Loss: 0.00000667\n","Iter: 052/100 | Train Loss: 0.00000615\n","Iter: 053/100 | Train Loss: 0.00000578\n","Iter: 054/100 | Train Loss: 0.00000534\n","Iter: 055/100 | Train Loss: 0.00000479\n","Iter: 056/100 | Train Loss: 0.00000421\n","Iter: 057/100 | Train Loss: 0.00000372\n","Iter: 058/100 | Train Loss: 0.00000337\n","Iter: 059/100 | Train Loss: 0.00000317\n","Iter: 060/100 | Train Loss: 0.00000304\n","Iter: 061/100 | Train Loss: 0.00000285\n","Iter: 062/100 | Train Loss: 0.00000254\n","Iter: 063/100 | Train Loss: 0.00000219\n","Iter: 064/100 | Train Loss: 0.00000192\n","Iter: 065/100 | Train Loss: 0.00000177\n","Iter: 066/100 | Train Loss: 0.00000166\n","Iter: 067/100 | Train Loss: 0.00000150\n","Iter: 068/100 | Train Loss: 0.00000131\n","Iter: 069/100 | Train Loss: 0.00000115\n","Iter: 070/100 | Train Loss: 0.00000108\n","Iter: 071/100 | Train Loss: 0.00000106\n","Iter: 072/100 | Train Loss: 0.00000103\n","Iter: 073/100 | Train Loss: 0.00000095\n","Iter: 074/100 | Train Loss: 0.00000085\n","Iter: 075/100 | Train Loss: 0.00000076\n","Iter: 076/100 | Train Loss: 0.00000071\n","Iter: 077/100 | Train Loss: 0.00000067\n","Iter: 078/100 | Train Loss: 0.00000063\n","Iter: 079/100 | Train Loss: 0.00000058\n","Iter: 080/100 | Train Loss: 0.00000053\n","Iter: 081/100 | Train Loss: 0.00000050\n","Iter: 082/100 | Train Loss: 0.00000048\n","Iter: 083/100 | Train Loss: 0.00000045\n","Iter: 084/100 | Train Loss: 0.00000043\n","Iter: 085/100 | Train Loss: 0.00000040\n","Iter: 086/100 | Train Loss: 0.00000037\n","Iter: 087/100 | Train Loss: 0.00000034\n","Iter: 088/100 | Train Loss: 0.00000032\n","Iter: 089/100 | Train Loss: 0.00000030\n","Iter: 090/100 | Train Loss: 0.00000028\n","Iter: 091/100 | Train Loss: 0.00000027\n","Iter: 092/100 | Train Loss: 0.00000026\n","Iter: 093/100 | Train Loss: 0.00000025\n","Iter: 094/100 | Train Loss: 0.00000023\n","Iter: 095/100 | Train Loss: 0.00000022\n","Iter: 096/100 | Train Loss: 0.00000022\n","Iter: 097/100 | Train Loss: 0.00000021\n","Iter: 098/100 | Train Loss: 0.00000020\n","Iter: 099/100 | Train Loss: 0.00000019\n","\n","Iter: 099/100 | Test Loss: 0.00116387 | Test acc: 58.1400\n","scale:1.100000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00134215\n","Iter: 001/100 | Train Loss: 0.00139134\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 002/100 | Train Loss: 0.00116913\n","Iter: 003/100 | Train Loss: 0.00079525\n","Iter: 004/100 | Train Loss: 0.00080281\n","Adjusting Layer 1, Kernel Nodes: 588, Adptive Nodes:212\n","Iter: 005/100 | Train Loss: 0.00069932\n","Iter: 006/100 | Train Loss: 0.00050760\n","Iter: 007/100 | Train Loss: 0.00052254\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 008/100 | Train Loss: 0.00040941\n","Iter: 009/100 | Train Loss: 0.00034819\n","Iter: 010/100 | Train Loss: 0.00032382\n","Iter: 011/100 | Train Loss: 0.00024672\n","Iter: 012/100 | Train Loss: 0.00022767\n","Iter: 013/100 | Train Loss: 0.00017249\n","Iter: 014/100 | Train Loss: 0.00016267\n","Iter: 015/100 | Train Loss: 0.00014008\n","Iter: 016/100 | Train Loss: 0.00012403\n","Iter: 017/100 | Train Loss: 0.00012433\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 018/100 | Train Loss: 0.00010031\n","Iter: 019/100 | Train Loss: 0.00009394\n","Iter: 020/100 | Train Loss: 0.00006831\n","Iter: 021/100 | Train Loss: 0.00006931\n","Adjusting Layer 1, Kernel Nodes: 626, Adptive Nodes:174\n","Iter: 022/100 | Train Loss: 0.00006151\n","Iter: 023/100 | Train Loss: 0.00004985\n","Iter: 024/100 | Train Loss: 0.00005473\n","Adjusting Layer 1, Kernel Nodes: 432, Adptive Nodes:368\n","Iter: 025/100 | Train Loss: 0.00004110\n","Iter: 026/100 | Train Loss: 0.00004699\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 027/100 | Train Loss: 0.00004211\n","Iter: 028/100 | Train Loss: 0.00003224\n","Iter: 029/100 | Train Loss: 0.00002750\n","Iter: 030/100 | Train Loss: 0.00002627\n","Iter: 031/100 | Train Loss: 0.00002496\n","Iter: 032/100 | Train Loss: 0.00002257\n","Iter: 033/100 | Train Loss: 0.00001908\n","Iter: 034/100 | Train Loss: 0.00001672\n","Iter: 035/100 | Train Loss: 0.00001406\n","Iter: 036/100 | Train Loss: 0.00001306\n","Iter: 037/100 | Train Loss: 0.00001192\n","Iter: 038/100 | Train Loss: 0.00000898\n","Iter: 039/100 | Train Loss: 0.00000953\n","Adjusting Layer 1, Kernel Nodes: 179, Adptive Nodes:621\n","Iter: 040/100 | Train Loss: 0.00001071\n","Adjusting Layer 1, Kernel Nodes: 470, Adptive Nodes:330\n","Iter: 041/100 | Train Loss: 0.00001718\n","Adjusting Layer 1, Kernel Nodes: 339, Adptive Nodes:461\n","Iter: 042/100 | Train Loss: 0.00002255\n","Adjusting Layer 1, Kernel Nodes: 464, Adptive Nodes:336\n","Iter: 043/100 | Train Loss: 0.00002316\n","Adjusting Layer 1, Kernel Nodes: 332, Adptive Nodes:468\n","Iter: 044/100 | Train Loss: 0.00001535\n","Iter: 045/100 | Train Loss: 0.00001062\n","Iter: 046/100 | Train Loss: 0.00000972\n","Iter: 047/100 | Train Loss: 0.00000797\n","Iter: 048/100 | Train Loss: 0.00001027\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 049/100 | Train Loss: 0.00003099\n","Adjusting Layer 1, Kernel Nodes: 440, Adptive Nodes:360\n","Iter: 050/100 | Train Loss: 0.00005745\n","Adjusting Layer 1, Kernel Nodes: 187, Adptive Nodes:613\n","Iter: 051/100 | Train Loss: 0.00003224\n","Iter: 052/100 | Train Loss: 0.00000516\n","Iter: 053/100 | Train Loss: 0.00001523\n","Adjusting Layer 1, Kernel Nodes: 3, Adptive Nodes:797\n","Iter: 054/100 | Train Loss: 0.00000810\n","Iter: 055/100 | Train Loss: 0.00000424\n","Iter: 056/100 | Train Loss: 0.00000883\n","Adjusting Layer 1, Kernel Nodes: 552, Adptive Nodes:248\n","Iter: 057/100 | Train Loss: 0.00000472\n","Iter: 058/100 | Train Loss: 0.00000480\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 059/100 | Train Loss: 0.00001412\n","Adjusting Layer 1, Kernel Nodes: 3, Adptive Nodes:797\n","Iter: 060/100 | Train Loss: 0.00000427\n","Iter: 061/100 | Train Loss: 0.00000830\n","Adjusting Layer 1, Kernel Nodes: 178, Adptive Nodes:622\n","Iter: 062/100 | Train Loss: 0.00000541\n","Iter: 063/100 | Train Loss: 0.00000433\n","Iter: 064/100 | Train Loss: 0.00000553\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 065/100 | Train Loss: 0.00000268\n","Iter: 066/100 | Train Loss: 0.00000467\n","Adjusting Layer 1, Kernel Nodes: 206, Adptive Nodes:594\n","Iter: 067/100 | Train Loss: 0.00000220\n","Iter: 068/100 | Train Loss: 0.00000347\n","Adjusting Layer 1, Kernel Nodes: 593, Adptive Nodes:207\n","Iter: 069/100 | Train Loss: 0.00000264\n","Iter: 070/100 | Train Loss: 0.00000256\n","Iter: 071/100 | Train Loss: 0.00000237\n","Iter: 072/100 | Train Loss: 0.00000190\n","Iter: 073/100 | Train Loss: 0.00000210\n","Adjusting Layer 1, Kernel Nodes: 208, Adptive Nodes:592\n","Iter: 074/100 | Train Loss: 0.00000159\n","Iter: 075/100 | Train Loss: 0.00000213\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 076/100 | Train Loss: 0.00000186\n","Iter: 077/100 | Train Loss: 0.00000203\n","Adjusting Layer 1, Kernel Nodes: 207, Adptive Nodes:593\n","Iter: 078/100 | Train Loss: 0.00000217\n","Adjusting Layer 1, Kernel Nodes: 593, Adptive Nodes:207\n","Iter: 079/100 | Train Loss: 0.00000171\n","Iter: 080/100 | Train Loss: 0.00000264\n","Adjusting Layer 1, Kernel Nodes: 208, Adptive Nodes:592\n","Iter: 081/100 | Train Loss: 0.00000160\n","Iter: 082/100 | Train Loss: 0.00000289\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 083/100 | Train Loss: 0.00000281\n","Iter: 084/100 | Train Loss: 0.00000275\n","Iter: 085/100 | Train Loss: 0.00000285\n","Adjusting Layer 1, Kernel Nodes: 208, Adptive Nodes:592\n","Iter: 086/100 | Train Loss: 0.00000287\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 087/100 | Train Loss: 0.00000298\n","Adjusting Layer 1, Kernel Nodes: 208, Adptive Nodes:592\n","Iter: 088/100 | Train Loss: 0.00000186\n","Iter: 089/100 | Train Loss: 0.00000347\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 090/100 | Train Loss: 0.00000135\n","Iter: 091/100 | Train Loss: 0.00000343\n","Adjusting Layer 1, Kernel Nodes: 208, Adptive Nodes:592\n","Iter: 092/100 | Train Loss: 0.00000184\n","Iter: 093/100 | Train Loss: 0.00000339\n","Adjusting Layer 1, Kernel Nodes: 592, Adptive Nodes:208\n","Iter: 094/100 | Train Loss: 0.00000364\n","Adjusting Layer 1, Kernel Nodes: 208, Adptive Nodes:592\n","Iter: 095/100 | Train Loss: 0.00000159\n","Iter: 096/100 | Train Loss: 0.00000418\n","Adjusting Layer 1, Kernel Nodes: 591, Adptive Nodes:209\n","Iter: 097/100 | Train Loss: 0.00000047\n","Iter: 098/100 | Train Loss: 0.00000435\n","Adjusting Layer 1, Kernel Nodes: 210, Adptive Nodes:590\n","Iter: 099/100 | Train Loss: 0.00000072\n","\n","Iter: 099/100 | Test Loss: 0.00107684 | Test acc: 63.1000\n","scale:1.100000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00133750\n","Iter: 001/100 | Train Loss: 0.00138963\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 002/100 | Train Loss: 0.00105252\n","Iter: 003/100 | Train Loss: 0.00077020\n","Iter: 004/100 | Train Loss: 0.00082284\n","Adjusting Layer 1, Kernel Nodes: 630, Adptive Nodes:170\n","Iter: 005/100 | Train Loss: 0.00059583\n","Iter: 006/100 | Train Loss: 0.00057718\n","Iter: 007/100 | Train Loss: 0.00043817\n","Iter: 008/100 | Train Loss: 0.00043361\n","Iter: 009/100 | Train Loss: 0.00032688\n","Iter: 010/100 | Train Loss: 0.00034168\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 011/100 | Train Loss: 0.00025845\n","Iter: 012/100 | Train Loss: 0.00026197\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 013/100 | Train Loss: 0.00018438\n","Iter: 014/100 | Train Loss: 0.00018704\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 015/100 | Train Loss: 0.00013658\n","Iter: 016/100 | Train Loss: 0.00014601\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 017/100 | Train Loss: 0.00011026\n","Iter: 018/100 | Train Loss: 0.00012001\n","Adjusting Layer 1, Kernel Nodes: 740, Adptive Nodes:60\n","Iter: 019/100 | Train Loss: 0.00008959\n","Iter: 020/100 | Train Loss: 0.00008452\n","Iter: 021/100 | Train Loss: 0.00006690\n","Iter: 022/100 | Train Loss: 0.00005471\n","Iter: 023/100 | Train Loss: 0.00005601\n","Adjusting Layer 1, Kernel Nodes: 498, Adptive Nodes:302\n","Iter: 024/100 | Train Loss: 0.00004184\n","Iter: 025/100 | Train Loss: 0.00004787\n","Adjusting Layer 1, Kernel Nodes: 792, Adptive Nodes:8\n","Iter: 026/100 | Train Loss: 0.00004131\n","Iter: 027/100 | Train Loss: 0.00003197\n","Iter: 028/100 | Train Loss: 0.00002995\n","Iter: 029/100 | Train Loss: 0.00002562\n","Iter: 030/100 | Train Loss: 0.00002463\n","Iter: 031/100 | Train Loss: 0.00002216\n","Iter: 032/100 | Train Loss: 0.00002068\n","Iter: 033/100 | Train Loss: 0.00001824\n","Iter: 034/100 | Train Loss: 0.00001591\n","Iter: 035/100 | Train Loss: 0.00001464\n","Iter: 036/100 | Train Loss: 0.00001276\n","Iter: 037/100 | Train Loss: 0.00001193\n","Iter: 038/100 | Train Loss: 0.00001003\n","Iter: 039/100 | Train Loss: 0.00000903\n","Iter: 040/100 | Train Loss: 0.00000736\n","Iter: 041/100 | Train Loss: 0.00000683\n","Iter: 042/100 | Train Loss: 0.00000587\n","Iter: 043/100 | Train Loss: 0.00000595\n","Adjusting Layer 1, Kernel Nodes: 317, Adptive Nodes:483\n","Iter: 044/100 | Train Loss: 0.00000579\n","Iter: 045/100 | Train Loss: 0.00000519\n","Iter: 046/100 | Train Loss: 0.00000454\n","Iter: 047/100 | Train Loss: 0.00000397\n","Iter: 048/100 | Train Loss: 0.00000364\n","Iter: 049/100 | Train Loss: 0.00000294\n","Iter: 050/100 | Train Loss: 0.00000322\n","Adjusting Layer 1, Kernel Nodes: 289, Adptive Nodes:511\n","Iter: 051/100 | Train Loss: 0.00000326\n","Adjusting Layer 1, Kernel Nodes: 436, Adptive Nodes:364\n","Iter: 052/100 | Train Loss: 0.00000386\n","Adjusting Layer 1, Kernel Nodes: 367, Adptive Nodes:433\n","Iter: 053/100 | Train Loss: 0.00000305\n","Iter: 054/100 | Train Loss: 0.00000216\n","Iter: 055/100 | Train Loss: 0.00000325\n","Adjusting Layer 1, Kernel Nodes: 412, Adptive Nodes:388\n","Iter: 056/100 | Train Loss: 0.00000194\n","Iter: 057/100 | Train Loss: 0.00000185\n","Iter: 058/100 | Train Loss: 0.00000196\n","Adjusting Layer 1, Kernel Nodes: 384, Adptive Nodes:416\n","Iter: 059/100 | Train Loss: 0.00000215\n","Adjusting Layer 1, Kernel Nodes: 413, Adptive Nodes:387\n","Iter: 060/100 | Train Loss: 0.00000170\n","Iter: 061/100 | Train Loss: 0.00000125\n","Iter: 062/100 | Train Loss: 0.00000199\n","Adjusting Layer 1, Kernel Nodes: 386, Adptive Nodes:414\n","Iter: 063/100 | Train Loss: 0.00000108\n","Iter: 064/100 | Train Loss: 0.00000103\n","Iter: 065/100 | Train Loss: 0.00000132\n","Adjusting Layer 1, Kernel Nodes: 412, Adptive Nodes:388\n","Iter: 066/100 | Train Loss: 0.00000135\n","Adjusting Layer 1, Kernel Nodes: 757, Adptive Nodes:43\n","Iter: 067/100 | Train Loss: 0.00000295\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 068/100 | Train Loss: 0.00000912\n","Adjusting Layer 1, Kernel Nodes: 522, Adptive Nodes:278\n","Iter: 069/100 | Train Loss: 0.00001261\n","Adjusting Layer 1, Kernel Nodes: 286, Adptive Nodes:514\n","Iter: 070/100 | Train Loss: 0.00001467\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 071/100 | Train Loss: 0.00002605\n","Adjusting Layer 1, Kernel Nodes: 2, Adptive Nodes:798\n","Iter: 072/100 | Train Loss: 0.00001612\n","Iter: 073/100 | Train Loss: 0.00000243\n","Iter: 074/100 | Train Loss: 0.00000818\n","Adjusting Layer 1, Kernel Nodes: 139, Adptive Nodes:661\n","Iter: 075/100 | Train Loss: 0.00000862\n","Adjusting Layer 1, Kernel Nodes: 550, Adptive Nodes:250\n","Iter: 076/100 | Train Loss: 0.00000596\n","Iter: 077/100 | Train Loss: 0.00000533\n","Iter: 078/100 | Train Loss: 0.00000767\n","Adjusting Layer 1, Kernel Nodes: 240, Adptive Nodes:560\n","Iter: 079/100 | Train Loss: 0.00000348\n","Iter: 080/100 | Train Loss: 0.00000371\n","Adjusting Layer 1, Kernel Nodes: 557, Adptive Nodes:243\n","Iter: 081/100 | Train Loss: 0.00000418\n","Adjusting Layer 1, Kernel Nodes: 244, Adptive Nodes:556\n","Iter: 082/100 | Train Loss: 0.00000235\n","Iter: 083/100 | Train Loss: 0.00000377\n","Adjusting Layer 1, Kernel Nodes: 557, Adptive Nodes:243\n","Iter: 084/100 | Train Loss: 0.00000365\n","Iter: 085/100 | Train Loss: 0.00000303\n","Iter: 086/100 | Train Loss: 0.00000255\n","Iter: 087/100 | Train Loss: 0.00000248\n","Iter: 088/100 | Train Loss: 0.00000251\n","Adjusting Layer 1, Kernel Nodes: 244, Adptive Nodes:556\n","Iter: 089/100 | Train Loss: 0.00000249\n","Iter: 090/100 | Train Loss: 0.00000307\n","Adjusting Layer 1, Kernel Nodes: 558, Adptive Nodes:242\n","Iter: 091/100 | Train Loss: 0.00000249\n","Iter: 092/100 | Train Loss: 0.00000335\n","Adjusting Layer 1, Kernel Nodes: 242, Adptive Nodes:558\n","Iter: 093/100 | Train Loss: 0.00000215\n","Iter: 094/100 | Train Loss: 0.00000297\n","Adjusting Layer 1, Kernel Nodes: 558, Adptive Nodes:242\n","Iter: 095/100 | Train Loss: 0.00000300\n","Adjusting Layer 1, Kernel Nodes: 243, Adptive Nodes:557\n","Iter: 096/100 | Train Loss: 0.00000322\n","Adjusting Layer 1, Kernel Nodes: 557, Adptive Nodes:243\n","Iter: 097/100 | Train Loss: 0.00000295\n","Iter: 098/100 | Train Loss: 0.00000224\n","Iter: 099/100 | Train Loss: 0.00000420\n","\n","Iter: 099/100 | Test Loss: 0.00104716 | Test acc: 64.0300\n","scale:1.100000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 000/100 | Train Loss: 0.00133666\n","Iter: 001/100 | Train Loss: 0.00139641\n","Adjusting Layer 1, Kernel Nodes: 646, Adptive Nodes:154\n","Iter: 002/100 | Train Loss: 0.00102055\n","Iter: 003/100 | Train Loss: 0.00077990\n","Iter: 004/100 | Train Loss: 0.00082805\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 005/100 | Train Loss: 0.00056265\n","Iter: 006/100 | Train Loss: 0.00058481\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 007/100 | Train Loss: 0.00041206\n","Iter: 008/100 | Train Loss: 0.00045208\n","Adjusting Layer 1, Kernel Nodes: 694, Adptive Nodes:106\n","Iter: 009/100 | Train Loss: 0.00031217\n","Iter: 010/100 | Train Loss: 0.00034787\n","Adjusting Layer 1, Kernel Nodes: 763, Adptive Nodes:37\n","Iter: 011/100 | Train Loss: 0.00025070\n","Iter: 012/100 | Train Loss: 0.00025528\n","Adjusting Layer 1, Kernel Nodes: 755, Adptive Nodes:45\n","Iter: 013/100 | Train Loss: 0.00019924\n","Iter: 014/100 | Train Loss: 0.00017206\n","Iter: 015/100 | Train Loss: 0.00016704\n","Iter: 016/100 | Train Loss: 0.00011962\n","Iter: 017/100 | Train Loss: 0.00013637\n","Adjusting Layer 1, Kernel Nodes: 638, Adptive Nodes:162\n","Iter: 018/100 | Train Loss: 0.00010123\n","Iter: 019/100 | Train Loss: 0.00009588\n","Iter: 020/100 | Train Loss: 0.00008963\n","Iter: 021/100 | Train Loss: 0.00006463\n","Iter: 022/100 | Train Loss: 0.00006752\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 023/100 | Train Loss: 0.00005470\n","Iter: 024/100 | Train Loss: 0.00004528\n","Iter: 025/100 | Train Loss: 0.00004696\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 026/100 | Train Loss: 0.00003443\n","Iter: 027/100 | Train Loss: 0.00003547\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 028/100 | Train Loss: 0.00003213\n","Iter: 029/100 | Train Loss: 0.00002436\n","Iter: 030/100 | Train Loss: 0.00002835\n","Adjusting Layer 1, Kernel Nodes: 707, Adptive Nodes:93\n","Iter: 031/100 | Train Loss: 0.00001907\n","Iter: 032/100 | Train Loss: 0.00002208\n","Adjusting Layer 1, Kernel Nodes: 764, Adptive Nodes:36\n","Iter: 033/100 | Train Loss: 0.00001664\n","Iter: 034/100 | Train Loss: 0.00001517\n","Iter: 035/100 | Train Loss: 0.00001544\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 036/100 | Train Loss: 0.00001183\n","Iter: 037/100 | Train Loss: 0.00001420\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 038/100 | Train Loss: 0.00000932\n","Iter: 039/100 | Train Loss: 0.00001133\n","Adjusting Layer 1, Kernel Nodes: 778, Adptive Nodes:22\n","Iter: 040/100 | Train Loss: 0.00000721\n","Iter: 041/100 | Train Loss: 0.00000895\n","Adjusting Layer 1, Kernel Nodes: 516, Adptive Nodes:284\n","Iter: 042/100 | Train Loss: 0.00000579\n","Iter: 043/100 | Train Loss: 0.00000709\n","Adjusting Layer 1, Kernel Nodes: 720, Adptive Nodes:80\n","Iter: 044/100 | Train Loss: 0.00000554\n","Iter: 045/100 | Train Loss: 0.00000545\n","Iter: 046/100 | Train Loss: 0.00000441\n","Iter: 047/100 | Train Loss: 0.00000397\n","Iter: 048/100 | Train Loss: 0.00000348\n","Iter: 049/100 | Train Loss: 0.00000331\n","Iter: 050/100 | Train Loss: 0.00000323\n","Iter: 051/100 | Train Loss: 0.00000290\n","Iter: 052/100 | Train Loss: 0.00000276\n","Iter: 053/100 | Train Loss: 0.00000214\n","Iter: 054/100 | Train Loss: 0.00000206\n","Iter: 055/100 | Train Loss: 0.00000160\n","Iter: 056/100 | Train Loss: 0.00000172\n","Adjusting Layer 1, Kernel Nodes: 406, Adptive Nodes:394\n","Iter: 057/100 | Train Loss: 0.00000143\n","Iter: 058/100 | Train Loss: 0.00000148\n","Adjusting Layer 1, Kernel Nodes: 444, Adptive Nodes:356\n","Iter: 059/100 | Train Loss: 0.00000121\n","Iter: 060/100 | Train Loss: 0.00000119\n","Iter: 061/100 | Train Loss: 0.00000092\n","Iter: 062/100 | Train Loss: 0.00000095\n","Adjusting Layer 1, Kernel Nodes: 498, Adptive Nodes:302\n","Iter: 063/100 | Train Loss: 0.00000102\n","Adjusting Layer 1, Kernel Nodes: 471, Adptive Nodes:329\n","Iter: 064/100 | Train Loss: 0.00000092\n","Iter: 065/100 | Train Loss: 0.00000082\n","Iter: 066/100 | Train Loss: 0.00000068\n","Iter: 067/100 | Train Loss: 0.00000071\n","Adjusting Layer 1, Kernel Nodes: 369, Adptive Nodes:431\n","Iter: 068/100 | Train Loss: 0.00000063\n","Iter: 069/100 | Train Loss: 0.00000061\n","Iter: 070/100 | Train Loss: 0.00000059\n","Iter: 071/100 | Train Loss: 0.00000051\n","Iter: 072/100 | Train Loss: 0.00000052\n","Adjusting Layer 1, Kernel Nodes: 462, Adptive Nodes:338\n","Iter: 073/100 | Train Loss: 0.00000037\n","Iter: 074/100 | Train Loss: 0.00000039\n","Adjusting Layer 1, Kernel Nodes: 483, Adptive Nodes:317\n","Iter: 075/100 | Train Loss: 0.00000048\n","Adjusting Layer 1, Kernel Nodes: 416, Adptive Nodes:384\n","Iter: 076/100 | Train Loss: 0.00000063\n","Adjusting Layer 1, Kernel Nodes: 447, Adptive Nodes:353\n","Iter: 077/100 | Train Loss: 0.00000060\n","Iter: 078/100 | Train Loss: 0.00000039\n","Iter: 079/100 | Train Loss: 0.00000048\n","Adjusting Layer 1, Kernel Nodes: 377, Adptive Nodes:423\n","Iter: 080/100 | Train Loss: 0.00000035\n","Iter: 081/100 | Train Loss: 0.00000027\n","Iter: 082/100 | Train Loss: 0.00000031\n","Adjusting Layer 1, Kernel Nodes: 462, Adptive Nodes:338\n","Iter: 083/100 | Train Loss: 0.00000052\n","Adjusting Layer 1, Kernel Nodes: 369, Adptive Nodes:431\n","Iter: 084/100 | Train Loss: 0.00000053\n","Adjusting Layer 1, Kernel Nodes: 439, Adptive Nodes:361\n","Iter: 085/100 | Train Loss: 0.00000041\n","Iter: 086/100 | Train Loss: 0.00000033\n","Iter: 087/100 | Train Loss: 0.00000044\n","Adjusting Layer 1, Kernel Nodes: 351, Adptive Nodes:449\n","Iter: 088/100 | Train Loss: 0.00000024\n","Iter: 089/100 | Train Loss: 0.00000019\n","Iter: 090/100 | Train Loss: 0.00000022\n","Adjusting Layer 1, Kernel Nodes: 428, Adptive Nodes:372\n","Iter: 091/100 | Train Loss: 0.00000032\n","Adjusting Layer 1, Kernel Nodes: 378, Adptive Nodes:422\n","Iter: 092/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 420, Adptive Nodes:380\n","Iter: 093/100 | Train Loss: 0.00000021\n","Iter: 094/100 | Train Loss: 0.00000013\n","Iter: 095/100 | Train Loss: 0.00000025\n","Adjusting Layer 1, Kernel Nodes: 374, Adptive Nodes:426\n","Iter: 096/100 | Train Loss: 0.00000017\n","Iter: 097/100 | Train Loss: 0.00000013\n","Iter: 098/100 | Train Loss: 0.00000013\n","Iter: 099/100 | Train Loss: 0.00000011\n","\n","Iter: 099/100 | Test Loss: 0.00100239 | Test acc: 64.6900\n","scale:1.100000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00133722\n","Iter: 001/100 | Train Loss: 0.00141421\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 002/100 | Train Loss: 0.00104846\n","Iter: 003/100 | Train Loss: 0.00078005\n","Iter: 004/100 | Train Loss: 0.00084013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00054681\n","Iter: 006/100 | Train Loss: 0.00058120\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00040279\n","Iter: 008/100 | Train Loss: 0.00044358\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 009/100 | Train Loss: 0.00032107\n","Iter: 010/100 | Train Loss: 0.00031803\n","Iter: 011/100 | Train Loss: 0.00029115\n","Iter: 012/100 | Train Loss: 0.00020976\n","Iter: 013/100 | Train Loss: 0.00023724\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00017081\n","Iter: 015/100 | Train Loss: 0.00014750\n","Iter: 016/100 | Train Loss: 0.00015620\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00011256\n","Iter: 018/100 | Train Loss: 0.00010147\n","Iter: 019/100 | Train Loss: 0.00010772\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 020/100 | Train Loss: 0.00007552\n","Iter: 021/100 | Train Loss: 0.00006475\n","Iter: 022/100 | Train Loss: 0.00007281\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 023/100 | Train Loss: 0.00005227\n","Iter: 024/100 | Train Loss: 0.00004478\n","Iter: 025/100 | Train Loss: 0.00005006\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 026/100 | Train Loss: 0.00003672\n","Iter: 027/100 | Train Loss: 0.00002916\n","Iter: 028/100 | Train Loss: 0.00003101\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 029/100 | Train Loss: 0.00003015\n","Iter: 030/100 | Train Loss: 0.00002160\n","Iter: 031/100 | Train Loss: 0.00002118\n","Iter: 032/100 | Train Loss: 0.00002297\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 033/100 | Train Loss: 0.00001576\n","Iter: 034/100 | Train Loss: 0.00001315\n","Iter: 035/100 | Train Loss: 0.00001501\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 036/100 | Train Loss: 0.00001376\n","Iter: 037/100 | Train Loss: 0.00001008\n","Iter: 038/100 | Train Loss: 0.00001042\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 039/100 | Train Loss: 0.00001081\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 040/100 | Train Loss: 0.00000777\n","Iter: 041/100 | Train Loss: 0.00000637\n","Iter: 042/100 | Train Loss: 0.00000704\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 043/100 | Train Loss: 0.00000647\n","Iter: 044/100 | Train Loss: 0.00000479\n","Iter: 045/100 | Train Loss: 0.00000450\n","Iter: 046/100 | Train Loss: 0.00000481\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 047/100 | Train Loss: 0.00000388\n","Iter: 048/100 | Train Loss: 0.00000281\n","Iter: 049/100 | Train Loss: 0.00000319\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 050/100 | Train Loss: 0.00000325\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 051/100 | Train Loss: 0.00000233\n","Iter: 052/100 | Train Loss: 0.00000206\n","Iter: 053/100 | Train Loss: 0.00000228\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 054/100 | Train Loss: 0.00000197\n","Iter: 055/100 | Train Loss: 0.00000149\n","Iter: 056/100 | Train Loss: 0.00000150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 057/100 | Train Loss: 0.00000165\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 058/100 | Train Loss: 0.00000129\n","Iter: 059/100 | Train Loss: 0.00000103\n","Iter: 060/100 | Train Loss: 0.00000106\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 061/100 | Train Loss: 0.00000103\n","Iter: 062/100 | Train Loss: 0.00000080\n","Iter: 063/100 | Train Loss: 0.00000068\n","Iter: 064/100 | Train Loss: 0.00000080\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 065/100 | Train Loss: 0.00000072\n","Iter: 066/100 | Train Loss: 0.00000054\n","Iter: 067/100 | Train Loss: 0.00000056\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 068/100 | Train Loss: 0.00000058\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 069/100 | Train Loss: 0.00000049\n","Iter: 070/100 | Train Loss: 0.00000040\n","Iter: 071/100 | Train Loss: 0.00000043\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 072/100 | Train Loss: 0.00000042\n","Iter: 073/100 | Train Loss: 0.00000034\n","Iter: 074/100 | Train Loss: 0.00000031\n","Iter: 075/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 076/100 | Train Loss: 0.00000031\n","Iter: 077/100 | Train Loss: 0.00000026\n","Iter: 078/100 | Train Loss: 0.00000025\n","Iter: 079/100 | Train Loss: 0.00000026\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 080/100 | Train Loss: 0.00000023\n","Iter: 081/100 | Train Loss: 0.00000019\n","Iter: 082/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 083/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 084/100 | Train Loss: 0.00000018\n","Iter: 085/100 | Train Loss: 0.00000017\n","Iter: 086/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 087/100 | Train Loss: 0.00000017\n","Iter: 088/100 | Train Loss: 0.00000015\n","Iter: 089/100 | Train Loss: 0.00000014\n","Iter: 090/100 | Train Loss: 0.00000015\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 091/100 | Train Loss: 0.00000014\n","Iter: 092/100 | Train Loss: 0.00000013\n","Iter: 093/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 094/100 | Train Loss: 0.00000013\n","Iter: 095/100 | Train Loss: 0.00000012\n","Iter: 096/100 | Train Loss: 0.00000012\n","Iter: 097/100 | Train Loss: 0.00000012\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 098/100 | Train Loss: 0.00000012\n","Iter: 099/100 | Train Loss: 0.00000011\n","\n","Iter: 099/100 | Test Loss: 0.00096208 | Test acc: 65.5500\n","scale:1.100000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00133257\n","Iter: 001/100 | Train Loss: 0.00143726\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 002/100 | Train Loss: 0.00108134\n","Iter: 003/100 | Train Loss: 0.00080986\n","Iter: 004/100 | Train Loss: 0.00083446\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00055710\n","Iter: 006/100 | Train Loss: 0.00054429\n","Iter: 007/100 | Train Loss: 0.00042219\n","Iter: 008/100 | Train Loss: 0.00039288\n","Iter: 009/100 | Train Loss: 0.00037302\n","Iter: 010/100 | Train Loss: 0.00026685\n","Iter: 011/100 | Train Loss: 0.00030330\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00022728\n","Iter: 013/100 | Train Loss: 0.00018166\n","Iter: 014/100 | Train Loss: 0.00020449\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00014511\n","Iter: 016/100 | Train Loss: 0.00011822\n","Iter: 017/100 | Train Loss: 0.00012684\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 018/100 | Train Loss: 0.00010179\n","Iter: 019/100 | Train Loss: 0.00007376\n","Iter: 020/100 | Train Loss: 0.00008578\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 021/100 | Train Loss: 0.00006637\n","Iter: 022/100 | Train Loss: 0.00005265\n","Iter: 023/100 | Train Loss: 0.00004804\n","Iter: 024/100 | Train Loss: 0.00005185\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 025/100 | Train Loss: 0.00003401\n","Iter: 026/100 | Train Loss: 0.00003391\n","Iter: 027/100 | Train Loss: 0.00003392\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 028/100 | Train Loss: 0.00002983\n","Iter: 029/100 | Train Loss: 0.00002029\n","Iter: 030/100 | Train Loss: 0.00002087\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 031/100 | Train Loss: 0.00002245\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 032/100 | Train Loss: 0.00001967\n","Iter: 033/100 | Train Loss: 0.00001734\n","Iter: 034/100 | Train Loss: 0.00001633\n","Iter: 035/100 | Train Loss: 0.00001063\n","Iter: 036/100 | Train Loss: 0.00000874\n","Iter: 037/100 | Train Loss: 0.00000901\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 038/100 | Train Loss: 0.00001399\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 039/100 | Train Loss: 0.00001110\n","Iter: 040/100 | Train Loss: 0.00001642\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 041/100 | Train Loss: 0.00002459\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 042/100 | Train Loss: 0.00012781\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 043/100 | Train Loss: 0.00064777\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 044/100 | Train Loss: 0.00247564\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 045/100 | Train Loss: 0.00068894\n","Iter: 046/100 | Train Loss: 0.00082247\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 047/100 | Train Loss: 0.00098332\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 048/100 | Train Loss: 0.00088863\n","Iter: 049/100 | Train Loss: 0.00076555\n","Iter: 050/100 | Train Loss: 0.00049823\n","Iter: 051/100 | Train Loss: 0.00055710\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 052/100 | Train Loss: 0.00049139\n","Iter: 053/100 | Train Loss: 0.00041207\n","Iter: 054/100 | Train Loss: 0.00052731\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 055/100 | Train Loss: 0.00082652\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 056/100 | Train Loss: 0.00074256\n","Iter: 057/100 | Train Loss: 0.00131659\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 058/100 | Train Loss: 0.00067162\n","Iter: 059/100 | Train Loss: 0.00088343\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 060/100 | Train Loss: 0.00066492\n","Iter: 061/100 | Train Loss: 0.00057747\n","Iter: 062/100 | Train Loss: 0.00044696\n","Iter: 063/100 | Train Loss: 0.00049190\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 064/100 | Train Loss: 0.00035606\n","Iter: 065/100 | Train Loss: 0.00042789\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 066/100 | Train Loss: 0.00055717\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 067/100 | Train Loss: 0.00035640\n","Iter: 068/100 | Train Loss: 0.00029786\n","Iter: 069/100 | Train Loss: 0.00028268\n","Iter: 070/100 | Train Loss: 0.00024635\n","Iter: 071/100 | Train Loss: 0.00028131\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 072/100 | Train Loss: 0.00024064\n","Iter: 073/100 | Train Loss: 0.00029700\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 074/100 | Train Loss: 0.00034921\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 075/100 | Train Loss: 0.00026837\n","Iter: 076/100 | Train Loss: 0.00018133\n","Iter: 077/100 | Train Loss: 0.00014544\n","Iter: 078/100 | Train Loss: 0.00014840\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 079/100 | Train Loss: 0.00018914\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 080/100 | Train Loss: 0.00018336\n","Iter: 081/100 | Train Loss: 0.00018455\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 082/100 | Train Loss: 0.00011918\n","Iter: 083/100 | Train Loss: 0.00013419\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 084/100 | Train Loss: 0.00009089\n","Iter: 085/100 | Train Loss: 0.00012513\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 086/100 | Train Loss: 0.00010026\n","Iter: 087/100 | Train Loss: 0.00008754\n","Iter: 088/100 | Train Loss: 0.00007647\n","Iter: 089/100 | Train Loss: 0.00006058\n","Iter: 090/100 | Train Loss: 0.00006305\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 091/100 | Train Loss: 0.00004455\n","Iter: 092/100 | Train Loss: 0.00004740\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 093/100 | Train Loss: 0.00004220\n","Iter: 094/100 | Train Loss: 0.00003756\n","Iter: 095/100 | Train Loss: 0.00003006\n","Iter: 096/100 | Train Loss: 0.00003007\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 097/100 | Train Loss: 0.00002825\n","Iter: 098/100 | Train Loss: 0.00002409\n","Iter: 099/100 | Train Loss: 0.00002090\n","\n","Iter: 099/100 | Test Loss: 0.00083617 | Test acc: 64.5100\n","scale:1.150000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00159854\n","Iter: 001/100 | Train Loss: 0.00296060\n","Adjusting Layer 1, Kernel Nodes: 534, Adptive Nodes:266\n","Iter: 002/100 | Train Loss: 0.01462660\n","Adjusting Layer 1, Kernel Nodes: 386, Adptive Nodes:414\n","Iter: 003/100 | Train Loss: 0.03432768\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 004/100 | Train Loss: 0.03587815\n","Adjusting Layer 1, Kernel Nodes: 333, Adptive Nodes:467\n","Iter: 005/100 | Train Loss: 0.00159534\n","Iter: 006/100 | Train Loss: 0.00403741\n","Adjusting Layer 1, Kernel Nodes: 730, Adptive Nodes:70\n","Iter: 007/100 | Train Loss: 0.00367581\n","Iter: 008/100 | Train Loss: 0.00193629\n","Iter: 009/100 | Train Loss: 0.00117525\n","Iter: 010/100 | Train Loss: 0.00101823\n","Iter: 011/100 | Train Loss: 0.00100400\n","Iter: 012/100 | Train Loss: 0.00100134\n","Iter: 013/100 | Train Loss: 0.00098270\n","Iter: 014/100 | Train Loss: 0.00093506\n","Iter: 015/100 | Train Loss: 0.00086715\n","Iter: 016/100 | Train Loss: 0.00078839\n","Iter: 017/100 | Train Loss: 0.00070919\n","Iter: 018/100 | Train Loss: 0.00063539\n","Iter: 019/100 | Train Loss: 0.00057174\n","Iter: 020/100 | Train Loss: 0.00052082\n","Iter: 021/100 | Train Loss: 0.00048204\n","Iter: 022/100 | Train Loss: 0.00044783\n","Iter: 023/100 | Train Loss: 0.00041325\n","Iter: 024/100 | Train Loss: 0.00037648\n","Iter: 025/100 | Train Loss: 0.00034191\n","Iter: 026/100 | Train Loss: 0.00031498\n","Iter: 027/100 | Train Loss: 0.00029615\n","Iter: 028/100 | Train Loss: 0.00027710\n","Iter: 029/100 | Train Loss: 0.00025273\n","Iter: 030/100 | Train Loss: 0.00022854\n","Iter: 031/100 | Train Loss: 0.00020961\n","Iter: 032/100 | Train Loss: 0.00019405\n","Iter: 033/100 | Train Loss: 0.00017798\n","Iter: 034/100 | Train Loss: 0.00016225\n","Iter: 035/100 | Train Loss: 0.00014956\n","Iter: 036/100 | Train Loss: 0.00013999\n","Iter: 037/100 | Train Loss: 0.00013058\n","Iter: 038/100 | Train Loss: 0.00011921\n","Iter: 039/100 | Train Loss: 0.00010849\n","Iter: 040/100 | Train Loss: 0.00010048\n","Iter: 041/100 | Train Loss: 0.00009369\n","Iter: 042/100 | Train Loss: 0.00008604\n","Iter: 043/100 | Train Loss: 0.00007878\n","Iter: 044/100 | Train Loss: 0.00007259\n","Iter: 045/100 | Train Loss: 0.00006689\n","Iter: 046/100 | Train Loss: 0.00006180\n","Iter: 047/100 | Train Loss: 0.00005718\n","Iter: 048/100 | Train Loss: 0.00005277\n","Iter: 049/100 | Train Loss: 0.00004892\n","Iter: 050/100 | Train Loss: 0.00004547\n","Iter: 051/100 | Train Loss: 0.00004252\n","Iter: 052/100 | Train Loss: 0.00004000\n","Iter: 053/100 | Train Loss: 0.00003721\n","Iter: 054/100 | Train Loss: 0.00003444\n","Iter: 055/100 | Train Loss: 0.00003208\n","Iter: 056/100 | Train Loss: 0.00002974\n","Iter: 057/100 | Train Loss: 0.00002734\n","Iter: 058/100 | Train Loss: 0.00002525\n","Iter: 059/100 | Train Loss: 0.00002358\n","Iter: 060/100 | Train Loss: 0.00002212\n","Iter: 061/100 | Train Loss: 0.00002067\n","Iter: 062/100 | Train Loss: 0.00001928\n","Iter: 063/100 | Train Loss: 0.00001817\n","Iter: 064/100 | Train Loss: 0.00001710\n","Iter: 065/100 | Train Loss: 0.00001596\n","Iter: 066/100 | Train Loss: 0.00001501\n","Iter: 067/100 | Train Loss: 0.00001428\n","Iter: 068/100 | Train Loss: 0.00001347\n","Iter: 069/100 | Train Loss: 0.00001261\n","Iter: 070/100 | Train Loss: 0.00001186\n","Iter: 071/100 | Train Loss: 0.00001119\n","Iter: 072/100 | Train Loss: 0.00001060\n","Iter: 073/100 | Train Loss: 0.00001003\n","Iter: 074/100 | Train Loss: 0.00000949\n","Iter: 075/100 | Train Loss: 0.00000903\n","Iter: 076/100 | Train Loss: 0.00000859\n","Iter: 077/100 | Train Loss: 0.00000817\n","Iter: 078/100 | Train Loss: 0.00000778\n","Iter: 079/100 | Train Loss: 0.00000741\n","Iter: 080/100 | Train Loss: 0.00000708\n","Iter: 081/100 | Train Loss: 0.00000678\n","Iter: 082/100 | Train Loss: 0.00000648\n","Iter: 083/100 | Train Loss: 0.00000619\n","Iter: 084/100 | Train Loss: 0.00000593\n","Iter: 085/100 | Train Loss: 0.00000569\n","Iter: 086/100 | Train Loss: 0.00000545\n","Iter: 087/100 | Train Loss: 0.00000522\n","Iter: 088/100 | Train Loss: 0.00000502\n","Iter: 089/100 | Train Loss: 0.00000481\n","Iter: 090/100 | Train Loss: 0.00000460\n","Iter: 091/100 | Train Loss: 0.00000443\n","Iter: 092/100 | Train Loss: 0.00000426\n","Iter: 093/100 | Train Loss: 0.00000410\n","Iter: 094/100 | Train Loss: 0.00000395\n","Iter: 095/100 | Train Loss: 0.00000380\n","Iter: 096/100 | Train Loss: 0.00000367\n","Iter: 097/100 | Train Loss: 0.00000354\n","Iter: 098/100 | Train Loss: 0.00000342\n","Iter: 099/100 | Train Loss: 0.00000330\n","\n","Iter: 099/100 | Test Loss: 0.00104850 | Test acc: 59.2100\n","scale:1.150000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00149396\n","Iter: 001/100 | Train Loss: 0.00203905\n","Adjusting Layer 1, Kernel Nodes: 554, Adptive Nodes:246\n","Iter: 002/100 | Train Loss: 0.00600387\n","Adjusting Layer 1, Kernel Nodes: 501, Adptive Nodes:299\n","Iter: 003/100 | Train Loss: 0.02606399\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 004/100 | Train Loss: 0.27595553\n","Adjusting Layer 1, Kernel Nodes: 210, Adptive Nodes:590\n","Iter: 005/100 | Train Loss: 0.00870833\n","Iter: 006/100 | Train Loss: 0.00155513\n","Iter: 007/100 | Train Loss: 0.00137043\n","Iter: 008/100 | Train Loss: 0.00130947\n","Iter: 009/100 | Train Loss: 0.00126089\n","Iter: 010/100 | Train Loss: 0.00117177\n","Iter: 011/100 | Train Loss: 0.00108881\n","Iter: 012/100 | Train Loss: 0.00101264\n","Iter: 013/100 | Train Loss: 0.00094994\n","Iter: 014/100 | Train Loss: 0.00091662\n","Iter: 015/100 | Train Loss: 0.00088380\n","Iter: 016/100 | Train Loss: 0.00080563\n","Iter: 017/100 | Train Loss: 0.00073471\n","Iter: 018/100 | Train Loss: 0.00069390\n","Iter: 019/100 | Train Loss: 0.00065844\n","Iter: 020/100 | Train Loss: 0.00062578\n","Iter: 021/100 | Train Loss: 0.00058532\n","Iter: 022/100 | Train Loss: 0.00054462\n","Iter: 023/100 | Train Loss: 0.00051723\n","Iter: 024/100 | Train Loss: 0.00048575\n","Iter: 025/100 | Train Loss: 0.00044841\n","Iter: 026/100 | Train Loss: 0.00041877\n","Iter: 027/100 | Train Loss: 0.00038688\n","Iter: 028/100 | Train Loss: 0.00035622\n","Iter: 029/100 | Train Loss: 0.00033092\n","Iter: 030/100 | Train Loss: 0.00030802\n","Iter: 031/100 | Train Loss: 0.00029196\n","Iter: 032/100 | Train Loss: 0.00027313\n","Iter: 033/100 | Train Loss: 0.00025040\n","Iter: 034/100 | Train Loss: 0.00023209\n","Iter: 035/100 | Train Loss: 0.00021417\n","Iter: 036/100 | Train Loss: 0.00019772\n","Iter: 037/100 | Train Loss: 0.00018149\n","Iter: 038/100 | Train Loss: 0.00016564\n","Iter: 039/100 | Train Loss: 0.00015317\n","Iter: 040/100 | Train Loss: 0.00014118\n","Iter: 041/100 | Train Loss: 0.00013061\n","Iter: 042/100 | Train Loss: 0.00011948\n","Iter: 043/100 | Train Loss: 0.00010927\n","Iter: 044/100 | Train Loss: 0.00010262\n","Iter: 045/100 | Train Loss: 0.00009382\n","Iter: 046/100 | Train Loss: 0.00008728\n","Iter: 047/100 | Train Loss: 0.00008203\n","Iter: 048/100 | Train Loss: 0.00007533\n","Iter: 049/100 | Train Loss: 0.00006977\n","Iter: 050/100 | Train Loss: 0.00006428\n","Iter: 051/100 | Train Loss: 0.00005932\n","Iter: 052/100 | Train Loss: 0.00005425\n","Iter: 053/100 | Train Loss: 0.00005018\n","Iter: 054/100 | Train Loss: 0.00004697\n","Iter: 055/100 | Train Loss: 0.00004298\n","Iter: 056/100 | Train Loss: 0.00003993\n","Iter: 057/100 | Train Loss: 0.00003727\n","Iter: 058/100 | Train Loss: 0.00003465\n","Iter: 059/100 | Train Loss: 0.00003207\n","Iter: 060/100 | Train Loss: 0.00002994\n","Iter: 061/100 | Train Loss: 0.00002803\n","Iter: 062/100 | Train Loss: 0.00002588\n","Iter: 063/100 | Train Loss: 0.00002411\n","Iter: 064/100 | Train Loss: 0.00002255\n","Iter: 065/100 | Train Loss: 0.00002088\n","Iter: 066/100 | Train Loss: 0.00001941\n","Iter: 067/100 | Train Loss: 0.00001806\n","Iter: 068/100 | Train Loss: 0.00001702\n","Iter: 069/100 | Train Loss: 0.00001601\n","Iter: 070/100 | Train Loss: 0.00001497\n","Iter: 071/100 | Train Loss: 0.00001416\n","Iter: 072/100 | Train Loss: 0.00001335\n","Iter: 073/100 | Train Loss: 0.00001257\n","Iter: 074/100 | Train Loss: 0.00001184\n","Iter: 075/100 | Train Loss: 0.00001117\n","Iter: 076/100 | Train Loss: 0.00001060\n","Iter: 077/100 | Train Loss: 0.00001004\n","Iter: 078/100 | Train Loss: 0.00000948\n","Iter: 079/100 | Train Loss: 0.00000898\n","Iter: 080/100 | Train Loss: 0.00000855\n","Iter: 081/100 | Train Loss: 0.00000811\n","Iter: 082/100 | Train Loss: 0.00000769\n","Iter: 083/100 | Train Loss: 0.00000731\n","Iter: 084/100 | Train Loss: 0.00000696\n","Iter: 085/100 | Train Loss: 0.00000662\n","Iter: 086/100 | Train Loss: 0.00000632\n","Iter: 087/100 | Train Loss: 0.00000604\n","Iter: 088/100 | Train Loss: 0.00000578\n","Iter: 089/100 | Train Loss: 0.00000553\n","Iter: 090/100 | Train Loss: 0.00000529\n","Iter: 091/100 | Train Loss: 0.00000507\n","Iter: 092/100 | Train Loss: 0.00000487\n","Iter: 093/100 | Train Loss: 0.00000467\n","Iter: 094/100 | Train Loss: 0.00000449\n","Iter: 095/100 | Train Loss: 0.00000431\n","Iter: 096/100 | Train Loss: 0.00000415\n","Iter: 097/100 | Train Loss: 0.00000400\n","Iter: 098/100 | Train Loss: 0.00000385\n","Iter: 099/100 | Train Loss: 0.00000372\n","\n","Iter: 099/100 | Test Loss: 0.00094469 | Test acc: 63.4100\n","scale:1.150000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00142510\n","Iter: 001/100 | Train Loss: 0.00164450\n","Adjusting Layer 1, Kernel Nodes: 575, Adptive Nodes:225\n","Iter: 002/100 | Train Loss: 0.00284957\n","Adjusting Layer 1, Kernel Nodes: 564, Adptive Nodes:236\n","Iter: 003/100 | Train Loss: 0.01104486\n","Adjusting Layer 1, Kernel Nodes: 743, Adptive Nodes:57\n","Iter: 004/100 | Train Loss: 0.15179944\n","Adjusting Layer 1, Kernel Nodes: 162, Adptive Nodes:638\n","Iter: 005/100 | Train Loss: 0.01076284\n","Iter: 006/100 | Train Loss: 0.00171834\n","Iter: 007/100 | Train Loss: 0.00125830\n","Iter: 008/100 | Train Loss: 0.00117749\n","Iter: 009/100 | Train Loss: 0.00108765\n","Iter: 010/100 | Train Loss: 0.00100828\n","Iter: 011/100 | Train Loss: 0.00095932\n","Iter: 012/100 | Train Loss: 0.00091925\n","Iter: 013/100 | Train Loss: 0.00083672\n","Iter: 014/100 | Train Loss: 0.00074737\n","Iter: 015/100 | Train Loss: 0.00069855\n","Iter: 016/100 | Train Loss: 0.00065950\n","Iter: 017/100 | Train Loss: 0.00059518\n","Iter: 018/100 | Train Loss: 0.00054980\n","Iter: 019/100 | Train Loss: 0.00052426\n","Iter: 020/100 | Train Loss: 0.00048270\n","Iter: 021/100 | Train Loss: 0.00044361\n","Iter: 022/100 | Train Loss: 0.00041584\n","Iter: 023/100 | Train Loss: 0.00038985\n","Iter: 024/100 | Train Loss: 0.00034789\n","Iter: 025/100 | Train Loss: 0.00032753\n","Iter: 026/100 | Train Loss: 0.00030504\n","Iter: 027/100 | Train Loss: 0.00028125\n","Iter: 028/100 | Train Loss: 0.00026149\n","Iter: 029/100 | Train Loss: 0.00023570\n","Iter: 030/100 | Train Loss: 0.00022268\n","Iter: 031/100 | Train Loss: 0.00020244\n","Iter: 032/100 | Train Loss: 0.00018107\n","Iter: 033/100 | Train Loss: 0.00016385\n","Iter: 034/100 | Train Loss: 0.00015156\n","Iter: 035/100 | Train Loss: 0.00013755\n","Iter: 036/100 | Train Loss: 0.00012612\n","Iter: 037/100 | Train Loss: 0.00011607\n","Iter: 038/100 | Train Loss: 0.00010540\n","Iter: 039/100 | Train Loss: 0.00009827\n","Iter: 040/100 | Train Loss: 0.00008696\n","Iter: 041/100 | Train Loss: 0.00007929\n","Iter: 042/100 | Train Loss: 0.00007345\n","Iter: 043/100 | Train Loss: 0.00006687\n","Iter: 044/100 | Train Loss: 0.00006098\n","Iter: 045/100 | Train Loss: 0.00005673\n","Iter: 046/100 | Train Loss: 0.00005169\n","Iter: 047/100 | Train Loss: 0.00004835\n","Iter: 048/100 | Train Loss: 0.00004390\n","Iter: 049/100 | Train Loss: 0.00004048\n","Iter: 050/100 | Train Loss: 0.00003745\n","Iter: 051/100 | Train Loss: 0.00003479\n","Iter: 052/100 | Train Loss: 0.00003182\n","Iter: 053/100 | Train Loss: 0.00002965\n","Iter: 054/100 | Train Loss: 0.00002726\n","Iter: 055/100 | Train Loss: 0.00002526\n","Iter: 056/100 | Train Loss: 0.00002280\n","Iter: 057/100 | Train Loss: 0.00002124\n","Iter: 058/100 | Train Loss: 0.00001952\n","Iter: 059/100 | Train Loss: 0.00001811\n","Iter: 060/100 | Train Loss: 0.00001679\n","Iter: 061/100 | Train Loss: 0.00001561\n","Iter: 062/100 | Train Loss: 0.00001443\n","Iter: 063/100 | Train Loss: 0.00001355\n","Iter: 064/100 | Train Loss: 0.00001242\n","Iter: 065/100 | Train Loss: 0.00001158\n","Iter: 066/100 | Train Loss: 0.00001086\n","Iter: 067/100 | Train Loss: 0.00001017\n","Iter: 068/100 | Train Loss: 0.00000945\n","Iter: 069/100 | Train Loss: 0.00000891\n","Iter: 070/100 | Train Loss: 0.00000830\n","Iter: 071/100 | Train Loss: 0.00000779\n","Iter: 072/100 | Train Loss: 0.00000726\n","Iter: 073/100 | Train Loss: 0.00000684\n","Iter: 074/100 | Train Loss: 0.00000643\n","Iter: 075/100 | Train Loss: 0.00000607\n","Iter: 076/100 | Train Loss: 0.00000571\n","Iter: 077/100 | Train Loss: 0.00000537\n","Iter: 078/100 | Train Loss: 0.00000507\n","Iter: 079/100 | Train Loss: 0.00000480\n","Iter: 080/100 | Train Loss: 0.00000451\n","Iter: 081/100 | Train Loss: 0.00000428\n","Iter: 082/100 | Train Loss: 0.00000408\n","Iter: 083/100 | Train Loss: 0.00000386\n","Iter: 084/100 | Train Loss: 0.00000366\n","Iter: 085/100 | Train Loss: 0.00000348\n","Iter: 086/100 | Train Loss: 0.00000330\n","Iter: 087/100 | Train Loss: 0.00000314\n","Iter: 088/100 | Train Loss: 0.00000299\n","Iter: 089/100 | Train Loss: 0.00000284\n","Iter: 090/100 | Train Loss: 0.00000271\n","Iter: 091/100 | Train Loss: 0.00000259\n","Iter: 092/100 | Train Loss: 0.00000247\n","Iter: 093/100 | Train Loss: 0.00000236\n","Iter: 094/100 | Train Loss: 0.00000226\n","Iter: 095/100 | Train Loss: 0.00000216\n","Iter: 096/100 | Train Loss: 0.00000207\n","Iter: 097/100 | Train Loss: 0.00000198\n","Iter: 098/100 | Train Loss: 0.00000191\n","Iter: 099/100 | Train Loss: 0.00000182\n","\n","Iter: 099/100 | Test Loss: 0.00098811 | Test acc: 62.8300\n","scale:1.150000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00138178\n","Iter: 001/100 | Train Loss: 0.00147799\n","Adjusting Layer 1, Kernel Nodes: 616, Adptive Nodes:184\n","Iter: 002/100 | Train Loss: 0.00178924\n","Adjusting Layer 1, Kernel Nodes: 606, Adptive Nodes:194\n","Iter: 003/100 | Train Loss: 0.00455818\n","Adjusting Layer 1, Kernel Nodes: 339, Adptive Nodes:461\n","Iter: 004/100 | Train Loss: 0.00565206\n","Adjusting Layer 1, Kernel Nodes: 542, Adptive Nodes:258\n","Iter: 005/100 | Train Loss: 0.00313987\n","Iter: 006/100 | Train Loss: 0.00104863\n","Iter: 007/100 | Train Loss: 0.00057311\n","Iter: 008/100 | Train Loss: 0.00087683\n","Adjusting Layer 1, Kernel Nodes: 682, Adptive Nodes:118\n","Iter: 009/100 | Train Loss: 0.00085350\n","Iter: 010/100 | Train Loss: 0.00060107\n","Iter: 011/100 | Train Loss: 0.00045378\n","Iter: 012/100 | Train Loss: 0.00044974\n","Iter: 013/100 | Train Loss: 0.00048456\n","Adjusting Layer 1, Kernel Nodes: 775, Adptive Nodes:25\n","Iter: 014/100 | Train Loss: 0.00042699\n","Iter: 015/100 | Train Loss: 0.00031022\n","Iter: 016/100 | Train Loss: 0.00023324\n","Iter: 017/100 | Train Loss: 0.00022708\n","Iter: 018/100 | Train Loss: 0.00023240\n","Adjusting Layer 1, Kernel Nodes: 334, Adptive Nodes:466\n","Iter: 019/100 | Train Loss: 0.00019437\n","Iter: 020/100 | Train Loss: 0.00014618\n","Iter: 021/100 | Train Loss: 0.00012657\n","Iter: 022/100 | Train Loss: 0.00012459\n","Iter: 023/100 | Train Loss: 0.00011648\n","Iter: 024/100 | Train Loss: 0.00009727\n","Iter: 025/100 | Train Loss: 0.00008090\n","Iter: 026/100 | Train Loss: 0.00007760\n","Iter: 027/100 | Train Loss: 0.00007668\n","Iter: 028/100 | Train Loss: 0.00006390\n","Iter: 029/100 | Train Loss: 0.00004776\n","Iter: 030/100 | Train Loss: 0.00004371\n","Iter: 031/100 | Train Loss: 0.00004756\n","Adjusting Layer 1, Kernel Nodes: 209, Adptive Nodes:591\n","Iter: 032/100 | Train Loss: 0.00004389\n","Iter: 033/100 | Train Loss: 0.00003348\n","Iter: 034/100 | Train Loss: 0.00002744\n","Iter: 035/100 | Train Loss: 0.00002707\n","Iter: 036/100 | Train Loss: 0.00002651\n","Iter: 037/100 | Train Loss: 0.00002320\n","Iter: 038/100 | Train Loss: 0.00001836\n","Iter: 039/100 | Train Loss: 0.00001543\n","Iter: 040/100 | Train Loss: 0.00001582\n","Adjusting Layer 1, Kernel Nodes: 508, Adptive Nodes:292\n","Iter: 041/100 | Train Loss: 0.00001575\n","Iter: 042/100 | Train Loss: 0.00001283\n","Iter: 043/100 | Train Loss: 0.00000998\n","Iter: 044/100 | Train Loss: 0.00000981\n","Iter: 045/100 | Train Loss: 0.00001047\n","Adjusting Layer 1, Kernel Nodes: 277, Adptive Nodes:523\n","Iter: 046/100 | Train Loss: 0.00000925\n","Iter: 047/100 | Train Loss: 0.00000710\n","Iter: 048/100 | Train Loss: 0.00000637\n","Iter: 049/100 | Train Loss: 0.00000650\n","Adjusting Layer 1, Kernel Nodes: 521, Adptive Nodes:279\n","Iter: 050/100 | Train Loss: 0.00000585\n","Iter: 051/100 | Train Loss: 0.00000474\n","Iter: 052/100 | Train Loss: 0.00000423\n","Iter: 053/100 | Train Loss: 0.00000418\n","Iter: 054/100 | Train Loss: 0.00000395\n","Iter: 055/100 | Train Loss: 0.00000342\n","Iter: 056/100 | Train Loss: 0.00000297\n","Iter: 057/100 | Train Loss: 0.00000273\n","Iter: 058/100 | Train Loss: 0.00000258\n","Iter: 059/100 | Train Loss: 0.00000236\n","Iter: 060/100 | Train Loss: 0.00000208\n","Iter: 061/100 | Train Loss: 0.00000185\n","Iter: 062/100 | Train Loss: 0.00000172\n","Iter: 063/100 | Train Loss: 0.00000164\n","Iter: 064/100 | Train Loss: 0.00000152\n","Iter: 065/100 | Train Loss: 0.00000137\n","Iter: 066/100 | Train Loss: 0.00000123\n","Iter: 067/100 | Train Loss: 0.00000114\n","Iter: 068/100 | Train Loss: 0.00000106\n","Iter: 069/100 | Train Loss: 0.00000098\n","Iter: 070/100 | Train Loss: 0.00000091\n","Iter: 071/100 | Train Loss: 0.00000083\n","Iter: 072/100 | Train Loss: 0.00000075\n","Iter: 073/100 | Train Loss: 0.00000070\n","Iter: 074/100 | Train Loss: 0.00000065\n","Iter: 075/100 | Train Loss: 0.00000059\n","Iter: 076/100 | Train Loss: 0.00000051\n","Iter: 077/100 | Train Loss: 0.00000047\n","Iter: 078/100 | Train Loss: 0.00000047\n","Iter: 079/100 | Train Loss: 0.00000045\n","Iter: 080/100 | Train Loss: 0.00000041\n","Iter: 081/100 | Train Loss: 0.00000036\n","Iter: 082/100 | Train Loss: 0.00000035\n","Iter: 083/100 | Train Loss: 0.00000034\n","Iter: 084/100 | Train Loss: 0.00000032\n","Iter: 085/100 | Train Loss: 0.00000028\n","Iter: 086/100 | Train Loss: 0.00000026\n","Iter: 087/100 | Train Loss: 0.00000026\n","Iter: 088/100 | Train Loss: 0.00000025\n","Iter: 089/100 | Train Loss: 0.00000023\n","Iter: 090/100 | Train Loss: 0.00000021\n","Iter: 091/100 | Train Loss: 0.00000020\n","Iter: 092/100 | Train Loss: 0.00000020\n","Iter: 093/100 | Train Loss: 0.00000019\n","Iter: 094/100 | Train Loss: 0.00000018\n","Iter: 095/100 | Train Loss: 0.00000017\n","Iter: 096/100 | Train Loss: 0.00000016\n","Iter: 097/100 | Train Loss: 0.00000016\n","Iter: 098/100 | Train Loss: 0.00000015\n","Iter: 099/100 | Train Loss: 0.00000014\n","\n","Iter: 099/100 | Test Loss: 0.00123134 | Test acc: 58.8400\n","scale:1.150000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00135706\n","Iter: 001/100 | Train Loss: 0.00141517\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 002/100 | Train Loss: 0.00181485\n","Adjusting Layer 1, Kernel Nodes: 477, Adptive Nodes:323\n","Iter: 003/100 | Train Loss: 0.00244184\n","Adjusting Layer 1, Kernel Nodes: 451, Adptive Nodes:349\n","Iter: 004/100 | Train Loss: 0.00236059\n","Iter: 005/100 | Train Loss: 0.00085693\n","Iter: 006/100 | Train Loss: 0.00094464\n","Adjusting Layer 1, Kernel Nodes: 714, Adptive Nodes:86\n","Iter: 007/100 | Train Loss: 0.00054327\n","Iter: 008/100 | Train Loss: 0.00062766\n","Adjusting Layer 1, Kernel Nodes: 437, Adptive Nodes:363\n","Iter: 009/100 | Train Loss: 0.00037461\n","Iter: 010/100 | Train Loss: 0.00038701\n","Adjusting Layer 1, Kernel Nodes: 777, Adptive Nodes:23\n","Iter: 011/100 | Train Loss: 0.00029662\n","Iter: 012/100 | Train Loss: 0.00018966\n","Iter: 013/100 | Train Loss: 0.00022241\n","Adjusting Layer 1, Kernel Nodes: 741, Adptive Nodes:59\n","Iter: 014/100 | Train Loss: 0.00015724\n","Iter: 015/100 | Train Loss: 0.00014982\n","Iter: 016/100 | Train Loss: 0.00013033\n","Iter: 017/100 | Train Loss: 0.00008562\n","Iter: 018/100 | Train Loss: 0.00009972\n","Adjusting Layer 1, Kernel Nodes: 82, Adptive Nodes:718\n","Iter: 019/100 | Train Loss: 0.00009103\n","Iter: 020/100 | Train Loss: 0.00006404\n","Iter: 021/100 | Train Loss: 0.00006260\n","Iter: 022/100 | Train Loss: 0.00006192\n","Iter: 023/100 | Train Loss: 0.00004218\n","Iter: 024/100 | Train Loss: 0.00003464\n","Iter: 025/100 | Train Loss: 0.00004325\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 026/100 | Train Loss: 0.00003705\n","Iter: 027/100 | Train Loss: 0.00002524\n","Iter: 028/100 | Train Loss: 0.00002690\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 029/100 | Train Loss: 0.00002649\n","Iter: 030/100 | Train Loss: 0.00001846\n","Iter: 031/100 | Train Loss: 0.00001648\n","Iter: 032/100 | Train Loss: 0.00001959\n","Adjusting Layer 1, Kernel Nodes: 374, Adptive Nodes:426\n","Iter: 033/100 | Train Loss: 0.00001555\n","Iter: 034/100 | Train Loss: 0.00001074\n","Iter: 035/100 | Train Loss: 0.00001161\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 036/100 | Train Loss: 0.00001109\n","Iter: 037/100 | Train Loss: 0.00000815\n","Iter: 038/100 | Train Loss: 0.00000827\n","Adjusting Layer 1, Kernel Nodes: 1, Adptive Nodes:799\n","Iter: 039/100 | Train Loss: 0.00000889\n","Adjusting Layer 1, Kernel Nodes: 475, Adptive Nodes:325\n","Iter: 040/100 | Train Loss: 0.00000729\n","Iter: 041/100 | Train Loss: 0.00000553\n","Iter: 042/100 | Train Loss: 0.00000553\n","Adjusting Layer 1, Kernel Nodes: 327, Adptive Nodes:473\n","Iter: 043/100 | Train Loss: 0.00000569\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 044/100 | Train Loss: 0.00000451\n","Iter: 045/100 | Train Loss: 0.00000390\n","Iter: 046/100 | Train Loss: 0.00000387\n","Iter: 047/100 | Train Loss: 0.00000358\n","Iter: 048/100 | Train Loss: 0.00000289\n","Iter: 049/100 | Train Loss: 0.00000245\n","Iter: 050/100 | Train Loss: 0.00000242\n","Iter: 051/100 | Train Loss: 0.00000229\n","Iter: 052/100 | Train Loss: 0.00000188\n","Iter: 053/100 | Train Loss: 0.00000161\n","Iter: 054/100 | Train Loss: 0.00000166\n","Adjusting Layer 1, Kernel Nodes: 331, Adptive Nodes:469\n","Iter: 055/100 | Train Loss: 0.00000165\n","Iter: 056/100 | Train Loss: 0.00000143\n","Iter: 057/100 | Train Loss: 0.00000119\n","Iter: 058/100 | Train Loss: 0.00000115\n","Iter: 059/100 | Train Loss: 0.00000109\n","Iter: 060/100 | Train Loss: 0.00000088\n","Iter: 061/100 | Train Loss: 0.00000073\n","Iter: 062/100 | Train Loss: 0.00000076\n","Adjusting Layer 1, Kernel Nodes: 465, Adptive Nodes:335\n","Iter: 063/100 | Train Loss: 0.00000085\n","Adjusting Layer 1, Kernel Nodes: 335, Adptive Nodes:465\n","Iter: 064/100 | Train Loss: 0.00000061\n","Iter: 065/100 | Train Loss: 0.00000055\n","Iter: 066/100 | Train Loss: 0.00000055\n","Iter: 067/100 | Train Loss: 0.00000048\n","Iter: 068/100 | Train Loss: 0.00000039\n","Iter: 069/100 | Train Loss: 0.00000036\n","Iter: 070/100 | Train Loss: 0.00000036\n","Iter: 071/100 | Train Loss: 0.00000033\n","Iter: 072/100 | Train Loss: 0.00000029\n","Iter: 073/100 | Train Loss: 0.00000027\n","Iter: 074/100 | Train Loss: 0.00000027\n","Iter: 075/100 | Train Loss: 0.00000025\n","Iter: 076/100 | Train Loss: 0.00000022\n","Iter: 077/100 | Train Loss: 0.00000020\n","Iter: 078/100 | Train Loss: 0.00000019\n","Iter: 079/100 | Train Loss: 0.00000018\n","Iter: 080/100 | Train Loss: 0.00000017\n","Iter: 081/100 | Train Loss: 0.00000015\n","Iter: 082/100 | Train Loss: 0.00000015\n","Iter: 083/100 | Train Loss: 0.00000014\n","Iter: 084/100 | Train Loss: 0.00000012\n","Iter: 085/100 | Train Loss: 0.00000011\n","Iter: 086/100 | Train Loss: 0.00000011\n","Iter: 087/100 | Train Loss: 0.00000010\n","Iter: 088/100 | Train Loss: 0.00000009\n","Iter: 089/100 | Train Loss: 0.00000008\n","Iter: 090/100 | Train Loss: 0.00000008\n","Adjusting Layer 1, Kernel Nodes: 590, Adptive Nodes:210\n","Iter: 091/100 | Train Loss: 0.00000015\n","Adjusting Layer 1, Kernel Nodes: 193, Adptive Nodes:607\n","Iter: 092/100 | Train Loss: 0.00000017\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 093/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 1, Adptive Nodes:799\n","Iter: 094/100 | Train Loss: 0.00000051\n","Adjusting Layer 1, Kernel Nodes: 454, Adptive Nodes:346\n","Iter: 095/100 | Train Loss: 0.00000015\n","Iter: 096/100 | Train Loss: 0.00000029\n","Adjusting Layer 1, Kernel Nodes: 284, Adptive Nodes:516\n","Iter: 097/100 | Train Loss: 0.00000049\n","Adjusting Layer 1, Kernel Nodes: 517, Adptive Nodes:283\n","Iter: 098/100 | Train Loss: 0.00000035\n","Iter: 099/100 | Train Loss: 0.00000009\n","\n","Iter: 099/100 | Test Loss: 0.00120962 | Test acc: 59.8200\n","scale:1.150000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00134443\n","Iter: 001/100 | Train Loss: 0.00139384\n","Adjusting Layer 1, Kernel Nodes: 648, Adptive Nodes:152\n","Iter: 002/100 | Train Loss: 0.00115969\n","Iter: 003/100 | Train Loss: 0.00079291\n","Iter: 004/100 | Train Loss: 0.00080734\n","Adjusting Layer 1, Kernel Nodes: 584, Adptive Nodes:216\n","Iter: 005/100 | Train Loss: 0.00069401\n","Iter: 006/100 | Train Loss: 0.00051249\n","Iter: 007/100 | Train Loss: 0.00051997\n","Adjusting Layer 1, Kernel Nodes: 779, Adptive Nodes:21\n","Iter: 008/100 | Train Loss: 0.00040455\n","Iter: 009/100 | Train Loss: 0.00035945\n","Iter: 010/100 | Train Loss: 0.00031912\n","Iter: 011/100 | Train Loss: 0.00025155\n","Iter: 012/100 | Train Loss: 0.00022617\n","Iter: 013/100 | Train Loss: 0.00017680\n","Iter: 014/100 | Train Loss: 0.00015901\n","Iter: 015/100 | Train Loss: 0.00014555\n","Iter: 016/100 | Train Loss: 0.00012007\n","Iter: 017/100 | Train Loss: 0.00012846\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 018/100 | Train Loss: 0.00009415\n","Iter: 019/100 | Train Loss: 0.00009825\n","Adjusting Layer 1, Kernel Nodes: 577, Adptive Nodes:223\n","Iter: 020/100 | Train Loss: 0.00007372\n","Iter: 021/100 | Train Loss: 0.00005817\n","Iter: 022/100 | Train Loss: 0.00006218\n","Adjusting Layer 1, Kernel Nodes: 455, Adptive Nodes:345\n","Iter: 023/100 | Train Loss: 0.00004413\n","Iter: 024/100 | Train Loss: 0.00005040\n","Adjusting Layer 1, Kernel Nodes: 496, Adptive Nodes:304\n","Iter: 025/100 | Train Loss: 0.00005344\n","Adjusting Layer 1, Kernel Nodes: 756, Adptive Nodes:44\n","Iter: 026/100 | Train Loss: 0.00013332\n","Adjusting Layer 1, Kernel Nodes: 35, Adptive Nodes:765\n","Iter: 027/100 | Train Loss: 0.00032097\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 028/100 | Train Loss: 0.00129942\n","Adjusting Layer 1, Kernel Nodes: 316, Adptive Nodes:484\n","Iter: 029/100 | Train Loss: 0.00340647\n","Adjusting Layer 1, Kernel Nodes: 755, Adptive Nodes:45\n","Iter: 030/100 | Train Loss: 0.01180588\n","Adjusting Layer 1, Kernel Nodes: 9, Adptive Nodes:791\n","Iter: 031/100 | Train Loss: 0.00097299\n","Iter: 032/100 | Train Loss: 0.00289135\n","Adjusting Layer 1, Kernel Nodes: 129, Adptive Nodes:671\n","Iter: 033/100 | Train Loss: 0.00141895\n","Iter: 034/100 | Train Loss: 0.00066826\n","Iter: 035/100 | Train Loss: 0.00059349\n","Iter: 036/100 | Train Loss: 0.00067249\n","Adjusting Layer 1, Kernel Nodes: 739, Adptive Nodes:61\n","Iter: 037/100 | Train Loss: 0.00072113\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 038/100 | Train Loss: 0.00070961\n","Iter: 039/100 | Train Loss: 0.00062405\n","Iter: 040/100 | Train Loss: 0.00049755\n","Iter: 041/100 | Train Loss: 0.00037617\n","Iter: 042/100 | Train Loss: 0.00029736\n","Iter: 043/100 | Train Loss: 0.00026723\n","Iter: 044/100 | Train Loss: 0.00026470\n","Iter: 045/100 | Train Loss: 0.00025628\n","Iter: 046/100 | Train Loss: 0.00023177\n","Iter: 047/100 | Train Loss: 0.00019923\n","Iter: 048/100 | Train Loss: 0.00016980\n","Iter: 049/100 | Train Loss: 0.00015157\n","Iter: 050/100 | Train Loss: 0.00014146\n","Iter: 051/100 | Train Loss: 0.00012535\n","Iter: 052/100 | Train Loss: 0.00010051\n","Iter: 053/100 | Train Loss: 0.00007821\n","Iter: 054/100 | Train Loss: 0.00006682\n","Iter: 055/100 | Train Loss: 0.00006361\n","Iter: 056/100 | Train Loss: 0.00006301\n","Iter: 057/100 | Train Loss: 0.00006094\n","Iter: 058/100 | Train Loss: 0.00005560\n","Iter: 059/100 | Train Loss: 0.00004768\n","Iter: 060/100 | Train Loss: 0.00004042\n","Iter: 061/100 | Train Loss: 0.00003614\n","Iter: 062/100 | Train Loss: 0.00003428\n","Iter: 063/100 | Train Loss: 0.00003257\n","Iter: 064/100 | Train Loss: 0.00002939\n","Iter: 065/100 | Train Loss: 0.00002535\n","Iter: 066/100 | Train Loss: 0.00002188\n","Iter: 067/100 | Train Loss: 0.00001961\n","Iter: 068/100 | Train Loss: 0.00001823\n","Iter: 069/100 | Train Loss: 0.00001723\n","Iter: 070/100 | Train Loss: 0.00001615\n","Iter: 071/100 | Train Loss: 0.00001462\n","Iter: 072/100 | Train Loss: 0.00001263\n","Iter: 073/100 | Train Loss: 0.00001082\n","Iter: 074/100 | Train Loss: 0.00000974\n","Iter: 075/100 | Train Loss: 0.00000922\n","Iter: 076/100 | Train Loss: 0.00000869\n","Iter: 077/100 | Train Loss: 0.00000787\n","Iter: 078/100 | Train Loss: 0.00000697\n","Iter: 079/100 | Train Loss: 0.00000622\n","Iter: 080/100 | Train Loss: 0.00000561\n","Iter: 081/100 | Train Loss: 0.00000502\n","Iter: 082/100 | Train Loss: 0.00000444\n","Iter: 083/100 | Train Loss: 0.00000396\n","Iter: 084/100 | Train Loss: 0.00000357\n","Iter: 085/100 | Train Loss: 0.00000328\n","Iter: 086/100 | Train Loss: 0.00000309\n","Iter: 087/100 | Train Loss: 0.00000297\n","Iter: 088/100 | Train Loss: 0.00000282\n","Iter: 089/100 | Train Loss: 0.00000256\n","Iter: 090/100 | Train Loss: 0.00000226\n","Iter: 091/100 | Train Loss: 0.00000203\n","Iter: 092/100 | Train Loss: 0.00000191\n","Iter: 093/100 | Train Loss: 0.00000182\n","Iter: 094/100 | Train Loss: 0.00000172\n","Iter: 095/100 | Train Loss: 0.00000159\n","Iter: 096/100 | Train Loss: 0.00000145\n","Iter: 097/100 | Train Loss: 0.00000133\n","Iter: 098/100 | Train Loss: 0.00000124\n","Iter: 099/100 | Train Loss: 0.00000118\n","\n","Iter: 099/100 | Test Loss: 0.00102698 | Test acc: 60.5800\n","scale:1.150000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00133852\n","Iter: 001/100 | Train Loss: 0.00138982\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 002/100 | Train Loss: 0.00104905\n","Iter: 003/100 | Train Loss: 0.00077212\n","Iter: 004/100 | Train Loss: 0.00082387\n","Adjusting Layer 1, Kernel Nodes: 627, Adptive Nodes:173\n","Iter: 005/100 | Train Loss: 0.00059488\n","Iter: 006/100 | Train Loss: 0.00057987\n","Iter: 007/100 | Train Loss: 0.00043560\n","Iter: 008/100 | Train Loss: 0.00043752\n","Adjusting Layer 1, Kernel Nodes: 735, Adptive Nodes:65\n","Iter: 009/100 | Train Loss: 0.00034900\n","Iter: 010/100 | Train Loss: 0.00032858\n","Iter: 011/100 | Train Loss: 0.00026915\n","Iter: 012/100 | Train Loss: 0.00025248\n","Iter: 013/100 | Train Loss: 0.00018516\n","Iter: 014/100 | Train Loss: 0.00018915\n","Adjusting Layer 1, Kernel Nodes: 711, Adptive Nodes:89\n","Iter: 015/100 | Train Loss: 0.00013755\n","Iter: 016/100 | Train Loss: 0.00014566\n","Adjusting Layer 1, Kernel Nodes: 711, Adptive Nodes:89\n","Iter: 017/100 | Train Loss: 0.00011292\n","Iter: 018/100 | Train Loss: 0.00011327\n","Adjusting Layer 1, Kernel Nodes: 787, Adptive Nodes:13\n","Iter: 019/100 | Train Loss: 0.00008829\n","Iter: 020/100 | Train Loss: 0.00008701\n","Iter: 021/100 | Train Loss: 0.00006247\n","Iter: 022/100 | Train Loss: 0.00006175\n","Iter: 023/100 | Train Loss: 0.00004771\n","Iter: 024/100 | Train Loss: 0.00004666\n","Iter: 025/100 | Train Loss: 0.00004137\n","Iter: 026/100 | Train Loss: 0.00003655\n","Iter: 027/100 | Train Loss: 0.00003454\n","Iter: 028/100 | Train Loss: 0.00002812\n","Iter: 029/100 | Train Loss: 0.00002612\n","Iter: 030/100 | Train Loss: 0.00002265\n","Iter: 031/100 | Train Loss: 0.00001892\n","Iter: 032/100 | Train Loss: 0.00001981\n","Adjusting Layer 1, Kernel Nodes: 655, Adptive Nodes:145\n","Iter: 033/100 | Train Loss: 0.00001602\n","Iter: 034/100 | Train Loss: 0.00001789\n","Adjusting Layer 1, Kernel Nodes: 694, Adptive Nodes:106\n","Iter: 035/100 | Train Loss: 0.00001415\n","Iter: 036/100 | Train Loss: 0.00001377\n","Iter: 037/100 | Train Loss: 0.00001071\n","Iter: 038/100 | Train Loss: 0.00001062\n","Iter: 039/100 | Train Loss: 0.00000787\n","Iter: 040/100 | Train Loss: 0.00000871\n","Adjusting Layer 1, Kernel Nodes: 435, Adptive Nodes:365\n","Iter: 041/100 | Train Loss: 0.00000714\n","Iter: 042/100 | Train Loss: 0.00000713\n","Iter: 043/100 | Train Loss: 0.00000517\n","Iter: 044/100 | Train Loss: 0.00000554\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 045/100 | Train Loss: 0.00000502\n","Iter: 046/100 | Train Loss: 0.00000405\n","Iter: 047/100 | Train Loss: 0.00000544\n","Adjusting Layer 1, Kernel Nodes: 459, Adptive Nodes:341\n","Iter: 048/100 | Train Loss: 0.00000334\n","Iter: 049/100 | Train Loss: 0.00000392\n","Adjusting Layer 1, Kernel Nodes: 548, Adptive Nodes:252\n","Iter: 050/100 | Train Loss: 0.00000432\n","Adjusting Layer 1, Kernel Nodes: 380, Adptive Nodes:420\n","Iter: 051/100 | Train Loss: 0.00000281\n","Iter: 052/100 | Train Loss: 0.00000287\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 053/100 | Train Loss: 0.00000374\n","Adjusting Layer 1, Kernel Nodes: 338, Adptive Nodes:462\n","Iter: 054/100 | Train Loss: 0.00000381\n","Adjusting Layer 1, Kernel Nodes: 472, Adptive Nodes:328\n","Iter: 055/100 | Train Loss: 0.00000307\n","Iter: 056/100 | Train Loss: 0.00000243\n","Iter: 057/100 | Train Loss: 0.00000258\n","Adjusting Layer 1, Kernel Nodes: 332, Adptive Nodes:468\n","Iter: 058/100 | Train Loss: 0.00000242\n","Iter: 059/100 | Train Loss: 0.00000424\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 060/100 | Train Loss: 0.00003005\n","Adjusting Layer 1, Kernel Nodes: 4, Adptive Nodes:796\n","Iter: 061/100 | Train Loss: 0.00011505\n","Adjusting Layer 1, Kernel Nodes: 5, Adptive Nodes:795\n","Iter: 062/100 | Train Loss: 0.00013037\n","Adjusting Layer 1, Kernel Nodes: 467, Adptive Nodes:333\n","Iter: 063/100 | Train Loss: 0.00002492\n","Iter: 064/100 | Train Loss: 0.00004084\n","Adjusting Layer 1, Kernel Nodes: 338, Adptive Nodes:462\n","Iter: 065/100 | Train Loss: 0.00008093\n","Adjusting Layer 1, Kernel Nodes: 470, Adptive Nodes:330\n","Iter: 066/100 | Train Loss: 0.00001839\n","Iter: 067/100 | Train Loss: 0.00003941\n","Adjusting Layer 1, Kernel Nodes: 509, Adptive Nodes:291\n","Iter: 068/100 | Train Loss: 0.00006548\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 069/100 | Train Loss: 0.00005790\n","Iter: 070/100 | Train Loss: 0.00004830\n","Iter: 071/100 | Train Loss: 0.00003515\n","Iter: 072/100 | Train Loss: 0.00002265\n","Iter: 073/100 | Train Loss: 0.00002007\n","Iter: 074/100 | Train Loss: 0.00002439\n","Adjusting Layer 1, Kernel Nodes: 7, Adptive Nodes:793\n","Iter: 075/100 | Train Loss: 0.00000882\n","Iter: 076/100 | Train Loss: 0.00001426\n","Adjusting Layer 1, Kernel Nodes: 771, Adptive Nodes:29\n","Iter: 077/100 | Train Loss: 0.00002148\n","Adjusting Layer 1, Kernel Nodes: 37, Adptive Nodes:763\n","Iter: 078/100 | Train Loss: 0.00000400\n","Iter: 079/100 | Train Loss: 0.00001453\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 080/100 | Train Loss: 0.00001745\n","Adjusting Layer 1, Kernel Nodes: 41, Adptive Nodes:759\n","Iter: 081/100 | Train Loss: 0.00000218\n","Iter: 082/100 | Train Loss: 0.00001407\n","Adjusting Layer 1, Kernel Nodes: 771, Adptive Nodes:29\n","Iter: 083/100 | Train Loss: 0.00001426\n","Adjusting Layer 1, Kernel Nodes: 37, Adptive Nodes:763\n","Iter: 084/100 | Train Loss: 0.00000219\n","Iter: 085/100 | Train Loss: 0.00001222\n","Adjusting Layer 1, Kernel Nodes: 767, Adptive Nodes:33\n","Iter: 086/100 | Train Loss: 0.00001383\n","Adjusting Layer 1, Kernel Nodes: 32, Adptive Nodes:768\n","Iter: 087/100 | Train Loss: 0.00000267\n","Iter: 088/100 | Train Loss: 0.00001045\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 089/100 | Train Loss: 0.00001453\n","Adjusting Layer 1, Kernel Nodes: 34, Adptive Nodes:766\n","Iter: 090/100 | Train Loss: 0.00000352\n","Iter: 091/100 | Train Loss: 0.00000894\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 092/100 | Train Loss: 0.00001809\n","Adjusting Layer 1, Kernel Nodes: 31, Adptive Nodes:769\n","Iter: 093/100 | Train Loss: 0.00000498\n","Iter: 094/100 | Train Loss: 0.00000832\n","Adjusting Layer 1, Kernel Nodes: 771, Adptive Nodes:29\n","Iter: 095/100 | Train Loss: 0.00002438\n","Adjusting Layer 1, Kernel Nodes: 31, Adptive Nodes:769\n","Iter: 096/100 | Train Loss: 0.00000729\n","Iter: 097/100 | Train Loss: 0.00000897\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 098/100 | Train Loss: 0.00003238\n","Adjusting Layer 1, Kernel Nodes: 31, Adptive Nodes:769\n","Iter: 099/100 | Train Loss: 0.00001031\n","\n","Iter: 099/100 | Test Loss: 0.00101248 | Test acc: 64.5500\n","scale:1.150000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 000/100 | Train Loss: 0.00133589\n","Iter: 001/100 | Train Loss: 0.00139428\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 002/100 | Train Loss: 0.00101932\n","Iter: 003/100 | Train Loss: 0.00078035\n","Iter: 004/100 | Train Loss: 0.00082723\n","Adjusting Layer 1, Kernel Nodes: 650, Adptive Nodes:150\n","Iter: 005/100 | Train Loss: 0.00056246\n","Iter: 006/100 | Train Loss: 0.00058414\n","Adjusting Layer 1, Kernel Nodes: 659, Adptive Nodes:141\n","Iter: 007/100 | Train Loss: 0.00041132\n","Iter: 008/100 | Train Loss: 0.00045152\n","Adjusting Layer 1, Kernel Nodes: 695, Adptive Nodes:105\n","Iter: 009/100 | Train Loss: 0.00031212\n","Iter: 010/100 | Train Loss: 0.00034693\n","Adjusting Layer 1, Kernel Nodes: 759, Adptive Nodes:41\n","Iter: 011/100 | Train Loss: 0.00025124\n","Iter: 012/100 | Train Loss: 0.00025342\n","Adjusting Layer 1, Kernel Nodes: 784, Adptive Nodes:16\n","Iter: 013/100 | Train Loss: 0.00020032\n","Iter: 014/100 | Train Loss: 0.00017140\n","Iter: 015/100 | Train Loss: 0.00016777\n","Iter: 016/100 | Train Loss: 0.00012017\n","Iter: 017/100 | Train Loss: 0.00013573\n","Adjusting Layer 1, Kernel Nodes: 638, Adptive Nodes:162\n","Iter: 018/100 | Train Loss: 0.00010281\n","Iter: 019/100 | Train Loss: 0.00009476\n","Iter: 020/100 | Train Loss: 0.00009081\n","Iter: 021/100 | Train Loss: 0.00006430\n","Iter: 022/100 | Train Loss: 0.00006749\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 023/100 | Train Loss: 0.00005497\n","Iter: 024/100 | Train Loss: 0.00004532\n","Iter: 025/100 | Train Loss: 0.00004678\n","Adjusting Layer 1, Kernel Nodes: 774, Adptive Nodes:26\n","Iter: 026/100 | Train Loss: 0.00003496\n","Iter: 027/100 | Train Loss: 0.00003485\n","Iter: 028/100 | Train Loss: 0.00003311\n","Iter: 029/100 | Train Loss: 0.00002351\n","Iter: 030/100 | Train Loss: 0.00002884\n","Adjusting Layer 1, Kernel Nodes: 703, Adptive Nodes:97\n","Iter: 031/100 | Train Loss: 0.00002003\n","Iter: 032/100 | Train Loss: 0.00002061\n","Adjusting Layer 1, Kernel Nodes: 722, Adptive Nodes:78\n","Iter: 033/100 | Train Loss: 0.00001881\n","Iter: 034/100 | Train Loss: 0.00001368\n","Iter: 035/100 | Train Loss: 0.00001614\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 036/100 | Train Loss: 0.00001188\n","Iter: 037/100 | Train Loss: 0.00001293\n","Adjusting Layer 1, Kernel Nodes: 614, Adptive Nodes:186\n","Iter: 038/100 | Train Loss: 0.00001115\n","Iter: 039/100 | Train Loss: 0.00000913\n","Iter: 040/100 | Train Loss: 0.00000943\n","Adjusting Layer 1, Kernel Nodes: 635, Adptive Nodes:165\n","Iter: 041/100 | Train Loss: 0.00000652\n","Iter: 042/100 | Train Loss: 0.00000768\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 043/100 | Train Loss: 0.00000518\n","Iter: 044/100 | Train Loss: 0.00000621\n","Adjusting Layer 1, Kernel Nodes: 760, Adptive Nodes:40\n","Iter: 045/100 | Train Loss: 0.00000451\n","Iter: 046/100 | Train Loss: 0.00000488\n","Adjusting Layer 1, Kernel Nodes: 777, Adptive Nodes:23\n","Iter: 047/100 | Train Loss: 0.00000343\n","Iter: 048/100 | Train Loss: 0.00000346\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 049/100 | Train Loss: 0.00000313\n","Iter: 050/100 | Train Loss: 0.00000290\n","Iter: 051/100 | Train Loss: 0.00000299\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 052/100 | Train Loss: 0.00000273\n","Iter: 053/100 | Train Loss: 0.00000238\n","Iter: 054/100 | Train Loss: 0.00000194\n","Iter: 055/100 | Train Loss: 0.00000184\n","Iter: 056/100 | Train Loss: 0.00000169\n","Iter: 057/100 | Train Loss: 0.00000151\n","Iter: 058/100 | Train Loss: 0.00000158\n","Adjusting Layer 1, Kernel Nodes: 492, Adptive Nodes:308\n","Iter: 059/100 | Train Loss: 0.00000139\n","Iter: 060/100 | Train Loss: 0.00000104\n","Iter: 061/100 | Train Loss: 0.00000112\n","Adjusting Layer 1, Kernel Nodes: 426, Adptive Nodes:374\n","Iter: 062/100 | Train Loss: 0.00000138\n","Adjusting Layer 1, Kernel Nodes: 455, Adptive Nodes:345\n","Iter: 063/100 | Train Loss: 0.00000095\n","Iter: 064/100 | Train Loss: 0.00000096\n","Adjusting Layer 1, Kernel Nodes: 494, Adptive Nodes:306\n","Iter: 065/100 | Train Loss: 0.00000109\n","Adjusting Layer 1, Kernel Nodes: 491, Adptive Nodes:309\n","Iter: 066/100 | Train Loss: 0.00000098\n","Iter: 067/100 | Train Loss: 0.00000099\n","Adjusting Layer 1, Kernel Nodes: 408, Adptive Nodes:392\n","Iter: 068/100 | Train Loss: 0.00000090\n","Iter: 069/100 | Train Loss: 0.00000079\n","Iter: 070/100 | Train Loss: 0.00000063\n","Iter: 071/100 | Train Loss: 0.00000064\n","Adjusting Layer 1, Kernel Nodes: 326, Adptive Nodes:474\n","Iter: 072/100 | Train Loss: 0.00000110\n","Adjusting Layer 1, Kernel Nodes: 500, Adptive Nodes:300\n","Iter: 073/100 | Train Loss: 0.00000185\n","Adjusting Layer 1, Kernel Nodes: 390, Adptive Nodes:410\n","Iter: 074/100 | Train Loss: 0.00000360\n","Adjusting Layer 1, Kernel Nodes: 475, Adptive Nodes:325\n","Iter: 075/100 | Train Loss: 0.00000773\n","Adjusting Layer 1, Kernel Nodes: 371, Adptive Nodes:429\n","Iter: 076/100 | Train Loss: 0.00001863\n","Adjusting Layer 1, Kernel Nodes: 456, Adptive Nodes:344\n","Iter: 077/100 | Train Loss: 0.00004511\n","Adjusting Layer 1, Kernel Nodes: 358, Adptive Nodes:442\n","Iter: 078/100 | Train Loss: 0.00010746\n","Adjusting Layer 1, Kernel Nodes: 447, Adptive Nodes:353\n","Iter: 079/100 | Train Loss: 0.00025121\n","Adjusting Layer 1, Kernel Nodes: 307, Adptive Nodes:493\n","Iter: 080/100 | Train Loss: 0.00049413\n","Adjusting Layer 1, Kernel Nodes: 575, Adptive Nodes:225\n","Iter: 081/100 | Train Loss: 0.00100993\n","Adjusting Layer 1, Kernel Nodes: 188, Adptive Nodes:612\n","Iter: 082/100 | Train Loss: 0.00125410\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 083/100 | Train Loss: 0.00103997\n","Iter: 084/100 | Train Loss: 0.00011164\n","Iter: 085/100 | Train Loss: 0.00039219\n","Adjusting Layer 1, Kernel Nodes: 221, Adptive Nodes:579\n","Iter: 086/100 | Train Loss: 0.00017893\n","Iter: 087/100 | Train Loss: 0.00024650\n","Adjusting Layer 1, Kernel Nodes: 359, Adptive Nodes:441\n","Iter: 088/100 | Train Loss: 0.00006134\n","Iter: 089/100 | Train Loss: 0.00021811\n","Adjusting Layer 1, Kernel Nodes: 435, Adptive Nodes:365\n","Iter: 090/100 | Train Loss: 0.00004817\n","Iter: 091/100 | Train Loss: 0.00008933\n","Adjusting Layer 1, Kernel Nodes: 786, Adptive Nodes:14\n","Iter: 092/100 | Train Loss: 0.00011627\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 093/100 | Train Loss: 0.00002011\n","Iter: 094/100 | Train Loss: 0.00007678\n","Adjusting Layer 1, Kernel Nodes: 77, Adptive Nodes:723\n","Iter: 095/100 | Train Loss: 0.00006741\n","Iter: 096/100 | Train Loss: 0.00001342\n","Iter: 097/100 | Train Loss: 0.00004584\n","Adjusting Layer 1, Kernel Nodes: 678, Adptive Nodes:122\n","Iter: 098/100 | Train Loss: 0.00004894\n","Adjusting Layer 1, Kernel Nodes: 224, Adptive Nodes:576\n","Iter: 099/100 | Train Loss: 0.00001110\n","\n","Iter: 099/100 | Test Loss: 0.00098530 | Test acc: 64.7400\n","scale:1.150000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00133722\n","Iter: 001/100 | Train Loss: 0.00141421\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 002/100 | Train Loss: 0.00104846\n","Iter: 003/100 | Train Loss: 0.00078005\n","Iter: 004/100 | Train Loss: 0.00084013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00054681\n","Iter: 006/100 | Train Loss: 0.00058120\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00040279\n","Iter: 008/100 | Train Loss: 0.00044358\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 009/100 | Train Loss: 0.00032107\n","Iter: 010/100 | Train Loss: 0.00031803\n","Iter: 011/100 | Train Loss: 0.00029115\n","Iter: 012/100 | Train Loss: 0.00020976\n","Iter: 013/100 | Train Loss: 0.00023724\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00017081\n","Iter: 015/100 | Train Loss: 0.00014750\n","Iter: 016/100 | Train Loss: 0.00015620\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00011256\n","Iter: 018/100 | Train Loss: 0.00010147\n","Iter: 019/100 | Train Loss: 0.00010772\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 020/100 | Train Loss: 0.00007552\n","Iter: 021/100 | Train Loss: 0.00006475\n","Iter: 022/100 | Train Loss: 0.00007281\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 023/100 | Train Loss: 0.00005227\n","Iter: 024/100 | Train Loss: 0.00004478\n","Iter: 025/100 | Train Loss: 0.00005006\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 026/100 | Train Loss: 0.00003672\n","Iter: 027/100 | Train Loss: 0.00002916\n","Iter: 028/100 | Train Loss: 0.00003101\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 029/100 | Train Loss: 0.00003015\n","Iter: 030/100 | Train Loss: 0.00002160\n","Iter: 031/100 | Train Loss: 0.00002118\n","Iter: 032/100 | Train Loss: 0.00002297\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 033/100 | Train Loss: 0.00001576\n","Iter: 034/100 | Train Loss: 0.00001315\n","Iter: 035/100 | Train Loss: 0.00001501\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 036/100 | Train Loss: 0.00001376\n","Iter: 037/100 | Train Loss: 0.00001008\n","Iter: 038/100 | Train Loss: 0.00001042\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 039/100 | Train Loss: 0.00001081\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 040/100 | Train Loss: 0.00000777\n","Iter: 041/100 | Train Loss: 0.00000637\n","Iter: 042/100 | Train Loss: 0.00000704\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 043/100 | Train Loss: 0.00000647\n","Iter: 044/100 | Train Loss: 0.00000479\n","Iter: 045/100 | Train Loss: 0.00000450\n","Iter: 046/100 | Train Loss: 0.00000481\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 047/100 | Train Loss: 0.00000388\n","Iter: 048/100 | Train Loss: 0.00000281\n","Iter: 049/100 | Train Loss: 0.00000319\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 050/100 | Train Loss: 0.00000325\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 051/100 | Train Loss: 0.00000233\n","Iter: 052/100 | Train Loss: 0.00000206\n","Iter: 053/100 | Train Loss: 0.00000228\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 054/100 | Train Loss: 0.00000197\n","Iter: 055/100 | Train Loss: 0.00000149\n","Iter: 056/100 | Train Loss: 0.00000150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 057/100 | Train Loss: 0.00000165\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 058/100 | Train Loss: 0.00000129\n","Iter: 059/100 | Train Loss: 0.00000103\n","Iter: 060/100 | Train Loss: 0.00000106\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 061/100 | Train Loss: 0.00000103\n","Iter: 062/100 | Train Loss: 0.00000080\n","Iter: 063/100 | Train Loss: 0.00000068\n","Iter: 064/100 | Train Loss: 0.00000080\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 065/100 | Train Loss: 0.00000072\n","Iter: 066/100 | Train Loss: 0.00000054\n","Iter: 067/100 | Train Loss: 0.00000056\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 068/100 | Train Loss: 0.00000058\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 069/100 | Train Loss: 0.00000049\n","Iter: 070/100 | Train Loss: 0.00000040\n","Iter: 071/100 | Train Loss: 0.00000043\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 072/100 | Train Loss: 0.00000042\n","Iter: 073/100 | Train Loss: 0.00000034\n","Iter: 074/100 | Train Loss: 0.00000031\n","Iter: 075/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 076/100 | Train Loss: 0.00000031\n","Iter: 077/100 | Train Loss: 0.00000026\n","Iter: 078/100 | Train Loss: 0.00000025\n","Iter: 079/100 | Train Loss: 0.00000026\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 080/100 | Train Loss: 0.00000023\n","Iter: 081/100 | Train Loss: 0.00000019\n","Iter: 082/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 083/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 084/100 | Train Loss: 0.00000018\n","Iter: 085/100 | Train Loss: 0.00000017\n","Iter: 086/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 087/100 | Train Loss: 0.00000017\n","Iter: 088/100 | Train Loss: 0.00000015\n","Iter: 089/100 | Train Loss: 0.00000014\n","Iter: 090/100 | Train Loss: 0.00000015\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 091/100 | Train Loss: 0.00000014\n","Iter: 092/100 | Train Loss: 0.00000013\n","Iter: 093/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 094/100 | Train Loss: 0.00000013\n","Iter: 095/100 | Train Loss: 0.00000012\n","Iter: 096/100 | Train Loss: 0.00000012\n","Iter: 097/100 | Train Loss: 0.00000012\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 098/100 | Train Loss: 0.00000012\n","Iter: 099/100 | Train Loss: 0.00000011\n","\n","Iter: 099/100 | Test Loss: 0.00096208 | Test acc: 65.5500\n","scale:1.150000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00133257\n","Iter: 001/100 | Train Loss: 0.00143726\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 002/100 | Train Loss: 0.00108134\n","Iter: 003/100 | Train Loss: 0.00080986\n","Iter: 004/100 | Train Loss: 0.00083446\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00055710\n","Iter: 006/100 | Train Loss: 0.00054429\n","Iter: 007/100 | Train Loss: 0.00042219\n","Iter: 008/100 | Train Loss: 0.00039288\n","Iter: 009/100 | Train Loss: 0.00037302\n","Iter: 010/100 | Train Loss: 0.00026685\n","Iter: 011/100 | Train Loss: 0.00030330\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00022728\n","Iter: 013/100 | Train Loss: 0.00018166\n","Iter: 014/100 | Train Loss: 0.00020449\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00014511\n","Iter: 016/100 | Train Loss: 0.00011822\n","Iter: 017/100 | Train Loss: 0.00012684\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 018/100 | Train Loss: 0.00010179\n","Iter: 019/100 | Train Loss: 0.00007376\n","Iter: 020/100 | Train Loss: 0.00008578\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 021/100 | Train Loss: 0.00006637\n","Iter: 022/100 | Train Loss: 0.00005265\n","Iter: 023/100 | Train Loss: 0.00004804\n","Iter: 024/100 | Train Loss: 0.00005185\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 025/100 | Train Loss: 0.00003401\n","Iter: 026/100 | Train Loss: 0.00003391\n","Iter: 027/100 | Train Loss: 0.00003392\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 028/100 | Train Loss: 0.00002983\n","Iter: 029/100 | Train Loss: 0.00002029\n","Iter: 030/100 | Train Loss: 0.00002087\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 031/100 | Train Loss: 0.00002245\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 032/100 | Train Loss: 0.00001967\n","Iter: 033/100 | Train Loss: 0.00001734\n","Iter: 034/100 | Train Loss: 0.00001633\n","Iter: 035/100 | Train Loss: 0.00001063\n","Iter: 036/100 | Train Loss: 0.00000874\n","Iter: 037/100 | Train Loss: 0.00000901\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 038/100 | Train Loss: 0.00001399\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 039/100 | Train Loss: 0.00001110\n","Iter: 040/100 | Train Loss: 0.00001642\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 041/100 | Train Loss: 0.00002459\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 042/100 | Train Loss: 0.00012781\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 043/100 | Train Loss: 0.00064777\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 044/100 | Train Loss: 0.00247564\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 045/100 | Train Loss: 0.00068894\n","Iter: 046/100 | Train Loss: 0.00082247\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 047/100 | Train Loss: 0.00098332\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 048/100 | Train Loss: 0.00088863\n","Iter: 049/100 | Train Loss: 0.00076555\n","Iter: 050/100 | Train Loss: 0.00049823\n","Iter: 051/100 | Train Loss: 0.00055710\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 052/100 | Train Loss: 0.00049139\n","Iter: 053/100 | Train Loss: 0.00041207\n","Iter: 054/100 | Train Loss: 0.00052731\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 055/100 | Train Loss: 0.00082652\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 056/100 | Train Loss: 0.00074256\n","Iter: 057/100 | Train Loss: 0.00131659\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 058/100 | Train Loss: 0.00067162\n","Iter: 059/100 | Train Loss: 0.00088343\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 060/100 | Train Loss: 0.00066492\n","Iter: 061/100 | Train Loss: 0.00057747\n","Iter: 062/100 | Train Loss: 0.00044696\n","Iter: 063/100 | Train Loss: 0.00049190\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 064/100 | Train Loss: 0.00035606\n","Iter: 065/100 | Train Loss: 0.00042789\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 066/100 | Train Loss: 0.00055717\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 067/100 | Train Loss: 0.00035640\n","Iter: 068/100 | Train Loss: 0.00029786\n","Iter: 069/100 | Train Loss: 0.00028268\n","Iter: 070/100 | Train Loss: 0.00024635\n","Iter: 071/100 | Train Loss: 0.00028131\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 072/100 | Train Loss: 0.00024064\n","Iter: 073/100 | Train Loss: 0.00029700\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 074/100 | Train Loss: 0.00034921\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 075/100 | Train Loss: 0.00026837\n","Iter: 076/100 | Train Loss: 0.00018133\n","Iter: 077/100 | Train Loss: 0.00014544\n","Iter: 078/100 | Train Loss: 0.00014840\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 079/100 | Train Loss: 0.00018914\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 080/100 | Train Loss: 0.00018336\n","Iter: 081/100 | Train Loss: 0.00018455\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 082/100 | Train Loss: 0.00011918\n","Iter: 083/100 | Train Loss: 0.00013419\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 084/100 | Train Loss: 0.00009089\n","Iter: 085/100 | Train Loss: 0.00012513\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 086/100 | Train Loss: 0.00010026\n","Iter: 087/100 | Train Loss: 0.00008754\n","Iter: 088/100 | Train Loss: 0.00007647\n","Iter: 089/100 | Train Loss: 0.00006058\n","Iter: 090/100 | Train Loss: 0.00006305\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 091/100 | Train Loss: 0.00004455\n","Iter: 092/100 | Train Loss: 0.00004740\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 093/100 | Train Loss: 0.00004220\n","Iter: 094/100 | Train Loss: 0.00003756\n","Iter: 095/100 | Train Loss: 0.00003006\n","Iter: 096/100 | Train Loss: 0.00003007\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 097/100 | Train Loss: 0.00002825\n","Iter: 098/100 | Train Loss: 0.00002409\n","Iter: 099/100 | Train Loss: 0.00002090\n","\n","Iter: 099/100 | Test Loss: 0.00083617 | Test acc: 64.5100\n","scale:1.200000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00160233\n","Iter: 001/100 | Train Loss: 0.00296334\n","Adjusting Layer 1, Kernel Nodes: 525, Adptive Nodes:275\n","Iter: 002/100 | Train Loss: 0.01386543\n","Adjusting Layer 1, Kernel Nodes: 439, Adptive Nodes:361\n","Iter: 003/100 | Train Loss: 0.04564315\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 004/100 | Train Loss: 0.11916858\n","Adjusting Layer 1, Kernel Nodes: 211, Adptive Nodes:589\n","Iter: 005/100 | Train Loss: 0.01139109\n","Iter: 006/100 | Train Loss: 0.00396883\n","Iter: 007/100 | Train Loss: 0.00148070\n","Iter: 008/100 | Train Loss: 0.00135362\n","Iter: 009/100 | Train Loss: 0.00131841\n","Iter: 010/100 | Train Loss: 0.00130057\n","Iter: 011/100 | Train Loss: 0.00124752\n","Iter: 012/100 | Train Loss: 0.00116465\n","Iter: 013/100 | Train Loss: 0.00109615\n","Iter: 014/100 | Train Loss: 0.00104033\n","Iter: 015/100 | Train Loss: 0.00099590\n","Iter: 016/100 | Train Loss: 0.00096367\n","Iter: 017/100 | Train Loss: 0.00093287\n","Iter: 018/100 | Train Loss: 0.00088001\n","Iter: 019/100 | Train Loss: 0.06276224\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 020/100 | Train Loss: 0.02763360\n","Iter: 021/100 | Train Loss: 0.00133185\n","Iter: 022/100 | Train Loss: 0.00137938\n","Adjusting Layer 1, Kernel Nodes: 511, Adptive Nodes:289\n","Iter: 023/100 | Train Loss: 0.00137380\n","Iter: 024/100 | Train Loss: 0.00135002\n","Iter: 025/100 | Train Loss: 0.00132074\n","Iter: 026/100 | Train Loss: 0.00128531\n","Iter: 027/100 | Train Loss: 0.00124914\n","Iter: 028/100 | Train Loss: 0.00121328\n","Iter: 029/100 | Train Loss: 0.00117766\n","Iter: 030/100 | Train Loss: 0.00113957\n","Iter: 031/100 | Train Loss: 0.00109553\n","Iter: 032/100 | Train Loss: 0.00104386\n","Iter: 033/100 | Train Loss: 0.00099385\n","Iter: 034/100 | Train Loss: 0.00095821\n","Iter: 035/100 | Train Loss: 0.00093901\n","Iter: 036/100 | Train Loss: 0.00475884\n","Adjusting Layer 1, Kernel Nodes: 338, Adptive Nodes:462\n","Iter: 037/100 | Train Loss: 0.00106943\n","Iter: 038/100 | Train Loss: 0.00132960\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 039/100 | Train Loss: 0.00111158\n","Iter: 040/100 | Train Loss: 0.00080979\n","Iter: 041/100 | Train Loss: 0.00084120\n","Adjusting Layer 1, Kernel Nodes: 23, Adptive Nodes:777\n","Iter: 042/100 | Train Loss: 0.00090387\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 043/100 | Train Loss: 0.00076151\n","Iter: 044/100 | Train Loss: 0.00073457\n","Iter: 045/100 | Train Loss: 0.00080024\n","Adjusting Layer 1, Kernel Nodes: 466, Adptive Nodes:334\n","Iter: 046/100 | Train Loss: 0.00066184\n","Iter: 047/100 | Train Loss: 0.00073568\n","Adjusting Layer 1, Kernel Nodes: 260, Adptive Nodes:540\n","Iter: 048/100 | Train Loss: 0.00070013\n","Iter: 049/100 | Train Loss: 0.00063250\n","Iter: 050/100 | Train Loss: 0.00064415\n","Adjusting Layer 1, Kernel Nodes: 489, Adptive Nodes:311\n","Iter: 051/100 | Train Loss: 0.00060355\n","Iter: 052/100 | Train Loss: 0.00058200\n","Iter: 053/100 | Train Loss: 0.00056697\n","Iter: 054/100 | Train Loss: 0.00054182\n","Iter: 055/100 | Train Loss: 0.00054243\n","Adjusting Layer 1, Kernel Nodes: 313, Adptive Nodes:487\n","Iter: 056/100 | Train Loss: 0.00051382\n","Iter: 057/100 | Train Loss: 0.00049522\n","Iter: 058/100 | Train Loss: 0.00049387\n","Iter: 059/100 | Train Loss: 0.00047781\n","Iter: 060/100 | Train Loss: 0.00044863\n","Iter: 061/100 | Train Loss: 0.00043111\n","Iter: 062/100 | Train Loss: 0.00042009\n","Iter: 063/100 | Train Loss: 0.00040160\n","Iter: 064/100 | Train Loss: 0.00038463\n","Iter: 065/100 | Train Loss: 0.00037602\n","Iter: 066/100 | Train Loss: 0.00036683\n","Iter: 067/100 | Train Loss: 0.00035535\n","Iter: 068/100 | Train Loss: 0.00034644\n","Iter: 069/100 | Train Loss: 0.00033997\n","Iter: 070/100 | Train Loss: 0.00033322\n","Iter: 071/100 | Train Loss: 0.00032430\n","Iter: 072/100 | Train Loss: 0.00031461\n","Iter: 073/100 | Train Loss: 0.00030818\n","Iter: 074/100 | Train Loss: 0.00030224\n","Iter: 075/100 | Train Loss: 0.00029305\n","Iter: 076/100 | Train Loss: 0.00028574\n","Iter: 077/100 | Train Loss: 0.00028104\n","Iter: 078/100 | Train Loss: 0.00027428\n","Iter: 079/100 | Train Loss: 0.00026765\n","Iter: 080/100 | Train Loss: 0.00026279\n","Iter: 081/100 | Train Loss: 0.00025731\n","Iter: 082/100 | Train Loss: 0.00025150\n","Iter: 083/100 | Train Loss: 0.00024567\n","Iter: 084/100 | Train Loss: 0.00024047\n","Iter: 085/100 | Train Loss: 0.00023591\n","Iter: 086/100 | Train Loss: 0.00023040\n","Iter: 087/100 | Train Loss: 0.00022539\n","Iter: 088/100 | Train Loss: 0.00022121\n","Iter: 089/100 | Train Loss: 0.00021628\n","Iter: 090/100 | Train Loss: 0.00021155\n","Iter: 091/100 | Train Loss: 0.00020730\n","Iter: 092/100 | Train Loss: 0.00020293\n","Iter: 093/100 | Train Loss: 0.00019865\n","Iter: 094/100 | Train Loss: 0.00019453\n","Iter: 095/100 | Train Loss: 0.00019092\n","Iter: 096/100 | Train Loss: 0.00018729\n","Iter: 097/100 | Train Loss: 0.00018346\n","Iter: 098/100 | Train Loss: 0.00018003\n","Iter: 099/100 | Train Loss: 0.00017663\n","\n","Iter: 099/100 | Test Loss: 0.00227219 | Test acc: 45.2300\n","scale:1.200000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00149512\n","Iter: 001/100 | Train Loss: 0.00204672\n","Adjusting Layer 1, Kernel Nodes: 548, Adptive Nodes:252\n","Iter: 002/100 | Train Loss: 0.00579770\n","Adjusting Layer 1, Kernel Nodes: 480, Adptive Nodes:320\n","Iter: 003/100 | Train Loss: 0.01880391\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 004/100 | Train Loss: 0.17348734\n","Adjusting Layer 1, Kernel Nodes: 205, Adptive Nodes:595\n","Iter: 005/100 | Train Loss: 0.01987439\n","Iter: 006/100 | Train Loss: 0.00161128\n","Iter: 007/100 | Train Loss: 0.00135332\n","Iter: 008/100 | Train Loss: 0.00130991\n","Iter: 009/100 | Train Loss: 0.00125173\n","Iter: 010/100 | Train Loss: 0.00119264\n","Iter: 011/100 | Train Loss: 0.00113573\n","Iter: 012/100 | Train Loss: 0.00106573\n","Iter: 013/100 | Train Loss: 0.00098723\n","Iter: 014/100 | Train Loss: 0.00092491\n","Iter: 015/100 | Train Loss: 0.00088101\n","Iter: 016/100 | Train Loss: 0.00083466\n","Iter: 017/100 | Train Loss: 0.00077960\n","Iter: 018/100 | Train Loss: 0.00073084\n","Iter: 019/100 | Train Loss: 0.00069214\n","Iter: 020/100 | Train Loss: 0.00065581\n","Iter: 021/100 | Train Loss: 0.00061874\n","Iter: 022/100 | Train Loss: 0.00058144\n","Iter: 023/100 | Train Loss: 0.00054814\n","Iter: 024/100 | Train Loss: 0.00051968\n","Iter: 025/100 | Train Loss: 0.00048805\n","Iter: 026/100 | Train Loss: 0.00045026\n","Iter: 027/100 | Train Loss: 0.00041977\n","Iter: 028/100 | Train Loss: 0.00039468\n","Iter: 029/100 | Train Loss: 0.00036840\n","Iter: 030/100 | Train Loss: 0.00034210\n","Iter: 031/100 | Train Loss: 0.00031863\n","Iter: 032/100 | Train Loss: 0.00029855\n","Iter: 033/100 | Train Loss: 0.00027737\n","Iter: 034/100 | Train Loss: 0.00025617\n","Iter: 035/100 | Train Loss: 0.00023580\n","Iter: 036/100 | Train Loss: 0.00021725\n","Iter: 037/100 | Train Loss: 0.00020108\n","Iter: 038/100 | Train Loss: 0.00018479\n","Iter: 039/100 | Train Loss: 0.00016862\n","Iter: 040/100 | Train Loss: 0.00015694\n","Iter: 041/100 | Train Loss: 0.00014358\n","Iter: 042/100 | Train Loss: 0.00013255\n","Iter: 043/100 | Train Loss: 0.00012404\n","Iter: 044/100 | Train Loss: 0.00011378\n","Iter: 045/100 | Train Loss: 0.00010449\n","Iter: 046/100 | Train Loss: 0.00009693\n","Iter: 047/100 | Train Loss: 0.00008873\n","Iter: 048/100 | Train Loss: 0.00008223\n","Iter: 049/100 | Train Loss: 0.00007653\n","Iter: 050/100 | Train Loss: 0.00007057\n","Iter: 051/100 | Train Loss: 0.00006479\n","Iter: 052/100 | Train Loss: 0.00006017\n","Iter: 053/100 | Train Loss: 0.00005529\n","Iter: 054/100 | Train Loss: 0.00005125\n","Iter: 055/100 | Train Loss: 0.00004773\n","Iter: 056/100 | Train Loss: 0.00004372\n","Iter: 057/100 | Train Loss: 0.00004041\n","Iter: 058/100 | Train Loss: 0.00003752\n","Iter: 059/100 | Train Loss: 0.00003421\n","Iter: 060/100 | Train Loss: 0.00003192\n","Iter: 061/100 | Train Loss: 0.00002963\n","Iter: 062/100 | Train Loss: 0.00002736\n","Iter: 063/100 | Train Loss: 0.00002570\n","Iter: 064/100 | Train Loss: 0.00002402\n","Iter: 065/100 | Train Loss: 0.00002234\n","Iter: 066/100 | Train Loss: 0.00002103\n","Iter: 067/100 | Train Loss: 0.00001952\n","Iter: 068/100 | Train Loss: 0.00001826\n","Iter: 069/100 | Train Loss: 0.00001719\n","Iter: 070/100 | Train Loss: 0.00001608\n","Iter: 071/100 | Train Loss: 0.00001516\n","Iter: 072/100 | Train Loss: 0.00001428\n","Iter: 073/100 | Train Loss: 0.00001333\n","Iter: 074/100 | Train Loss: 0.00001260\n","Iter: 075/100 | Train Loss: 0.00001186\n","Iter: 076/100 | Train Loss: 0.00001114\n","Iter: 077/100 | Train Loss: 0.00001057\n","Iter: 078/100 | Train Loss: 0.00000994\n","Iter: 079/100 | Train Loss: 0.00000938\n","Iter: 080/100 | Train Loss: 0.00000887\n","Iter: 081/100 | Train Loss: 0.00000833\n","Iter: 082/100 | Train Loss: 0.00000789\n","Iter: 083/100 | Train Loss: 0.00000747\n","Iter: 084/100 | Train Loss: 0.00000705\n","Iter: 085/100 | Train Loss: 0.00000673\n","Iter: 086/100 | Train Loss: 0.00000638\n","Iter: 087/100 | Train Loss: 0.00000607\n","Iter: 088/100 | Train Loss: 0.00000580\n","Iter: 089/100 | Train Loss: 0.00000551\n","Iter: 090/100 | Train Loss: 0.00000526\n","Iter: 091/100 | Train Loss: 0.00000503\n","Iter: 092/100 | Train Loss: 0.00000480\n","Iter: 093/100 | Train Loss: 0.00000459\n","Iter: 094/100 | Train Loss: 0.00000439\n","Iter: 095/100 | Train Loss: 0.00000421\n","Iter: 096/100 | Train Loss: 0.00000404\n","Iter: 097/100 | Train Loss: 0.00000387\n","Iter: 098/100 | Train Loss: 0.00000371\n","Iter: 099/100 | Train Loss: 0.00000357\n","\n","Iter: 099/100 | Test Loss: 0.00093949 | Test acc: 64.1700\n","scale:1.200000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00142665\n","Iter: 001/100 | Train Loss: 0.00165013\n","Adjusting Layer 1, Kernel Nodes: 574, Adptive Nodes:226\n","Iter: 002/100 | Train Loss: 0.00276494\n","Adjusting Layer 1, Kernel Nodes: 570, Adptive Nodes:230\n","Iter: 003/100 | Train Loss: 0.01050984\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 004/100 | Train Loss: 0.13280061\n","Adjusting Layer 1, Kernel Nodes: 228, Adptive Nodes:572\n","Iter: 005/100 | Train Loss: 0.02750582\n","Iter: 006/100 | Train Loss: 0.00151115\n","Iter: 007/100 | Train Loss: 0.00129594\n","Iter: 008/100 | Train Loss: 0.00126352\n","Iter: 009/100 | Train Loss: 0.00120716\n","Iter: 010/100 | Train Loss: 0.00113998\n","Iter: 011/100 | Train Loss: 0.00108132\n","Iter: 012/100 | Train Loss: 0.00101962\n","Iter: 013/100 | Train Loss: 0.00095652\n","Iter: 014/100 | Train Loss: 0.00088494\n","Iter: 015/100 | Train Loss: 0.00080672\n","Iter: 016/100 | Train Loss: 0.00074397\n","Iter: 017/100 | Train Loss: 0.00068979\n","Iter: 018/100 | Train Loss: 0.00062986\n","Iter: 019/100 | Train Loss: 0.00057790\n","Iter: 020/100 | Train Loss: 0.00054856\n","Iter: 021/100 | Train Loss: 0.00051370\n","Iter: 022/100 | Train Loss: 0.00047345\n","Iter: 023/100 | Train Loss: 0.00044983\n","Iter: 024/100 | Train Loss: 0.00041745\n","Iter: 025/100 | Train Loss: 0.00038153\n","Iter: 026/100 | Train Loss: 0.00035485\n","Iter: 027/100 | Train Loss: 0.00032596\n","Iter: 028/100 | Train Loss: 0.00030001\n","Iter: 029/100 | Train Loss: 0.00027424\n","Iter: 030/100 | Train Loss: 0.00025353\n","Iter: 031/100 | Train Loss: 0.00023261\n","Iter: 032/100 | Train Loss: 0.00021393\n","Iter: 033/100 | Train Loss: 0.00019451\n","Iter: 034/100 | Train Loss: 0.00017887\n","Iter: 035/100 | Train Loss: 0.00016762\n","Iter: 036/100 | Train Loss: 0.00015214\n","Iter: 037/100 | Train Loss: 0.00014194\n","Iter: 038/100 | Train Loss: 0.00012940\n","Iter: 039/100 | Train Loss: 0.00011979\n","Iter: 040/100 | Train Loss: 0.00010675\n","Iter: 041/100 | Train Loss: 0.00009757\n","Iter: 042/100 | Train Loss: 0.00009032\n","Iter: 043/100 | Train Loss: 0.00008245\n","Iter: 044/100 | Train Loss: 0.00007533\n","Iter: 045/100 | Train Loss: 0.00007008\n","Iter: 046/100 | Train Loss: 0.00006520\n","Iter: 047/100 | Train Loss: 0.00005874\n","Iter: 048/100 | Train Loss: 0.00005418\n","Iter: 049/100 | Train Loss: 0.00004995\n","Iter: 050/100 | Train Loss: 0.00004554\n","Iter: 051/100 | Train Loss: 0.00004172\n","Iter: 052/100 | Train Loss: 0.00003875\n","Iter: 053/100 | Train Loss: 0.00003591\n","Iter: 054/100 | Train Loss: 0.00003352\n","Iter: 055/100 | Train Loss: 0.00003106\n","Iter: 056/100 | Train Loss: 0.00002890\n","Iter: 057/100 | Train Loss: 0.00002684\n","Iter: 058/100 | Train Loss: 0.00002471\n","Iter: 059/100 | Train Loss: 0.00002279\n","Iter: 060/100 | Train Loss: 0.00002119\n","Iter: 061/100 | Train Loss: 0.00001955\n","Iter: 062/100 | Train Loss: 0.00001825\n","Iter: 063/100 | Train Loss: 0.00001680\n","Iter: 064/100 | Train Loss: 0.00001563\n","Iter: 065/100 | Train Loss: 0.00001451\n","Iter: 066/100 | Train Loss: 0.00001351\n","Iter: 067/100 | Train Loss: 0.00001263\n","Iter: 068/100 | Train Loss: 0.00001189\n","Iter: 069/100 | Train Loss: 0.00001111\n","Iter: 070/100 | Train Loss: 0.00001048\n","Iter: 071/100 | Train Loss: 0.00000993\n","Iter: 072/100 | Train Loss: 0.00000933\n","Iter: 073/100 | Train Loss: 0.00000872\n","Iter: 074/100 | Train Loss: 0.00000827\n","Iter: 075/100 | Train Loss: 0.00000778\n","Iter: 076/100 | Train Loss: 0.00000730\n","Iter: 077/100 | Train Loss: 0.00000693\n","Iter: 078/100 | Train Loss: 0.00000658\n","Iter: 079/100 | Train Loss: 0.00000623\n","Iter: 080/100 | Train Loss: 0.00000591\n","Iter: 081/100 | Train Loss: 0.00000559\n","Iter: 082/100 | Train Loss: 0.00000532\n","Iter: 083/100 | Train Loss: 0.00000505\n","Iter: 084/100 | Train Loss: 0.00000479\n","Iter: 085/100 | Train Loss: 0.00000455\n","Iter: 086/100 | Train Loss: 0.00000434\n","Iter: 087/100 | Train Loss: 0.00000413\n","Iter: 088/100 | Train Loss: 0.00000394\n","Iter: 089/100 | Train Loss: 0.00000376\n","Iter: 090/100 | Train Loss: 0.00000360\n","Iter: 091/100 | Train Loss: 0.00000344\n","Iter: 092/100 | Train Loss: 0.00000329\n","Iter: 093/100 | Train Loss: 0.00000316\n","Iter: 094/100 | Train Loss: 0.00000303\n","Iter: 095/100 | Train Loss: 0.00000291\n","Iter: 096/100 | Train Loss: 0.00000279\n","Iter: 097/100 | Train Loss: 0.00000269\n","Iter: 098/100 | Train Loss: 0.00000258\n","Iter: 099/100 | Train Loss: 0.00000248\n","\n","Iter: 099/100 | Test Loss: 0.00096654 | Test acc: 62.7100\n","scale:1.200000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00138488\n","Iter: 001/100 | Train Loss: 0.00148762\n","Adjusting Layer 1, Kernel Nodes: 610, Adptive Nodes:190\n","Iter: 002/100 | Train Loss: 0.00174379\n","Adjusting Layer 1, Kernel Nodes: 602, Adptive Nodes:198\n","Iter: 003/100 | Train Loss: 0.00406974\n","Adjusting Layer 1, Kernel Nodes: 333, Adptive Nodes:467\n","Iter: 004/100 | Train Loss: 0.00480822\n","Adjusting Layer 1, Kernel Nodes: 561, Adptive Nodes:239\n","Iter: 005/100 | Train Loss: 0.00327285\n","Iter: 006/100 | Train Loss: 0.00076798\n","Iter: 007/100 | Train Loss: 0.00069026\n","Iter: 008/100 | Train Loss: 0.00092497\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 009/100 | Train Loss: 0.00068334\n","Iter: 010/100 | Train Loss: 0.00045638\n","Iter: 011/100 | Train Loss: 0.00046458\n","Adjusting Layer 1, Kernel Nodes: 789, Adptive Nodes:11\n","Iter: 012/100 | Train Loss: 0.00050268\n","Adjusting Layer 1, Kernel Nodes: 277, Adptive Nodes:523\n","Iter: 013/100 | Train Loss: 0.00036863\n","Iter: 014/100 | Train Loss: 0.00024186\n","Iter: 015/100 | Train Loss: 0.00026154\n","Adjusting Layer 1, Kernel Nodes: 53, Adptive Nodes:747\n","Iter: 016/100 | Train Loss: 0.00027207\n","Adjusting Layer 1, Kernel Nodes: 506, Adptive Nodes:294\n","Iter: 017/100 | Train Loss: 0.00022591\n","Iter: 018/100 | Train Loss: 0.00015807\n","Iter: 019/100 | Train Loss: 0.00013515\n","Iter: 020/100 | Train Loss: 0.00014088\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 021/100 | Train Loss: 0.00011890\n","Iter: 022/100 | Train Loss: 0.00008651\n","Iter: 023/100 | Train Loss: 0.00007845\n","Iter: 024/100 | Train Loss: 0.00008625\n","Adjusting Layer 1, Kernel Nodes: 260, Adptive Nodes:540\n","Iter: 025/100 | Train Loss: 0.00007651\n","Iter: 026/100 | Train Loss: 0.00005571\n","Iter: 027/100 | Train Loss: 0.00004792\n","Iter: 028/100 | Train Loss: 0.00005375\n","Adjusting Layer 1, Kernel Nodes: 545, Adptive Nodes:255\n","Iter: 029/100 | Train Loss: 0.00005110\n","Iter: 030/100 | Train Loss: 0.00003989\n","Iter: 031/100 | Train Loss: 0.00003282\n","Iter: 032/100 | Train Loss: 0.00003320\n","Adjusting Layer 1, Kernel Nodes: 309, Adptive Nodes:491\n","Iter: 033/100 | Train Loss: 0.00003049\n","Iter: 034/100 | Train Loss: 0.00002337\n","Iter: 035/100 | Train Loss: 0.00001960\n","Iter: 036/100 | Train Loss: 0.00002091\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 037/100 | Train Loss: 0.00001890\n","Iter: 038/100 | Train Loss: 0.00001448\n","Iter: 039/100 | Train Loss: 0.00001302\n","Iter: 040/100 | Train Loss: 0.00001235\n","Iter: 041/100 | Train Loss: 0.00000993\n","Iter: 042/100 | Train Loss: 0.00000916\n","Iter: 043/100 | Train Loss: 0.00000921\n","Adjusting Layer 1, Kernel Nodes: 16, Adptive Nodes:784\n","Iter: 044/100 | Train Loss: 0.00000852\n","Iter: 045/100 | Train Loss: 0.00000671\n","Iter: 046/100 | Train Loss: 0.00000670\n","Iter: 047/100 | Train Loss: 0.00000713\n","Adjusting Layer 1, Kernel Nodes: 700, Adptive Nodes:100\n","Iter: 048/100 | Train Loss: 0.00000580\n","Iter: 049/100 | Train Loss: 0.00000483\n","Iter: 050/100 | Train Loss: 0.00000559\n","Adjusting Layer 1, Kernel Nodes: 33, Adptive Nodes:767\n","Iter: 051/100 | Train Loss: 0.00000581\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 052/100 | Train Loss: 0.00000381\n","Iter: 053/100 | Train Loss: 0.00000386\n","Adjusting Layer 1, Kernel Nodes: 183, Adptive Nodes:617\n","Iter: 054/100 | Train Loss: 0.00000395\n","Adjusting Layer 1, Kernel Nodes: 618, Adptive Nodes:182\n","Iter: 055/100 | Train Loss: 0.00000368\n","Iter: 056/100 | Train Loss: 0.00000246\n","Iter: 057/100 | Train Loss: 0.00000248\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 058/100 | Train Loss: 0.00000311\n","Adjusting Layer 1, Kernel Nodes: 618, Adptive Nodes:182\n","Iter: 059/100 | Train Loss: 0.00000234\n","Iter: 060/100 | Train Loss: 0.00000190\n","Iter: 061/100 | Train Loss: 0.00000163\n","Iter: 062/100 | Train Loss: 0.00000154\n","Iter: 063/100 | Train Loss: 0.00000142\n","Iter: 064/100 | Train Loss: 0.00000123\n","Iter: 065/100 | Train Loss: 0.00000108\n","Iter: 066/100 | Train Loss: 0.00000104\n","Iter: 067/100 | Train Loss: 0.00000101\n","Iter: 068/100 | Train Loss: 0.00000089\n","Iter: 069/100 | Train Loss: 0.00000073\n","Iter: 070/100 | Train Loss: 0.00000064\n","Iter: 071/100 | Train Loss: 0.00000065\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 072/100 | Train Loss: 0.00000076\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 073/100 | Train Loss: 0.00000047\n","Iter: 074/100 | Train Loss: 0.00000042\n","Iter: 075/100 | Train Loss: 0.00000045\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 076/100 | Train Loss: 0.00000054\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 077/100 | Train Loss: 0.00000039\n","Iter: 078/100 | Train Loss: 0.00000030\n","Iter: 079/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 080/100 | Train Loss: 0.00000043\n","Adjusting Layer 1, Kernel Nodes: 618, Adptive Nodes:182\n","Iter: 081/100 | Train Loss: 0.00000028\n","Iter: 082/100 | Train Loss: 0.00000023\n","Iter: 083/100 | Train Loss: 0.00000022\n","Iter: 084/100 | Train Loss: 0.00000021\n","Iter: 085/100 | Train Loss: 0.00000018\n","Iter: 086/100 | Train Loss: 0.00000014\n","Iter: 087/100 | Train Loss: 0.00000014\n","Iter: 088/100 | Train Loss: 0.00000014\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 089/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 618, Adptive Nodes:182\n","Iter: 090/100 | Train Loss: 0.00000020\n","Iter: 091/100 | Train Loss: 0.00000012\n","Iter: 092/100 | Train Loss: 0.00000009\n","Iter: 093/100 | Train Loss: 0.00000014\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 094/100 | Train Loss: 0.00000028\n","Adjusting Layer 1, Kernel Nodes: 618, Adptive Nodes:182\n","Iter: 095/100 | Train Loss: 0.00000011\n","Iter: 096/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 182, Adptive Nodes:618\n","Iter: 097/100 | Train Loss: 0.00000010\n","Iter: 098/100 | Train Loss: 0.00000011\n","Adjusting Layer 1, Kernel Nodes: 618, Adptive Nodes:182\n","Iter: 099/100 | Train Loss: 0.00000017\n","\n","Iter: 099/100 | Test Loss: 0.00129063 | Test acc: 58.0800\n","scale:1.200000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00136131\n","Iter: 001/100 | Train Loss: 0.00142167\n","Adjusting Layer 1, Kernel Nodes: 761, Adptive Nodes:39\n","Iter: 002/100 | Train Loss: 0.00172666\n","Adjusting Layer 1, Kernel Nodes: 470, Adptive Nodes:330\n","Iter: 003/100 | Train Loss: 0.00216828\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 004/100 | Train Loss: 0.00217324\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 005/100 | Train Loss: 0.00459582\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 006/100 | Train Loss: 0.00687210\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 007/100 | Train Loss: 0.00630867\n","Iter: 008/100 | Train Loss: 0.00102317\n","Iter: 009/100 | Train Loss: 0.00074076\n","Iter: 010/100 | Train Loss: 0.00123284\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 011/100 | Train Loss: 0.00116255\n","Iter: 012/100 | Train Loss: 0.00081410\n","Iter: 013/100 | Train Loss: 0.00058943\n","Iter: 014/100 | Train Loss: 0.00053561\n","Iter: 015/100 | Train Loss: 0.00056499\n","Adjusting Layer 1, Kernel Nodes: 673, Adptive Nodes:127\n","Iter: 016/100 | Train Loss: 0.00058456\n","Adjusting Layer 1, Kernel Nodes: 156, Adptive Nodes:644\n","Iter: 017/100 | Train Loss: 0.00053218\n","Iter: 018/100 | Train Loss: 0.00041355\n","Iter: 019/100 | Train Loss: 0.00030360\n","Iter: 020/100 | Train Loss: 0.00024743\n","Iter: 021/100 | Train Loss: 0.00023911\n","Iter: 022/100 | Train Loss: 0.00024711\n","Adjusting Layer 1, Kernel Nodes: 675, Adptive Nodes:125\n","Iter: 023/100 | Train Loss: 0.00024135\n","Iter: 024/100 | Train Loss: 0.00020694\n","Iter: 025/100 | Train Loss: 0.00016299\n","Iter: 026/100 | Train Loss: 0.00013207\n","Iter: 027/100 | Train Loss: 0.00011764\n","Iter: 028/100 | Train Loss: 0.00010904\n","Iter: 029/100 | Train Loss: 0.00009882\n","Iter: 030/100 | Train Loss: 0.00008913\n","Iter: 031/100 | Train Loss: 0.00008379\n","Iter: 032/100 | Train Loss: 0.00008184\n","Iter: 033/100 | Train Loss: 0.00007872\n","Iter: 034/100 | Train Loss: 0.00007095\n","Iter: 035/100 | Train Loss: 0.00005916\n","Iter: 036/100 | Train Loss: 0.00004739\n","Iter: 037/100 | Train Loss: 0.00003949\n","Iter: 038/100 | Train Loss: 0.00003629\n","Iter: 039/100 | Train Loss: 0.00003529\n","Iter: 040/100 | Train Loss: 0.00003350\n","Iter: 041/100 | Train Loss: 0.00002991\n","Iter: 042/100 | Train Loss: 0.00002557\n","Iter: 043/100 | Train Loss: 0.00002208\n","Iter: 044/100 | Train Loss: 0.00002003\n","Iter: 045/100 | Train Loss: 0.00001871\n","Iter: 046/100 | Train Loss: 0.00001714\n","Iter: 047/100 | Train Loss: 0.00001498\n","Iter: 048/100 | Train Loss: 0.00001276\n","Iter: 049/100 | Train Loss: 0.00001122\n","Iter: 050/100 | Train Loss: 0.00001060\n","Iter: 051/100 | Train Loss: 0.00001042\n","Iter: 052/100 | Train Loss: 0.00000997\n","Iter: 053/100 | Train Loss: 0.00000889\n","Iter: 054/100 | Train Loss: 0.00000746\n","Iter: 055/100 | Train Loss: 0.00000624\n","Iter: 056/100 | Train Loss: 0.00000555\n","Iter: 057/100 | Train Loss: 0.00000528\n","Iter: 058/100 | Train Loss: 0.00000504\n","Iter: 059/100 | Train Loss: 0.00000458\n","Iter: 060/100 | Train Loss: 0.00000401\n","Iter: 061/100 | Train Loss: 0.00000360\n","Iter: 062/100 | Train Loss: 0.00000345\n","Iter: 063/100 | Train Loss: 0.00000341\n","Iter: 064/100 | Train Loss: 0.00000326\n","Iter: 065/100 | Train Loss: 0.00000291\n","Iter: 066/100 | Train Loss: 0.00000249\n","Iter: 067/100 | Train Loss: 0.00000215\n","Iter: 068/100 | Train Loss: 0.00000196\n","Iter: 069/100 | Train Loss: 0.00000186\n","Iter: 070/100 | Train Loss: 0.00000176\n","Iter: 071/100 | Train Loss: 0.00000164\n","Iter: 072/100 | Train Loss: 0.00000152\n","Iter: 073/100 | Train Loss: 0.00000143\n","Iter: 074/100 | Train Loss: 0.00000136\n","Iter: 075/100 | Train Loss: 0.00000128\n","Iter: 076/100 | Train Loss: 0.00000118\n","Iter: 077/100 | Train Loss: 0.00000106\n","Iter: 078/100 | Train Loss: 0.00000097\n","Iter: 079/100 | Train Loss: 0.00000090\n","Iter: 080/100 | Train Loss: 0.00000085\n","Iter: 081/100 | Train Loss: 0.00000081\n","Iter: 082/100 | Train Loss: 0.00000076\n","Iter: 083/100 | Train Loss: 0.00000071\n","Iter: 084/100 | Train Loss: 0.00000067\n","Iter: 085/100 | Train Loss: 0.00000064\n","Iter: 086/100 | Train Loss: 0.00000060\n","Iter: 087/100 | Train Loss: 0.00000055\n","Iter: 088/100 | Train Loss: 0.00000051\n","Iter: 089/100 | Train Loss: 0.00000048\n","Iter: 090/100 | Train Loss: 0.00000046\n","Iter: 091/100 | Train Loss: 0.00000045\n","Iter: 092/100 | Train Loss: 0.00000043\n","Iter: 093/100 | Train Loss: 0.00000041\n","Iter: 094/100 | Train Loss: 0.00000038\n","Iter: 095/100 | Train Loss: 0.00000037\n","Iter: 096/100 | Train Loss: 0.00000035\n","Iter: 097/100 | Train Loss: 0.00000034\n","Iter: 098/100 | Train Loss: 0.00000032\n","Iter: 099/100 | Train Loss: 0.00000031\n","\n","Iter: 099/100 | Test Loss: 0.00130200 | Test acc: 55.1400\n","scale:1.200000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00134749\n","Iter: 001/100 | Train Loss: 0.00139513\n","Adjusting Layer 1, Kernel Nodes: 644, Adptive Nodes:156\n","Iter: 002/100 | Train Loss: 0.00115117\n","Iter: 003/100 | Train Loss: 0.00079174\n","Iter: 004/100 | Train Loss: 0.00080978\n","Adjusting Layer 1, Kernel Nodes: 582, Adptive Nodes:218\n","Iter: 005/100 | Train Loss: 0.00068476\n","Iter: 006/100 | Train Loss: 0.00051525\n","Iter: 007/100 | Train Loss: 0.00051636\n","Adjusting Layer 1, Kernel Nodes: 775, Adptive Nodes:25\n","Iter: 008/100 | Train Loss: 0.00039936\n","Iter: 009/100 | Train Loss: 0.00036402\n","Iter: 010/100 | Train Loss: 0.00031396\n","Iter: 011/100 | Train Loss: 0.00025627\n","Iter: 012/100 | Train Loss: 0.00022223\n","Iter: 013/100 | Train Loss: 0.00018006\n","Iter: 014/100 | Train Loss: 0.00015615\n","Iter: 015/100 | Train Loss: 0.00014823\n","Iter: 016/100 | Train Loss: 0.00011838\n","Iter: 017/100 | Train Loss: 0.00012972\n","Adjusting Layer 1, Kernel Nodes: 722, Adptive Nodes:78\n","Iter: 018/100 | Train Loss: 0.00009219\n","Iter: 019/100 | Train Loss: 0.00009957\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 020/100 | Train Loss: 0.00008265\n","Iter: 021/100 | Train Loss: 0.00005731\n","Iter: 022/100 | Train Loss: 0.00006826\n","Adjusting Layer 1, Kernel Nodes: 576, Adptive Nodes:224\n","Iter: 023/100 | Train Loss: 0.00005085\n","Iter: 024/100 | Train Loss: 0.00004844\n","Iter: 025/100 | Train Loss: 0.00004988\n","Adjusting Layer 1, Kernel Nodes: 544, Adptive Nodes:256\n","Iter: 026/100 | Train Loss: 0.00006003\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 027/100 | Train Loss: 0.00023873\n","Adjusting Layer 1, Kernel Nodes: 151, Adptive Nodes:649\n","Iter: 028/100 | Train Loss: 0.00143703\n","Adjusting Layer 1, Kernel Nodes: 57, Adptive Nodes:743\n","Iter: 029/100 | Train Loss: 0.00305618\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 030/100 | Train Loss: 0.00565360\n","Adjusting Layer 1, Kernel Nodes: 21, Adptive Nodes:779\n","Iter: 031/100 | Train Loss: 0.00065009\n","Iter: 032/100 | Train Loss: 0.00179322\n","Adjusting Layer 1, Kernel Nodes: 389, Adptive Nodes:411\n","Iter: 033/100 | Train Loss: 0.00046919\n","Iter: 034/100 | Train Loss: 0.00035329\n","Iter: 035/100 | Train Loss: 0.00065873\n","Adjusting Layer 1, Kernel Nodes: 325, Adptive Nodes:475\n","Iter: 036/100 | Train Loss: 0.00067550\n","Adjusting Layer 1, Kernel Nodes: 779, Adptive Nodes:21\n","Iter: 037/100 | Train Loss: 0.00044898\n","Iter: 038/100 | Train Loss: 0.00024805\n","Iter: 039/100 | Train Loss: 0.00021101\n","Iter: 040/100 | Train Loss: 0.00027480\n","Adjusting Layer 1, Kernel Nodes: 369, Adptive Nodes:431\n","Iter: 041/100 | Train Loss: 0.00031216\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 042/100 | Train Loss: 0.00024247\n","Iter: 043/100 | Train Loss: 0.00014618\n","Iter: 044/100 | Train Loss: 0.00010696\n","Iter: 045/100 | Train Loss: 0.00012786\n","Adjusting Layer 1, Kernel Nodes: 195, Adptive Nodes:605\n","Iter: 046/100 | Train Loss: 0.00015049\n","Adjusting Layer 1, Kernel Nodes: 772, Adptive Nodes:28\n","Iter: 047/100 | Train Loss: 0.00012701\n","Iter: 048/100 | Train Loss: 0.00007718\n","Iter: 049/100 | Train Loss: 0.00005856\n","Iter: 050/100 | Train Loss: 0.00007132\n","Adjusting Layer 1, Kernel Nodes: 619, Adptive Nodes:181\n","Iter: 051/100 | Train Loss: 0.00007653\n","Adjusting Layer 1, Kernel Nodes: 387, Adptive Nodes:413\n","Iter: 052/100 | Train Loss: 0.00005718\n","Iter: 053/100 | Train Loss: 0.00003681\n","Iter: 054/100 | Train Loss: 0.00003918\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 055/100 | Train Loss: 0.00004937\n","Adjusting Layer 1, Kernel Nodes: 39, Adptive Nodes:761\n","Iter: 056/100 | Train Loss: 0.00005092\n","Adjusting Layer 1, Kernel Nodes: 494, Adptive Nodes:306\n","Iter: 057/100 | Train Loss: 0.00004189\n","Iter: 058/100 | Train Loss: 0.00002790\n","Iter: 059/100 | Train Loss: 0.00001897\n","Iter: 060/100 | Train Loss: 0.00001796\n","Iter: 061/100 | Train Loss: 0.00002095\n","Adjusting Layer 1, Kernel Nodes: 306, Adptive Nodes:494\n","Iter: 062/100 | Train Loss: 0.00002267\n","Adjusting Layer 1, Kernel Nodes: 496, Adptive Nodes:304\n","Iter: 063/100 | Train Loss: 0.00001989\n","Iter: 064/100 | Train Loss: 0.00001544\n","Iter: 065/100 | Train Loss: 0.00001202\n","Iter: 066/100 | Train Loss: 0.00001110\n","Iter: 067/100 | Train Loss: 0.00001176\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 068/100 | Train Loss: 0.00001225\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 069/100 | Train Loss: 0.00000993\n","Iter: 070/100 | Train Loss: 0.00000670\n","Iter: 071/100 | Train Loss: 0.00000572\n","Iter: 072/100 | Train Loss: 0.00000670\n","Adjusting Layer 1, Kernel Nodes: 5, Adptive Nodes:795\n","Iter: 073/100 | Train Loss: 0.00000770\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 074/100 | Train Loss: 0.00000637\n","Iter: 075/100 | Train Loss: 0.00000459\n","Iter: 076/100 | Train Loss: 0.00000361\n","Iter: 077/100 | Train Loss: 0.00000381\n","Adjusting Layer 1, Kernel Nodes: 1, Adptive Nodes:799\n","Iter: 078/100 | Train Loss: 0.00000412\n","Adjusting Layer 1, Kernel Nodes: 175, Adptive Nodes:625\n","Iter: 079/100 | Train Loss: 0.00000404\n","Iter: 080/100 | Train Loss: 0.00000347\n","Iter: 081/100 | Train Loss: 0.00000268\n","Iter: 082/100 | Train Loss: 0.00000213\n","Iter: 083/100 | Train Loss: 0.00000200\n","Iter: 084/100 | Train Loss: 0.00000210\n","Adjusting Layer 1, Kernel Nodes: 289, Adptive Nodes:511\n","Iter: 085/100 | Train Loss: 0.00000220\n","Adjusting Layer 1, Kernel Nodes: 455, Adptive Nodes:345\n","Iter: 086/100 | Train Loss: 0.00000193\n","Iter: 087/100 | Train Loss: 0.00000160\n","Iter: 088/100 | Train Loss: 0.00000132\n","Iter: 089/100 | Train Loss: 0.00000120\n","Iter: 090/100 | Train Loss: 0.00000120\n","Adjusting Layer 1, Kernel Nodes: 329, Adptive Nodes:471\n","Iter: 091/100 | Train Loss: 0.00000117\n","Iter: 092/100 | Train Loss: 0.00000110\n","Iter: 093/100 | Train Loss: 0.00000093\n","Iter: 094/100 | Train Loss: 0.00000074\n","Iter: 095/100 | Train Loss: 0.00000063\n","Iter: 096/100 | Train Loss: 0.00000059\n","Iter: 097/100 | Train Loss: 0.00000061\n","Adjusting Layer 1, Kernel Nodes: 468, Adptive Nodes:332\n","Iter: 098/100 | Train Loss: 0.00000066\n","Adjusting Layer 1, Kernel Nodes: 332, Adptive Nodes:468\n","Iter: 099/100 | Train Loss: 0.00000060\n","\n","Iter: 099/100 | Test Loss: 0.00110437 | Test acc: 60.6800\n","scale:1.200000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00133846\n","Iter: 001/100 | Train Loss: 0.00138617\n","Adjusting Layer 1, Kernel Nodes: 645, Adptive Nodes:155\n","Iter: 002/100 | Train Loss: 0.00104616\n","Iter: 003/100 | Train Loss: 0.00077418\n","Iter: 004/100 | Train Loss: 0.00082365\n","Adjusting Layer 1, Kernel Nodes: 623, Adptive Nodes:177\n","Iter: 005/100 | Train Loss: 0.00059549\n","Iter: 006/100 | Train Loss: 0.00058078\n","Iter: 007/100 | Train Loss: 0.00043621\n","Iter: 008/100 | Train Loss: 0.00043849\n","Adjusting Layer 1, Kernel Nodes: 728, Adptive Nodes:72\n","Iter: 009/100 | Train Loss: 0.00035149\n","Iter: 010/100 | Train Loss: 0.00032870\n","Iter: 011/100 | Train Loss: 0.00027154\n","Iter: 012/100 | Train Loss: 0.00025220\n","Iter: 013/100 | Train Loss: 0.00018692\n","Iter: 014/100 | Train Loss: 0.00018966\n","Adjusting Layer 1, Kernel Nodes: 685, Adptive Nodes:115\n","Iter: 015/100 | Train Loss: 0.00013889\n","Iter: 016/100 | Train Loss: 0.00014602\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 017/100 | Train Loss: 0.00011394\n","Iter: 018/100 | Train Loss: 0.00011411\n","Adjusting Layer 1, Kernel Nodes: 781, Adptive Nodes:19\n","Iter: 019/100 | Train Loss: 0.00008819\n","Iter: 020/100 | Train Loss: 0.00008810\n","Iter: 021/100 | Train Loss: 0.00006250\n","Iter: 022/100 | Train Loss: 0.00006283\n","Adjusting Layer 1, Kernel Nodes: 736, Adptive Nodes:64\n","Iter: 023/100 | Train Loss: 0.00004719\n","Iter: 024/100 | Train Loss: 0.00004910\n","Adjusting Layer 1, Kernel Nodes: 459, Adptive Nodes:341\n","Iter: 025/100 | Train Loss: 0.00004330\n","Iter: 026/100 | Train Loss: 0.00003787\n","Iter: 027/100 | Train Loss: 0.00003425\n","Iter: 028/100 | Train Loss: 0.00003009\n","Iter: 029/100 | Train Loss: 0.00002447\n","Iter: 030/100 | Train Loss: 0.00002454\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 031/100 | Train Loss: 0.00001917\n","Iter: 032/100 | Train Loss: 0.00002070\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 033/100 | Train Loss: 0.00002206\n","Adjusting Layer 1, Kernel Nodes: 417, Adptive Nodes:383\n","Iter: 034/100 | Train Loss: 0.00002195\n","Iter: 035/100 | Train Loss: 0.00001759\n","Iter: 036/100 | Train Loss: 0.00001436\n","Iter: 037/100 | Train Loss: 0.00001294\n","Iter: 038/100 | Train Loss: 0.00001201\n","Iter: 039/100 | Train Loss: 0.00001036\n","Iter: 040/100 | Train Loss: 0.00000775\n","Iter: 041/100 | Train Loss: 0.00000684\n","Iter: 042/100 | Train Loss: 0.00000815\n","Adjusting Layer 1, Kernel Nodes: 551, Adptive Nodes:249\n","Iter: 043/100 | Train Loss: 0.00001094\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 044/100 | Train Loss: 0.00002264\n","Adjusting Layer 1, Kernel Nodes: 385, Adptive Nodes:415\n","Iter: 045/100 | Train Loss: 0.00008281\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 046/100 | Train Loss: 0.00083829\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 047/100 | Train Loss: 0.01664303\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 048/100 | Train Loss: 0.36059475\n","Adjusting Layer 1, Kernel Nodes: 313, Adptive Nodes:487\n","Iter: 049/100 | Train Loss: 0.02089591\n","Iter: 050/100 | Train Loss: 0.00383599\n","Iter: 051/100 | Train Loss: 0.00160161\n","Iter: 052/100 | Train Loss: 0.00134070\n","Iter: 053/100 | Train Loss: 0.00127574\n","Iter: 054/100 | Train Loss: 0.00122712\n","Iter: 055/100 | Train Loss: 0.00118202\n","Iter: 056/100 | Train Loss: 0.00113316\n","Iter: 057/100 | Train Loss: 0.00107491\n","Iter: 058/100 | Train Loss: 0.00101065\n","Iter: 059/100 | Train Loss: 0.00095096\n","Iter: 060/100 | Train Loss: 0.00088928\n","Iter: 061/100 | Train Loss: 0.00081560\n","Iter: 062/100 | Train Loss: 0.00074831\n","Iter: 063/100 | Train Loss: 0.37574771\n","Adjusting Layer 1, Kernel Nodes: 743, Adptive Nodes:57\n","Iter: 064/100 | Train Loss: 0.04868015\n","Iter: 065/100 | Train Loss: 0.00140351\n","Iter: 066/100 | Train Loss: 0.06729569\n","Adjusting Layer 1, Kernel Nodes: 772, Adptive Nodes:28\n","Iter: 067/100 | Train Loss: 0.00142637\n","Iter: 068/100 | Train Loss: 0.00142038\n","Iter: 069/100 | Train Loss: 0.00141320\n","Iter: 070/100 | Train Loss: 0.00140834\n","Iter: 071/100 | Train Loss: 0.02969335\n","Adjusting Layer 1, Kernel Nodes: 774, Adptive Nodes:26\n","Iter: 072/100 | Train Loss: 0.00151769\n","Iter: 073/100 | Train Loss: 0.00250009\n","Adjusting Layer 1, Kernel Nodes: 638, Adptive Nodes:162\n","Iter: 074/100 | Train Loss: 0.00290260\n","Adjusting Layer 1, Kernel Nodes: 549, Adptive Nodes:251\n","Iter: 075/100 | Train Loss: 0.00151869\n","Iter: 076/100 | Train Loss: 0.00129244\n","Iter: 077/100 | Train Loss: 0.00128385\n","Iter: 078/100 | Train Loss: 0.00125868\n","Iter: 079/100 | Train Loss: 0.00123096\n","Iter: 080/100 | Train Loss: 0.00119971\n","Iter: 081/100 | Train Loss: 0.00116573\n","Iter: 082/100 | Train Loss: 0.00113040\n","Iter: 083/100 | Train Loss: 0.00109416\n","Iter: 084/100 | Train Loss: 0.00105669\n","Iter: 085/100 | Train Loss: 0.00101741\n","Iter: 086/100 | Train Loss: 0.00097943\n","Iter: 087/100 | Train Loss: 0.00094502\n","Iter: 088/100 | Train Loss: 0.00091388\n","Iter: 089/100 | Train Loss: 0.00088435\n","Iter: 090/100 | Train Loss: 0.00085416\n","Iter: 091/100 | Train Loss: 0.00082323\n","Iter: 092/100 | Train Loss: 0.00079793\n","Iter: 093/100 | Train Loss: 0.00078415\n","Iter: 094/100 | Train Loss: 0.00077464\n","Iter: 095/100 | Train Loss: 0.00075413\n","Iter: 096/100 | Train Loss: 0.00072351\n","Iter: 097/100 | Train Loss: 0.00069880\n","Iter: 098/100 | Train Loss: 0.00068512\n","Iter: 099/100 | Train Loss: 0.00067666\n","\n","Iter: 099/100 | Test Loss: 0.00127295 | Test acc: 45.3500\n","scale:1.200000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 000/100 | Train Loss: 0.00133481\n","Iter: 001/100 | Train Loss: 0.00139199\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 002/100 | Train Loss: 0.00101915\n","Iter: 003/100 | Train Loss: 0.00078000\n","Iter: 004/100 | Train Loss: 0.00082667\n","Adjusting Layer 1, Kernel Nodes: 646, Adptive Nodes:154\n","Iter: 005/100 | Train Loss: 0.00056265\n","Iter: 006/100 | Train Loss: 0.00058357\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 007/100 | Train Loss: 0.00041083\n","Iter: 008/100 | Train Loss: 0.00045115\n","Adjusting Layer 1, Kernel Nodes: 691, Adptive Nodes:109\n","Iter: 009/100 | Train Loss: 0.00031201\n","Iter: 010/100 | Train Loss: 0.00034635\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 011/100 | Train Loss: 0.00025111\n","Iter: 012/100 | Train Loss: 0.00025300\n","Adjusting Layer 1, Kernel Nodes: 762, Adptive Nodes:38\n","Iter: 013/100 | Train Loss: 0.00020031\n","Iter: 014/100 | Train Loss: 0.00017013\n","Iter: 015/100 | Train Loss: 0.00016776\n","Iter: 016/100 | Train Loss: 0.00011949\n","Iter: 017/100 | Train Loss: 0.00013523\n","Adjusting Layer 1, Kernel Nodes: 638, Adptive Nodes:162\n","Iter: 018/100 | Train Loss: 0.00010280\n","Iter: 019/100 | Train Loss: 0.00009426\n","Iter: 020/100 | Train Loss: 0.00009041\n","Iter: 021/100 | Train Loss: 0.00006462\n","Iter: 022/100 | Train Loss: 0.00006639\n","Adjusting Layer 1, Kernel Nodes: 786, Adptive Nodes:14\n","Iter: 023/100 | Train Loss: 0.00005558\n","Iter: 024/100 | Train Loss: 0.00004470\n","Iter: 025/100 | Train Loss: 0.00004692\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 026/100 | Train Loss: 0.00003514\n","Iter: 027/100 | Train Loss: 0.00003438\n","Iter: 028/100 | Train Loss: 0.00003314\n","Iter: 029/100 | Train Loss: 0.00002320\n","Iter: 030/100 | Train Loss: 0.00002820\n","Adjusting Layer 1, Kernel Nodes: 725, Adptive Nodes:75\n","Iter: 031/100 | Train Loss: 0.00002023\n","Iter: 032/100 | Train Loss: 0.00001988\n","Iter: 033/100 | Train Loss: 0.00001946\n","Iter: 034/100 | Train Loss: 0.00001360\n","Iter: 035/100 | Train Loss: 0.00001610\n","Adjusting Layer 1, Kernel Nodes: 712, Adptive Nodes:88\n","Iter: 036/100 | Train Loss: 0.00001276\n","Iter: 037/100 | Train Loss: 0.00001199\n","Iter: 038/100 | Train Loss: 0.00001241\n","Adjusting Layer 1, Kernel Nodes: 648, Adptive Nodes:152\n","Iter: 039/100 | Train Loss: 0.00000858\n","Iter: 040/100 | Train Loss: 0.00000982\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 041/100 | Train Loss: 0.00000671\n","Iter: 042/100 | Train Loss: 0.00000712\n","Adjusting Layer 1, Kernel Nodes: 758, Adptive Nodes:42\n","Iter: 043/100 | Train Loss: 0.00000603\n","Iter: 044/100 | Train Loss: 0.00000538\n","Iter: 045/100 | Train Loss: 0.00000545\n","Adjusting Layer 1, Kernel Nodes: 643, Adptive Nodes:157\n","Iter: 046/100 | Train Loss: 0.00000409\n","Iter: 047/100 | Train Loss: 0.00000437\n","Adjusting Layer 1, Kernel Nodes: 485, Adptive Nodes:315\n","Iter: 048/100 | Train Loss: 0.00000291\n","Iter: 049/100 | Train Loss: 0.00000336\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 050/100 | Train Loss: 0.00000245\n","Iter: 051/100 | Train Loss: 0.00000306\n","Adjusting Layer 1, Kernel Nodes: 512, Adptive Nodes:288\n","Iter: 052/100 | Train Loss: 0.00000228\n","Iter: 053/100 | Train Loss: 0.00000245\n","Adjusting Layer 1, Kernel Nodes: 553, Adptive Nodes:247\n","Iter: 054/100 | Train Loss: 0.00000183\n","Iter: 055/100 | Train Loss: 0.00000174\n","Iter: 056/100 | Train Loss: 0.00000144\n","Iter: 057/100 | Train Loss: 0.00000144\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 058/100 | Train Loss: 0.00000157\n","Adjusting Layer 1, Kernel Nodes: 573, Adptive Nodes:227\n","Iter: 059/100 | Train Loss: 0.00000128\n","Iter: 060/100 | Train Loss: 0.00000116\n","Iter: 061/100 | Train Loss: 0.00000100\n","Iter: 062/100 | Train Loss: 0.00000100\n","Adjusting Layer 1, Kernel Nodes: 626, Adptive Nodes:174\n","Iter: 063/100 | Train Loss: 0.00000091\n","Iter: 064/100 | Train Loss: 0.00000099\n","Adjusting Layer 1, Kernel Nodes: 193, Adptive Nodes:607\n","Iter: 065/100 | Train Loss: 0.00000107\n","Adjusting Layer 1, Kernel Nodes: 570, Adptive Nodes:230\n","Iter: 066/100 | Train Loss: 0.00000107\n","Adjusting Layer 1, Kernel Nodes: 305, Adptive Nodes:495\n","Iter: 067/100 | Train Loss: 0.00000093\n","Iter: 068/100 | Train Loss: 0.00000058\n","Iter: 069/100 | Train Loss: 0.00000089\n","Adjusting Layer 1, Kernel Nodes: 329, Adptive Nodes:471\n","Iter: 070/100 | Train Loss: 0.00000093\n","Adjusting Layer 1, Kernel Nodes: 386, Adptive Nodes:414\n","Iter: 071/100 | Train Loss: 0.00000053\n","Iter: 072/100 | Train Loss: 0.00000075\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 073/100 | Train Loss: 0.00000051\n","Iter: 074/100 | Train Loss: 0.00000043\n","Iter: 075/100 | Train Loss: 0.00000045\n","Adjusting Layer 1, Kernel Nodes: 282, Adptive Nodes:518\n","Iter: 076/100 | Train Loss: 0.00000072\n","Adjusting Layer 1, Kernel Nodes: 432, Adptive Nodes:368\n","Iter: 077/100 | Train Loss: 0.00000063\n","Iter: 078/100 | Train Loss: 0.00000032\n","Iter: 079/100 | Train Loss: 0.00000051\n","Adjusting Layer 1, Kernel Nodes: 346, Adptive Nodes:454\n","Iter: 080/100 | Train Loss: 0.00000042\n","Iter: 081/100 | Train Loss: 0.00000021\n","Iter: 082/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 472, Adptive Nodes:328\n","Iter: 083/100 | Train Loss: 0.00000051\n","Adjusting Layer 1, Kernel Nodes: 320, Adptive Nodes:480\n","Iter: 084/100 | Train Loss: 0.00000044\n","Iter: 085/100 | Train Loss: 0.00000019\n","Iter: 086/100 | Train Loss: 0.00000042\n","Adjusting Layer 1, Kernel Nodes: 466, Adptive Nodes:334\n","Iter: 087/100 | Train Loss: 0.00000028\n","Iter: 088/100 | Train Loss: 0.00000016\n","Iter: 089/100 | Train Loss: 0.00000022\n","Adjusting Layer 1, Kernel Nodes: 321, Adptive Nodes:479\n","Iter: 090/100 | Train Loss: 0.00000045\n","Adjusting Layer 1, Kernel Nodes: 478, Adptive Nodes:322\n","Iter: 091/100 | Train Loss: 0.00000027\n","Iter: 092/100 | Train Loss: 0.00000015\n","Iter: 093/100 | Train Loss: 0.00000040\n","Adjusting Layer 1, Kernel Nodes: 321, Adptive Nodes:479\n","Iter: 094/100 | Train Loss: 0.00000016\n","Iter: 095/100 | Train Loss: 0.00000014\n","Iter: 096/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 479, Adptive Nodes:321\n","Iter: 097/100 | Train Loss: 0.00000024\n","Adjusting Layer 1, Kernel Nodes: 322, Adptive Nodes:478\n","Iter: 098/100 | Train Loss: 0.00000015\n","Iter: 099/100 | Train Loss: 0.00000013\n","\n","Iter: 099/100 | Test Loss: 0.00099925 | Test acc: 64.8000\n","scale:1.200000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00133722\n","Iter: 001/100 | Train Loss: 0.00141421\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 002/100 | Train Loss: 0.00104846\n","Iter: 003/100 | Train Loss: 0.00078005\n","Iter: 004/100 | Train Loss: 0.00084013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00054681\n","Iter: 006/100 | Train Loss: 0.00058120\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00040279\n","Iter: 008/100 | Train Loss: 0.00044358\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 009/100 | Train Loss: 0.00032107\n","Iter: 010/100 | Train Loss: 0.00031803\n","Iter: 011/100 | Train Loss: 0.00029115\n","Iter: 012/100 | Train Loss: 0.00020976\n","Iter: 013/100 | Train Loss: 0.00023724\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00017081\n","Iter: 015/100 | Train Loss: 0.00014750\n","Iter: 016/100 | Train Loss: 0.00015620\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00011256\n","Iter: 018/100 | Train Loss: 0.00010147\n","Iter: 019/100 | Train Loss: 0.00010772\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 020/100 | Train Loss: 0.00007552\n","Iter: 021/100 | Train Loss: 0.00006475\n","Iter: 022/100 | Train Loss: 0.00007281\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 023/100 | Train Loss: 0.00005227\n","Iter: 024/100 | Train Loss: 0.00004478\n","Iter: 025/100 | Train Loss: 0.00005006\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 026/100 | Train Loss: 0.00003672\n","Iter: 027/100 | Train Loss: 0.00002916\n","Iter: 028/100 | Train Loss: 0.00003101\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 029/100 | Train Loss: 0.00003015\n","Iter: 030/100 | Train Loss: 0.00002160\n","Iter: 031/100 | Train Loss: 0.00002118\n","Iter: 032/100 | Train Loss: 0.00002297\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 033/100 | Train Loss: 0.00001576\n","Iter: 034/100 | Train Loss: 0.00001315\n","Iter: 035/100 | Train Loss: 0.00001501\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 036/100 | Train Loss: 0.00001376\n","Iter: 037/100 | Train Loss: 0.00001008\n","Iter: 038/100 | Train Loss: 0.00001042\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 039/100 | Train Loss: 0.00001081\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 040/100 | Train Loss: 0.00000777\n","Iter: 041/100 | Train Loss: 0.00000637\n","Iter: 042/100 | Train Loss: 0.00000704\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 043/100 | Train Loss: 0.00000647\n","Iter: 044/100 | Train Loss: 0.00000479\n","Iter: 045/100 | Train Loss: 0.00000450\n","Iter: 046/100 | Train Loss: 0.00000481\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 047/100 | Train Loss: 0.00000388\n","Iter: 048/100 | Train Loss: 0.00000281\n","Iter: 049/100 | Train Loss: 0.00000319\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 050/100 | Train Loss: 0.00000325\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 051/100 | Train Loss: 0.00000233\n","Iter: 052/100 | Train Loss: 0.00000206\n","Iter: 053/100 | Train Loss: 0.00000228\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 054/100 | Train Loss: 0.00000197\n","Iter: 055/100 | Train Loss: 0.00000149\n","Iter: 056/100 | Train Loss: 0.00000150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 057/100 | Train Loss: 0.00000165\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 058/100 | Train Loss: 0.00000129\n","Iter: 059/100 | Train Loss: 0.00000103\n","Iter: 060/100 | Train Loss: 0.00000106\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 061/100 | Train Loss: 0.00000103\n","Iter: 062/100 | Train Loss: 0.00000080\n","Iter: 063/100 | Train Loss: 0.00000068\n","Iter: 064/100 | Train Loss: 0.00000080\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 065/100 | Train Loss: 0.00000072\n","Iter: 066/100 | Train Loss: 0.00000054\n","Iter: 067/100 | Train Loss: 0.00000056\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 068/100 | Train Loss: 0.00000058\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 069/100 | Train Loss: 0.00000049\n","Iter: 070/100 | Train Loss: 0.00000040\n","Iter: 071/100 | Train Loss: 0.00000043\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 072/100 | Train Loss: 0.00000042\n","Iter: 073/100 | Train Loss: 0.00000034\n","Iter: 074/100 | Train Loss: 0.00000031\n","Iter: 075/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 076/100 | Train Loss: 0.00000031\n","Iter: 077/100 | Train Loss: 0.00000026\n","Iter: 078/100 | Train Loss: 0.00000025\n","Iter: 079/100 | Train Loss: 0.00000026\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 080/100 | Train Loss: 0.00000023\n","Iter: 081/100 | Train Loss: 0.00000019\n","Iter: 082/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 083/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 084/100 | Train Loss: 0.00000018\n","Iter: 085/100 | Train Loss: 0.00000017\n","Iter: 086/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 087/100 | Train Loss: 0.00000017\n","Iter: 088/100 | Train Loss: 0.00000015\n","Iter: 089/100 | Train Loss: 0.00000014\n","Iter: 090/100 | Train Loss: 0.00000015\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 091/100 | Train Loss: 0.00000014\n","Iter: 092/100 | Train Loss: 0.00000013\n","Iter: 093/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 094/100 | Train Loss: 0.00000013\n","Iter: 095/100 | Train Loss: 0.00000012\n","Iter: 096/100 | Train Loss: 0.00000012\n","Iter: 097/100 | Train Loss: 0.00000012\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 098/100 | Train Loss: 0.00000012\n","Iter: 099/100 | Train Loss: 0.00000011\n","\n","Iter: 099/100 | Test Loss: 0.00096208 | Test acc: 65.5500\n","scale:1.200000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00133257\n","Iter: 001/100 | Train Loss: 0.00143726\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 002/100 | Train Loss: 0.00108134\n","Iter: 003/100 | Train Loss: 0.00080986\n","Iter: 004/100 | Train Loss: 0.00083446\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00055710\n","Iter: 006/100 | Train Loss: 0.00054429\n","Iter: 007/100 | Train Loss: 0.00042219\n","Iter: 008/100 | Train Loss: 0.00039288\n","Iter: 009/100 | Train Loss: 0.00037302\n","Iter: 010/100 | Train Loss: 0.00026685\n","Iter: 011/100 | Train Loss: 0.00030330\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00022728\n","Iter: 013/100 | Train Loss: 0.00018166\n","Iter: 014/100 | Train Loss: 0.00020449\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00014511\n","Iter: 016/100 | Train Loss: 0.00011822\n","Iter: 017/100 | Train Loss: 0.00012684\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 018/100 | Train Loss: 0.00010179\n","Iter: 019/100 | Train Loss: 0.00007376\n","Iter: 020/100 | Train Loss: 0.00008578\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 021/100 | Train Loss: 0.00006637\n","Iter: 022/100 | Train Loss: 0.00005265\n","Iter: 023/100 | Train Loss: 0.00004804\n","Iter: 024/100 | Train Loss: 0.00005185\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 025/100 | Train Loss: 0.00003401\n","Iter: 026/100 | Train Loss: 0.00003391\n","Iter: 027/100 | Train Loss: 0.00003392\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 028/100 | Train Loss: 0.00002983\n","Iter: 029/100 | Train Loss: 0.00002029\n","Iter: 030/100 | Train Loss: 0.00002087\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 031/100 | Train Loss: 0.00002245\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 032/100 | Train Loss: 0.00001967\n","Iter: 033/100 | Train Loss: 0.00001734\n","Iter: 034/100 | Train Loss: 0.00001633\n","Iter: 035/100 | Train Loss: 0.00001063\n","Iter: 036/100 | Train Loss: 0.00000874\n","Iter: 037/100 | Train Loss: 0.00000901\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 038/100 | Train Loss: 0.00001399\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 039/100 | Train Loss: 0.00001110\n","Iter: 040/100 | Train Loss: 0.00001642\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 041/100 | Train Loss: 0.00002459\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 042/100 | Train Loss: 0.00012781\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 043/100 | Train Loss: 0.00064777\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 044/100 | Train Loss: 0.00247564\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 045/100 | Train Loss: 0.00068894\n","Iter: 046/100 | Train Loss: 0.00082247\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 047/100 | Train Loss: 0.00098332\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 048/100 | Train Loss: 0.00088863\n","Iter: 049/100 | Train Loss: 0.00076555\n","Iter: 050/100 | Train Loss: 0.00049823\n","Iter: 051/100 | Train Loss: 0.00055710\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 052/100 | Train Loss: 0.00049139\n","Iter: 053/100 | Train Loss: 0.00041207\n","Iter: 054/100 | Train Loss: 0.00052731\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 055/100 | Train Loss: 0.00082652\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 056/100 | Train Loss: 0.00074256\n","Iter: 057/100 | Train Loss: 0.00131659\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 058/100 | Train Loss: 0.00067162\n","Iter: 059/100 | Train Loss: 0.00088343\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 060/100 | Train Loss: 0.00066492\n","Iter: 061/100 | Train Loss: 0.00057747\n","Iter: 062/100 | Train Loss: 0.00044696\n","Iter: 063/100 | Train Loss: 0.00049190\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 064/100 | Train Loss: 0.00035606\n","Iter: 065/100 | Train Loss: 0.00042789\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 066/100 | Train Loss: 0.00055717\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 067/100 | Train Loss: 0.00035640\n","Iter: 068/100 | Train Loss: 0.00029786\n","Iter: 069/100 | Train Loss: 0.00028268\n","Iter: 070/100 | Train Loss: 0.00024635\n","Iter: 071/100 | Train Loss: 0.00028131\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 072/100 | Train Loss: 0.00024064\n","Iter: 073/100 | Train Loss: 0.00029700\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 074/100 | Train Loss: 0.00034921\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 075/100 | Train Loss: 0.00026837\n","Iter: 076/100 | Train Loss: 0.00018133\n","Iter: 077/100 | Train Loss: 0.00014544\n","Iter: 078/100 | Train Loss: 0.00014840\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 079/100 | Train Loss: 0.00018914\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 080/100 | Train Loss: 0.00018336\n","Iter: 081/100 | Train Loss: 0.00018455\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 082/100 | Train Loss: 0.00011918\n","Iter: 083/100 | Train Loss: 0.00013419\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 084/100 | Train Loss: 0.00009089\n","Iter: 085/100 | Train Loss: 0.00012513\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 086/100 | Train Loss: 0.00010026\n","Iter: 087/100 | Train Loss: 0.00008754\n","Iter: 088/100 | Train Loss: 0.00007647\n","Iter: 089/100 | Train Loss: 0.00006058\n","Iter: 090/100 | Train Loss: 0.00006305\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 091/100 | Train Loss: 0.00004455\n","Iter: 092/100 | Train Loss: 0.00004740\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 093/100 | Train Loss: 0.00004220\n","Iter: 094/100 | Train Loss: 0.00003756\n","Iter: 095/100 | Train Loss: 0.00003006\n","Iter: 096/100 | Train Loss: 0.00003007\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 097/100 | Train Loss: 0.00002825\n","Iter: 098/100 | Train Loss: 0.00002409\n","Iter: 099/100 | Train Loss: 0.00002090\n","\n","Iter: 099/100 | Test Loss: 0.00083617 | Test acc: 64.5100\n","scale:1.250000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00160607\n","Iter: 001/100 | Train Loss: 0.00296825\n","Adjusting Layer 1, Kernel Nodes: 520, Adptive Nodes:280\n","Iter: 002/100 | Train Loss: 0.01313308\n","Adjusting Layer 1, Kernel Nodes: 404, Adptive Nodes:396\n","Iter: 003/100 | Train Loss: 0.03244377\n","Adjusting Layer 1, Kernel Nodes: 583, Adptive Nodes:217\n","Iter: 004/100 | Train Loss: 0.05685818\n","Adjusting Layer 1, Kernel Nodes: 333, Adptive Nodes:467\n","Iter: 005/100 | Train Loss: 0.00605356\n","Iter: 006/100 | Train Loss: 0.00573990\n","Iter: 007/100 | Train Loss: 0.00203449\n","Iter: 008/100 | Train Loss: 0.00130652\n","Iter: 009/100 | Train Loss: 0.00119645\n","Iter: 010/100 | Train Loss: 0.00113134\n","Iter: 011/100 | Train Loss: 0.00107415\n","Iter: 012/100 | Train Loss: 0.00101918\n","Iter: 013/100 | Train Loss: 0.00095291\n","Iter: 014/100 | Train Loss: 0.00090923\n","Iter: 015/100 | Train Loss: 0.00086540\n","Iter: 016/100 | Train Loss: 0.00078756\n","Iter: 017/100 | Train Loss: 0.00071874\n","Iter: 018/100 | Train Loss: 0.00067958\n","Iter: 019/100 | Train Loss: 0.00064467\n","Iter: 020/100 | Train Loss: 0.00059726\n","Iter: 021/100 | Train Loss: 0.00055307\n","Iter: 022/100 | Train Loss: 0.00051499\n","Iter: 023/100 | Train Loss: 0.00047389\n","Iter: 024/100 | Train Loss: 0.00043472\n","Iter: 025/100 | Train Loss: 0.00040497\n","Iter: 026/100 | Train Loss: 0.00037679\n","Iter: 027/100 | Train Loss: 0.00034621\n","Iter: 028/100 | Train Loss: 0.00032110\n","Iter: 029/100 | Train Loss: 0.00030021\n","Iter: 030/100 | Train Loss: 0.00027567\n","Iter: 031/100 | Train Loss: 0.00025259\n","Iter: 032/100 | Train Loss: 0.00023408\n","Iter: 033/100 | Train Loss: 0.00021398\n","Iter: 034/100 | Train Loss: 0.00019373\n","Iter: 035/100 | Train Loss: 0.00017854\n","Iter: 036/100 | Train Loss: 0.00016412\n","Iter: 037/100 | Train Loss: 0.00014928\n","Iter: 038/100 | Train Loss: 0.00013625\n","Iter: 039/100 | Train Loss: 0.00012369\n","Iter: 040/100 | Train Loss: 0.00011140\n","Iter: 041/100 | Train Loss: 0.00010210\n","Iter: 042/100 | Train Loss: 0.00009354\n","Iter: 043/100 | Train Loss: 0.00008359\n","Iter: 044/100 | Train Loss: 0.00007553\n","Iter: 045/100 | Train Loss: 0.00006947\n","Iter: 046/100 | Train Loss: 0.00006361\n","Iter: 047/100 | Train Loss: 0.00005788\n","Iter: 048/100 | Train Loss: 0.00005270\n","Iter: 049/100 | Train Loss: 0.00004828\n","Iter: 050/100 | Train Loss: 0.00004489\n","Iter: 051/100 | Train Loss: 0.00004172\n","Iter: 052/100 | Train Loss: 0.00003814\n","Iter: 053/100 | Train Loss: 0.00003502\n","Iter: 054/100 | Train Loss: 0.00003273\n","Iter: 055/100 | Train Loss: 0.00003076\n","Iter: 056/100 | Train Loss: 0.00002867\n","Iter: 057/100 | Train Loss: 0.00002624\n","Iter: 058/100 | Train Loss: 0.00002408\n","Iter: 059/100 | Train Loss: 0.00002255\n","Iter: 060/100 | Train Loss: 0.00002110\n","Iter: 061/100 | Train Loss: 0.00001945\n","Iter: 062/100 | Train Loss: 0.00001801\n","Iter: 063/100 | Train Loss: 0.00001692\n","Iter: 064/100 | Train Loss: 0.00001592\n","Iter: 065/100 | Train Loss: 0.00001491\n","Iter: 066/100 | Train Loss: 0.00001389\n","Iter: 067/100 | Train Loss: 0.00001287\n","Iter: 068/100 | Train Loss: 0.00001206\n","Iter: 069/100 | Train Loss: 0.00001132\n","Iter: 070/100 | Train Loss: 0.00001054\n","Iter: 071/100 | Train Loss: 0.00000983\n","Iter: 072/100 | Train Loss: 0.00000927\n","Iter: 073/100 | Train Loss: 0.00000877\n","Iter: 074/100 | Train Loss: 0.00000825\n","Iter: 075/100 | Train Loss: 0.00000776\n","Iter: 076/100 | Train Loss: 0.00000734\n","Iter: 077/100 | Train Loss: 0.00000697\n","Iter: 078/100 | Train Loss: 0.00000662\n","Iter: 079/100 | Train Loss: 0.00000627\n","Iter: 080/100 | Train Loss: 0.00000596\n","Iter: 081/100 | Train Loss: 0.00000567\n","Iter: 082/100 | Train Loss: 0.00000541\n","Iter: 083/100 | Train Loss: 0.00000517\n","Iter: 084/100 | Train Loss: 0.00000492\n","Iter: 085/100 | Train Loss: 0.00000470\n","Iter: 086/100 | Train Loss: 0.00000450\n","Iter: 087/100 | Train Loss: 0.00000430\n","Iter: 088/100 | Train Loss: 0.00000411\n","Iter: 089/100 | Train Loss: 0.00000393\n","Iter: 090/100 | Train Loss: 0.00000377\n","Iter: 091/100 | Train Loss: 0.00000361\n","Iter: 092/100 | Train Loss: 0.00000346\n","Iter: 093/100 | Train Loss: 0.00000331\n","Iter: 094/100 | Train Loss: 0.00000318\n","Iter: 095/100 | Train Loss: 0.00000305\n","Iter: 096/100 | Train Loss: 0.00000294\n","Iter: 097/100 | Train Loss: 0.00000283\n","Iter: 098/100 | Train Loss: 0.00000272\n","Iter: 099/100 | Train Loss: 0.00000262\n","\n","Iter: 099/100 | Test Loss: 0.00089441 | Test acc: 64.7800\n","scale:1.250000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00149732\n","Iter: 001/100 | Train Loss: 0.00205796\n","Adjusting Layer 1, Kernel Nodes: 545, Adptive Nodes:255\n","Iter: 002/100 | Train Loss: 0.00556318\n","Adjusting Layer 1, Kernel Nodes: 466, Adptive Nodes:334\n","Iter: 003/100 | Train Loss: 0.01451533\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 004/100 | Train Loss: 0.11782005\n","Adjusting Layer 1, Kernel Nodes: 249, Adptive Nodes:551\n","Iter: 005/100 | Train Loss: 0.05161486\n","Iter: 006/100 | Train Loss: 0.00145904\n","Iter: 007/100 | Train Loss: 0.00135467\n","Iter: 008/100 | Train Loss: 0.00134008\n","Iter: 009/100 | Train Loss: 0.00129549\n","Iter: 010/100 | Train Loss: 0.00123990\n","Iter: 011/100 | Train Loss: 0.00119171\n","Iter: 012/100 | Train Loss: 0.00113598\n","Iter: 013/100 | Train Loss: 0.00105821\n","Iter: 014/100 | Train Loss: 0.00099117\n","Iter: 015/100 | Train Loss: 0.00093538\n","Iter: 016/100 | Train Loss: 0.00088515\n","Iter: 017/100 | Train Loss: 0.00083145\n","Iter: 018/100 | Train Loss: 0.00077412\n","Iter: 019/100 | Train Loss: 0.00072556\n","Iter: 020/100 | Train Loss: 0.00067925\n","Iter: 021/100 | Train Loss: 0.00063462\n","Iter: 022/100 | Train Loss: 0.00059223\n","Iter: 023/100 | Train Loss: 0.00055446\n","Iter: 024/100 | Train Loss: 0.00052435\n","Iter: 025/100 | Train Loss: 0.00049738\n","Iter: 026/100 | Train Loss: 0.00046453\n","Iter: 027/100 | Train Loss: 0.00043497\n","Iter: 028/100 | Train Loss: 0.00041295\n","Iter: 029/100 | Train Loss: 0.00038961\n","Iter: 030/100 | Train Loss: 0.00035775\n","Iter: 031/100 | Train Loss: 0.00032645\n","Iter: 032/100 | Train Loss: 0.00030514\n","Iter: 033/100 | Train Loss: 0.00028698\n","Iter: 034/100 | Train Loss: 0.00026121\n","Iter: 035/100 | Train Loss: 0.00023636\n","Iter: 036/100 | Train Loss: 0.00021840\n","Iter: 037/100 | Train Loss: 0.00020047\n","Iter: 038/100 | Train Loss: 0.00018102\n","Iter: 039/100 | Train Loss: 0.00016459\n","Iter: 040/100 | Train Loss: 0.00015136\n","Iter: 041/100 | Train Loss: 0.00013924\n","Iter: 042/100 | Train Loss: 0.00012840\n","Iter: 043/100 | Train Loss: 0.00011872\n","Iter: 044/100 | Train Loss: 0.00010879\n","Iter: 045/100 | Train Loss: 0.00009972\n","Iter: 046/100 | Train Loss: 0.00009257\n","Iter: 047/100 | Train Loss: 0.00008542\n","Iter: 048/100 | Train Loss: 0.00007838\n","Iter: 049/100 | Train Loss: 0.00007297\n","Iter: 050/100 | Train Loss: 0.00006806\n","Iter: 051/100 | Train Loss: 0.00006303\n","Iter: 052/100 | Train Loss: 0.00005867\n","Iter: 053/100 | Train Loss: 0.00005498\n","Iter: 054/100 | Train Loss: 0.00005095\n","Iter: 055/100 | Train Loss: 0.00004734\n","Iter: 056/100 | Train Loss: 0.00004447\n","Iter: 057/100 | Train Loss: 0.00004167\n","Iter: 058/100 | Train Loss: 0.00003864\n","Iter: 059/100 | Train Loss: 0.00003588\n","Iter: 060/100 | Train Loss: 0.00003344\n","Iter: 061/100 | Train Loss: 0.00003124\n","Iter: 062/100 | Train Loss: 0.00002913\n","Iter: 063/100 | Train Loss: 0.00002696\n","Iter: 064/100 | Train Loss: 0.00002513\n","Iter: 065/100 | Train Loss: 0.00002364\n","Iter: 066/100 | Train Loss: 0.00002210\n","Iter: 067/100 | Train Loss: 0.00002052\n","Iter: 068/100 | Train Loss: 0.00001922\n","Iter: 069/100 | Train Loss: 0.00001815\n","Iter: 070/100 | Train Loss: 0.00001707\n","Iter: 071/100 | Train Loss: 0.00001599\n","Iter: 072/100 | Train Loss: 0.00001503\n","Iter: 073/100 | Train Loss: 0.00001418\n","Iter: 074/100 | Train Loss: 0.00001337\n","Iter: 075/100 | Train Loss: 0.00001259\n","Iter: 076/100 | Train Loss: 0.00001183\n","Iter: 077/100 | Train Loss: 0.00001112\n","Iter: 078/100 | Train Loss: 0.00001053\n","Iter: 079/100 | Train Loss: 0.00000995\n","Iter: 080/100 | Train Loss: 0.00000938\n","Iter: 081/100 | Train Loss: 0.00000890\n","Iter: 082/100 | Train Loss: 0.00000846\n","Iter: 083/100 | Train Loss: 0.00000800\n","Iter: 084/100 | Train Loss: 0.00000759\n","Iter: 085/100 | Train Loss: 0.00000721\n","Iter: 086/100 | Train Loss: 0.00000684\n","Iter: 087/100 | Train Loss: 0.00000649\n","Iter: 088/100 | Train Loss: 0.00000618\n","Iter: 089/100 | Train Loss: 0.00000589\n","Iter: 090/100 | Train Loss: 0.00000561\n","Iter: 091/100 | Train Loss: 0.00000536\n","Iter: 092/100 | Train Loss: 0.00000513\n","Iter: 093/100 | Train Loss: 0.00000490\n","Iter: 094/100 | Train Loss: 0.00000470\n","Iter: 095/100 | Train Loss: 0.00000450\n","Iter: 096/100 | Train Loss: 0.00000431\n","Iter: 097/100 | Train Loss: 0.00000414\n","Iter: 098/100 | Train Loss: 0.00000398\n","Iter: 099/100 | Train Loss: 0.00000382\n","\n","Iter: 099/100 | Test Loss: 0.00092511 | Test acc: 63.7000\n","scale:1.250000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00142944\n","Iter: 001/100 | Train Loss: 0.00166049\n","Adjusting Layer 1, Kernel Nodes: 569, Adptive Nodes:231\n","Iter: 002/100 | Train Loss: 0.00269826\n","Adjusting Layer 1, Kernel Nodes: 576, Adptive Nodes:224\n","Iter: 003/100 | Train Loss: 0.01015708\n","Adjusting Layer 1, Kernel Nodes: 739, Adptive Nodes:61\n","Iter: 004/100 | Train Loss: 0.14084624\n","Adjusting Layer 1, Kernel Nodes: 117, Adptive Nodes:683\n","Iter: 005/100 | Train Loss: 0.00588830\n","Iter: 006/100 | Train Loss: 0.00179929\n","Iter: 007/100 | Train Loss: 0.00130035\n","Iter: 008/100 | Train Loss: 0.00117962\n","Iter: 009/100 | Train Loss: 0.00107751\n","Iter: 010/100 | Train Loss: 0.00098037\n","Iter: 011/100 | Train Loss: 0.00092856\n","Iter: 012/100 | Train Loss: 0.00090355\n","Iter: 013/100 | Train Loss: 0.00082982\n","Iter: 014/100 | Train Loss: 0.00074898\n","Iter: 015/100 | Train Loss: 0.00069460\n","Iter: 016/100 | Train Loss: 0.00065808\n","Iter: 017/100 | Train Loss: 0.00060343\n","Iter: 018/100 | Train Loss: 0.00054967\n","Iter: 019/100 | Train Loss: 0.00052193\n","Iter: 020/100 | Train Loss: 0.00048358\n","Iter: 021/100 | Train Loss: 0.00043316\n","Iter: 022/100 | Train Loss: 0.00041249\n","Iter: 023/100 | Train Loss: 0.00038513\n","Iter: 024/100 | Train Loss: 0.00034513\n","Iter: 025/100 | Train Loss: 0.00032646\n","Iter: 026/100 | Train Loss: 0.00030265\n","Iter: 027/100 | Train Loss: 0.00028335\n","Iter: 028/100 | Train Loss: 0.00026153\n","Iter: 029/100 | Train Loss: 0.00023575\n","Iter: 030/100 | Train Loss: 0.00022120\n","Iter: 031/100 | Train Loss: 0.00019920\n","Iter: 032/100 | Train Loss: 0.00018248\n","Iter: 033/100 | Train Loss: 0.00016167\n","Iter: 034/100 | Train Loss: 0.00014840\n","Iter: 035/100 | Train Loss: 0.00013924\n","Iter: 036/100 | Train Loss: 0.00012586\n","Iter: 037/100 | Train Loss: 0.00011513\n","Iter: 038/100 | Train Loss: 0.00010315\n","Iter: 039/100 | Train Loss: 0.00009857\n","Iter: 040/100 | Train Loss: 0.00008596\n","Iter: 041/100 | Train Loss: 0.00007884\n","Iter: 042/100 | Train Loss: 0.00007362\n","Iter: 043/100 | Train Loss: 0.00006852\n","Iter: 044/100 | Train Loss: 0.00006145\n","Iter: 045/100 | Train Loss: 0.00005618\n","Iter: 046/100 | Train Loss: 0.00005183\n","Iter: 047/100 | Train Loss: 0.00004773\n","Iter: 048/100 | Train Loss: 0.00004310\n","Iter: 049/100 | Train Loss: 0.00003960\n","Iter: 050/100 | Train Loss: 0.00003763\n","Iter: 051/100 | Train Loss: 0.00003410\n","Iter: 052/100 | Train Loss: 0.00003175\n","Iter: 053/100 | Train Loss: 0.00002921\n","Iter: 054/100 | Train Loss: 0.00002709\n","Iter: 055/100 | Train Loss: 0.00002486\n","Iter: 056/100 | Train Loss: 0.00002288\n","Iter: 057/100 | Train Loss: 0.00002110\n","Iter: 058/100 | Train Loss: 0.00001969\n","Iter: 059/100 | Train Loss: 0.00001816\n","Iter: 060/100 | Train Loss: 0.00001670\n","Iter: 061/100 | Train Loss: 0.00001575\n","Iter: 062/100 | Train Loss: 0.00001438\n","Iter: 063/100 | Train Loss: 0.00001352\n","Iter: 064/100 | Train Loss: 0.00001247\n","Iter: 065/100 | Train Loss: 0.00001170\n","Iter: 066/100 | Train Loss: 0.00001089\n","Iter: 067/100 | Train Loss: 0.00001032\n","Iter: 068/100 | Train Loss: 0.00000956\n","Iter: 069/100 | Train Loss: 0.00000908\n","Iter: 070/100 | Train Loss: 0.00000847\n","Iter: 071/100 | Train Loss: 0.00000796\n","Iter: 072/100 | Train Loss: 0.00000745\n","Iter: 073/100 | Train Loss: 0.00000699\n","Iter: 074/100 | Train Loss: 0.00000660\n","Iter: 075/100 | Train Loss: 0.00000619\n","Iter: 076/100 | Train Loss: 0.00000582\n","Iter: 077/100 | Train Loss: 0.00000551\n","Iter: 078/100 | Train Loss: 0.00000522\n","Iter: 079/100 | Train Loss: 0.00000490\n","Iter: 080/100 | Train Loss: 0.00000468\n","Iter: 081/100 | Train Loss: 0.00000444\n","Iter: 082/100 | Train Loss: 0.00000421\n","Iter: 083/100 | Train Loss: 0.00000399\n","Iter: 084/100 | Train Loss: 0.00000380\n","Iter: 085/100 | Train Loss: 0.00000361\n","Iter: 086/100 | Train Loss: 0.00000343\n","Iter: 087/100 | Train Loss: 0.00000327\n","Iter: 088/100 | Train Loss: 0.00000313\n","Iter: 089/100 | Train Loss: 0.00000298\n","Iter: 090/100 | Train Loss: 0.00000284\n","Iter: 091/100 | Train Loss: 0.00000273\n","Iter: 092/100 | Train Loss: 0.00000261\n","Iter: 093/100 | Train Loss: 0.00000250\n","Iter: 094/100 | Train Loss: 0.00000239\n","Iter: 095/100 | Train Loss: 0.00000230\n","Iter: 096/100 | Train Loss: 0.00000220\n","Iter: 097/100 | Train Loss: 0.00000212\n","Iter: 098/100 | Train Loss: 0.00000203\n","Iter: 099/100 | Train Loss: 0.00000195\n","\n","Iter: 099/100 | Test Loss: 0.00098737 | Test acc: 62.8800\n","scale:1.250000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00138989\n","Iter: 001/100 | Train Loss: 0.00149824\n","Adjusting Layer 1, Kernel Nodes: 596, Adptive Nodes:204\n","Iter: 002/100 | Train Loss: 0.00170627\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 003/100 | Train Loss: 0.00358338\n","Adjusting Layer 1, Kernel Nodes: 341, Adptive Nodes:459\n","Iter: 004/100 | Train Loss: 0.00385473\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 005/100 | Train Loss: 0.00193184\n","Iter: 006/100 | Train Loss: 0.00086206\n","Iter: 007/100 | Train Loss: 0.00053817\n","Iter: 008/100 | Train Loss: 0.00074729\n","Adjusting Layer 1, Kernel Nodes: 696, Adptive Nodes:104\n","Iter: 009/100 | Train Loss: 0.00056763\n","Iter: 010/100 | Train Loss: 0.00037454\n","Iter: 011/100 | Train Loss: 0.00040203\n","Adjusting Layer 1, Kernel Nodes: 363, Adptive Nodes:437\n","Iter: 012/100 | Train Loss: 0.00042029\n","Adjusting Layer 1, Kernel Nodes: 669, Adptive Nodes:131\n","Iter: 013/100 | Train Loss: 0.00027949\n","Iter: 014/100 | Train Loss: 0.00019844\n","Iter: 015/100 | Train Loss: 0.00022667\n","Adjusting Layer 1, Kernel Nodes: 362, Adptive Nodes:438\n","Iter: 016/100 | Train Loss: 0.00020907\n","Iter: 017/100 | Train Loss: 0.00013640\n","Iter: 018/100 | Train Loss: 0.00012651\n","Iter: 019/100 | Train Loss: 0.00014277\n","Adjusting Layer 1, Kernel Nodes: 328, Adptive Nodes:472\n","Iter: 020/100 | Train Loss: 0.00010858\n","Iter: 021/100 | Train Loss: 0.00007205\n","Iter: 022/100 | Train Loss: 0.00007947\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 023/100 | Train Loss: 0.00008697\n","Adjusting Layer 1, Kernel Nodes: 385, Adptive Nodes:415\n","Iter: 024/100 | Train Loss: 0.00005591\n","Iter: 025/100 | Train Loss: 0.00005390\n","Iter: 026/100 | Train Loss: 0.00006272\n","Adjusting Layer 1, Kernel Nodes: 79, Adptive Nodes:721\n","Iter: 027/100 | Train Loss: 0.00005055\n","Iter: 028/100 | Train Loss: 0.00003499\n","Iter: 029/100 | Train Loss: 0.00003784\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 030/100 | Train Loss: 0.00003877\n","Adjusting Layer 1, Kernel Nodes: 25, Adptive Nodes:775\n","Iter: 031/100 | Train Loss: 0.00002874\n","Iter: 032/100 | Train Loss: 0.00002254\n","Iter: 033/100 | Train Loss: 0.00002429\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 034/100 | Train Loss: 0.00002435\n","Adjusting Layer 1, Kernel Nodes: 220, Adptive Nodes:580\n","Iter: 035/100 | Train Loss: 0.00001878\n","Iter: 036/100 | Train Loss: 0.00001431\n","Iter: 037/100 | Train Loss: 0.00001505\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 038/100 | Train Loss: 0.00001393\n","Iter: 039/100 | Train Loss: 0.00001044\n","Iter: 040/100 | Train Loss: 0.00001109\n","Adjusting Layer 1, Kernel Nodes: 9, Adptive Nodes:791\n","Iter: 041/100 | Train Loss: 0.00001135\n","Adjusting Layer 1, Kernel Nodes: 596, Adptive Nodes:204\n","Iter: 042/100 | Train Loss: 0.00000818\n","Iter: 043/100 | Train Loss: 0.00000793\n","Iter: 044/100 | Train Loss: 0.00000795\n","Adjusting Layer 1, Kernel Nodes: 205, Adptive Nodes:595\n","Iter: 045/100 | Train Loss: 0.00000663\n","Iter: 046/100 | Train Loss: 0.00000548\n","Iter: 047/100 | Train Loss: 0.00000523\n","Iter: 048/100 | Train Loss: 0.00000519\n","Iter: 049/100 | Train Loss: 0.00000440\n","Iter: 050/100 | Train Loss: 0.00000347\n","Iter: 051/100 | Train Loss: 0.00000324\n","Iter: 052/100 | Train Loss: 0.00000329\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 053/100 | Train Loss: 0.00000276\n","Iter: 054/100 | Train Loss: 0.00000321\n","Adjusting Layer 1, Kernel Nodes: 560, Adptive Nodes:240\n","Iter: 055/100 | Train Loss: 0.00000286\n","Iter: 056/100 | Train Loss: 0.00000202\n","Iter: 057/100 | Train Loss: 0.00000241\n","Adjusting Layer 1, Kernel Nodes: 158, Adptive Nodes:642\n","Iter: 058/100 | Train Loss: 0.00000224\n","Iter: 059/100 | Train Loss: 0.00000170\n","Iter: 060/100 | Train Loss: 0.00000140\n","Iter: 061/100 | Train Loss: 0.00000145\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 062/100 | Train Loss: 0.00000121\n","Iter: 063/100 | Train Loss: 0.00000109\n","Iter: 064/100 | Train Loss: 0.00000099\n","Iter: 065/100 | Train Loss: 0.00000087\n","Iter: 066/100 | Train Loss: 0.00000076\n","Iter: 067/100 | Train Loss: 0.00000072\n","Iter: 068/100 | Train Loss: 0.00000067\n","Iter: 069/100 | Train Loss: 0.00000058\n","Iter: 070/100 | Train Loss: 0.00000053\n","Iter: 071/100 | Train Loss: 0.00000049\n","Iter: 072/100 | Train Loss: 0.00000044\n","Iter: 073/100 | Train Loss: 0.00000042\n","Iter: 074/100 | Train Loss: 0.00000037\n","Iter: 075/100 | Train Loss: 0.00000031\n","Iter: 076/100 | Train Loss: 0.00000029\n","Iter: 077/100 | Train Loss: 0.00000026\n","Iter: 078/100 | Train Loss: 0.00000023\n","Iter: 079/100 | Train Loss: 0.00000021\n","Iter: 080/100 | Train Loss: 0.00000018\n","Iter: 081/100 | Train Loss: 0.00000016\n","Iter: 082/100 | Train Loss: 0.00000016\n","Iter: 083/100 | Train Loss: 0.00000014\n","Iter: 084/100 | Train Loss: 0.00000013\n","Iter: 085/100 | Train Loss: 0.00000012\n","Iter: 086/100 | Train Loss: 0.00000010\n","Iter: 087/100 | Train Loss: 0.00000009\n","Iter: 088/100 | Train Loss: 0.00000008\n","Iter: 089/100 | Train Loss: 0.00000007\n","Iter: 090/100 | Train Loss: 0.00000007\n","Iter: 091/100 | Train Loss: 0.00000006\n","Iter: 092/100 | Train Loss: 0.00000005\n","Iter: 093/100 | Train Loss: 0.00000005\n","Iter: 094/100 | Train Loss: 0.00000004\n","Iter: 095/100 | Train Loss: 0.00000004\n","Iter: 096/100 | Train Loss: 0.00000004\n","Iter: 097/100 | Train Loss: 0.00000003\n","Iter: 098/100 | Train Loss: 0.00000003\n","Iter: 099/100 | Train Loss: 0.00000002\n","\n","Iter: 099/100 | Test Loss: 0.00125929 | Test acc: 58.8500\n","scale:1.250000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00136613\n","Iter: 001/100 | Train Loss: 0.00142660\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 002/100 | Train Loss: 0.00175292\n","Adjusting Layer 1, Kernel Nodes: 472, Adptive Nodes:328\n","Iter: 003/100 | Train Loss: 0.00227536\n","Adjusting Layer 1, Kernel Nodes: 437, Adptive Nodes:363\n","Iter: 004/100 | Train Loss: 0.00221550\n","Iter: 005/100 | Train Loss: 0.00122345\n","Iter: 006/100 | Train Loss: 0.00086145\n","Iter: 007/100 | Train Loss: 0.00063061\n","Iter: 008/100 | Train Loss: 0.00044927\n","Iter: 009/100 | Train Loss: 0.00052826\n","Adjusting Layer 1, Kernel Nodes: 784, Adptive Nodes:16\n","Iter: 010/100 | Train Loss: 0.00042046\n","Iter: 011/100 | Train Loss: 0.00030356\n","Iter: 012/100 | Train Loss: 0.00029942\n","Iter: 013/100 | Train Loss: 0.00031234\n","Adjusting Layer 1, Kernel Nodes: 574, Adptive Nodes:226\n","Iter: 014/100 | Train Loss: 0.00024506\n","Iter: 015/100 | Train Loss: 0.00017709\n","Iter: 016/100 | Train Loss: 0.00015749\n","Iter: 017/100 | Train Loss: 0.00016097\n","Adjusting Layer 1, Kernel Nodes: 516, Adptive Nodes:284\n","Iter: 018/100 | Train Loss: 0.00013667\n","Iter: 019/100 | Train Loss: 0.00009428\n","Iter: 020/100 | Train Loss: 0.00008337\n","Iter: 021/100 | Train Loss: 0.00009142\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 022/100 | Train Loss: 0.00007735\n","Iter: 023/100 | Train Loss: 0.00005807\n","Iter: 024/100 | Train Loss: 0.00005893\n","Adjusting Layer 1, Kernel Nodes: 770, Adptive Nodes:30\n","Iter: 025/100 | Train Loss: 0.00005801\n","Iter: 026/100 | Train Loss: 0.00004112\n","Iter: 027/100 | Train Loss: 0.00003916\n","Iter: 028/100 | Train Loss: 0.00004222\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 029/100 | Train Loss: 0.00003071\n","Iter: 030/100 | Train Loss: 0.00003103\n","Adjusting Layer 1, Kernel Nodes: 12, Adptive Nodes:788\n","Iter: 031/100 | Train Loss: 0.00002738\n","Iter: 032/100 | Train Loss: 0.00001872\n","Iter: 033/100 | Train Loss: 0.00001982\n","Adjusting Layer 1, Kernel Nodes: 569, Adptive Nodes:231\n","Iter: 034/100 | Train Loss: 0.00002036\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 035/100 | Train Loss: 0.00001699\n","Iter: 036/100 | Train Loss: 0.00001767\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 037/100 | Train Loss: 0.00001292\n","Iter: 038/100 | Train Loss: 0.00001328\n","Adjusting Layer 1, Kernel Nodes: 165, Adptive Nodes:635\n","Iter: 039/100 | Train Loss: 0.00001371\n","Adjusting Layer 1, Kernel Nodes: 644, Adptive Nodes:156\n","Iter: 040/100 | Train Loss: 0.00000992\n","Iter: 041/100 | Train Loss: 0.00001010\n","Adjusting Layer 1, Kernel Nodes: 147, Adptive Nodes:653\n","Iter: 042/100 | Train Loss: 0.00001026\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 043/100 | Train Loss: 0.00000735\n","Iter: 044/100 | Train Loss: 0.00000715\n","Iter: 045/100 | Train Loss: 0.00000659\n","Iter: 046/100 | Train Loss: 0.00000564\n","Iter: 047/100 | Train Loss: 0.00000509\n","Iter: 048/100 | Train Loss: 0.00000460\n","Iter: 049/100 | Train Loss: 0.00000429\n","Iter: 050/100 | Train Loss: 0.00000368\n","Iter: 051/100 | Train Loss: 0.00000324\n","Iter: 052/100 | Train Loss: 0.00000325\n","Adjusting Layer 1, Kernel Nodes: 144, Adptive Nodes:656\n","Iter: 053/100 | Train Loss: 0.00000291\n","Iter: 054/100 | Train Loss: 0.00000204\n","Iter: 055/100 | Train Loss: 0.00000279\n","Adjusting Layer 1, Kernel Nodes: 656, Adptive Nodes:144\n","Iter: 056/100 | Train Loss: 0.00000233\n","Iter: 057/100 | Train Loss: 0.00000165\n","Iter: 058/100 | Train Loss: 0.00000221\n","Adjusting Layer 1, Kernel Nodes: 156, Adptive Nodes:644\n","Iter: 059/100 | Train Loss: 0.00000194\n","Iter: 060/100 | Train Loss: 0.00000130\n","Iter: 061/100 | Train Loss: 0.00000154\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 062/100 | Train Loss: 0.00000151\n","Iter: 063/100 | Train Loss: 0.00000139\n","Iter: 064/100 | Train Loss: 0.00000182\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 065/100 | Train Loss: 0.00000109\n","Iter: 066/100 | Train Loss: 0.00000088\n","Iter: 067/100 | Train Loss: 0.00000080\n","Iter: 068/100 | Train Loss: 0.00000078\n","Iter: 069/100 | Train Loss: 0.00000069\n","Iter: 070/100 | Train Loss: 0.00000068\n","Iter: 071/100 | Train Loss: 0.00000059\n","Iter: 072/100 | Train Loss: 0.00000052\n","Iter: 073/100 | Train Loss: 0.00000053\n","Adjusting Layer 1, Kernel Nodes: 5, Adptive Nodes:795\n","Iter: 074/100 | Train Loss: 0.00000049\n","Iter: 075/100 | Train Loss: 0.00000048\n","Iter: 076/100 | Train Loss: 0.00000065\n","Adjusting Layer 1, Kernel Nodes: 279, Adptive Nodes:521\n","Iter: 077/100 | Train Loss: 0.00000035\n","Iter: 078/100 | Train Loss: 0.00000028\n","Iter: 079/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 080/100 | Train Loss: 0.00000028\n","Iter: 081/100 | Train Loss: 0.00000024\n","Iter: 082/100 | Train Loss: 0.00000023\n","Iter: 083/100 | Train Loss: 0.00000020\n","Iter: 084/100 | Train Loss: 0.00000018\n","Iter: 085/100 | Train Loss: 0.00000018\n","Iter: 086/100 | Train Loss: 0.00000017\n","Iter: 087/100 | Train Loss: 0.00000014\n","Iter: 088/100 | Train Loss: 0.00000013\n","Iter: 089/100 | Train Loss: 0.00000012\n","Iter: 090/100 | Train Loss: 0.00000010\n","Iter: 091/100 | Train Loss: 0.00000010\n","Iter: 092/100 | Train Loss: 0.00000010\n","Adjusting Layer 1, Kernel Nodes: 308, Adptive Nodes:492\n","Iter: 093/100 | Train Loss: 0.00000012\n","Adjusting Layer 1, Kernel Nodes: 492, Adptive Nodes:308\n","Iter: 094/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 308, Adptive Nodes:492\n","Iter: 095/100 | Train Loss: 0.00000011\n","Iter: 096/100 | Train Loss: 0.00000010\n","Iter: 097/100 | Train Loss: 0.00000008\n","Iter: 098/100 | Train Loss: 0.00000007\n","Iter: 099/100 | Train Loss: 0.00000007\n","\n","Iter: 099/100 | Test Loss: 0.00111543 | Test acc: 61.7900\n","scale:1.250000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00134941\n","Iter: 001/100 | Train Loss: 0.00139333\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 002/100 | Train Loss: 0.00114484\n","Iter: 003/100 | Train Loss: 0.00079316\n","Iter: 004/100 | Train Loss: 0.00081084\n","Adjusting Layer 1, Kernel Nodes: 577, Adptive Nodes:223\n","Iter: 005/100 | Train Loss: 0.00067602\n","Iter: 006/100 | Train Loss: 0.00051888\n","Iter: 007/100 | Train Loss: 0.00051172\n","Iter: 008/100 | Train Loss: 0.00036358\n","Iter: 009/100 | Train Loss: 0.00038371\n","Adjusting Layer 1, Kernel Nodes: 726, Adptive Nodes:74\n","Iter: 010/100 | Train Loss: 0.00030178\n","Iter: 011/100 | Train Loss: 0.00028373\n","Iter: 012/100 | Train Loss: 0.00021138\n","Iter: 013/100 | Train Loss: 0.00019682\n","Iter: 014/100 | Train Loss: 0.00015057\n","Iter: 015/100 | Train Loss: 0.00015049\n","Iter: 016/100 | Train Loss: 0.00012818\n","Iter: 017/100 | Train Loss: 0.00011233\n","Iter: 018/100 | Train Loss: 0.00011182\n","Iter: 019/100 | Train Loss: 0.00007703\n","Iter: 020/100 | Train Loss: 0.00008495\n","Adjusting Layer 1, Kernel Nodes: 745, Adptive Nodes:55\n","Iter: 021/100 | Train Loss: 0.00005757\n","Iter: 022/100 | Train Loss: 0.00005995\n","Adjusting Layer 1, Kernel Nodes: 789, Adptive Nodes:11\n","Iter: 023/100 | Train Loss: 0.00004795\n","Iter: 024/100 | Train Loss: 0.00004515\n","Iter: 025/100 | Train Loss: 0.00004005\n","Iter: 026/100 | Train Loss: 0.00003568\n","Iter: 027/100 | Train Loss: 0.00003195\n","Iter: 028/100 | Train Loss: 0.00002751\n","Iter: 029/100 | Train Loss: 0.00002424\n","Iter: 030/100 | Train Loss: 0.00002119\n","Iter: 031/100 | Train Loss: 0.00001991\n","Iter: 032/100 | Train Loss: 0.00001799\n","Iter: 033/100 | Train Loss: 0.00001785\n","Iter: 034/100 | Train Loss: 0.00001516\n","Iter: 035/100 | Train Loss: 0.00001464\n","Iter: 036/100 | Train Loss: 0.00001151\n","Iter: 037/100 | Train Loss: 0.00001100\n","Iter: 038/100 | Train Loss: 0.00000912\n","Iter: 039/100 | Train Loss: 0.00000843\n","Iter: 040/100 | Train Loss: 0.00000781\n","Iter: 041/100 | Train Loss: 0.00000616\n","Iter: 042/100 | Train Loss: 0.00000634\n","Adjusting Layer 1, Kernel Nodes: 465, Adptive Nodes:335\n","Iter: 043/100 | Train Loss: 0.00000500\n","Iter: 044/100 | Train Loss: 0.00000495\n","Iter: 045/100 | Train Loss: 0.00000449\n","Iter: 046/100 | Train Loss: 0.00000455\n","Adjusting Layer 1, Kernel Nodes: 704, Adptive Nodes:96\n","Iter: 047/100 | Train Loss: 0.00000823\n","Adjusting Layer 1, Kernel Nodes: 445, Adptive Nodes:355\n","Iter: 048/100 | Train Loss: 0.00000736\n","Iter: 049/100 | Train Loss: 0.00000609\n","Iter: 050/100 | Train Loss: 0.00000674\n","Adjusting Layer 1, Kernel Nodes: 95, Adptive Nodes:705\n","Iter: 051/100 | Train Loss: 0.00000815\n","Adjusting Layer 1, Kernel Nodes: 431, Adptive Nodes:369\n","Iter: 052/100 | Train Loss: 0.00000941\n","Adjusting Layer 1, Kernel Nodes: 405, Adptive Nodes:395\n","Iter: 053/100 | Train Loss: 0.00000727\n","Iter: 054/100 | Train Loss: 0.00000421\n","Iter: 055/100 | Train Loss: 0.00000850\n","Adjusting Layer 1, Kernel Nodes: 377, Adptive Nodes:423\n","Iter: 056/100 | Train Loss: 0.00001260\n","Adjusting Layer 1, Kernel Nodes: 379, Adptive Nodes:421\n","Iter: 057/100 | Train Loss: 0.00000990\n","Iter: 058/100 | Train Loss: 0.00002148\n","Adjusting Layer 1, Kernel Nodes: 409, Adptive Nodes:391\n","Iter: 059/100 | Train Loss: 0.00004702\n","Adjusting Layer 1, Kernel Nodes: 384, Adptive Nodes:416\n","Iter: 060/100 | Train Loss: 0.00003790\n","Iter: 061/100 | Train Loss: 0.00002580\n","Iter: 062/100 | Train Loss: 0.00002293\n","Iter: 063/100 | Train Loss: 0.00000714\n","Iter: 064/100 | Train Loss: 0.00001883\n","Adjusting Layer 1, Kernel Nodes: 414, Adptive Nodes:386\n","Iter: 065/100 | Train Loss: 0.00001551\n","Iter: 066/100 | Train Loss: 0.00001039\n","Iter: 067/100 | Train Loss: 0.00000932\n","Iter: 068/100 | Train Loss: 0.00001176\n","Adjusting Layer 1, Kernel Nodes: 393, Adptive Nodes:407\n","Iter: 069/100 | Train Loss: 0.00001003\n","Iter: 070/100 | Train Loss: 0.00000448\n","Iter: 071/100 | Train Loss: 0.00001093\n","Adjusting Layer 1, Kernel Nodes: 410, Adptive Nodes:390\n","Iter: 072/100 | Train Loss: 0.00000464\n","Iter: 073/100 | Train Loss: 0.00000477\n","Adjusting Layer 1, Kernel Nodes: 392, Adptive Nodes:408\n","Iter: 074/100 | Train Loss: 0.00000499\n","Adjusting Layer 1, Kernel Nodes: 410, Adptive Nodes:390\n","Iter: 075/100 | Train Loss: 0.00000300\n","Iter: 076/100 | Train Loss: 0.00000372\n","Adjusting Layer 1, Kernel Nodes: 391, Adptive Nodes:409\n","Iter: 077/100 | Train Loss: 0.00000370\n","Iter: 078/100 | Train Loss: 0.00000225\n","Iter: 079/100 | Train Loss: 0.00000323\n","Adjusting Layer 1, Kernel Nodes: 410, Adptive Nodes:390\n","Iter: 080/100 | Train Loss: 0.00000217\n","Iter: 081/100 | Train Loss: 0.00000210\n","Iter: 082/100 | Train Loss: 0.00000195\n","Iter: 083/100 | Train Loss: 0.00000162\n","Iter: 084/100 | Train Loss: 0.00000151\n","Iter: 085/100 | Train Loss: 0.00000137\n","Iter: 086/100 | Train Loss: 0.00000118\n","Iter: 087/100 | Train Loss: 0.00000111\n","Iter: 088/100 | Train Loss: 0.00000101\n","Iter: 089/100 | Train Loss: 0.00000085\n","Iter: 090/100 | Train Loss: 0.00000085\n","Adjusting Layer 1, Kernel Nodes: 390, Adptive Nodes:410\n","Iter: 091/100 | Train Loss: 0.00000100\n","Adjusting Layer 1, Kernel Nodes: 410, Adptive Nodes:390\n","Iter: 092/100 | Train Loss: 0.00000078\n","Iter: 093/100 | Train Loss: 0.00000085\n","Adjusting Layer 1, Kernel Nodes: 390, Adptive Nodes:410\n","Iter: 094/100 | Train Loss: 0.00000162\n","Adjusting Layer 1, Kernel Nodes: 411, Adptive Nodes:389\n","Iter: 095/100 | Train Loss: 0.00000024\n","Iter: 096/100 | Train Loss: 0.00000200\n","Adjusting Layer 1, Kernel Nodes: 389, Adptive Nodes:411\n","Iter: 097/100 | Train Loss: 0.00000098\n","Iter: 098/100 | Train Loss: 0.00000139\n","Adjusting Layer 1, Kernel Nodes: 411, Adptive Nodes:389\n","Iter: 099/100 | Train Loss: 0.00000235\n","\n","Iter: 099/100 | Test Loss: 0.00107081 | Test acc: 62.6000\n","scale:1.250000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00133809\n","Iter: 001/100 | Train Loss: 0.00138299\n","Adjusting Layer 1, Kernel Nodes: 637, Adptive Nodes:163\n","Iter: 002/100 | Train Loss: 0.00104402\n","Iter: 003/100 | Train Loss: 0.00077464\n","Iter: 004/100 | Train Loss: 0.00082321\n","Adjusting Layer 1, Kernel Nodes: 624, Adptive Nodes:176\n","Iter: 005/100 | Train Loss: 0.00059409\n","Iter: 006/100 | Train Loss: 0.00058052\n","Iter: 007/100 | Train Loss: 0.00043590\n","Iter: 008/100 | Train Loss: 0.00043878\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 009/100 | Train Loss: 0.00035114\n","Iter: 010/100 | Train Loss: 0.00032904\n","Iter: 011/100 | Train Loss: 0.00027133\n","Iter: 012/100 | Train Loss: 0.00025221\n","Iter: 013/100 | Train Loss: 0.00018703\n","Iter: 014/100 | Train Loss: 0.00018998\n","Adjusting Layer 1, Kernel Nodes: 687, Adptive Nodes:113\n","Iter: 015/100 | Train Loss: 0.00013909\n","Iter: 016/100 | Train Loss: 0.00014658\n","Adjusting Layer 1, Kernel Nodes: 747, Adptive Nodes:53\n","Iter: 017/100 | Train Loss: 0.00011378\n","Iter: 018/100 | Train Loss: 0.00011448\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 019/100 | Train Loss: 0.00008775\n","Iter: 020/100 | Train Loss: 0.00008829\n","Adjusting Layer 1, Kernel Nodes: 612, Adptive Nodes:188\n","Iter: 021/100 | Train Loss: 0.00006145\n","Iter: 022/100 | Train Loss: 0.00006311\n","Adjusting Layer 1, Kernel Nodes: 715, Adptive Nodes:85\n","Iter: 023/100 | Train Loss: 0.00005094\n","Iter: 024/100 | Train Loss: 0.00004631\n","Iter: 025/100 | Train Loss: 0.00004289\n","Iter: 026/100 | Train Loss: 0.00003869\n","Iter: 027/100 | Train Loss: 0.00003206\n","Iter: 028/100 | Train Loss: 0.00002985\n","Iter: 029/100 | Train Loss: 0.00002424\n","Iter: 030/100 | Train Loss: 0.00002276\n","Iter: 031/100 | Train Loss: 0.00002112\n","Iter: 032/100 | Train Loss: 0.00001868\n","Iter: 033/100 | Train Loss: 0.00001858\n","Iter: 034/100 | Train Loss: 0.00001515\n","Iter: 035/100 | Train Loss: 0.00001473\n","Iter: 036/100 | Train Loss: 0.00001201\n","Iter: 037/100 | Train Loss: 0.00001125\n","Iter: 038/100 | Train Loss: 0.00000982\n","Iter: 039/100 | Train Loss: 0.00000886\n","Iter: 040/100 | Train Loss: 0.00000778\n","Iter: 041/100 | Train Loss: 0.00000675\n","Iter: 042/100 | Train Loss: 0.00000568\n","Iter: 043/100 | Train Loss: 0.00000524\n","Iter: 044/100 | Train Loss: 0.00000458\n","Iter: 045/100 | Train Loss: 0.00000471\n","Adjusting Layer 1, Kernel Nodes: 794, Adptive Nodes:6\n","Iter: 046/100 | Train Loss: 0.00000450\n","Iter: 047/100 | Train Loss: 0.00000458\n","Adjusting Layer 1, Kernel Nodes: 738, Adptive Nodes:62\n","Iter: 048/100 | Train Loss: 0.00000361\n","Iter: 049/100 | Train Loss: 0.00000308\n","Iter: 050/100 | Train Loss: 0.00000284\n","Iter: 051/100 | Train Loss: 0.00000260\n","Iter: 052/100 | Train Loss: 0.00000263\n","Adjusting Layer 1, Kernel Nodes: 550, Adptive Nodes:250\n","Iter: 053/100 | Train Loss: 0.00000278\n","Adjusting Layer 1, Kernel Nodes: 304, Adptive Nodes:496\n","Iter: 054/100 | Train Loss: 0.00000229\n","Iter: 055/100 | Train Loss: 0.00000258\n","Adjusting Layer 1, Kernel Nodes: 195, Adptive Nodes:605\n","Iter: 056/100 | Train Loss: 0.00000623\n","Adjusting Layer 1, Kernel Nodes: 722, Adptive Nodes:78\n","Iter: 057/100 | Train Loss: 0.00002049\n","Adjusting Layer 1, Kernel Nodes: 33, Adptive Nodes:767\n","Iter: 058/100 | Train Loss: 0.00005016\n","Adjusting Layer 1, Kernel Nodes: 203, Adptive Nodes:597\n","Iter: 059/100 | Train Loss: 0.00005054\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 060/100 | Train Loss: 0.00009969\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 061/100 | Train Loss: 0.00063460\n","Adjusting Layer 1, Kernel Nodes: 3, Adptive Nodes:797\n","Iter: 062/100 | Train Loss: 0.00178268\n","Adjusting Layer 1, Kernel Nodes: 2, Adptive Nodes:798\n","Iter: 063/100 | Train Loss: 0.00087043\n","Iter: 064/100 | Train Loss: 0.00022709\n","Iter: 065/100 | Train Loss: 0.00055906\n","Adjusting Layer 1, Kernel Nodes: 66, Adptive Nodes:734\n","Iter: 066/100 | Train Loss: 0.00026779\n","Iter: 067/100 | Train Loss: 0.00012633\n","Iter: 068/100 | Train Loss: 0.00033225\n","Adjusting Layer 1, Kernel Nodes: 233, Adptive Nodes:567\n","Iter: 069/100 | Train Loss: 0.00015327\n","Iter: 070/100 | Train Loss: 0.00006886\n","Iter: 071/100 | Train Loss: 0.00016629\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 072/100 | Train Loss: 0.00016508\n","Iter: 073/100 | Train Loss: 0.00006662\n","Iter: 074/100 | Train Loss: 0.00005334\n","Iter: 075/100 | Train Loss: 0.00009907\n","Adjusting Layer 1, Kernel Nodes: 105, Adptive Nodes:695\n","Iter: 076/100 | Train Loss: 0.00009633\n","Iter: 077/100 | Train Loss: 0.00004923\n","Iter: 078/100 | Train Loss: 0.00002639\n","Iter: 079/100 | Train Loss: 0.00004312\n","Adjusting Layer 1, Kernel Nodes: 570, Adptive Nodes:230\n","Iter: 080/100 | Train Loss: 0.00005796\n","Adjusting Layer 1, Kernel Nodes: 267, Adptive Nodes:533\n","Iter: 081/100 | Train Loss: 0.00004248\n","Iter: 082/100 | Train Loss: 0.00002100\n","Iter: 083/100 | Train Loss: 0.00001744\n","Iter: 084/100 | Train Loss: 0.00002717\n","Adjusting Layer 1, Kernel Nodes: 586, Adptive Nodes:214\n","Iter: 085/100 | Train Loss: 0.00003052\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 086/100 | Train Loss: 0.00002010\n","Iter: 087/100 | Train Loss: 0.00001082\n","Iter: 088/100 | Train Loss: 0.00001279\n","Adjusting Layer 1, Kernel Nodes: 126, Adptive Nodes:674\n","Iter: 089/100 | Train Loss: 0.00001791\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 090/100 | Train Loss: 0.00001557\n","Iter: 091/100 | Train Loss: 0.00000857\n","Iter: 092/100 | Train Loss: 0.00000652\n","Iter: 093/100 | Train Loss: 0.00000961\n","Adjusting Layer 1, Kernel Nodes: 2, Adptive Nodes:798\n","Iter: 094/100 | Train Loss: 0.00001091\n","Adjusting Layer 1, Kernel Nodes: 267, Adptive Nodes:533\n","Iter: 095/100 | Train Loss: 0.00000809\n","Iter: 096/100 | Train Loss: 0.00000462\n","Iter: 097/100 | Train Loss: 0.00000418\n","Iter: 098/100 | Train Loss: 0.00000596\n","Adjusting Layer 1, Kernel Nodes: 422, Adptive Nodes:378\n","Iter: 099/100 | Train Loss: 0.00000669\n","\n","Iter: 099/100 | Test Loss: 0.00101989 | Test acc: 63.3800\n","scale:1.250000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00133485\n","Iter: 001/100 | Train Loss: 0.00139033\n","Adjusting Layer 1, Kernel Nodes: 639, Adptive Nodes:161\n","Iter: 002/100 | Train Loss: 0.00101787\n","Iter: 003/100 | Train Loss: 0.00078049\n","Iter: 004/100 | Train Loss: 0.00082644\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 005/100 | Train Loss: 0.00056225\n","Iter: 006/100 | Train Loss: 0.00058284\n","Adjusting Layer 1, Kernel Nodes: 649, Adptive Nodes:151\n","Iter: 007/100 | Train Loss: 0.00041041\n","Iter: 008/100 | Train Loss: 0.00045047\n","Adjusting Layer 1, Kernel Nodes: 689, Adptive Nodes:111\n","Iter: 009/100 | Train Loss: 0.00031224\n","Iter: 010/100 | Train Loss: 0.00034585\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 011/100 | Train Loss: 0.00025144\n","Iter: 012/100 | Train Loss: 0.00025251\n","Adjusting Layer 1, Kernel Nodes: 768, Adptive Nodes:32\n","Iter: 013/100 | Train Loss: 0.00020059\n","Iter: 014/100 | Train Loss: 0.00017018\n","Iter: 015/100 | Train Loss: 0.00016749\n","Iter: 016/100 | Train Loss: 0.00011937\n","Iter: 017/100 | Train Loss: 0.00013487\n","Adjusting Layer 1, Kernel Nodes: 631, Adptive Nodes:169\n","Iter: 018/100 | Train Loss: 0.00010258\n","Iter: 019/100 | Train Loss: 0.00009423\n","Iter: 020/100 | Train Loss: 0.00009027\n","Iter: 021/100 | Train Loss: 0.00006489\n","Iter: 022/100 | Train Loss: 0.00006611\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 023/100 | Train Loss: 0.00005604\n","Iter: 024/100 | Train Loss: 0.00004439\n","Iter: 025/100 | Train Loss: 0.00004707\n","Adjusting Layer 1, Kernel Nodes: 765, Adptive Nodes:35\n","Iter: 026/100 | Train Loss: 0.00003489\n","Iter: 027/100 | Train Loss: 0.00003426\n","Iter: 028/100 | Train Loss: 0.00003317\n","Iter: 029/100 | Train Loss: 0.00002318\n","Iter: 030/100 | Train Loss: 0.00002845\n","Adjusting Layer 1, Kernel Nodes: 740, Adptive Nodes:60\n","Iter: 031/100 | Train Loss: 0.00002036\n","Iter: 032/100 | Train Loss: 0.00002006\n","Iter: 033/100 | Train Loss: 0.00001945\n","Iter: 034/100 | Train Loss: 0.00001362\n","Iter: 035/100 | Train Loss: 0.00001603\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 036/100 | Train Loss: 0.00001268\n","Iter: 037/100 | Train Loss: 0.00001204\n","Iter: 038/100 | Train Loss: 0.00001241\n","Adjusting Layer 1, Kernel Nodes: 662, Adptive Nodes:138\n","Iter: 039/100 | Train Loss: 0.00000867\n","Iter: 040/100 | Train Loss: 0.00000993\n","Adjusting Layer 1, Kernel Nodes: 582, Adptive Nodes:218\n","Iter: 041/100 | Train Loss: 0.00000677\n","Iter: 042/100 | Train Loss: 0.00000716\n","Adjusting Layer 1, Kernel Nodes: 575, Adptive Nodes:225\n","Iter: 043/100 | Train Loss: 0.00000596\n","Iter: 044/100 | Train Loss: 0.00000548\n","Iter: 045/100 | Train Loss: 0.00000544\n","Iter: 046/100 | Train Loss: 0.00000415\n","Iter: 047/100 | Train Loss: 0.00000429\n","Adjusting Layer 1, Kernel Nodes: 497, Adptive Nodes:303\n","Iter: 048/100 | Train Loss: 0.00000307\n","Iter: 049/100 | Train Loss: 0.00000313\n","Adjusting Layer 1, Kernel Nodes: 575, Adptive Nodes:225\n","Iter: 050/100 | Train Loss: 0.00000253\n","Iter: 051/100 | Train Loss: 0.00000286\n","Adjusting Layer 1, Kernel Nodes: 534, Adptive Nodes:266\n","Iter: 052/100 | Train Loss: 0.00000242\n","Iter: 053/100 | Train Loss: 0.00000241\n","Iter: 054/100 | Train Loss: 0.00000181\n","Iter: 055/100 | Train Loss: 0.00000177\n","Iter: 056/100 | Train Loss: 0.00000133\n","Iter: 057/100 | Train Loss: 0.00000145\n","Adjusting Layer 1, Kernel Nodes: 460, Adptive Nodes:340\n","Iter: 058/100 | Train Loss: 0.00000121\n","Iter: 059/100 | Train Loss: 0.00000126\n","Adjusting Layer 1, Kernel Nodes: 563, Adptive Nodes:237\n","Iter: 060/100 | Train Loss: 0.00000097\n","Iter: 061/100 | Train Loss: 0.00000093\n","Iter: 062/100 | Train Loss: 0.00000073\n","Iter: 063/100 | Train Loss: 0.00000076\n","Adjusting Layer 1, Kernel Nodes: 467, Adptive Nodes:333\n","Iter: 064/100 | Train Loss: 0.00000073\n","Iter: 065/100 | Train Loss: 0.00000072\n","Iter: 066/100 | Train Loss: 0.00000060\n","Iter: 067/100 | Train Loss: 0.00000060\n","Iter: 068/100 | Train Loss: 0.00000047\n","Iter: 069/100 | Train Loss: 0.00000049\n","Adjusting Layer 1, Kernel Nodes: 450, Adptive Nodes:350\n","Iter: 070/100 | Train Loss: 0.00000044\n","Iter: 071/100 | Train Loss: 0.00000044\n","Adjusting Layer 1, Kernel Nodes: 492, Adptive Nodes:308\n","Iter: 072/100 | Train Loss: 0.00000042\n","Iter: 073/100 | Train Loss: 0.00000037\n","Iter: 074/100 | Train Loss: 0.00000035\n","Iter: 075/100 | Train Loss: 0.00000032\n","Iter: 076/100 | Train Loss: 0.00000031\n","Iter: 077/100 | Train Loss: 0.00000027\n","Iter: 078/100 | Train Loss: 0.00000029\n","Adjusting Layer 1, Kernel Nodes: 434, Adptive Nodes:366\n","Iter: 079/100 | Train Loss: 0.00000032\n","Adjusting Layer 1, Kernel Nodes: 463, Adptive Nodes:337\n","Iter: 080/100 | Train Loss: 0.00000027\n","Iter: 081/100 | Train Loss: 0.00000023\n","Iter: 082/100 | Train Loss: 0.00000027\n","Adjusting Layer 1, Kernel Nodes: 400, Adptive Nodes:400\n","Iter: 083/100 | Train Loss: 0.00000021\n","Iter: 084/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 442, Adptive Nodes:358\n","Iter: 085/100 | Train Loss: 0.00000018\n","Iter: 086/100 | Train Loss: 0.00000018\n","Iter: 087/100 | Train Loss: 0.00000016\n","Iter: 088/100 | Train Loss: 0.00000015\n","Iter: 089/100 | Train Loss: 0.00000014\n","Iter: 090/100 | Train Loss: 0.00000014\n","Adjusting Layer 1, Kernel Nodes: 382, Adptive Nodes:418\n","Iter: 091/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 432, Adptive Nodes:368\n","Iter: 092/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 372, Adptive Nodes:428\n","Iter: 093/100 | Train Loss: 0.00000018\n","Iter: 094/100 | Train Loss: 0.00000012\n","Iter: 095/100 | Train Loss: 0.00000020\n","Adjusting Layer 1, Kernel Nodes: 429, Adptive Nodes:371\n","Iter: 096/100 | Train Loss: 0.00000011\n","Iter: 097/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 372, Adptive Nodes:428\n","Iter: 098/100 | Train Loss: 0.00000010\n","Iter: 099/100 | Train Loss: 0.00000009\n","\n","Iter: 099/100 | Test Loss: 0.00099990 | Test acc: 64.9100\n","scale:1.250000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00133722\n","Iter: 001/100 | Train Loss: 0.00141421\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 002/100 | Train Loss: 0.00104846\n","Iter: 003/100 | Train Loss: 0.00078005\n","Iter: 004/100 | Train Loss: 0.00084013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00054681\n","Iter: 006/100 | Train Loss: 0.00058120\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00040279\n","Iter: 008/100 | Train Loss: 0.00044358\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 009/100 | Train Loss: 0.00032107\n","Iter: 010/100 | Train Loss: 0.00031803\n","Iter: 011/100 | Train Loss: 0.00029115\n","Iter: 012/100 | Train Loss: 0.00020976\n","Iter: 013/100 | Train Loss: 0.00023724\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00017081\n","Iter: 015/100 | Train Loss: 0.00014750\n","Iter: 016/100 | Train Loss: 0.00015620\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00011256\n","Iter: 018/100 | Train Loss: 0.00010147\n","Iter: 019/100 | Train Loss: 0.00010772\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 020/100 | Train Loss: 0.00007552\n","Iter: 021/100 | Train Loss: 0.00006475\n","Iter: 022/100 | Train Loss: 0.00007281\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 023/100 | Train Loss: 0.00005227\n","Iter: 024/100 | Train Loss: 0.00004478\n","Iter: 025/100 | Train Loss: 0.00005006\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 026/100 | Train Loss: 0.00003672\n","Iter: 027/100 | Train Loss: 0.00002916\n","Iter: 028/100 | Train Loss: 0.00003101\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 029/100 | Train Loss: 0.00003015\n","Iter: 030/100 | Train Loss: 0.00002160\n","Iter: 031/100 | Train Loss: 0.00002118\n","Iter: 032/100 | Train Loss: 0.00002297\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 033/100 | Train Loss: 0.00001576\n","Iter: 034/100 | Train Loss: 0.00001315\n","Iter: 035/100 | Train Loss: 0.00001501\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 036/100 | Train Loss: 0.00001376\n","Iter: 037/100 | Train Loss: 0.00001008\n","Iter: 038/100 | Train Loss: 0.00001042\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 039/100 | Train Loss: 0.00001081\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 040/100 | Train Loss: 0.00000777\n","Iter: 041/100 | Train Loss: 0.00000637\n","Iter: 042/100 | Train Loss: 0.00000704\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 043/100 | Train Loss: 0.00000647\n","Iter: 044/100 | Train Loss: 0.00000479\n","Iter: 045/100 | Train Loss: 0.00000450\n","Iter: 046/100 | Train Loss: 0.00000481\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 047/100 | Train Loss: 0.00000388\n","Iter: 048/100 | Train Loss: 0.00000281\n","Iter: 049/100 | Train Loss: 0.00000319\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 050/100 | Train Loss: 0.00000325\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 051/100 | Train Loss: 0.00000233\n","Iter: 052/100 | Train Loss: 0.00000206\n","Iter: 053/100 | Train Loss: 0.00000228\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 054/100 | Train Loss: 0.00000197\n","Iter: 055/100 | Train Loss: 0.00000149\n","Iter: 056/100 | Train Loss: 0.00000150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 057/100 | Train Loss: 0.00000165\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 058/100 | Train Loss: 0.00000129\n","Iter: 059/100 | Train Loss: 0.00000103\n","Iter: 060/100 | Train Loss: 0.00000106\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 061/100 | Train Loss: 0.00000103\n","Iter: 062/100 | Train Loss: 0.00000080\n","Iter: 063/100 | Train Loss: 0.00000068\n","Iter: 064/100 | Train Loss: 0.00000080\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 065/100 | Train Loss: 0.00000072\n","Iter: 066/100 | Train Loss: 0.00000054\n","Iter: 067/100 | Train Loss: 0.00000056\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 068/100 | Train Loss: 0.00000058\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 069/100 | Train Loss: 0.00000049\n","Iter: 070/100 | Train Loss: 0.00000040\n","Iter: 071/100 | Train Loss: 0.00000043\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 072/100 | Train Loss: 0.00000042\n","Iter: 073/100 | Train Loss: 0.00000034\n","Iter: 074/100 | Train Loss: 0.00000031\n","Iter: 075/100 | Train Loss: 0.00000033\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 076/100 | Train Loss: 0.00000031\n","Iter: 077/100 | Train Loss: 0.00000026\n","Iter: 078/100 | Train Loss: 0.00000025\n","Iter: 079/100 | Train Loss: 0.00000026\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 080/100 | Train Loss: 0.00000023\n","Iter: 081/100 | Train Loss: 0.00000019\n","Iter: 082/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 083/100 | Train Loss: 0.00000021\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 084/100 | Train Loss: 0.00000018\n","Iter: 085/100 | Train Loss: 0.00000017\n","Iter: 086/100 | Train Loss: 0.00000018\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 087/100 | Train Loss: 0.00000017\n","Iter: 088/100 | Train Loss: 0.00000015\n","Iter: 089/100 | Train Loss: 0.00000014\n","Iter: 090/100 | Train Loss: 0.00000015\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 091/100 | Train Loss: 0.00000014\n","Iter: 092/100 | Train Loss: 0.00000013\n","Iter: 093/100 | Train Loss: 0.00000013\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 094/100 | Train Loss: 0.00000013\n","Iter: 095/100 | Train Loss: 0.00000012\n","Iter: 096/100 | Train Loss: 0.00000012\n","Iter: 097/100 | Train Loss: 0.00000012\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 098/100 | Train Loss: 0.00000012\n","Iter: 099/100 | Train Loss: 0.00000011\n","\n","Iter: 099/100 | Test Loss: 0.00096208 | Test acc: 65.5500\n","scale:1.250000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00133257\n","Iter: 001/100 | Train Loss: 0.00143726\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 002/100 | Train Loss: 0.00108134\n","Iter: 003/100 | Train Loss: 0.00080986\n","Iter: 004/100 | Train Loss: 0.00083446\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00055710\n","Iter: 006/100 | Train Loss: 0.00054429\n","Iter: 007/100 | Train Loss: 0.00042219\n","Iter: 008/100 | Train Loss: 0.00039288\n","Iter: 009/100 | Train Loss: 0.00037302\n","Iter: 010/100 | Train Loss: 0.00026685\n","Iter: 011/100 | Train Loss: 0.00030330\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00022728\n","Iter: 013/100 | Train Loss: 0.00018166\n","Iter: 014/100 | Train Loss: 0.00020449\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00014511\n","Iter: 016/100 | Train Loss: 0.00011822\n","Iter: 017/100 | Train Loss: 0.00012684\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 018/100 | Train Loss: 0.00010179\n","Iter: 019/100 | Train Loss: 0.00007376\n","Iter: 020/100 | Train Loss: 0.00008578\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 021/100 | Train Loss: 0.00006637\n","Iter: 022/100 | Train Loss: 0.00005265\n","Iter: 023/100 | Train Loss: 0.00004804\n","Iter: 024/100 | Train Loss: 0.00005185\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 025/100 | Train Loss: 0.00003401\n","Iter: 026/100 | Train Loss: 0.00003391\n","Iter: 027/100 | Train Loss: 0.00003392\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 028/100 | Train Loss: 0.00002983\n","Iter: 029/100 | Train Loss: 0.00002029\n","Iter: 030/100 | Train Loss: 0.00002087\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 031/100 | Train Loss: 0.00002245\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 032/100 | Train Loss: 0.00001967\n","Iter: 033/100 | Train Loss: 0.00001734\n","Iter: 034/100 | Train Loss: 0.00001633\n","Iter: 035/100 | Train Loss: 0.00001063\n","Iter: 036/100 | Train Loss: 0.00000874\n","Iter: 037/100 | Train Loss: 0.00000901\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 038/100 | Train Loss: 0.00001399\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 039/100 | Train Loss: 0.00001110\n","Iter: 040/100 | Train Loss: 0.00001642\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 041/100 | Train Loss: 0.00002459\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 042/100 | Train Loss: 0.00012781\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 043/100 | Train Loss: 0.00064777\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 044/100 | Train Loss: 0.00247564\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 045/100 | Train Loss: 0.00068894\n","Iter: 046/100 | Train Loss: 0.00082247\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 047/100 | Train Loss: 0.00098332\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 048/100 | Train Loss: 0.00088863\n","Iter: 049/100 | Train Loss: 0.00076555\n","Iter: 050/100 | Train Loss: 0.00049823\n","Iter: 051/100 | Train Loss: 0.00055710\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 052/100 | Train Loss: 0.00049139\n","Iter: 053/100 | Train Loss: 0.00041207\n","Iter: 054/100 | Train Loss: 0.00052731\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 055/100 | Train Loss: 0.00082652\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 056/100 | Train Loss: 0.00074256\n","Iter: 057/100 | Train Loss: 0.00131659\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 058/100 | Train Loss: 0.00067162\n","Iter: 059/100 | Train Loss: 0.00088343\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 060/100 | Train Loss: 0.00066492\n","Iter: 061/100 | Train Loss: 0.00057747\n","Iter: 062/100 | Train Loss: 0.00044696\n","Iter: 063/100 | Train Loss: 0.00049190\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 064/100 | Train Loss: 0.00035606\n","Iter: 065/100 | Train Loss: 0.00042789\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 066/100 | Train Loss: 0.00055717\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 067/100 | Train Loss: 0.00035640\n","Iter: 068/100 | Train Loss: 0.00029786\n","Iter: 069/100 | Train Loss: 0.00028268\n","Iter: 070/100 | Train Loss: 0.00024635\n","Iter: 071/100 | Train Loss: 0.00028131\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 072/100 | Train Loss: 0.00024064\n","Iter: 073/100 | Train Loss: 0.00029700\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 074/100 | Train Loss: 0.00034921\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 075/100 | Train Loss: 0.00026837\n","Iter: 076/100 | Train Loss: 0.00018133\n","Iter: 077/100 | Train Loss: 0.00014544\n","Iter: 078/100 | Train Loss: 0.00014840\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 079/100 | Train Loss: 0.00018914\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 080/100 | Train Loss: 0.00018336\n","Iter: 081/100 | Train Loss: 0.00018455\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 082/100 | Train Loss: 0.00011918\n","Iter: 083/100 | Train Loss: 0.00013419\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 084/100 | Train Loss: 0.00009089\n","Iter: 085/100 | Train Loss: 0.00012513\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 086/100 | Train Loss: 0.00010026\n","Iter: 087/100 | Train Loss: 0.00008754\n","Iter: 088/100 | Train Loss: 0.00007647\n","Iter: 089/100 | Train Loss: 0.00006058\n","Iter: 090/100 | Train Loss: 0.00006305\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 091/100 | Train Loss: 0.00004455\n","Iter: 092/100 | Train Loss: 0.00004740\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 093/100 | Train Loss: 0.00004220\n","Iter: 094/100 | Train Loss: 0.00003756\n","Iter: 095/100 | Train Loss: 0.00003006\n","Iter: 096/100 | Train Loss: 0.00003007\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 097/100 | Train Loss: 0.00002825\n","Iter: 098/100 | Train Loss: 0.00002409\n","Iter: 099/100 | Train Loss: 0.00002090\n","\n","Iter: 099/100 | Test Loss: 0.00083617 | Test acc: 64.5100\n","tensor(2.4566e-08) 1.25 0.75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C5Xs3vX59jvN","colab_type":"code","colab":{}},"source":["tensor(6.6297e-08) 1.15 0.7999999999999999"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pp7Q3iWOc2m4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597610785144,"user_tz":-120,"elapsed":135199,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"e8b7c6ea-ff6b-4d41-ea9b-6218906beb2f"},"source":["step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(1.25,0.75,2,True,initialize = 'LeCun', batchnorm = True, learning_rate = 0.1, ** shared_model_param_dict)"],"execution_count":192,"outputs":[{"output_type":"stream","text":["Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 000/100 | Train Loss: 0.00138989\n","Iter: 001/100 | Train Loss: 0.00149824\n","Adjusting Layer 1, Kernel Nodes: 596, Adptive Nodes:204\n","Iter: 002/100 | Train Loss: 0.00170627\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 003/100 | Train Loss: 0.00358338\n","Adjusting Layer 1, Kernel Nodes: 341, Adptive Nodes:459\n","Iter: 004/100 | Train Loss: 0.00385473\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 005/100 | Train Loss: 0.00193184\n","Iter: 006/100 | Train Loss: 0.00086206\n","Iter: 007/100 | Train Loss: 0.00053817\n","Iter: 008/100 | Train Loss: 0.00074729\n","Adjusting Layer 1, Kernel Nodes: 696, Adptive Nodes:104\n","Iter: 009/100 | Train Loss: 0.00056763\n","Iter: 010/100 | Train Loss: 0.00037454\n","Iter: 011/100 | Train Loss: 0.00040203\n","Adjusting Layer 1, Kernel Nodes: 363, Adptive Nodes:437\n","Iter: 012/100 | Train Loss: 0.00042029\n","Adjusting Layer 1, Kernel Nodes: 669, Adptive Nodes:131\n","Iter: 013/100 | Train Loss: 0.00027949\n","Iter: 014/100 | Train Loss: 0.00019844\n","Iter: 015/100 | Train Loss: 0.00022667\n","Adjusting Layer 1, Kernel Nodes: 362, Adptive Nodes:438\n","Iter: 016/100 | Train Loss: 0.00020907\n","Iter: 017/100 | Train Loss: 0.00013640\n","Iter: 018/100 | Train Loss: 0.00012651\n","Iter: 019/100 | Train Loss: 0.00014277\n","Adjusting Layer 1, Kernel Nodes: 328, Adptive Nodes:472\n","Iter: 020/100 | Train Loss: 0.00010858\n","Iter: 021/100 | Train Loss: 0.00007205\n","Iter: 022/100 | Train Loss: 0.00007947\n","Adjusting Layer 1, Kernel Nodes: 796, Adptive Nodes:4\n","Iter: 023/100 | Train Loss: 0.00008697\n","Adjusting Layer 1, Kernel Nodes: 385, Adptive Nodes:415\n","Iter: 024/100 | Train Loss: 0.00005591\n","Iter: 025/100 | Train Loss: 0.00005390\n","Iter: 026/100 | Train Loss: 0.00006272\n","Adjusting Layer 1, Kernel Nodes: 79, Adptive Nodes:721\n","Iter: 027/100 | Train Loss: 0.00005055\n","Iter: 028/100 | Train Loss: 0.00003499\n","Iter: 029/100 | Train Loss: 0.00003784\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 030/100 | Train Loss: 0.00003877\n","Adjusting Layer 1, Kernel Nodes: 25, Adptive Nodes:775\n","Iter: 031/100 | Train Loss: 0.00002874\n","Iter: 032/100 | Train Loss: 0.00002254\n","Iter: 033/100 | Train Loss: 0.00002429\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 034/100 | Train Loss: 0.00002435\n","Adjusting Layer 1, Kernel Nodes: 220, Adptive Nodes:580\n","Iter: 035/100 | Train Loss: 0.00001878\n","Iter: 036/100 | Train Loss: 0.00001431\n","Iter: 037/100 | Train Loss: 0.00001505\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 038/100 | Train Loss: 0.00001393\n","Iter: 039/100 | Train Loss: 0.00001044\n","Iter: 040/100 | Train Loss: 0.00001109\n","Adjusting Layer 1, Kernel Nodes: 9, Adptive Nodes:791\n","Iter: 041/100 | Train Loss: 0.00001135\n","Adjusting Layer 1, Kernel Nodes: 596, Adptive Nodes:204\n","Iter: 042/100 | Train Loss: 0.00000818\n","Iter: 043/100 | Train Loss: 0.00000793\n","Iter: 044/100 | Train Loss: 0.00000795\n","Adjusting Layer 1, Kernel Nodes: 205, Adptive Nodes:595\n","Iter: 045/100 | Train Loss: 0.00000663\n","Iter: 046/100 | Train Loss: 0.00000548\n","Iter: 047/100 | Train Loss: 0.00000523\n","Iter: 048/100 | Train Loss: 0.00000519\n","Iter: 049/100 | Train Loss: 0.00000440\n","Iter: 050/100 | Train Loss: 0.00000347\n","Iter: 051/100 | Train Loss: 0.00000324\n","Iter: 052/100 | Train Loss: 0.00000329\n","Adjusting Layer 1, Kernel Nodes: 799, Adptive Nodes:1\n","Iter: 053/100 | Train Loss: 0.00000276\n","Iter: 054/100 | Train Loss: 0.00000321\n","Adjusting Layer 1, Kernel Nodes: 560, Adptive Nodes:240\n","Iter: 055/100 | Train Loss: 0.00000286\n","Iter: 056/100 | Train Loss: 0.00000202\n","Iter: 057/100 | Train Loss: 0.00000241\n","Adjusting Layer 1, Kernel Nodes: 158, Adptive Nodes:642\n","Iter: 058/100 | Train Loss: 0.00000224\n","Iter: 059/100 | Train Loss: 0.00000170\n","Iter: 060/100 | Train Loss: 0.00000140\n","Iter: 061/100 | Train Loss: 0.00000145\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 062/100 | Train Loss: 0.00000121\n","Iter: 063/100 | Train Loss: 0.00000109\n","Iter: 064/100 | Train Loss: 0.00000099\n","Iter: 065/100 | Train Loss: 0.00000087\n","Iter: 066/100 | Train Loss: 0.00000076\n","Iter: 067/100 | Train Loss: 0.00000072\n","Iter: 068/100 | Train Loss: 0.00000067\n","Iter: 069/100 | Train Loss: 0.00000058\n","Iter: 070/100 | Train Loss: 0.00000053\n","Iter: 071/100 | Train Loss: 0.00000049\n","Iter: 072/100 | Train Loss: 0.00000044\n","Iter: 073/100 | Train Loss: 0.00000042\n","Iter: 074/100 | Train Loss: 0.00000037\n","Iter: 075/100 | Train Loss: 0.00000031\n","Iter: 076/100 | Train Loss: 0.00000029\n","Iter: 077/100 | Train Loss: 0.00000026\n","Iter: 078/100 | Train Loss: 0.00000023\n","Iter: 079/100 | Train Loss: 0.00000021\n","Iter: 080/100 | Train Loss: 0.00000018\n","Iter: 081/100 | Train Loss: 0.00000016\n","Iter: 082/100 | Train Loss: 0.00000016\n","Iter: 083/100 | Train Loss: 0.00000014\n","Iter: 084/100 | Train Loss: 0.00000013\n","Iter: 085/100 | Train Loss: 0.00000012\n","Iter: 086/100 | Train Loss: 0.00000010\n","Iter: 087/100 | Train Loss: 0.00000009\n","Iter: 088/100 | Train Loss: 0.00000008\n","Iter: 089/100 | Train Loss: 0.00000007\n","Iter: 090/100 | Train Loss: 0.00000007\n","Iter: 091/100 | Train Loss: 0.00000006\n","Iter: 092/100 | Train Loss: 0.00000005\n","Iter: 093/100 | Train Loss: 0.00000005\n","Iter: 094/100 | Train Loss: 0.00000004\n","Iter: 095/100 | Train Loss: 0.00000004\n","Iter: 096/100 | Train Loss: 0.00000004\n","Iter: 097/100 | Train Loss: 0.00000003\n","Iter: 098/100 | Train Loss: 0.00000003\n","Iter: 099/100 | Train Loss: 0.00000002\n","\n","Iter: 099/100 | Test Loss: 0.00125929 | Test acc: 58.8500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lY9piNMTcT1J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597608873900,"user_tz":-120,"elapsed":131680,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"cc4a0557-b26b-4940-ab6c-a8d5541b78f7"},"source":["step_list, loss_no_scale_bn,train_loss = full_batch_train(initialize = 'LeCun', batchnorm = True, learning_rate = 0.1, ** shared_model_param_dict)"],"execution_count":178,"outputs":[{"output_type":"stream","text":["Iter: 000/100 | Train Loss: 0.00134803\n","Iter: 001/100 | Train Loss: 0.00147709\n","Iter: 002/100 | Train Loss: 0.00111018\n","Iter: 003/100 | Train Loss: 0.00078829\n","Iter: 004/100 | Train Loss: 0.00088465\n","Iter: 005/100 | Train Loss: 0.00056639\n","Iter: 006/100 | Train Loss: 0.00061762\n","Iter: 007/100 | Train Loss: 0.00041767\n","Iter: 008/100 | Train Loss: 0.00046920\n","Iter: 009/100 | Train Loss: 0.00033135\n","Iter: 010/100 | Train Loss: 0.00033680\n","Iter: 011/100 | Train Loss: 0.00030543\n","Iter: 012/100 | Train Loss: 0.00022692\n","Iter: 013/100 | Train Loss: 0.00025334\n","Iter: 014/100 | Train Loss: 0.00019167\n","Iter: 015/100 | Train Loss: 0.00015860\n","Iter: 016/100 | Train Loss: 0.00016925\n","Iter: 017/100 | Train Loss: 0.00013062\n","Iter: 018/100 | Train Loss: 0.00010653\n","Iter: 019/100 | Train Loss: 0.00011745\n","Iter: 020/100 | Train Loss: 0.00009153\n","Iter: 021/100 | Train Loss: 0.00006846\n","Iter: 022/100 | Train Loss: 0.00007593\n","Iter: 023/100 | Train Loss: 0.00006663\n","Iter: 024/100 | Train Loss: 0.00004766\n","Iter: 025/100 | Train Loss: 0.00005117\n","Iter: 026/100 | Train Loss: 0.00004759\n","Iter: 027/100 | Train Loss: 0.00003377\n","Iter: 028/100 | Train Loss: 0.00003062\n","Iter: 029/100 | Train Loss: 0.00003238\n","Iter: 030/100 | Train Loss: 0.00002929\n","Iter: 031/100 | Train Loss: 0.00002138\n","Iter: 032/100 | Train Loss: 0.00002257\n","Iter: 033/100 | Train Loss: 0.00002262\n","Iter: 034/100 | Train Loss: 0.00001570\n","Iter: 035/100 | Train Loss: 0.00001371\n","Iter: 036/100 | Train Loss: 0.00001519\n","Iter: 037/100 | Train Loss: 0.00001393\n","Iter: 038/100 | Train Loss: 0.00001043\n","Iter: 039/100 | Train Loss: 0.00001021\n","Iter: 040/100 | Train Loss: 0.00001076\n","Iter: 041/100 | Train Loss: 0.00000832\n","Iter: 042/100 | Train Loss: 0.00000667\n","Iter: 043/100 | Train Loss: 0.00000692\n","Iter: 044/100 | Train Loss: 0.00000693\n","Iter: 045/100 | Train Loss: 0.00000547\n","Iter: 046/100 | Train Loss: 0.00000448\n","Iter: 047/100 | Train Loss: 0.00000474\n","Iter: 048/100 | Train Loss: 0.00000446\n","Iter: 049/100 | Train Loss: 0.00000334\n","Iter: 050/100 | Train Loss: 0.00000299\n","Iter: 051/100 | Train Loss: 0.00000337\n","Iter: 052/100 | Train Loss: 0.00000291\n","Iter: 053/100 | Train Loss: 0.00000211\n","Iter: 054/100 | Train Loss: 0.00000213\n","Iter: 055/100 | Train Loss: 0.00000218\n","Iter: 056/100 | Train Loss: 0.00000185\n","Iter: 057/100 | Train Loss: 0.00000151\n","Iter: 058/100 | Train Loss: 0.00000154\n","Iter: 059/100 | Train Loss: 0.00000157\n","Iter: 060/100 | Train Loss: 0.00000122\n","Iter: 061/100 | Train Loss: 0.00000101\n","Iter: 062/100 | Train Loss: 0.00000103\n","Iter: 063/100 | Train Loss: 0.00000102\n","Iter: 064/100 | Train Loss: 0.00000082\n","Iter: 065/100 | Train Loss: 0.00000069\n","Iter: 066/100 | Train Loss: 0.00000076\n","Iter: 067/100 | Train Loss: 0.00000070\n","Iter: 068/100 | Train Loss: 0.00000055\n","Iter: 069/100 | Train Loss: 0.00000052\n","Iter: 070/100 | Train Loss: 0.00000056\n","Iter: 071/100 | Train Loss: 0.00000050\n","Iter: 072/100 | Train Loss: 0.00000040\n","Iter: 073/100 | Train Loss: 0.00000039\n","Iter: 074/100 | Train Loss: 0.00000040\n","Iter: 075/100 | Train Loss: 0.00000035\n","Iter: 076/100 | Train Loss: 0.00000030\n","Iter: 077/100 | Train Loss: 0.00000030\n","Iter: 078/100 | Train Loss: 0.00000031\n","Iter: 079/100 | Train Loss: 0.00000027\n","Iter: 080/100 | Train Loss: 0.00000023\n","Iter: 081/100 | Train Loss: 0.00000023\n","Iter: 082/100 | Train Loss: 0.00000023\n","Iter: 083/100 | Train Loss: 0.00000020\n","Iter: 084/100 | Train Loss: 0.00000019\n","Iter: 085/100 | Train Loss: 0.00000020\n","Iter: 086/100 | Train Loss: 0.00000019\n","Iter: 087/100 | Train Loss: 0.00000017\n","Iter: 088/100 | Train Loss: 0.00000016\n","Iter: 089/100 | Train Loss: 0.00000016\n","Iter: 090/100 | Train Loss: 0.00000015\n","Iter: 091/100 | Train Loss: 0.00000014\n","Iter: 092/100 | Train Loss: 0.00000014\n","Iter: 093/100 | Train Loss: 0.00000014\n","Iter: 094/100 | Train Loss: 0.00000013\n","Iter: 095/100 | Train Loss: 0.00000012\n","Iter: 096/100 | Train Loss: 0.00000012\n","Iter: 097/100 | Train Loss: 0.00000012\n","Iter: 098/100 | Train Loss: 0.00000011\n","Iter: 099/100 | Train Loss: 0.00000011\n","\n","Iter: 099/100 | Test Loss: 0.00095899 | Test acc: 65.7000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k7u0irBYcT1L","colab_type":"code","colab":{},"outputId":"230ed927-f0b6-41cd-8613-d6f10291a8e1"},"source":["title = '784-800-10; no scaling; with bn; full batch; train data size = 64'\n","fig = plot_loss(step_list, loss_no_scale_bn, title, fig_save_path)\n","del title"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAioAAADdCAYAAACVOFk7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU5f3A8c83BwmQQLgPOSJyKKhgxaOKNwpSrRZQUVGpitX+eqm1QmsVxaq1tdZWrQcoFcQbiyj1FgVvq6JconLfd8hB7u/vj2c2mWw2yW6yYXeT7/v12lcyzzzzzDO7M7vfeZ5nZkRVMcYYY4yJR0mxroAxxhhjTE0sUDHGGGNM3LJAxRhjjDFxywIVY4wxxsQtC1SMMcYYE7csUDHGGGNM3LJAxSQkEVkgIguC0lREpsSmRtElImtEZEaYeWeISGkD1jXFe+961LeMaBCRX4rItyJSIiJ76rF8lc9fRE720k6uY7lAvvGR17pxxGOdwhHuex6ldU3w1pXd2OuKtqb0XbU/1BmoeF+CWsvr4qD8/UXkaRFZLyIFIrJSRP4sIu3rWM8fvfK+i2QDIlmfiHQVkVkislNE8kTkbRE5spZyXxKRHBHZKyJzReSgCOo1QET+JiILRSS/roO3oeszTZuIZHkBxYmxrktj8I6NfwBfAFcCV8W0QlHgfV4/jnU9/EQk26vX4bGuSyyJyBDvfegV67okMhFpJSK3eb+7hSKyVUTm1xY8ikg7EdkeSSCeEkaeh4E3Q6TfDBzonyciBwKfAjnAv4DtwJHA9cBwETlKVctDVDwbmAzkh1Pp+qxPRFoD7wBdgHu8Zf4PeEdEjlbVFb5yuwMLgSLgVkCAa4H3RGSIqm4Po3o/BH4DrAC+Bo6pZTuisT4DLYF6tyzEmQGA/1jJAm7Bbd97MalR4xru/b1KVSNuTYlTtwDTgZdiXRGfbFy9vgO+aoTy38Mdh8WNUHY0DcG9D28C62Kw/oT/rhKRDOBtoB/wKO63rh1wNNAeWFPDonfgtj9sdQYqqvoh8GFQBTsDjwOvq+pW36yfAm2AYar6tZf2qIjsBW4ABuPOmILdBywCWgCRND9Hsr6rgYOBU1X1HW87ngFWArcDY33lTsa94Yeq6kov7yvAEuBG4Ldh1O0lIEtV93pRY42BSpTW1+ypamGs6xAtqloU6zrsZ50BmlCQ0iSISGtVDfsE0jsxbDLHYWNpIt9VfwL6A0NVNayeEK8HYyIuSLw93BXVd4zKRbgg54mg9Dbe381B6YHpguCCRGQUMAr4dT3qEcn6zgeWBoIUAK+l4lngLBFp5ct7Hi4IW+nLuwJ4C7ggnIqp6i5V3RvWVkSwPhHpKCIHB9U3JHHjHN4UkaNE5H0R2ed1kV0XIm+6iNzhLVPs/b1DRNLCWE9rr7vte6/5b6eIfCQiY4PydRGRB0RknYgUicgGEZktIgd481uIyK0i8omI7PLq+6WITKirDt7ywWMUAn3Yp4jInSKyxSvzDa81Lnj5c0Xka28bVorIRK95WIPyhfUZiMjd3vrSfGm/9er0SFDeFSLyH990xRgVr1tktTdrqlR2u04JKqOLiDzrdR3uFpFpIhLJmUuWiPzbW3aviDzlnZT417FARL4Tkb4i8pq4bs1tInKXiCQF5Q33fVLclxfB2yY1jNUR1yW9JoJtC0eSiNzkHSP7RGSRBHUNi0hvEblfRJZ7277XO8aO8+XJ9u0zV/i2aYYvT63HQtVVyvUistbbLz8SkR8EV9x7n2vtxvCOo8D330xfvSZ48wOf7aHeMZILPOnNO0FcF/sar77bxHWj9whaR7UxKt5nVdqQ/VNEjhWRD7z3YJ2ITMK1PAfn+7G4LvQNvvf1XyKS5cszBXeiDbDQ9z6cHG4ZddR1jIh8LK4bP997T/8VlCf4u2qN1Dy8YoIvX6a475XV4r6n14nIX8J9H6NFRNrgjtlHVfU7EUmtqw4iIsADwNPA+5GsL5yun1AuBfYCc4PSF+ACjsdF5BZgGzAU1yrwvKp+488sIum4fun7VXW5246IhLU+7wv0cOCpEGV8gusPHwR86n1RdPHSQ+U9Q0Q6Ras7ph7r+wUuGj0Ft/116QXMA2YCs3CBzz0islRVX/PqIMAc4Ewvz4fAcbiWnsOAs+tYx4PAhd7fJbgAcgiuFel5bx1dgI+B7sA0YDHQCfgR0BfY6C13NfAM8G8gFTgX9/mmquqjYWxvKH/BneXdAXTEtVA96W0jXv1GAi/gWthuwjVNTgW2hCgv3M/gPVzL3tG4rj2AE3FdOif51t0Z19XzSHABnuW4rsB7ce9n4LjzN90L8CqwDLf/HwVcgesOnVxLHf1mATuBP+Kac38OHCKua9TflJ+JazL/L/AiMMJb52pcV3FAuO/TJV5dT/b+D962/eU6oBXuOykdV/+3RWSoqn7r5TkKtz1zgLW4/ekKX74luPf8EtwxtwDX/QPwPYR9LAT8Gkjz6pSC259eFJG+qlriy7cceBf3HtbkPdwx8HtcV/kHXvoHvjxtgDdwLcLPAfu89POBDl59t+LOpK8CjhGRw1V1H7Wr9/4pIgNx+1su7iy82Ft3XojslwNlwP24fXmIt57DgGFenjlAN6+MqbhjHtx7GG4ZNdX1NNz7tgD4A1AC9MF9t9bmN0BGUFrgmNjmlZ1OZVfLI7juu8O9ZQ8VkVFay8P7xJ0wZdZRj4CcoP0r2Am478hvRORpYAyQIiJfAtf5GwR8rgQO9fL2C7MejqpG9ML9oCswrYb5U3BjTdT3ehhIDpH3FtxO39abXgB8F2F96lwf7stEgT+FWH6UN+8cb3qoNz0xRN6fe/MGR1jH8d5yJ4eYF9H6vO0NWVaI5dd4eUf60tK89/w5X9pZXr47g5b/i5c+qo717AYeqCPPY15Zp4SYJ97fZCAtxPw3gG+D0hYAC4LSFJjim57gpb0ftD/8xksf5EtbjPtCaOdLG4D7otEQ+1ydnwHQFveFd1NgO3FffE97y3f10sd600ODPrsZvulsL89NIdYzw5t3V1D6f4DtYR5DivsS9L9PE730nwW97wpcHVTGl8Cn9XmfvLzTgt/nUO9D0DavqePzPznMzymQbyfQwZd+CG4cwVO+tFYhlm/v7TuPhqhPte9JwjsWAnVaBbT0zT/XS/9RiHUtqG07g8odH2Je4LO9LsS8UNs9zMt/cW3veRT2zxdwx2F/X1onYI9XbnYd9Qx8/x7vS5vgpQ0Lc1urlVFDXe/FjX+s9ntX274aYv5I3HfHI760ybgTrsOC8l7llXd6HesMbHM4r7qOmcB36HbgM+/9uRwXPBUDR4Y4RnYAk+vaD0O96tP1c6n3N7jbJ2AtrstiIjAa19RzBe5Hr4K4pvdJwO9VNace9YhkfYEmqVD9/oVBeSLJGw0RrU9Vp6iqqOqCMMtfo6qv+pYvAj7CRfkBZ3l//xq07N1B82uyB3dm1TPUTK9FazTwhoaItDVw5KqWefXDa0psLyIdcT+gfUWkbR31qMnDqlrmm37X+9vHW1d33JnJLFXd7avXN7izwOD6hvUZePv117hWFHBnZO2BP+MO5kD6ibizxVDjtyLxYND0u0BHEQn3LOqfQe/TDNxnG/z5l+ACi+B1+fep+uyrsTZLVXcGJlR1OfAaMMprdURVK7qTRaSliHTAdaF/ghvIX6twjwWfGVq1taLKvutbTlT15LrWH4Zy4KEQ9fJvd6Z3XK7A7R91brcn4v1TRJJxP9rztWrX+Ha8bqlQ9RSnjVfPQDdDWPVsYBl7gNbAmYF9JlIi0h93MvMRrlUv4AJca/dmcd2qHb26BS5oObWOol8DTg/ztbiOsgKtP4ILuGep6mO41kbFtSb53Yl7b/5WR7khRdT14x1kF+GaeBeGmP8rXCtJP1Xd5SW/KO6eCH8QkSdU9Usv/R+4boLH61hnBkFNYqq6JcL1BQ70UOMt0r2/+4L+1pnX++H0By1lGnmXUCR1q481IdJ2436YA7JxZzY7/ZlUdbuI7MBd3VWb63HN3GtF5CtcC8jTqvo/b34nXOvC1zUsX0FELvPKG0T1MVRtcWcrkVobNB0IRgKXsPf2/n5LdaHSIvEecLmIpOACku2q+oWIfIrr/nnWS/8gKEiIVDmwISjNv525YZRRpWtWVUtEZDVu//DbqKrBVyzspvL9TFTf1JA2CrdtO0WkBe475xIgODBfHcY6wj4WPFX2XVXd7f3+NdZ7vcUflAR4wfzduKA1+IQhnLEb9d0/O+G642r6bILreTDuRGC4t1yk9WxoGQ/iujbmAVtF5B1cN9rzWntXSmDdbXFdu3nAGK3a5dof93tT029M5xrSAVDVzVQfz1lfgd+kl1S14rNT1fUi8h6+LjIROQrX7XOu1vMigUjHqJyKuypnaojIH1w/+ru+oCFgDi7CGgZ8KSKn4Hb4i4BevsAzHdfPlQ3kqeoO3HiCW4LKCywQ1vqAXbgWi+4h6tzN+7vJ+xv4IMPJex9wmW/+Wqp/qdclkvXVR00/fuFG+4KLkGukqnNEZBFuLMtwXBPg9SLyB1W907euWssRkQtwZ/Gv4CLvrbiz91G4z7q+g78b8h7U66zI5z3gl7gzsROpvLT4XeDH3gC9w3ABS0Oohrj03xPuNoT6fEIt25CAKlI17TPJ+2ldwdt/H66p/QHcWfZu3I/wZCCc+x6FdSz4NPT4jVS1kyLvBPUNoCuupXoZ7odUcWf+4RyX9d0/a3u/qiznDfB8F9cSfTPuJKMAt6+8Gk49G1qGd3L3A9xv5UjgDGAccIOIDAsVBPrWnQTMxv2GnBg4IfdJ8upW09Uytf5OeINdw22V3hUUJNW0rq0h5m3FXcUacA+ue+hrqby/Slfvb0cvbUOIk58KkQYqgYFuNXX7dCf0ILiUoL+BM9jZNZSzGjeYcoK3rkUNWZ+qlovIYtwArmDH4IKYZV7ejSKyrZa8G32tJnfjBiAGRNzyEeH6GssaYISIdPC3qnjNih2o+Xr4Cqq6DTdocLq4qzxeAW4Vkb/i+u9zqNqKE8o43Gd/tj8QFpG6mjQbKnDWGmqAV2SDvqoLBCYn4Qag3elLn4wbc5BE3fdGCfeHrSEOpnJAISKSivvSrOn42x92E/osNrsR1nVwiLT+uAsHAidD44AnVPVX/kwicluY6wj3WGgs9dmPDgMGAhNU9d+BRO+Hr12NS0XHNlygUNNn43cKrlXhZFUNdJEFulKC1fQ+RFJGSN4P7uveCxG5BtfSch7ud60md+JOyiao6qch5n8HtFHVUPc1C8cF1NGD4VPXAPhAa3mo24n0oGqrTy/cb36oFsd7vdeB1PI7E3agIu6GaaOBD7Xma6a/AU4RkQNU1T9yPXD3ucDGvQ38JMTyt+N+GK/BuwmPqq7CDShryPrAXS1xt4icHOgzF5FOuJ1nvla9V8DzwEQR6a+V9zU5GBcl/yOQSVWX4QU4DRTW+hrRPOBnuKse/H2LN/jmh+T1IWf4xxmpaoGIfIMbMNVaVfeIyBzgpyJySnDfvIiIF5gEzriS8M4kvTEAlzdk4+qiqpu8LqvxIjI1ME5FRAbgrmhpSNnbvPfictxZRODL733cNk7Cnb2F+mLyC+yfYTVf19MvReQlXxfUBG99rzTiOuvyHXCSiKT5xi/9AHfF1voor2u8iNwWCNZF5BDc5/+8L3AuJ+isWkROAI6l+o3D8gn6vLyTpnCOhcZSn/3If1z6XR8iLapUtUxEXgPODvp+7IRrkQ+nnjdQXU3vQyRlVBN8sucJjD2r8T0XkQuB3wF/9weDQZ7G3Z5gtKrOCVo+HUj1d8OEEBijEo5ax6io6koR+Rw4R0Q6eyeqgd+tYbihAAFXUb0L7VDcFVd/x30nbqttfZG0qPwEN1akptYUcDeAeRr4WNx14zuB03BXNbylqgsBVHUdIe4GKCK/AdJV9T/B8xqyPs+/cANu53hn+YE706biLkf1uwMXwLwpIvfimhivw72Zfw6nYl5f4y+9ycHe30tEZBiAqvqb78Jen7hr72/BDWBaEE5dwjAf16z5e3H3RfgY98V7CfCyqv63lmUzgY0i8iJu594FHIHrk/yvVt7A6/e4g+Q1EQlcktkedwZxE25nnYsLhl8Wd0+RzridfBPuEu7GNBl4GfhARKZTeXnqEtzliRXq8Rm8h9v39uCNTVDVPBH5AteS9m5dfbequkNE1gEXicj3uJaGJeouh42W9sDr3mfZF3d81DmOrCZR2lcfxrvPkLjLIA/A7RNLCL8ZO1zrgQ9F5FHcmLFf4lpJb/blmQtcJiJ5uG7lQ3D7+lKqX/r5P9ztBa7FdfGuVtWPCe9YiIi4+7a8G8aA2uXeNv1cRPbhfrA/VtXaxtcsx13Ce4+4e7VsxbUQHo/7zm1sN+MCxndF5H5cd/BVuJZQ/4//+7grS54QkX/iWmLOIvTYjc9xrSqTvZOhItwJdCRlhDJN3O0G3sL9xnXE3XIhnxruUOytf7q33i+l+m3lP/BO2P/q1eU5EZmFG8CdimtZOh/3u7egpopFeYwKuCt/3sJ9Zz6Eu2Hrr3C/rbf61vt68IJS+Syv/4X1e69hXBrkBfiv4T7MdnXkOxnX5LUZd2XD98BdhLjkK8SyC4j88uSw14frKpqN+zHNx938aGgN5Q7A/XDt9V4vAX0jqFc2tVz+Vd/14XbWcuDgMOqwBngzRPoMql/a2RLX9LjWex/X4gKo9DrW0QIXTP0P9+NZgLsa4DZcS4s/bzfc9f+bvHWsx3WddffluRZ3Fl2I+3L8NZWX1WUH7SsLgsqvcskfNVyC6PtsJgSlj8b9ABZ56/6p937vq+9n4OW/2FvfvKD0wOXft9Xw2c0Isa9/7tWvYlu9z7M0RBnV3rca6jfFy3co7kRkN25w4zN4l1DXdYwGymjAvhry8mRv3i+9/bHQ289Oq2EfDv78Tyayy5Mvxd1DZoO3rveBo4LyZuKa8Tfj9vWPceMQQtXnMFy3WYFX/gzfvFqPBWq/jDh4OzO8tKdq205f/vNwwUcJvuOgps/Wm9cP17K2B/dD9BJuTE6V/TTUe97Q/dPLexzuipdCXAAwCXd8Bn8vDMUFerm4IGoWLsio8p55eX+B644o9dc5kjJC1HMM7sRvM+443YhrMR8clM9//GZT+6XCE3zLtcIdayu88nfixn9MAdqH8/lH80XluLt8b7/4DzAgjOVq3L9DvQLX7JsEISKfAGtV9bxY16U5EJG5wEBV7edLs88gDPY+7R/i7u79Mu7HMNyriYxJGPW9M62JAW9E+mCqXmlkokDc5cOob+S51986CneDrkCafQZhsPdpvzoFdzsAC1JMk2QtKsYA3tic93DNvOtxN9S6GndZ4hGq+n0Mq2eMMc2WtagY4+Tg7gT5U1x/9D7c+II/WJBijDGxYy0qxhhjjIlbjXoNvDHGGGNMQ1igYowxxpi4ZWNUTNwbOXKkvvpqtYcYG2MaX2M9U8iYsFmLiol7O3bsiHUVjDHGxIgFKsYYY4yJWxaoGGOMMSZuWaBimgZV+K6+Tz83xhgTryxQMYmvrBRe+iXMGgMf3B/r2hhjjIkiC1RM4vvwn/DFTPf/63+Ar5+PbX2MMcZEjQUqJvEdczX0PLZy+sWrYdWCmFXHGGNM9FigYhJfaku48CnoOMBNl5fA0+Nh8+LY1ssYY0yDWaBimoZW7WH8C5DZ3U0X58KssbB7TUyrZYwxpmEsUDFNR1ZPF6yktXXT+dtg5mjItxvGGWNMorJAxTQtXQa6bqDkNDe963uYfT4U58e2XsYYY+rFAhXT9GQfD2OmUfGYko3/g+cmQFlJLGtlTKO6+uqrmTp1aqyrYUzUiarGug7G1Gro0KH62WefRb7gJ4/C/N9WTg+5GM55AMSes2biT3Z2NtOmTWP48OGxroqfHSwm5qxFxTRdR0+EE3yBypdPwtt2xmkST2lpaayrYEzMWKBimrZTb4IjxldOL7zHtbQYE0cuueQS1q1bx9lnn01GRgZ33303IsL06dPp1asXp556KgDnnXceXbt2pW3btpx44oksXbq0oowJEyZw0003AbBgwQJ69OjBPffcQ+fOnenWrRuPP/54TLbNmIZKiXUFjGlUInDWfZC3Hb59zaXNvwFad4JB58a2bibm7n1jJfe99W1YeS88uid3jj68StrkOV/x1Cfra1zm16f149rT+9dZ9syZM1m4cGFF18+aNWu48cYbeffdd1m+fDlJSe6c8swzz+Sxxx6jRYsW3HjjjVx88cV8+eWXIcvcsmULOTk5bNy4kTfeeIOxY8dy7rnn0q5du7C215h4YS0qpulLToHzHocDjvQSFOZMhDWLYlotY+oyZcoUWrduTcuWLQG4/PLLyczMJC0tjSlTprB48WJycnJCLpuamsrNN99Mamoqo0aNIiMjg2+++WZ/Vt+YqLBAxTQPLVrDRc9Bh75uuqwYnroIti6tfTljYqhnz54V/5eVlTFp0iQOOugg2rRpQ3Z2NgA7doS+T1CHDh1ISalsNG/VqhV5eXmNWl9jGoN1/Zjmo3UHd0O46WdA3lYoynF3r73idXezONPsXHt6/7C6Zmpy5+jDq3UH1ZeEuBrNnzZ79mzmzp3Lm2++SXZ2Njk5ObRr1w67ctM0ddaiYpqXdtlw8fPQItNN526CWWOgYFdMq2VMly5dWLVqVY3zc3NzSUtLo0OHDhQUFPD73/9+P9bOmNixQMU0P90Oh3FPQlKqm97xDTw1Dkr2xbZeplmbPHkyt99+O1lZWTz//PPV5l966aX07t2bAw44gIEDB3LssceGKMWYpsdu+GbiXiQ3fNuwu4Ae7VqFV/DXz8MLV1ROD/gRnP+EG3xrjAG74ZuJA9aiYpqEbbmFXP/sYk76ywKWbAx9FUQ1h42FEXdWTn/zCsy/Hix4N8aYuGGBimkSpry0lBc+30BZuTLlpaXhDzD84c/huF9VTv9vBrx7d6PU0RhjTOQsUDFNwm/PGEBqsmul/mztbl5avCn8hYffCoedXzm94A4XsBhjjIk5C1RMk9CnUwaXH39gxfQd85eTXxTm81GSktzDCvucUpn28rWwYn6Ua2mMMSZSFqg0EyLSXkReFJF8EVkrIhfVkvdaEdkiIjki8piIpIVTjoi0EJHnRWSNiKiInBxU7hQRKRGRPN+rT7S28Ren9qVjhqvq1r1FPPDOd+EvnNICLpgJ3Qa7aS2H538K6z6OVvWMMcbUgwUqzccDQDHQBbgY+JeIDArOJCIjgEnAaUA20Ae4NYJyFgHjgS011OMZVc3wvWq+cUSEMtNTmXTmwRXT0xauZu3O/PALSMt091hpl+2mSwvhqQtgu9123BhjYsUClWZARFoDY4A/qmqeqi4CXgIuCZH9MmC6qi5V1d3AVGBCOOWoarGq/t1LL2vs7Qpl9BEHMKRnFgDFZeVMfXl5ZAVkdIbxc6BVRze9b7e7IdzeCMa8GGOMiRoLVJqH/kCZqq70pS0GqrWoeGmLg/J1EZEOEZZTk7NFZJeILBWRayJYLixJScKUH1dW583lW3l35fbICulwEFz8HKS2dtM5692t9vftiWJNjTHGhMMCleYhAwi+uUgOkBlG3sD/mRGWE8qzwCFAJ2AicLOIXBgqo4hcJSKfichn27dHFmgM6ZnFeUf2qJi+dd5SikvLIyqDA37gbv6W5N38bdtSePpiKCmMrBxjjDENYoFK85AHtAlKawPkhpE38H9uhOVUo6rLVHWTqpap6gfAfcDYGvI+oqpDVXVop06dwim+it+NPJjMNBdkdG/bkr2FJRGXQb/h7mqggLWL4MWroDwmvVqmicvOzubNN99sUBkzZsxg2LBhUaqRMfHBApXmYSWQIiL9fGmDgaUh8i715vnzbVXVnRGWEw6lkW7R3SkzjZvPHsjDlxzJzCuOrrgaKGKDx7n7rAQsmwuvTrK71xpjzH5igUozoKr5wBzgNhFpLSLHA+cAM0NkfwK4QkQGikg74CZgRrjliEiaiKR7ky1EJF28Z9WLyDki0k6co4FfAXMbY5sBzhvakxGDuuKtvv6O/zUcc3Xl9CePwKJ7G1amMT6XXHIJ69at4+yzzyYjI4O7776bjz76iOOOO46srCwGDx7MggULKvLPmDGDPn36kJmZyYEHHsiTTz7J8uXLufrqq/nwww/JyMggKysrdhtkTBTZQwmbCRFpDzwGnA7sBCap6mwR6QUsAwaq6jov73XAjUBL4AXgalUtqq0c33rWAL2DVn+gqq4RkaeAM4A0YAPwoKr+o666R/JQwkZTXg4vXA5LX6xMO/dfMKTG29GYeDel7X5cV93Pn8rOzmbatGkMHz6cjRs3cvjhhzNz5kxGjhzJW2+9xbhx41ixYgWtWrWiW7dufPrppwwYMIDNmzeza9cuBg0axIwZM5g2bRqLFi2KVs3toYQm5uwxsc2Equ4Czg2Rvg43SNaf9jfgb5GU45ufXcu8kANn95ei0jLmfrGJsUf2ICkpwu/fpCT4ycOQvwPWLHRpc38BrTtBv9OjX1nTrM2aNYtRo0YxatQoAE4//XSGDh3K/PnzGTt2LElJSSxZsoRevXrRrVs3unXrFuMaG9N4rOvHNAvvfLONEfe+x+9e+Io5X2ysXyEpaTDuSehyqJvWMnj2Utjwv+hV1Bhg7dq1PPfcc2RlZVW8Fi1axObNm2ndujXPPPMMDz30EN26deNHP/oRK1asiHWVjWk01qJimoVPV+9izc4CAO767wpGDOpCZnpq5AWlt3V3r51+BuSsg5ICmH0eXPGGu/+KSRxhdMfsT/6xVD179uSSSy7h0UcfDZl3xIgRjBgxgn379nHTTTcxceJEFi5c2PDxWMbEIWtRMc3C/53Sly5t3JU/O/KKuP/tCJ4DFKxNNxj/ArRs56YLdsLMn0Du1ijU1DRXXbp0YdUq90SJ8ePHM2/ePF577TXKysooLCxkwYIFbNiwga1bt/LSSy+Rn59PWloaGRkZJCcnV5SxYcMGiouLY7kpxkSVBSqmWWidlsLvRx1SMf3Y+6v5fnte/Qvs1B8uehZSWrrpPWvhybFQFNYtZYypZvLkydx+++1kZWXxzDPPMHfuXO644w46depEz549+ctf/kHi2UYAAB/rSURBVEJ5eTnl5eXcc889dO/enfbt2/Puu+/y4IMPAnDqqacyaNAgunbtSseOHWO8RcZEh131Y+JetK76UVXOe+hDPlu7G4CTB3Rixk+Pblih37wKT1/kxqsA9DkZLnrOPY3ZmMRnfUkm5qxFxTQbIu45QIFu/AXfbOftFQ3srhkwEs7y3VNl1QKY+3N3ObMxxpgGs0DFNCuHHtCWcUf1qpi+bd4yikobeEv8Iy+DU/5QOf31c/DmzQ0r0xhjDGCBimmGfntGfzLT3QVva3YW8NiiNQ0v9MQbYOjlldMf/BM+uL/h5RpjTDNngYppdjpkpHHd6f0rpu9/+1u27m3gU5FFYNRf4eCzKtNe/wN8/XzDyjXGmGbOAhXTLI0/tjf9u2TQqkUy/3dqX7Ja1eOeKsGSkmHMNOj1w8q0F69241aMMcbUi131Y+JeYz3rZ9mmvbRv3YKubdPrzhyJfbvhsZGw3btbaItM+Okr0G1w7csZE3/sqh8Tc9aiYpqtgd3bRD9IAXcjuPEvQGZ3N12cC7PGwu410V+XMcY0cRaoGNMY2vaAS+a4W+4D5G+DmaPdQw2NMcaEzQIVYzzfbcvlF7M/J2dfSXQK7HwIXPg0JLtb97Pre3jyPCjOj075xhjTDFigYgzw4ILvGPn3hbz81Wbue/Pb6BXc+zg3wDbQ1b/pc3huApRFKRgyxpgmzgIVY4De7VtTWu4Glv/7wzV8uzWKz+wZ+GP40V8rp799Heb9GmwguzHG1MkCFWOAUYd15dg+7QEoK1dunbeMqF4Rd9SV7qZwAV8+CW9PjV75xhjTRFmgYgzuOUC3nD2IJK+HZtF3O3h9WQOfAxTslD/AEeMrpxfeAx8/Et11GGNME2OBijGeQ7q1YfyxvSumb39lGYUlDXwOkJ8InHUf9BtRmfbf38HS/0RvHcYY08RYoGKMz3Wn96+4S+36XfuYtnBVdFeQnALnPQ4HDPUSFOZMhDWLorseY4xpIixQMcYnq1ULrj9jQMX0A+98z+acfdFdSYvWcNGz0KGvmy4rhqcugq1Lo7seY4xpAixQMSbIRUf34pBubQDYV1LGnfNXRH8lrTvA+DmQ0cVNF+XArDGwZ33012WMMQnMAhVjgiQnCVPOHlgxPe+rTazanhf9FbXr7W61n+aCInI3u2ClYFf012WMMQnKAhVjQjimTwfOOrwbg3u05YVrjqNPp4zGWVHXw+CCWZDkPb15xzfw1DgoiXJ3kzHGJCh7enKCEpGDgYOBT1R1U6zr05ga6+nJdckrKqVVajJJSfvhAbJLXoDnL6+c7n8mDPsNZPWCjK6QZOcUJibs6ckm5lJiXQFTNxF5GFBVvdqbvgCYBSQDeSIyUlU/iGUdm6KMtP14eBw6BvK2wauT3PTK/7oXQHILaHOAC1qyekFWb8jqWTmd2Q2SkvdfXY0xZj+yQCUxjAQm+6anAk8BvwP+6U2fFoN6NTtFpWWkpTRSUHDsNW6cyvv3VU0vK4bdq90rlKSUoEDG92rb081LtkPdGJOY7NsrMXQG1gOISD+gLzBaVbeIyCPAM3UVICLtgenAGcAOYLKqzq4h77XAjUBL4AXgGlUtqqscEWkBzAaGAr2BU1R1ga9cAe4CrvSSpgM3agL0P5aVK09/uo573/iWWVcezcFd2zTOik6bAp0Ohm/+C3vWude+OgbXlpfCnrXuFYoke4FMz+pBTFYvaNsDklOjvinGGBMNFqgkhl2Adx0rw4EtqrrEmxZcF1BdHgCKvXKGAK+IyGJVrXLzDhEZAUwCTgU2AS8Ct3pp4ZSzCPg78FyIOlwFnAsMBhR4A1gFPBRG/WPq1nlLeeJDFwjc+tIyZk88Bhd3RVlSEgy5yL0CivIgZ31l4OJ/5ayH/O21l6llkLPOvda+X32+JLnuo+AApmK6B6SkRXc7jTEmTDaYNgGIyDTgOFyQ8DvgRVX9jTfvOuAyVR1cy/Ktgd3Aoaq60kubCWxU1UlBeWcDa1T19970acCTqto1wnI2AOODWlQ+AGao6iPe9BXARFU9trbtj9VgWr+VW3M5876FlHlPWH7w4h8w6rBuMa1TheICL5BZ77WsrKsa2OQ19JlFApldQwcxgUAmtWVUNsXEHRtMa2LOWlQSw/XAvcDVwHvAzb55PwFerWP5/kBZILjwLAZOCpF3EDA3KF8XEekA9IqgnFAGefn9yw4KlVFErsK1wNCrV68wi288/btkcukPe/P4+2sA+NMryzllQGdatoiDQawtWkGnAe4VSkkh5GwIHcTsWQe5W3ANXDVRN3YmdzOs/zh0lowuQUFMT2/Qby9o3QlaZEBKi4ZuqTGmGbJAJQGoag5weQ3zTgijiAwgJygtB8gMI2/g/8wIywmnHjlAhohI8DgVr9XlEXAtKmGW36h+M7w/c7/cxK78Yjbu2cdD737Ptaf3j3W16paaDh37ulcopUVeIBMcxHj/524CLa99HXlb3WtjLS1fyWmQluGClrQ2vv8z3f9pbbxpL61iXqYv3cuTkuYe8miMafIsUEkAIpICJAcGtHppZwADgXdV9Ys6isgDgkd/tgFyw8gb+D83wnLCqUcbIC8RBtMCtG2Zyg0jBjB5ztcAPPTu95w3tAc92rWKcc0aKCUNOhzkXqGUFsPejaGDmD3r3DwN4ynTZUVQUAQFOxte56QUL4DJDBHYZHjpmbUERr78qS0t6DEmjlmgkhiewbU+XA4gIr/CDVgtApJFZLSqvlzL8iuBFBHpp6rfemmDgVBPwVvqzXvWl2+rqu4UkcIIygklUPYn9Vg2Lpw/tCdPfryWJRv3UlRazh3zl/PgxUfGulqNK6UFtD/QvUIpK3WtLtWCmLUuuNm32w0IDieYCVd5qSt33+6GlyVJIQKbEK08LTJcQKPl3kuhvMw3HfzSWubVNL+sgcsHvcprmddpAFwcasy7MfHFApXEcCzwa9/0DcA9qnqDiDwI/AGoMVBR1XwRmQPcJiJX4q7WOQc3QDfYE8AMEXkS2AzcBMwItxwRSaNyAF4LEUkHirxWkyeA60RkPm5QxPW4+8AkDPccoEGMfehDAOZ/vYUPvtvBcX07xrhmMZScUjk2pSaq7rEAxXlQlOtexXkugCnKheJc3/95ULTXTVfkz/PyeP+Xl0Sv/lruHgpZFNyr2cSlNdIl9sZEmQUqiaEDsAVARA4DulN5Se9zwMVhlPFz4DFgG7ATd2+UpSLSC1gGDFTVdar6qojcDbxD5X1UbqmrHN/8b3D3UAF4zft7ILAGeBjoA3ztpU/z0hLK0Oz2/OSIA3jxi40ATJm3lPm/OoGUZLvNfY1E3KDfFq0go3PDyystCgp4QgQz1YKiGvKUFja8PomornFHxsQJC1QSw1YgG3ePkpHAWlX93pvXEqjzG0dVd+HuYRKcvg43yNWf9jfgb5GU45ufXcs8xV1e/bu66hvvJp15MK8t3UJBcRm7C0pYs7OAvp0b6cGFprqUNPdqHYWWrLKS8AKeYu8J2pLkvZJ9/ye5YKzKdPCrrvlJ7j46DS2jWp4a6mn3xjEJwgKVxPAc8GcRGQz8FLjfN+8I4NuQS5lG06VNOtcO78+O/CJ+eWq//ftcIBNdyanQqr17GWPijn27JoZJwF7gKOBfwJ2+eUcSxi30TfRNPLFPrKtgjDFNngUqCUBVS4Hbapg3ej9XxxhjjNlvLFBJICJyDDAMaI97/s8iVa3hVqEmFr7asIfDDmjbOM8BMsaYZsgClQTgPWPnOdxA2lLc1TYdcPdQeRU4T1ULYljFZm/jnn3cMX85r3y1mfvGDeGcIQfEukrGGNMk2PWUieFu4IfABUC6qnYD0oFxXvqfY1g3Azzx4Rpe+WozAHfOX0FBcWlsK2SMMU2EBSqJYQxwo6o+p+pufqCq5ar6HG6g7XkxrZ3hF6f0pWOGu9xzy95CHnzn+zqWMMYYEw4LVBJDW2B9DfPWU/35O2Y/y0xP5caRlU8vfuS9VazdmR/DGhljTNNggUpiWAxcI0EjNL3pa7z5JsbG/KAHg3tmAVBcVs7tryyPcY2MMSbxWaCSGH4PjABWiMhdInKtiNwJLAfO8OabGEtKEqacPbBi+o1lW3lv5fYY1sgYYxKfBSoJQFXfxt2B9gvceJQ/AecDn+MClSg+ltY0xBG92jH2yB4V07fOW0pJmT1TxRhj6ssClQShqstUdZyqHqSqrby/FwGdcA8QNHHidyMHVNxS//vt+fz7gzWxrZAxxiQwC1SMibLOmen8+rR+FdP3vfkt23OLYlgjY4xJXBaoGNMILjsumz6dWgNwTJ8OlJZb948xxtSH3ZnWmEbQIiWJO35yGIUlZZw8oHOsq2OMMQnLAhVjGsmxfTrEugrGGJPwLFCJUyKyHdAwsqY1dl2MMcaYWLFAJX49QHiBikkQ+4rLeOz91Vx2XHbFVUHGGGNqZ9+WcUpVp8S6DiZ63lq+lT/+ZwmbcgrZW1jC5DMPiXWVjDEmIdhVP8bsB7mFpWzKKQTgsUWrWb3DngNkjDHhsEDFmP3gnCHdObJ3OwBKypSpLy+LcY2MMSYxWKBizH4gItz640EEHiv59optvL1ia2wrZYwxCcACFWP2k0MPaMu4o3pWTE99eTnFpXYjOGOMqY0FKsbsR789YwCZ6W4M++od+Tz+/uoY18gYY+KbBSrG7EcdMtK47vT+FdP/eOtbtu0tjGGNjDEmvlmgYsx+Nv7Y3vTrnAFAfnEZd726IsY1MsaY+GWBijH7WWpyErecPahies7nG1m8fk8Ma2SMMfHLApVmQkTai8iLIpIvImtF5KJa8l4rIltEJEdEHhORtHDLEZHTRGSFiBSIyDsi0ts3b4qIlIhInu/Vp3G2OL4N69eRkYO6kpmews1nDWRg9zaxrpIxxsQluzNt8/EAUAx0AYYAr4jIYlVd6s8kIiOAScCpwCbgReBWL63WckSkIzAHuBKYB0wFngGO9a3iGVUd3zibmFhuPWcQKUlChwx7XJMxxtTEWlSaARFpDYwB/qiqeaq6CHgJuCRE9suA6aq6VFV344KNCWGWMxpYqqrPqWohMAUYLCIHN97WJa4ubdItSDHGmDpYoNI89AfKVHWlL20xMChE3kHePH++LiLSIYxyqiyrqvnA90HrOVtEdonIUhG5pqYKi8hVIvKZiHy2ffv2urfQGGNMk2SBSvOQAeQEpeUAmWHkDfyfGUY5dc1/FjgE6ARMBG4WkQtDVVhVH1HVoao6tFOnTqGyNDlLNuZw4SMf8d223FhXxRhj4oYFKs1DHhA8WrMNEOoXMThv4P/cMMqpdb6qLlPVTapapqofAPcBYyPYjibr8fdXc/b9i/hw1U5unbcMVY11lYwxJi5YoNI8rARSRKSfL20wsDRE3qXePH++raq6M4xyqizrjWk5qIb1ACggEWxHk3XMgR0q3oiF3+7gjWX2HCBjjAELVJoFb6zIHOA2EWktIscD5wAzQ2R/ArhCRAaKSDvgJmBGmOW8CBwqImNEJB24GfhKVVcAiMg5ItJOnKOBXwFzG2mzE8rA7m246JheFdO3v7KcwpKyGNbIGGPigwUqzcfPgZbANuAp4BrvkuJe3v1MegGo6qvA3cA7wFrvdUtd5XjLbsddFfQnYDdwDDDOt+w44DtcV9ATwJ9V9d+Ns7mJ5/rTB9C2ZSoA63YVcOZ9C5n50Vr2FVvAYoxpvsT6wk28Gzp0qH722WexrsZ+MfPDNfxxbtWesqxWqYw/pjeXHtebzpnpsamYaa6sa9bEnLWoGBNHxh/bmxtGVD5hGWBPQQn3v/MdJ/z5HXblF8ewdsYYs/9ZoGJMHBER/u+Uvnw0+TRuOXsgPdu3rJh3Uv9OtG/dIoa1M8aY/c9uoW9MHGqdlsJPjz+QS3+YzWtLt/DowlVceUL1xyLN/ngdKcnCOUO6k5aSHIOaGmNM47IxKibuNacxKrVRVUQqhwwUlpTxwzvfYndBCZ0y07jsh725+JjetLNWFxM9NkbFxJx1/RiTIPxBCsCczzeyu6AEgO25Rfz19ZX88K63+ON/lrB6R34sqmiMMVFnLSom7lmLSmg5BSXM/mQdMz5Yzda9RVXmicDwQ7ow8YQ+HJXdrlqQY0yYbMcxMWeBiol7FqjUrri0nJe/2sSjC1ezfPPeavMP79GWa0/vzykDOsegdibBWaBiYs4G0xqT4FqkJDH6Bz34yREH8OH3O3l04Sre+abyidNfbchh/a6CGNbQGGPqzwIVY5oIEeG4vh05rm9Hvt2ay/RFq5nzxUZapiYz9sge1fJv21tI5zZ2AzljTHyzrh8T96zrp/525BXxzZZcju/bsUr6/9bu5vyHP2TUYd2YeMKBHN4jK0Y1NHHOun5MzFmLijFNWMeMNDr2TauWPn3RKsrKlXmLNzFv8SaOzm7PlSccyPBDupCUZL9Nxpj4YYGKMc1MaVk5e/eVVkn7ZM0uPlmziwM7tubyYQcy9gc9aNnCbiBnjIk96/oxcc+6fhrHko05TFu4ipe/2kxpedXvAXsQovFY85qJOQtUTNyzQKVxbc7Zx4wP1jD743XkFlZtaWmRksQHk06lY0b17iPTLFigYmLO7kxrTDPXrW1LJp95CB+GeBDisL4dLUgxxsSUjVExxgCQEepBiMMOrJbv2c/WA9iDEI0x+4V1/Zi4Z10/sRPqQYjD/vwOO/KK7EGIzYN1/ZiYs64fY0yNgp8R9NLiTezIc88V8j8I8ab/fG0PQjTGNAprUTFxz1pU4oc9CLHZsQ/RxJwFKibuWaASf8J5EOJvhvfj1IO7xKB2JoosUDExZ10/xpiIBR6EOP9Xw5h95TGcMqBTlflfbchh1faqXUElZeUs2ZjDrvxi7ATJGBMuu+rHGFNv/gchfrfNPQjxhc83kpacxAVH9aySd92uAs765yIA0lOT6Na2Jd3aplf+zUqne9uWdMtyaW1bpsZik4wxccYCFWNMVPTtnMmdow/n+jMGsGzTXjLTqwYaW3IKK/4vLCln9Y78GgfgdsxI47ObhldJW7sznw+/30m3rJZ0b5tOt6yWZKTZV5gxTZ0d5caYqOqYkcaJ/TtVSy8rV/p1zmBzTiF5RaUhlqzUrW312/Z/snoXk+Z8XSUtMy2logWme1Zl60y/LpkM6WlPhDamKbBAxRizX5zYvxNvXHcSAHsLS9iSU8imPfvYnFPI5j372JRT6NJy9tGnU+tqy2/2tcgE5BaVkrs1j5Vb86qknzGwC49cOrRK2rzFm/h49c4qXU7ds9Lp2jbdblxnTByzQMUYs9+1SU+lTXoq/btkhr3MgK6ZjD7iADbleMFNTiHFpeUh83bPalkt7YPvd/DUJ+tD5u/QukVly0zbdE4f2JVh/TqGXTdjTOOxQMUYkxBGDOrKiEFdK6ZVlV35xWz2t8zkFLI5Zx8/6N2u2vKhWmQCduYXszO/mCUb3aXW3bNaVgtUrn3mS75Yt5u0lGRapCSRlpJEWmoSLZKTqqS1SEnisuOyqwVhL/xvA2WqbrmU6sv4pztlppGabBdlGgMWqDQbItIemA6cAewAJqvq7BryXgvcCLQEXgCuUdWicMoRkdOAB4BewMfABFVd680T4C7gSi/7dOBGtWtVTT2ICB0y0uiQkcahB7StM//EE/pwYr9ObM5x3UybveBm695CyoP2wK4hxsgs3ZTDmp0FYdVtxKCu1QKVqa8sY09BSVjL//fXJ3BItzYV0yVl5fxg6hukpSRXBDotagx4kph67qFVBjPvLSxh9sfrXFCVmsSwvh3p3aF695ox8cgClebjAaAY6AIMAV4RkcWqutSfSURGAJOAU4FNwIvArV5areWISEdgDi4QmQdMBZ4BjvWWvQo4FxgMKPAGsAp4qDE22Bi/4/t25Pi+1btzSsvK2Z5XxKY9rjVm857CkANxN++puUUmWIuU6q0hNXVThZIWtHxxaTm5haXkUvsg5IDbzj20yvSuvGLu+u+Kiun7LzrCAhWTMCxQaQZEpDUwBjhUVfOARSLyEnAJlQFIwGXA9EAAIyJTgSeBSWGUMxpYqqrPectOAXaIyMGqusIr+x5V3eDNvweYiAUqJoZSkgP3dGkJVO8yCnjt2hMpLCmjqLSc4tJyikrLKSotq/i/2Dfdp2P1IGDskT0oKA4s7/4WlZRTXOaWq/i/pJz01KqDe4siCHKgeqATvLwNHjaJxAKV5qE/UKaqK31pi4GTQuQdBMwNytdFRDrgunNqK2eQNw2AquaLyPde+org+d7/g0JVWESuwrXA0KtXr7q2z5hGF2qAbiRuO+fQujPVIKtlKotvPsMFNEGBkT9wCqS1CBrfktUqlZ+d2Kdi2Z7tG7YtxuxPFqg0DxlATlBaDhDqkovgvIH/M8MoJwPYXsf84LIzRESCx6mo6iPAI+Ce9ROinsY0G0lJQttWqUD97tbbpU06k0cdEt1KGbOf2LDy5iEPaBOU1gbIDSNv4P/cMMqJdH4bIM8G0xpjjKmJBSrNw0ogRUT6+dIGA0tD5F3qzfPn26qqO8Mop8qy3piWg2qaX0sdjDHGGMAClWZBVfNxV+PcJiKtReR44BxgZojsTwBXiMhAEWkH3ATMCLOcF4FDRWSMiKQDNwNfeQNpA2VfJyIHiEh34PpA2cYYY0woFqg0Hz/H3RdlG/AU7t4oS0Wkl4jkiUgvAFV9FbgbeAdY671uqascb9ntuKuC/gTsBo4BxvmWfRh32fLXwBLgFS/NGGOMCUlseICJdyKyHRcw1aUj7iZ0TYFtS3xqKtsS7nbsUNWRjV0ZY2pjgYppMkTkM1UdWnfO+GfbEp+ayrY0le0wzYN1/RhjjDEmblmgYowxxpi4ZYGKaUoeiXUFosi2JT41lW1pKtthmgEbo2KMMcaYuGUtKsYYY4yJWxaoGGOMMSZuWaBijDHGmLhlgYpJeCLSXkReFJF8EVkrIhfFuk7hEpFfiMhnIlIkIjOC5p0mIitEpEBE3hGR3jGqZp1EJE1Epnvvf66IfCEiZ/rmJ8y2AIjILBHZLCJ7RWSliFzpm5dQ2wIgIv1EpFBEZvnSEm47TPNkgYppCh4AioEuwMXAv0RkUGyrFLZNwO3AY/5EEemIe67SH4H2wGfAM/u9duFLAdYDJwFtcfV+VkSyE3BbAO4EslW1DfBj4HYROTJBtwXcMfJpYCKBt8M0Q3bVj0lo3hOadwOHqupKL20msFFVJ8W0chEQkduBHqo6wZu+Cpigqsd5061xtzw/wveQx7gmIl8BtwIdSOBtEZEBwALg10AWCbYtIjIOGA0sA/qq6vimsH+Z5sNaVEyi6w+UBYIUz2IgUVpUajIItx1AxZOrvydBtktEuuA+m6Uk6LaIyIMiUgCsADYD80mwbRGRNsBtuCeV+yXUdpjmzQIVk+gygJygtBwgMwZ1iaaE3S4RSQWeBP7tnZ0n5Lao6s9xdTwB101SROJty1RguqquD0pPtO0wzZgFKibR5QFtgtLaALkxqEs0JeR2iUgSMBM3ZugXXnJCbguAqpap6iKgB3ANCbQtIjIEGA7cG2J2wmyHMRaomES3EkgRkX6+tMG4LodEthS3HUDFGIKDiOPtEhEBpuMGNY9R1RJvVsJtSwgpVNY5UbblZCAbWCciW4DfAmNE5HMSaztMM2eBikloXt/6HOA2EWktIscD5+DO6uOeiKSISDqQDCSLSLqIpAAvAoeKyBhv/s3AV3E+0PFfwCHA2aq6z5eeUNsiIp1FZJyIZIhIsoiMAC4E3iaxtuURXPAxxHs9BLwCjCCxtsM0cxaomKbg50BLYBvwFHCNqibKmeFNwD5gEjDe+/8mVd0OjAH+hLuq6RhgXKwqWRfvHhw/w/0gbhGRPO91caJtC6C4bp4NuPr+FfiNqs5NpG1R1QJV3RJ44bp7ClV1eyJthzF2ebIxxhhj4pa1qBhjjDEmblmgYowxxpi4ZYGKMcYYY+KWBSrGGGOMiVsWqBhjjDEmblmgYowxxpi4ZYGKMc2AiEwRkR2+6f5eWlYs62WMMXWxQMWY5qk/cAtggYoxJq5ZoGKMaTARaRnrOhhjmiYLVIxpZkTkZGCeN7laRFRE1vjm9xKRp0Vkl4gUiMhrIjLANz/bW+ZiEXlCRPb4ygteVyDv+SLysIjkiMgGEbnVe9JyIN8MEfmshmXP8qWpiFwrIveIyE4R2SEiv/XmXSYiq0Rkj4g85j3DxhiT4FJiXQFjzH73Oe5Jun8FRgObgSIAEWkPLAJ2AlcDBbjnEL0pIv2DHjb4V9wDIc8DyupY593AC8BY4DTcQ/CWAs/Wo/7X4x6udyFwFvAXEekMHAX8CugF3It7svZd9SjfGBNHLFAxpplR1b0i8o03+YWqrvHNvhZoDQxR1V0AIvI+sAa4HHjAl/cjVf2/MFf7nqpe7/3/hoiMxAVJ9QlUvlXVn3l1exMXKE0EeqvqXi/9ZOAnWKBiTMKzQMUY4zcceAPYKyKB74dc4H/A0KC8r0RQ7utB08twLR/18VbgH1UtF5HVQEEgSPF8BxxXz/KNMXHExqgYY/w6AhcAJUGvU4CeQXm3RlDunqDpYqC+Y0hClRXN8o0xccRaVIwxfruAl4CpIeblBk1rFNdbCLQISmsfxfKNMQnKAhVjmqdi729wq8NbwPnA0qCBs41tA5AtIumqWuilnb4f12+MiVPW9WNM8xQYTPszETlGRA7zpv+Ga9l4W0QuEpGTvEuLHxCRCxuxPv8BMoBpIjJcRG4AftqI6zPGJAgLVIxphlR1Le4S5dHA+3j3QVHVHcCxwArcJb6v4y4tbgt81Yj1WYK7quiHuK6nk7xpY0wzJ6rR7GY2xhhjjIkea1ExxhhjTNyyQMUYY4wxccsCFWOMMcbELQtUjDHGGBO3LFAxxhhjTNyyQMUYY4wxccsCFWOMMcbELQtUjDHGGBO3/h/PTPNivMA/RgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 288x216 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"41PmKTtTgRpR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1597610821241,"user_tz":-120,"elapsed":1365,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"c51195dd-6059-4a73-b9a7-64459b3c6e93"},"source":["plt.plot(step_list,loss_no_scale_bn['train'],'r-',label='BN_Train',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['train'],'b-',label='Modification_Train',linewidth = 2)\n","plt.plot(step_list,loss_no_scale_bn['test'],'r--',label='BN_Test',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['test'],'b--',label='Modification_Test',linewidth = 2)\n","plt.title('784-800-10; no scaling; with bn; full batch; train data size = 64')\n","plt.xlabel('Iter num')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.legend()\n","plt.savefig('/content/drive/My Drive/LCNN/newplots/attack2.pdf')"],"execution_count":193,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"FS67vaDAcT1O","colab_type":"text"},"source":["# NTK scaling, no batch norm"]},{"cell_type":"code","metadata":{"id":"T5-5-vXwcT1P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597606824223,"user_tz":-120,"elapsed":127011,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"6fd95262-001f-4d20-8412-ca97738ba17d"},"source":["step_list,  loss_ntk_scale_no_bn, train_loss = full_batch_train(initialize = 'NTK', batchnorm = False, learning_rate = 0.1 * np.sqrt(784), ** shared_model_param_dict)"],"execution_count":169,"outputs":[{"output_type":"stream","text":["Iter: 000/100 | Train Loss: 0.00237281\n","Iter: 001/100 | Train Loss: 0.00200517\n","Iter: 002/100 | Train Loss: 0.00167512\n","Iter: 003/100 | Train Loss: 0.00149027\n","Iter: 004/100 | Train Loss: 0.00147401\n","Iter: 005/100 | Train Loss: 0.00157495\n","Iter: 006/100 | Train Loss: 0.00170266\n","Iter: 007/100 | Train Loss: 0.00177413\n","Iter: 008/100 | Train Loss: 0.00174776\n","Iter: 009/100 | Train Loss: 0.00163276\n","Iter: 010/100 | Train Loss: 0.00147414\n","Iter: 011/100 | Train Loss: 0.00132576\n","Iter: 012/100 | Train Loss: 0.00122586\n","Iter: 013/100 | Train Loss: 0.00118550\n","Iter: 014/100 | Train Loss: 0.00119055\n","Iter: 015/100 | Train Loss: 0.00121295\n","Iter: 016/100 | Train Loss: 0.00122473\n","Iter: 017/100 | Train Loss: 0.00120870\n","Iter: 018/100 | Train Loss: 0.00116260\n","Iter: 019/100 | Train Loss: 0.00109682\n","Iter: 020/100 | Train Loss: 0.00102778\n","Iter: 021/100 | Train Loss: 0.00097037\n","Iter: 022/100 | Train Loss: 0.00093247\n","Iter: 023/100 | Train Loss: 0.00091368\n","Iter: 024/100 | Train Loss: 0.00090696\n","Iter: 025/100 | Train Loss: 0.00090270\n","Iter: 026/100 | Train Loss: 0.00089270\n","Iter: 027/100 | Train Loss: 0.00087313\n","Iter: 028/100 | Train Loss: 0.00084502\n","Iter: 029/100 | Train Loss: 0.00081274\n","Iter: 030/100 | Train Loss: 0.00078169\n","Iter: 031/100 | Train Loss: 0.00075609\n","Iter: 032/100 | Train Loss: 0.00073761\n","Iter: 033/100 | Train Loss: 0.00072524\n","Iter: 034/100 | Train Loss: 0.00071620\n","Iter: 035/100 | Train Loss: 0.00070736\n","Iter: 036/100 | Train Loss: 0.00069639\n","Iter: 037/100 | Train Loss: 0.00068243\n","Iter: 038/100 | Train Loss: 0.00066612\n","Iter: 039/100 | Train Loss: 0.00064911\n","Iter: 040/100 | Train Loss: 0.00063317\n","Iter: 041/100 | Train Loss: 0.00061953\n","Iter: 042/100 | Train Loss: 0.00060851\n","Iter: 043/100 | Train Loss: 0.00059954\n","Iter: 044/100 | Train Loss: 0.00059155\n","Iter: 045/100 | Train Loss: 0.00058349\n","Iter: 046/100 | Train Loss: 0.00057469\n","Iter: 047/100 | Train Loss: 0.00056505\n","Iter: 048/100 | Train Loss: 0.00055489\n","Iter: 049/100 | Train Loss: 0.00054480\n","Iter: 050/100 | Train Loss: 0.00053536\n","Iter: 051/100 | Train Loss: 0.00052687\n","Iter: 052/100 | Train Loss: 0.00051933\n","Iter: 053/100 | Train Loss: 0.00051250\n","Iter: 054/100 | Train Loss: 0.00050600\n","Iter: 055/100 | Train Loss: 0.00049948\n","Iter: 056/100 | Train Loss: 0.00049274\n","Iter: 057/100 | Train Loss: 0.00048579\n","Iter: 058/100 | Train Loss: 0.00047880\n","Iter: 059/100 | Train Loss: 0.00047196\n","Iter: 060/100 | Train Loss: 0.00046546\n","Iter: 061/100 | Train Loss: 0.00045937\n","Iter: 062/100 | Train Loss: 0.00045367\n","Iter: 063/100 | Train Loss: 0.00044823\n","Iter: 064/100 | Train Loss: 0.00044291\n","Iter: 065/100 | Train Loss: 0.00043762\n","Iter: 066/100 | Train Loss: 0.00043230\n","Iter: 067/100 | Train Loss: 0.00042700\n","Iter: 068/100 | Train Loss: 0.00042176\n","Iter: 069/100 | Train Loss: 0.00041668\n","Iter: 070/100 | Train Loss: 0.00041179\n","Iter: 071/100 | Train Loss: 0.00040710\n","Iter: 072/100 | Train Loss: 0.00040258\n","Iter: 073/100 | Train Loss: 0.00039819\n","Iter: 074/100 | Train Loss: 0.00039387\n","Iter: 075/100 | Train Loss: 0.00038958\n","Iter: 076/100 | Train Loss: 0.00038533\n","Iter: 077/100 | Train Loss: 0.00038114\n","Iter: 078/100 | Train Loss: 0.00037702\n","Iter: 079/100 | Train Loss: 0.00037300\n","Iter: 080/100 | Train Loss: 0.00036909\n","Iter: 081/100 | Train Loss: 0.00036528\n","Iter: 082/100 | Train Loss: 0.00036156\n","Iter: 083/100 | Train Loss: 0.00035791\n","Iter: 084/100 | Train Loss: 0.00035431\n","Iter: 085/100 | Train Loss: 0.00035076\n","Iter: 086/100 | Train Loss: 0.00034725\n","Iter: 087/100 | Train Loss: 0.00034379\n","Iter: 088/100 | Train Loss: 0.00034039\n","Iter: 089/100 | Train Loss: 0.00033706\n","Iter: 090/100 | Train Loss: 0.00033380\n","Iter: 091/100 | Train Loss: 0.00033060\n","Iter: 092/100 | Train Loss: 0.00032745\n","Iter: 093/100 | Train Loss: 0.00032434\n","Iter: 094/100 | Train Loss: 0.00032128\n","Iter: 095/100 | Train Loss: 0.00031826\n","Iter: 096/100 | Train Loss: 0.00031528\n","Iter: 097/100 | Train Loss: 0.00031235\n","Iter: 098/100 | Train Loss: 0.00030947\n","Iter: 099/100 | Train Loss: 0.00030663\n","\n","Iter: 099/100 | Test Loss: 0.00098671 | Test acc: 64.2100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z4YFOPAXw6nH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597606454841,"user_tz":-120,"elapsed":1340,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"fb4eb380-2187-42ba-faa6-e9f39def7582"},"source":["print(train_loss)"],"execution_count":163,"outputs":[{"output_type":"stream","text":["tensor(0.0003)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AE2-BKT2qi-w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597611235475,"user_tz":-120,"elapsed":328049,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"79238e12-f779-4003-9515-99eb67bcd18a"},"source":["# 1.2 0.53. runstep=2\n","maxstep=1000\n","for scale in range(10):\n","    scale=(scale-2)*0.05+0.9\n","    for therd in range(10):\n","        therd=(therd-2)*0.05+0.7\n","        print('scale:{:03f},therd:{:03f}'.format(scale,therd))\n","        step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(scale,therd,2,False,initialize = 'NTK', batchnorm = False, learning_rate = 0.1 * np.sqrt(784), ** shared_model_param_dict)\n","        if maxstep>train_loss:\n","            maxstep=train_loss\n","            sb=scale\n","            th=therd\n","print(maxstep,sb,th)"],"execution_count":194,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Iter: 066/100 | Train Loss: 0.00018588\n","Iter: 067/100 | Train Loss: 0.00018201\n","Iter: 068/100 | Train Loss: 0.00017814\n","Iter: 069/100 | Train Loss: 0.00017443\n","Iter: 070/100 | Train Loss: 0.00017096\n","Iter: 071/100 | Train Loss: 0.00016765\n","Iter: 072/100 | Train Loss: 0.00016435\n","Iter: 073/100 | Train Loss: 0.00016106\n","Iter: 074/100 | Train Loss: 0.00015785\n","Iter: 075/100 | Train Loss: 0.00015481\n","Iter: 076/100 | Train Loss: 0.00015190\n","Iter: 077/100 | Train Loss: 0.00014904\n","Iter: 078/100 | Train Loss: 0.00014620\n","Iter: 079/100 | Train Loss: 0.00014341\n","Iter: 080/100 | Train Loss: 0.00014073\n","Iter: 081/100 | Train Loss: 0.00013816\n","Iter: 082/100 | Train Loss: 0.00013565\n","Iter: 083/100 | Train Loss: 0.00013317\n","Iter: 084/100 | Train Loss: 0.00013073\n","Iter: 085/100 | Train Loss: 0.00012836\n","Iter: 086/100 | Train Loss: 0.00012609\n","Iter: 087/100 | Train Loss: 0.00012387\n","Iter: 088/100 | Train Loss: 0.00012169\n","Iter: 089/100 | Train Loss: 0.00011955\n","Iter: 090/100 | Train Loss: 0.00011746\n","Iter: 091/100 | Train Loss: 0.00011543\n","Iter: 092/100 | Train Loss: 0.00011346\n","Iter: 093/100 | Train Loss: 0.00011153\n","Iter: 094/100 | Train Loss: 0.00010963\n","Iter: 095/100 | Train Loss: 0.00010778\n","Iter: 096/100 | Train Loss: 0.00010597\n","Iter: 097/100 | Train Loss: 0.00010421\n","Iter: 098/100 | Train Loss: 0.00010249\n","Iter: 099/100 | Train Loss: 0.00010080\n","\n","Iter: 099/100 | Test Loss: 0.00102142 | Test acc: 64.8800\n","scale:1.050000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00235675\n","Iter: 001/100 | Train Loss: 0.00197286\n","Iter: 002/100 | Train Loss: 0.00164250\n","Iter: 003/100 | Train Loss: 0.00147839\n","Iter: 004/100 | Train Loss: 0.00149470\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 005/100 | Train Loss: 0.00163388\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 006/100 | Train Loss: 0.00177174\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 007/100 | Train Loss: 0.00177203\n","Adjusting Layer 1, Kernel Nodes: 652, Adptive Nodes:148\n","Iter: 008/100 | Train Loss: 0.00158855\n","Iter: 009/100 | Train Loss: 0.00136182\n","Iter: 010/100 | Train Loss: 0.00123419\n","Iter: 011/100 | Train Loss: 0.00123849\n","Adjusting Layer 1, Kernel Nodes: 372, Adptive Nodes:428\n","Iter: 012/100 | Train Loss: 0.00129036\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 013/100 | Train Loss: 0.00128897\n","Iter: 014/100 | Train Loss: 0.00118433\n","Iter: 015/100 | Train Loss: 0.00103819\n","Iter: 016/100 | Train Loss: 0.00094074\n","Iter: 017/100 | Train Loss: 0.00092134\n","Iter: 018/100 | Train Loss: 0.00093374\n","Adjusting Layer 1, Kernel Nodes: 648, Adptive Nodes:152\n","Iter: 019/100 | Train Loss: 0.00090733\n","Iter: 020/100 | Train Loss: 0.00082655\n","Iter: 021/100 | Train Loss: 0.00074208\n","Iter: 022/100 | Train Loss: 0.00070164\n","Iter: 023/100 | Train Loss: 0.00069864\n","Iter: 024/100 | Train Loss: 0.00069071\n","Iter: 025/100 | Train Loss: 0.00065347\n","Iter: 026/100 | Train Loss: 0.00060255\n","Iter: 027/100 | Train Loss: 0.00056777\n","Iter: 028/100 | Train Loss: 0.00055732\n","Iter: 029/100 | Train Loss: 0.00055339\n","Iter: 030/100 | Train Loss: 0.00053701\n","Iter: 031/100 | Train Loss: 0.00050851\n","Iter: 032/100 | Train Loss: 0.00048254\n","Iter: 033/100 | Train Loss: 0.00046886\n","Iter: 034/100 | Train Loss: 0.00046290\n","Iter: 035/100 | Train Loss: 0.00045384\n","Iter: 036/100 | Train Loss: 0.00043749\n","Iter: 037/100 | Train Loss: 0.00041908\n","Iter: 038/100 | Train Loss: 0.00040539\n","Iter: 039/100 | Train Loss: 0.00039721\n","Iter: 040/100 | Train Loss: 0.00038991\n","Iter: 041/100 | Train Loss: 0.00037955\n","Iter: 042/100 | Train Loss: 0.00036678\n","Iter: 043/100 | Train Loss: 0.00035501\n","Iter: 044/100 | Train Loss: 0.00034620\n","Iter: 045/100 | Train Loss: 0.00033915\n","Iter: 046/100 | Train Loss: 0.00033154\n","Iter: 047/100 | Train Loss: 0.00032252\n","Iter: 048/100 | Train Loss: 0.00031320\n","Iter: 049/100 | Train Loss: 0.00030505\n","Iter: 050/100 | Train Loss: 0.00029832\n","Iter: 051/100 | Train Loss: 0.00029207\n","Iter: 052/100 | Train Loss: 0.00028535\n","Iter: 053/100 | Train Loss: 0.00027817\n","Iter: 054/100 | Train Loss: 0.00027126\n","Iter: 055/100 | Train Loss: 0.00026515\n","Iter: 056/100 | Train Loss: 0.00025970\n","Iter: 057/100 | Train Loss: 0.00025434\n","Iter: 058/100 | Train Loss: 0.00024875\n","Iter: 059/100 | Train Loss: 0.00024309\n","Iter: 060/100 | Train Loss: 0.00023777\n","Iter: 061/100 | Train Loss: 0.00023297\n","Iter: 062/100 | Train Loss: 0.00022847\n","Iter: 063/100 | Train Loss: 0.00022396\n","Iter: 064/100 | Train Loss: 0.00021936\n","Iter: 065/100 | Train Loss: 0.00021486\n","Iter: 066/100 | Train Loss: 0.00021066\n","Iter: 067/100 | Train Loss: 0.00020676\n","Iter: 068/100 | Train Loss: 0.00020298\n","Iter: 069/100 | Train Loss: 0.00019917\n","Iter: 070/100 | Train Loss: 0.00019536\n","Iter: 071/100 | Train Loss: 0.00019170\n","Iter: 072/100 | Train Loss: 0.00018827\n","Iter: 073/100 | Train Loss: 0.00018498\n","Iter: 074/100 | Train Loss: 0.00018172\n","Iter: 075/100 | Train Loss: 0.00017845\n","Iter: 076/100 | Train Loss: 0.00017525\n","Iter: 077/100 | Train Loss: 0.00017219\n","Iter: 078/100 | Train Loss: 0.00016927\n","Iter: 079/100 | Train Loss: 0.00016641\n","Iter: 080/100 | Train Loss: 0.00016356\n","Iter: 081/100 | Train Loss: 0.00016075\n","Iter: 082/100 | Train Loss: 0.00015802\n","Iter: 083/100 | Train Loss: 0.00015540\n","Iter: 084/100 | Train Loss: 0.00015286\n","Iter: 085/100 | Train Loss: 0.00015035\n","Iter: 086/100 | Train Loss: 0.00014787\n","Iter: 087/100 | Train Loss: 0.00014544\n","Iter: 088/100 | Train Loss: 0.00014309\n","Iter: 089/100 | Train Loss: 0.00014082\n","Iter: 090/100 | Train Loss: 0.00013858\n","Iter: 091/100 | Train Loss: 0.00013638\n","Iter: 092/100 | Train Loss: 0.00013422\n","Iter: 093/100 | Train Loss: 0.00013211\n","Iter: 094/100 | Train Loss: 0.00013006\n","Iter: 095/100 | Train Loss: 0.00012806\n","Iter: 096/100 | Train Loss: 0.00012609\n","Iter: 097/100 | Train Loss: 0.00012415\n","Iter: 098/100 | Train Loss: 0.00012226\n","Iter: 099/100 | Train Loss: 0.00012041\n","\n","Iter: 099/100 | Test Loss: 0.00101159 | Test acc: 65.0300\n","scale:1.050000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00236528\n","Iter: 001/100 | Train Loss: 0.00198989\n","Iter: 002/100 | Train Loss: 0.00165945\n","Iter: 003/100 | Train Loss: 0.00148430\n","Iter: 004/100 | Train Loss: 0.00148373\n","Iter: 005/100 | Train Loss: 0.00159782\n","Adjusting Layer 1, Kernel Nodes: 746, Adptive Nodes:54\n","Iter: 006/100 | Train Loss: 0.00173245\n","Adjusting Layer 1, Kernel Nodes: 691, Adptive Nodes:109\n","Iter: 007/100 | Train Loss: 0.00178912\n","Adjusting Layer 1, Kernel Nodes: 675, Adptive Nodes:125\n","Iter: 008/100 | Train Loss: 0.00171310\n","Iter: 009/100 | Train Loss: 0.00154341\n","Iter: 010/100 | Train Loss: 0.00136446\n","Iter: 011/100 | Train Loss: 0.00124800\n","Iter: 012/100 | Train Loss: 0.00121675\n","Iter: 013/100 | Train Loss: 0.00124347\n","Adjusting Layer 1, Kernel Nodes: 579, Adptive Nodes:221\n","Iter: 014/100 | Train Loss: 0.00127658\n","Adjusting Layer 1, Kernel Nodes: 648, Adptive Nodes:152\n","Iter: 015/100 | Train Loss: 0.00126681\n","Iter: 016/100 | Train Loss: 0.00119923\n","Iter: 017/100 | Train Loss: 0.00110012\n","Iter: 018/100 | Train Loss: 0.00101091\n","Iter: 019/100 | Train Loss: 0.00095938\n","Iter: 020/100 | Train Loss: 0.00094557\n","Iter: 021/100 | Train Loss: 0.00094728\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 022/100 | Train Loss: 0.00093739\n","Iter: 023/100 | Train Loss: 0.00090267\n","Iter: 024/100 | Train Loss: 0.00084987\n","Iter: 025/100 | Train Loss: 0.00079734\n","Iter: 026/100 | Train Loss: 0.00076048\n","Iter: 027/100 | Train Loss: 0.00074226\n","Iter: 028/100 | Train Loss: 0.00073392\n","Iter: 029/100 | Train Loss: 0.00072324\n","Iter: 030/100 | Train Loss: 0.00070306\n","Iter: 031/100 | Train Loss: 0.00067487\n","Iter: 032/100 | Train Loss: 0.00064590\n","Iter: 033/100 | Train Loss: 0.00062307\n","Iter: 034/100 | Train Loss: 0.00060850\n","Iter: 035/100 | Train Loss: 0.00059914\n","Iter: 036/100 | Train Loss: 0.00058989\n","Iter: 037/100 | Train Loss: 0.00057725\n","Iter: 038/100 | Train Loss: 0.00056120\n","Iter: 039/100 | Train Loss: 0.00054436\n","Iter: 040/100 | Train Loss: 0.00052971\n","Iter: 041/100 | Train Loss: 0.00051854\n","Iter: 042/100 | Train Loss: 0.00050995\n","Iter: 043/100 | Train Loss: 0.00050196\n","Iter: 044/100 | Train Loss: 0.00049298\n","Iter: 045/100 | Train Loss: 0.00048269\n","Iter: 046/100 | Train Loss: 0.00047195\n","Iter: 047/100 | Train Loss: 0.00046194\n","Iter: 048/100 | Train Loss: 0.00045333\n","Iter: 049/100 | Train Loss: 0.00044597\n","Iter: 050/100 | Train Loss: 0.00043915\n","Iter: 051/100 | Train Loss: 0.00043216\n","Iter: 052/100 | Train Loss: 0.00042477\n","Iter: 053/100 | Train Loss: 0.00041717\n","Iter: 054/100 | Train Loss: 0.00040980\n","Iter: 055/100 | Train Loss: 0.00040297\n","Iter: 056/100 | Train Loss: 0.00039670\n","Iter: 057/100 | Train Loss: 0.00039082\n","Iter: 058/100 | Train Loss: 0.00038504\n","Iter: 059/100 | Train Loss: 0.00037921\n","Iter: 060/100 | Train Loss: 0.00037333\n","Iter: 061/100 | Train Loss: 0.00036754\n","Iter: 062/100 | Train Loss: 0.00036197\n","Iter: 063/100 | Train Loss: 0.00035667\n","Iter: 064/100 | Train Loss: 0.00035163\n","Iter: 065/100 | Train Loss: 0.00034675\n","Iter: 066/100 | Train Loss: 0.00034192\n","Iter: 067/100 | Train Loss: 0.00033712\n","Iter: 068/100 | Train Loss: 0.00033236\n","Iter: 069/100 | Train Loss: 0.00032770\n","Iter: 070/100 | Train Loss: 0.00032320\n","Iter: 071/100 | Train Loss: 0.00031887\n","Iter: 072/100 | Train Loss: 0.00031466\n","Iter: 073/100 | Train Loss: 0.00031054\n","Iter: 074/100 | Train Loss: 0.00030646\n","Iter: 075/100 | Train Loss: 0.00030242\n","Iter: 076/100 | Train Loss: 0.00029845\n","Iter: 077/100 | Train Loss: 0.00029458\n","Iter: 078/100 | Train Loss: 0.00029081\n","Iter: 079/100 | Train Loss: 0.00028715\n","Iter: 080/100 | Train Loss: 0.00028357\n","Iter: 081/100 | Train Loss: 0.00028004\n","Iter: 082/100 | Train Loss: 0.00027657\n","Iter: 083/100 | Train Loss: 0.00027314\n","Iter: 084/100 | Train Loss: 0.00026978\n","Iter: 085/100 | Train Loss: 0.00026650\n","Iter: 086/100 | Train Loss: 0.00026330\n","Iter: 087/100 | Train Loss: 0.00026016\n","Iter: 088/100 | Train Loss: 0.00025708\n","Iter: 089/100 | Train Loss: 0.00025403\n","Iter: 090/100 | Train Loss: 0.00025104\n","Iter: 091/100 | Train Loss: 0.00024810\n","Iter: 092/100 | Train Loss: 0.00024522\n","Iter: 093/100 | Train Loss: 0.00024240\n","Iter: 094/100 | Train Loss: 0.00023962\n","Iter: 095/100 | Train Loss: 0.00023690\n","Iter: 096/100 | Train Loss: 0.00023421\n","Iter: 097/100 | Train Loss: 0.00023156\n","Iter: 098/100 | Train Loss: 0.00022896\n","Iter: 099/100 | Train Loss: 0.00022640\n","\n","Iter: 099/100 | Test Loss: 0.00098559 | Test acc: 65.1800\n","scale:1.050000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00237046\n","Iter: 001/100 | Train Loss: 0.00200039\n","Iter: 002/100 | Train Loss: 0.00167020\n","Iter: 003/100 | Train Loss: 0.00148847\n","Iter: 004/100 | Train Loss: 0.00147735\n","Iter: 005/100 | Train Loss: 0.00158282\n","Adjusting Layer 1, Kernel Nodes: 758, Adptive Nodes:42\n","Iter: 006/100 | Train Loss: 0.00171383\n","Adjusting Layer 1, Kernel Nodes: 730, Adptive Nodes:70\n","Iter: 007/100 | Train Loss: 0.00178246\n","Adjusting Layer 1, Kernel Nodes: 707, Adptive Nodes:93\n","Iter: 008/100 | Train Loss: 0.00174250\n","Iter: 009/100 | Train Loss: 0.00160991\n","Iter: 010/100 | Train Loss: 0.00144163\n","Iter: 011/100 | Train Loss: 0.00129915\n","Iter: 012/100 | Train Loss: 0.00121952\n","Iter: 013/100 | Train Loss: 0.00120432\n","Iter: 014/100 | Train Loss: 0.00122745\n","Adjusting Layer 1, Kernel Nodes: 692, Adptive Nodes:108\n","Iter: 015/100 | Train Loss: 0.00125288\n","Adjusting Layer 1, Kernel Nodes: 644, Adptive Nodes:156\n","Iter: 016/100 | Train Loss: 0.00124923\n","Iter: 017/100 | Train Loss: 0.00120555\n","Iter: 018/100 | Train Loss: 0.00113286\n","Iter: 019/100 | Train Loss: 0.00105403\n","Iter: 020/100 | Train Loss: 0.00099073\n","Iter: 021/100 | Train Loss: 0.00095385\n","Iter: 022/100 | Train Loss: 0.00094061\n","Iter: 023/100 | Train Loss: 0.00093868\n","Iter: 024/100 | Train Loss: 0.00093378\n","Iter: 025/100 | Train Loss: 0.00091653\n","Iter: 026/100 | Train Loss: 0.00088578\n","Iter: 027/100 | Train Loss: 0.00084734\n","Iter: 028/100 | Train Loss: 0.00080987\n","Iter: 029/100 | Train Loss: 0.00078027\n","Iter: 030/100 | Train Loss: 0.00076092\n","Iter: 031/100 | Train Loss: 0.00074961\n","Iter: 032/100 | Train Loss: 0.00074134\n","Iter: 033/100 | Train Loss: 0.00073123\n","Iter: 034/100 | Train Loss: 0.00071665\n","Iter: 035/100 | Train Loss: 0.00069785\n","Iter: 036/100 | Train Loss: 0.00067735\n","Iter: 037/100 | Train Loss: 0.00065818\n","Iter: 038/100 | Train Loss: 0.00064244\n","Iter: 039/100 | Train Loss: 0.00063058\n","Iter: 040/100 | Train Loss: 0.00062145\n","Iter: 041/100 | Train Loss: 0.00061318\n","Iter: 042/100 | Train Loss: 0.00060412\n","Iter: 043/100 | Train Loss: 0.00059354\n","Iter: 044/100 | Train Loss: 0.00058175\n","Iter: 045/100 | Train Loss: 0.00056974\n","Iter: 046/100 | Train Loss: 0.00055854\n","Iter: 047/100 | Train Loss: 0.00054880\n","Iter: 048/100 | Train Loss: 0.00054050\n","Iter: 049/100 | Train Loss: 0.00053315\n","Iter: 050/100 | Train Loss: 0.00052605\n","Iter: 051/100 | Train Loss: 0.00051872\n","Iter: 052/100 | Train Loss: 0.00051096\n","Iter: 053/100 | Train Loss: 0.00050295\n","Iter: 054/100 | Train Loss: 0.00049504\n","Iter: 055/100 | Train Loss: 0.00048756\n","Iter: 056/100 | Train Loss: 0.00048068\n","Iter: 057/100 | Train Loss: 0.00047434\n","Iter: 058/100 | Train Loss: 0.00046836\n","Iter: 059/100 | Train Loss: 0.00046251\n","Iter: 060/100 | Train Loss: 0.00045663\n","Iter: 061/100 | Train Loss: 0.00045069\n","Iter: 062/100 | Train Loss: 0.00044475\n","Iter: 063/100 | Train Loss: 0.00043895\n","Iter: 064/100 | Train Loss: 0.00043336\n","Iter: 065/100 | Train Loss: 0.00042804\n","Iter: 066/100 | Train Loss: 0.00042296\n","Iter: 067/100 | Train Loss: 0.00041804\n","Iter: 068/100 | Train Loss: 0.00041322\n","Iter: 069/100 | Train Loss: 0.00040843\n","Iter: 070/100 | Train Loss: 0.00040367\n","Iter: 071/100 | Train Loss: 0.00039898\n","Iter: 072/100 | Train Loss: 0.00039439\n","Iter: 073/100 | Train Loss: 0.00038991\n","Iter: 074/100 | Train Loss: 0.00038557\n","Iter: 075/100 | Train Loss: 0.00038134\n","Iter: 076/100 | Train Loss: 0.00037722\n","Iter: 077/100 | Train Loss: 0.00037318\n","Iter: 078/100 | Train Loss: 0.00036919\n","Iter: 079/100 | Train Loss: 0.00036525\n","Iter: 080/100 | Train Loss: 0.00036137\n","Iter: 081/100 | Train Loss: 0.00035757\n","Iter: 082/100 | Train Loss: 0.00035383\n","Iter: 083/100 | Train Loss: 0.00035017\n","Iter: 084/100 | Train Loss: 0.00034659\n","Iter: 085/100 | Train Loss: 0.00034306\n","Iter: 086/100 | Train Loss: 0.00033959\n","Iter: 087/100 | Train Loss: 0.00033617\n","Iter: 088/100 | Train Loss: 0.00033280\n","Iter: 089/100 | Train Loss: 0.00032949\n","Iter: 090/100 | Train Loss: 0.00032622\n","Iter: 091/100 | Train Loss: 0.00032300\n","Iter: 092/100 | Train Loss: 0.00031985\n","Iter: 093/100 | Train Loss: 0.00031674\n","Iter: 094/100 | Train Loss: 0.00031369\n","Iter: 095/100 | Train Loss: 0.00031068\n","Iter: 096/100 | Train Loss: 0.00030772\n","Iter: 097/100 | Train Loss: 0.00030480\n","Iter: 098/100 | Train Loss: 0.00030192\n","Iter: 099/100 | Train Loss: 0.00029908\n","\n","Iter: 099/100 | Test Loss: 0.00098750 | Test acc: 64.5700\n","scale:1.050000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00237281\n","Iter: 001/100 | Train Loss: 0.00200517\n","Iter: 002/100 | Train Loss: 0.00167512\n","Iter: 003/100 | Train Loss: 0.00149027\n","Iter: 004/100 | Train Loss: 0.00147401\n","Iter: 005/100 | Train Loss: 0.00157495\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00170266\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00177413\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 008/100 | Train Loss: 0.00174776\n","Iter: 009/100 | Train Loss: 0.00163275\n","Iter: 010/100 | Train Loss: 0.00147414\n","Iter: 011/100 | Train Loss: 0.00132576\n","Iter: 012/100 | Train Loss: 0.00122586\n","Iter: 013/100 | Train Loss: 0.00118550\n","Iter: 014/100 | Train Loss: 0.00119055\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 015/100 | Train Loss: 0.00121295\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 016/100 | Train Loss: 0.00122473\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00120870\n","Iter: 018/100 | Train Loss: 0.00116260\n","Iter: 019/100 | Train Loss: 0.00109682\n","Iter: 020/100 | Train Loss: 0.00102778\n","Iter: 021/100 | Train Loss: 0.00097037\n","Iter: 022/100 | Train Loss: 0.00093247\n","Iter: 023/100 | Train Loss: 0.00091368\n","Iter: 024/100 | Train Loss: 0.00090696\n","Iter: 025/100 | Train Loss: 0.00090270\n","Iter: 026/100 | Train Loss: 0.00089270\n","Iter: 027/100 | Train Loss: 0.00087313\n","Iter: 028/100 | Train Loss: 0.00084502\n","Iter: 029/100 | Train Loss: 0.00081274\n","Iter: 030/100 | Train Loss: 0.00078169\n","Iter: 031/100 | Train Loss: 0.00075609\n","Iter: 032/100 | Train Loss: 0.00073761\n","Iter: 033/100 | Train Loss: 0.00072524\n","Iter: 034/100 | Train Loss: 0.00071620\n","Iter: 035/100 | Train Loss: 0.00070736\n","Iter: 036/100 | Train Loss: 0.00069639\n","Iter: 037/100 | Train Loss: 0.00068243\n","Iter: 038/100 | Train Loss: 0.00066612\n","Iter: 039/100 | Train Loss: 0.00064911\n","Iter: 040/100 | Train Loss: 0.00063317\n","Iter: 041/100 | Train Loss: 0.00061953\n","Iter: 042/100 | Train Loss: 0.00060851\n","Iter: 043/100 | Train Loss: 0.00059954\n","Iter: 044/100 | Train Loss: 0.00059155\n","Iter: 045/100 | Train Loss: 0.00058349\n","Iter: 046/100 | Train Loss: 0.00057469\n","Iter: 047/100 | Train Loss: 0.00056505\n","Iter: 048/100 | Train Loss: 0.00055489\n","Iter: 049/100 | Train Loss: 0.00054480\n","Iter: 050/100 | Train Loss: 0.00053536\n","Iter: 051/100 | Train Loss: 0.00052687\n","Iter: 052/100 | Train Loss: 0.00051933\n","Iter: 053/100 | Train Loss: 0.00051250\n","Iter: 054/100 | Train Loss: 0.00050600\n","Iter: 055/100 | Train Loss: 0.00049948\n","Iter: 056/100 | Train Loss: 0.00049274\n","Iter: 057/100 | Train Loss: 0.00048579\n","Iter: 058/100 | Train Loss: 0.00047880\n","Iter: 059/100 | Train Loss: 0.00047196\n","Iter: 060/100 | Train Loss: 0.00046545\n","Iter: 061/100 | Train Loss: 0.00045937\n","Iter: 062/100 | Train Loss: 0.00045367\n","Iter: 063/100 | Train Loss: 0.00044823\n","Iter: 064/100 | Train Loss: 0.00044291\n","Iter: 065/100 | Train Loss: 0.00043762\n","Iter: 066/100 | Train Loss: 0.00043230\n","Iter: 067/100 | Train Loss: 0.00042700\n","Iter: 068/100 | Train Loss: 0.00042176\n","Iter: 069/100 | Train Loss: 0.00041668\n","Iter: 070/100 | Train Loss: 0.00041179\n","Iter: 071/100 | Train Loss: 0.00040710\n","Iter: 072/100 | Train Loss: 0.00040258\n","Iter: 073/100 | Train Loss: 0.00039819\n","Iter: 074/100 | Train Loss: 0.00039387\n","Iter: 075/100 | Train Loss: 0.00038958\n","Iter: 076/100 | Train Loss: 0.00038533\n","Iter: 077/100 | Train Loss: 0.00038114\n","Iter: 078/100 | Train Loss: 0.00037702\n","Iter: 079/100 | Train Loss: 0.00037300\n","Iter: 080/100 | Train Loss: 0.00036909\n","Iter: 081/100 | Train Loss: 0.00036528\n","Iter: 082/100 | Train Loss: 0.00036156\n","Iter: 083/100 | Train Loss: 0.00035791\n","Iter: 084/100 | Train Loss: 0.00035431\n","Iter: 085/100 | Train Loss: 0.00035076\n","Iter: 086/100 | Train Loss: 0.00034725\n","Iter: 087/100 | Train Loss: 0.00034379\n","Iter: 088/100 | Train Loss: 0.00034039\n","Iter: 089/100 | Train Loss: 0.00033706\n","Iter: 090/100 | Train Loss: 0.00033380\n","Iter: 091/100 | Train Loss: 0.00033060\n","Iter: 092/100 | Train Loss: 0.00032745\n","Iter: 093/100 | Train Loss: 0.00032434\n","Iter: 094/100 | Train Loss: 0.00032128\n","Iter: 095/100 | Train Loss: 0.00031826\n","Iter: 096/100 | Train Loss: 0.00031528\n","Iter: 097/100 | Train Loss: 0.00031235\n","Iter: 098/100 | Train Loss: 0.00030947\n","Iter: 099/100 | Train Loss: 0.00030663\n","\n","Iter: 099/100 | Test Loss: 0.00098671 | Test acc: 64.2000\n","scale:1.050000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00237513\n","Iter: 001/100 | Train Loss: 0.00200979\n","Iter: 002/100 | Train Loss: 0.00167977\n","Iter: 003/100 | Train Loss: 0.00149144\n","Iter: 004/100 | Train Loss: 0.00146956\n","Iter: 005/100 | Train Loss: 0.00156533\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00168952\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00175960\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 008/100 | Train Loss: 0.00173163\n","Iter: 009/100 | Train Loss: 0.00161543\n","Iter: 010/100 | Train Loss: 0.00145599\n","Iter: 011/100 | Train Loss: 0.00130687\n","Iter: 012/100 | Train Loss: 0.00120552\n","Iter: 013/100 | Train Loss: 0.00116211\n","Iter: 014/100 | Train Loss: 0.00116277\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00117961\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 016/100 | Train Loss: 0.00118500\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 017/100 | Train Loss: 0.00116069\n","Iter: 018/100 | Train Loss: 0.00110648\n","Iter: 019/100 | Train Loss: 0.00103504\n","Iter: 020/100 | Train Loss: 0.00096394\n","Iter: 021/100 | Train Loss: 0.00090774\n","Iter: 022/100 | Train Loss: 0.00087244\n","Iter: 023/100 | Train Loss: 0.00085476\n","Iter: 024/100 | Train Loss: 0.00084580\n","Iter: 025/100 | Train Loss: 0.00083583\n","Iter: 026/100 | Train Loss: 0.00081822\n","Iter: 027/100 | Train Loss: 0.00079154\n","Iter: 028/100 | Train Loss: 0.00075894\n","Iter: 029/100 | Train Loss: 0.00072574\n","Iter: 030/100 | Train Loss: 0.00069680\n","Iter: 031/100 | Train Loss: 0.00067467\n","Iter: 032/100 | Train Loss: 0.00065902\n","Iter: 033/100 | Train Loss: 0.00064754\n","Iter: 034/100 | Train Loss: 0.00063710\n","Iter: 035/100 | Train Loss: 0.00062521\n","Iter: 036/100 | Train Loss: 0.00061080\n","Iter: 037/100 | Train Loss: 0.00059425\n","Iter: 038/100 | Train Loss: 0.00057693\n","Iter: 039/100 | Train Loss: 0.00056052\n","Iter: 040/100 | Train Loss: 0.00054632\n","Iter: 041/100 | Train Loss: 0.00053469\n","Iter: 042/100 | Train Loss: 0.00052514\n","Iter: 043/100 | Train Loss: 0.00051665\n","Iter: 044/100 | Train Loss: 0.00050820\n","Iter: 045/100 | Train Loss: 0.00049906\n","Iter: 046/100 | Train Loss: 0.00048919\n","Iter: 047/100 | Train Loss: 0.00047893\n","Iter: 048/100 | Train Loss: 0.00046892\n","Iter: 049/100 | Train Loss: 0.00045972\n","Iter: 050/100 | Train Loss: 0.00045155\n","Iter: 051/100 | Train Loss: 0.00044432\n","Iter: 052/100 | Train Loss: 0.00043772\n","Iter: 053/100 | Train Loss: 0.00043132\n","Iter: 054/100 | Train Loss: 0.00042480\n","Iter: 055/100 | Train Loss: 0.00041803\n","Iter: 056/100 | Train Loss: 0.00041112\n","Iter: 057/100 | Train Loss: 0.00040429\n","Iter: 058/100 | Train Loss: 0.00039778\n","Iter: 059/100 | Train Loss: 0.00039172\n","Iter: 060/100 | Train Loss: 0.00038612\n","Iter: 061/100 | Train Loss: 0.00038085\n","Iter: 062/100 | Train Loss: 0.00037575\n","Iter: 063/100 | Train Loss: 0.00037064\n","Iter: 064/100 | Train Loss: 0.00036545\n","Iter: 065/100 | Train Loss: 0.00036023\n","Iter: 066/100 | Train Loss: 0.00035507\n","Iter: 067/100 | Train Loss: 0.00035007\n","Iter: 068/100 | Train Loss: 0.00034532\n","Iter: 069/100 | Train Loss: 0.00034080\n","Iter: 070/100 | Train Loss: 0.00033645\n","Iter: 071/100 | Train Loss: 0.00033220\n","Iter: 072/100 | Train Loss: 0.00032799\n","Iter: 073/100 | Train Loss: 0.00032380\n","Iter: 074/100 | Train Loss: 0.00031965\n","Iter: 075/100 | Train Loss: 0.00031560\n","Iter: 076/100 | Train Loss: 0.00031165\n","Iter: 077/100 | Train Loss: 0.00030781\n","Iter: 078/100 | Train Loss: 0.00030409\n","Iter: 079/100 | Train Loss: 0.00030048\n","Iter: 080/100 | Train Loss: 0.00029694\n","Iter: 081/100 | Train Loss: 0.00029346\n","Iter: 082/100 | Train Loss: 0.00029003\n","Iter: 083/100 | Train Loss: 0.00028665\n","Iter: 084/100 | Train Loss: 0.00028333\n","Iter: 085/100 | Train Loss: 0.00028007\n","Iter: 086/100 | Train Loss: 0.00027687\n","Iter: 087/100 | Train Loss: 0.00027375\n","Iter: 088/100 | Train Loss: 0.00027069\n","Iter: 089/100 | Train Loss: 0.00026768\n","Iter: 090/100 | Train Loss: 0.00026471\n","Iter: 091/100 | Train Loss: 0.00026177\n","Iter: 092/100 | Train Loss: 0.00025888\n","Iter: 093/100 | Train Loss: 0.00025604\n","Iter: 094/100 | Train Loss: 0.00025325\n","Iter: 095/100 | Train Loss: 0.00025051\n","Iter: 096/100 | Train Loss: 0.00024782\n","Iter: 097/100 | Train Loss: 0.00024518\n","Iter: 098/100 | Train Loss: 0.00024258\n","Iter: 099/100 | Train Loss: 0.00024001\n","\n","Iter: 099/100 | Test Loss: 0.00097254 | Test acc: 64.8800\n","scale:1.100000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00223584\n","Iter: 001/100 | Train Loss: 0.00175930\n","Iter: 002/100 | Train Loss: 0.00149062\n","Iter: 003/100 | Train Loss: 0.00152734\n","Adjusting Layer 1, Kernel Nodes: 607, Adptive Nodes:193\n","Iter: 004/100 | Train Loss: 0.00176017\n","Adjusting Layer 1, Kernel Nodes: 428, Adptive Nodes:372\n","Iter: 005/100 | Train Loss: 0.00184703\n","Adjusting Layer 1, Kernel Nodes: 339, Adptive Nodes:461\n","Iter: 006/100 | Train Loss: 0.00164374\n","Iter: 007/100 | Train Loss: 0.00135522\n","Iter: 008/100 | Train Loss: 0.00120680\n","Iter: 009/100 | Train Loss: 0.00122841\n","Adjusting Layer 1, Kernel Nodes: 447, Adptive Nodes:353\n","Iter: 010/100 | Train Loss: 0.00126285\n","Adjusting Layer 1, Kernel Nodes: 356, Adptive Nodes:444\n","Iter: 011/100 | Train Loss: 0.00124577\n","Iter: 012/100 | Train Loss: 0.00112903\n","Iter: 013/100 | Train Loss: 0.00098533\n","Iter: 014/100 | Train Loss: 0.00089979\n","Iter: 015/100 | Train Loss: 0.00089077\n","Iter: 016/100 | Train Loss: 0.00090659\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 017/100 | Train Loss: 0.00088319\n","Iter: 018/100 | Train Loss: 0.00081638\n","Iter: 019/100 | Train Loss: 0.00073857\n","Iter: 020/100 | Train Loss: 0.00068706\n","Iter: 021/100 | Train Loss: 0.00067141\n","Iter: 022/100 | Train Loss: 0.00067172\n","Adjusting Layer 1, Kernel Nodes: 348, Adptive Nodes:452\n","Iter: 023/100 | Train Loss: 0.00065596\n","Iter: 024/100 | Train Loss: 0.00062212\n","Iter: 025/100 | Train Loss: 0.00058483\n","Iter: 026/100 | Train Loss: 0.00055998\n","Iter: 027/100 | Train Loss: 0.00055059\n","Iter: 028/100 | Train Loss: 0.00054692\n","Iter: 029/100 | Train Loss: 0.00053730\n","Iter: 030/100 | Train Loss: 0.00051846\n","Iter: 031/100 | Train Loss: 0.00049649\n","Iter: 032/100 | Train Loss: 0.00047946\n","Iter: 033/100 | Train Loss: 0.00047005\n","Iter: 034/100 | Train Loss: 0.00046436\n","Iter: 035/100 | Train Loss: 0.00045666\n","Iter: 036/100 | Train Loss: 0.00044468\n","Iter: 037/100 | Train Loss: 0.00043057\n","Iter: 038/100 | Train Loss: 0.00041829\n","Iter: 039/100 | Train Loss: 0.00040961\n","Iter: 040/100 | Train Loss: 0.00040329\n","Iter: 041/100 | Train Loss: 0.00039663\n","Iter: 042/100 | Train Loss: 0.00038804\n","Iter: 043/100 | Train Loss: 0.00037814\n","Iter: 044/100 | Train Loss: 0.00036884\n","Iter: 045/100 | Train Loss: 0.00036131\n","Iter: 046/100 | Train Loss: 0.00035523\n","Iter: 047/100 | Train Loss: 0.00034934\n","Iter: 048/100 | Train Loss: 0.00034271\n","Iter: 049/100 | Train Loss: 0.00033541\n","Iter: 050/100 | Train Loss: 0.00032826\n","Iter: 051/100 | Train Loss: 0.00032193\n","Iter: 052/100 | Train Loss: 0.00031641\n","Iter: 053/100 | Train Loss: 0.00031115\n","Iter: 054/100 | Train Loss: 0.00030568\n","Iter: 055/100 | Train Loss: 0.00029996\n","Iter: 056/100 | Train Loss: 0.00029427\n","Iter: 057/100 | Train Loss: 0.00028894\n","Iter: 058/100 | Train Loss: 0.00028408\n","Iter: 059/100 | Train Loss: 0.00027950\n","Iter: 060/100 | Train Loss: 0.00027494\n","Iter: 061/100 | Train Loss: 0.00027029\n","Iter: 062/100 | Train Loss: 0.00026562\n","Iter: 063/100 | Train Loss: 0.00026113\n","Iter: 064/100 | Train Loss: 0.00025690\n","Iter: 065/100 | Train Loss: 0.00025290\n","Iter: 066/100 | Train Loss: 0.00024901\n","Iter: 067/100 | Train Loss: 0.00024509\n","Iter: 068/100 | Train Loss: 0.00024118\n","Iter: 069/100 | Train Loss: 0.00023734\n","Iter: 070/100 | Train Loss: 0.00023367\n","Iter: 071/100 | Train Loss: 0.00023018\n","Iter: 072/100 | Train Loss: 0.00022679\n","Iter: 073/100 | Train Loss: 0.00022344\n","Iter: 074/100 | Train Loss: 0.00022009\n","Iter: 075/100 | Train Loss: 0.00021680\n","Iter: 076/100 | Train Loss: 0.00021361\n","Iter: 077/100 | Train Loss: 0.00021053\n","Iter: 078/100 | Train Loss: 0.00020754\n","Iter: 079/100 | Train Loss: 0.00020459\n","Iter: 080/100 | Train Loss: 0.00020167\n","Iter: 081/100 | Train Loss: 0.00019879\n","Iter: 082/100 | Train Loss: 0.00019597\n","Iter: 083/100 | Train Loss: 0.00019325\n","Iter: 084/100 | Train Loss: 0.00019060\n","Iter: 085/100 | Train Loss: 0.00018799\n","Iter: 086/100 | Train Loss: 0.00018542\n","Iter: 087/100 | Train Loss: 0.00018290\n","Iter: 088/100 | Train Loss: 0.00018043\n","Iter: 089/100 | Train Loss: 0.00017802\n","Iter: 090/100 | Train Loss: 0.00017565\n","Iter: 091/100 | Train Loss: 0.00017333\n","Iter: 092/100 | Train Loss: 0.00017105\n","Iter: 093/100 | Train Loss: 0.00016881\n","Iter: 094/100 | Train Loss: 0.00016661\n","Iter: 095/100 | Train Loss: 0.00016445\n","Iter: 096/100 | Train Loss: 0.00016234\n","Iter: 097/100 | Train Loss: 0.00016027\n","Iter: 098/100 | Train Loss: 0.00015823\n","Iter: 099/100 | Train Loss: 0.00015623\n","\n","Iter: 099/100 | Test Loss: 0.00100242 | Test acc: 65.2800\n","scale:1.100000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00227386\n","Iter: 001/100 | Train Loss: 0.00182062\n","Iter: 002/100 | Train Loss: 0.00152110\n","Iter: 003/100 | Train Loss: 0.00148878\n","Iter: 004/100 | Train Loss: 0.00165196\n","Adjusting Layer 1, Kernel Nodes: 629, Adptive Nodes:171\n","Iter: 005/100 | Train Loss: 0.00183001\n","Adjusting Layer 1, Kernel Nodes: 536, Adptive Nodes:264\n","Iter: 006/100 | Train Loss: 0.00175360\n","Iter: 007/100 | Train Loss: 0.00146977\n","Iter: 008/100 | Train Loss: 0.00124264\n","Iter: 009/100 | Train Loss: 0.00121183\n","Iter: 010/100 | Train Loss: 0.00128225\n","Adjusting Layer 1, Kernel Nodes: 397, Adptive Nodes:403\n","Iter: 011/100 | Train Loss: 0.00126102\n","Iter: 012/100 | Train Loss: 0.00111663\n","Iter: 013/100 | Train Loss: 0.00095418\n","Iter: 014/100 | Train Loss: 0.00088067\n","Iter: 015/100 | Train Loss: 0.00089043\n","Adjusting Layer 1, Kernel Nodes: 341, Adptive Nodes:459\n","Iter: 016/100 | Train Loss: 0.00088237\n","Iter: 017/100 | Train Loss: 0.00082631\n","Iter: 018/100 | Train Loss: 0.00074477\n","Iter: 019/100 | Train Loss: 0.00068271\n","Iter: 020/100 | Train Loss: 0.00066126\n","Iter: 021/100 | Train Loss: 0.00066310\n","Adjusting Layer 1, Kernel Nodes: 280, Adptive Nodes:520\n","Iter: 022/100 | Train Loss: 0.00064994\n","Iter: 023/100 | Train Loss: 0.00061946\n","Iter: 024/100 | Train Loss: 0.00058264\n","Iter: 025/100 | Train Loss: 0.00055460\n","Iter: 026/100 | Train Loss: 0.00054150\n","Iter: 027/100 | Train Loss: 0.00053730\n","Iter: 028/100 | Train Loss: 0.00053089\n","Iter: 029/100 | Train Loss: 0.00051612\n","Iter: 030/100 | Train Loss: 0.00049550\n","Iter: 031/100 | Train Loss: 0.00047627\n","Iter: 032/100 | Train Loss: 0.00046347\n","Iter: 033/100 | Train Loss: 0.00045628\n","Iter: 034/100 | Train Loss: 0.00044998\n","Iter: 035/100 | Train Loss: 0.00044058\n","Iter: 036/100 | Train Loss: 0.00042783\n","Iter: 037/100 | Train Loss: 0.00041452\n","Iter: 038/100 | Train Loss: 0.00040361\n","Iter: 039/100 | Train Loss: 0.00039575\n","Iter: 040/100 | Train Loss: 0.00038926\n","Iter: 041/100 | Train Loss: 0.00038205\n","Iter: 042/100 | Train Loss: 0.00037333\n","Iter: 043/100 | Train Loss: 0.00036397\n","Iter: 044/100 | Train Loss: 0.00035536\n","Iter: 045/100 | Train Loss: 0.00034819\n","Iter: 046/100 | Train Loss: 0.00034204\n","Iter: 047/100 | Train Loss: 0.00033596\n","Iter: 048/100 | Train Loss: 0.00032940\n","Iter: 049/100 | Train Loss: 0.00032251\n","Iter: 050/100 | Train Loss: 0.00031584\n","Iter: 051/100 | Train Loss: 0.00030978\n","Iter: 052/100 | Train Loss: 0.00030433\n","Iter: 053/100 | Train Loss: 0.00029915\n","Iter: 054/100 | Train Loss: 0.00029389\n","Iter: 055/100 | Train Loss: 0.00028853\n","Iter: 056/100 | Train Loss: 0.00028325\n","Iter: 057/100 | Train Loss: 0.00027823\n","Iter: 058/100 | Train Loss: 0.00027352\n","Iter: 059/100 | Train Loss: 0.00026901\n","Iter: 060/100 | Train Loss: 0.00026457\n","Iter: 061/100 | Train Loss: 0.00026015\n","Iter: 062/100 | Train Loss: 0.00025578\n","Iter: 063/100 | Train Loss: 0.00025153\n","Iter: 064/100 | Train Loss: 0.00024745\n","Iter: 065/100 | Train Loss: 0.00024350\n","Iter: 066/100 | Train Loss: 0.00023964\n","Iter: 067/100 | Train Loss: 0.00023586\n","Iter: 068/100 | Train Loss: 0.00023216\n","Iter: 069/100 | Train Loss: 0.00022856\n","Iter: 070/100 | Train Loss: 0.00022506\n","Iter: 071/100 | Train Loss: 0.00022165\n","Iter: 072/100 | Train Loss: 0.00021832\n","Iter: 073/100 | Train Loss: 0.00021506\n","Iter: 074/100 | Train Loss: 0.00021187\n","Iter: 075/100 | Train Loss: 0.00020875\n","Iter: 076/100 | Train Loss: 0.00020571\n","Iter: 077/100 | Train Loss: 0.00020273\n","Iter: 078/100 | Train Loss: 0.00019981\n","Iter: 079/100 | Train Loss: 0.00019695\n","Iter: 080/100 | Train Loss: 0.00019415\n","Iter: 081/100 | Train Loss: 0.00019142\n","Iter: 082/100 | Train Loss: 0.00018874\n","Iter: 083/100 | Train Loss: 0.00018611\n","Iter: 084/100 | Train Loss: 0.00018353\n","Iter: 085/100 | Train Loss: 0.00018101\n","Iter: 086/100 | Train Loss: 0.00017853\n","Iter: 087/100 | Train Loss: 0.00017611\n","Iter: 088/100 | Train Loss: 0.00017373\n","Iter: 089/100 | Train Loss: 0.00017140\n","Iter: 090/100 | Train Loss: 0.00016911\n","Iter: 091/100 | Train Loss: 0.00016685\n","Iter: 092/100 | Train Loss: 0.00016465\n","Iter: 093/100 | Train Loss: 0.00016248\n","Iter: 094/100 | Train Loss: 0.00016035\n","Iter: 095/100 | Train Loss: 0.00015826\n","Iter: 096/100 | Train Loss: 0.00015621\n","Iter: 097/100 | Train Loss: 0.00015419\n","Iter: 098/100 | Train Loss: 0.00015220\n","Iter: 099/100 | Train Loss: 0.00015025\n","\n","Iter: 099/100 | Test Loss: 0.00100512 | Test acc: 65.2400\n","scale:1.100000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00230387\n","Iter: 001/100 | Train Loss: 0.00187283\n","Iter: 002/100 | Train Loss: 0.00155602\n","Iter: 003/100 | Train Loss: 0.00147190\n","Iter: 004/100 | Train Loss: 0.00158691\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 005/100 | Train Loss: 0.00177582\n","Adjusting Layer 1, Kernel Nodes: 587, Adptive Nodes:213\n","Iter: 006/100 | Train Loss: 0.00179277\n","Adjusting Layer 1, Kernel Nodes: 363, Adptive Nodes:437\n","Iter: 007/100 | Train Loss: 0.00158501\n","Iter: 008/100 | Train Loss: 0.00133110\n","Iter: 009/100 | Train Loss: 0.00120661\n","Iter: 010/100 | Train Loss: 0.00122456\n","Adjusting Layer 1, Kernel Nodes: 387, Adptive Nodes:413\n","Iter: 011/100 | Train Loss: 0.00126397\n","Adjusting Layer 1, Kernel Nodes: 381, Adptive Nodes:419\n","Iter: 012/100 | Train Loss: 0.00124194\n","Iter: 013/100 | Train Loss: 0.00113891\n","Iter: 014/100 | Train Loss: 0.00100305\n","Iter: 015/100 | Train Loss: 0.00090193\n","Iter: 016/100 | Train Loss: 0.00086588\n","Iter: 017/100 | Train Loss: 0.00087171\n","Adjusting Layer 1, Kernel Nodes: 567, Adptive Nodes:233\n","Iter: 018/100 | Train Loss: 0.00086619\n","Iter: 019/100 | Train Loss: 0.00081278\n","Iter: 020/100 | Train Loss: 0.00073399\n","Iter: 021/100 | Train Loss: 0.00067635\n","Iter: 022/100 | Train Loss: 0.00065902\n","Iter: 023/100 | Train Loss: 0.00066020\n","Adjusting Layer 1, Kernel Nodes: 131, Adptive Nodes:669\n","Iter: 024/100 | Train Loss: 0.00065004\n","Iter: 025/100 | Train Loss: 0.00062408\n","Iter: 026/100 | Train Loss: 0.00058948\n","Iter: 027/100 | Train Loss: 0.00055854\n","Iter: 028/100 | Train Loss: 0.00053950\n","Iter: 029/100 | Train Loss: 0.00053189\n","Iter: 030/100 | Train Loss: 0.00052849\n","Iter: 031/100 | Train Loss: 0.00052147\n","Iter: 032/100 | Train Loss: 0.00050760\n","Iter: 033/100 | Train Loss: 0.00048921\n","Iter: 034/100 | Train Loss: 0.00047146\n","Iter: 035/100 | Train Loss: 0.00045836\n","Iter: 036/100 | Train Loss: 0.00045035\n","Iter: 037/100 | Train Loss: 0.00044478\n","Iter: 038/100 | Train Loss: 0.00043832\n","Iter: 039/100 | Train Loss: 0.00042925\n","Iter: 040/100 | Train Loss: 0.00041806\n","Iter: 041/100 | Train Loss: 0.00040674\n","Iter: 042/100 | Train Loss: 0.00039714\n","Iter: 043/100 | Train Loss: 0.00038979\n","Iter: 044/100 | Train Loss: 0.00038385\n","Iter: 045/100 | Train Loss: 0.00037796\n","Iter: 046/100 | Train Loss: 0.00037118\n","Iter: 047/100 | Train Loss: 0.00036345\n","Iter: 048/100 | Train Loss: 0.00035548\n","Iter: 049/100 | Train Loss: 0.00034811\n","Iter: 050/100 | Train Loss: 0.00034177\n","Iter: 051/100 | Train Loss: 0.00033626\n","Iter: 052/100 | Train Loss: 0.00033100\n","Iter: 053/100 | Train Loss: 0.00032553\n","Iter: 054/100 | Train Loss: 0.00031970\n","Iter: 055/100 | Train Loss: 0.00031376\n","Iter: 056/100 | Train Loss: 0.00030805\n","Iter: 057/100 | Train Loss: 0.00030283\n","Iter: 058/100 | Train Loss: 0.00029803\n","Iter: 059/100 | Train Loss: 0.00029347\n","Iter: 060/100 | Train Loss: 0.00028890\n","Iter: 061/100 | Train Loss: 0.00028421\n","Iter: 062/100 | Train Loss: 0.00027950\n","Iter: 063/100 | Train Loss: 0.00027491\n","Iter: 064/100 | Train Loss: 0.00027055\n","Iter: 065/100 | Train Loss: 0.00026644\n","Iter: 066/100 | Train Loss: 0.00026250\n","Iter: 067/100 | Train Loss: 0.00025862\n","Iter: 068/100 | Train Loss: 0.00025474\n","Iter: 069/100 | Train Loss: 0.00025088\n","Iter: 070/100 | Train Loss: 0.00024709\n","Iter: 071/100 | Train Loss: 0.00024343\n","Iter: 072/100 | Train Loss: 0.00023990\n","Iter: 073/100 | Train Loss: 0.00023651\n","Iter: 074/100 | Train Loss: 0.00023319\n","Iter: 075/100 | Train Loss: 0.00022990\n","Iter: 076/100 | Train Loss: 0.00022664\n","Iter: 077/100 | Train Loss: 0.00022344\n","Iter: 078/100 | Train Loss: 0.00022031\n","Iter: 079/100 | Train Loss: 0.00021728\n","Iter: 080/100 | Train Loss: 0.00021433\n","Iter: 081/100 | Train Loss: 0.00021145\n","Iter: 082/100 | Train Loss: 0.00020861\n","Iter: 083/100 | Train Loss: 0.00020579\n","Iter: 084/100 | Train Loss: 0.00020303\n","Iter: 085/100 | Train Loss: 0.00020032\n","Iter: 086/100 | Train Loss: 0.00019768\n","Iter: 087/100 | Train Loss: 0.00019510\n","Iter: 088/100 | Train Loss: 0.00019257\n","Iter: 089/100 | Train Loss: 0.00019007\n","Iter: 090/100 | Train Loss: 0.00018763\n","Iter: 091/100 | Train Loss: 0.00018522\n","Iter: 092/100 | Train Loss: 0.00018284\n","Iter: 093/100 | Train Loss: 0.00018053\n","Iter: 094/100 | Train Loss: 0.00017824\n","Iter: 095/100 | Train Loss: 0.00017602\n","Iter: 096/100 | Train Loss: 0.00017381\n","Iter: 097/100 | Train Loss: 0.00017166\n","Iter: 098/100 | Train Loss: 0.00016952\n","Iter: 099/100 | Train Loss: 0.00016743\n","\n","Iter: 099/100 | Test Loss: 0.00099204 | Test acc: 65.5000\n","scale:1.100000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00232694\n","Iter: 001/100 | Train Loss: 0.00191521\n","Iter: 002/100 | Train Loss: 0.00158976\n","Iter: 003/100 | Train Loss: 0.00146857\n","Iter: 004/100 | Train Loss: 0.00154211\n","Adjusting Layer 1, Kernel Nodes: 683, Adptive Nodes:117\n","Iter: 005/100 | Train Loss: 0.00172094\n","Adjusting Layer 1, Kernel Nodes: 601, Adptive Nodes:199\n","Iter: 006/100 | Train Loss: 0.00180810\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00158374\n","Iter: 008/100 | Train Loss: 0.00129638\n","Iter: 009/100 | Train Loss: 0.00123303\n","Iter: 010/100 | Train Loss: 0.00132567\n","Adjusting Layer 1, Kernel Nodes: 126, Adptive Nodes:674\n","Iter: 011/100 | Train Loss: 0.00133371\n","Adjusting Layer 1, Kernel Nodes: 223, Adptive Nodes:577\n","Iter: 012/100 | Train Loss: 0.00122791\n","Iter: 013/100 | Train Loss: 0.00107079\n","Iter: 014/100 | Train Loss: 0.00094933\n","Iter: 015/100 | Train Loss: 0.00090747\n","Iter: 016/100 | Train Loss: 0.00092112\n","Adjusting Layer 1, Kernel Nodes: 621, Adptive Nodes:179\n","Iter: 017/100 | Train Loss: 0.00092211\n","Adjusting Layer 1, Kernel Nodes: 748, Adptive Nodes:52\n","Iter: 018/100 | Train Loss: 0.00084481\n","Iter: 019/100 | Train Loss: 0.00074426\n","Iter: 020/100 | Train Loss: 0.00069680\n","Iter: 021/100 | Train Loss: 0.00069697\n","Adjusting Layer 1, Kernel Nodes: 9, Adptive Nodes:791\n","Iter: 022/100 | Train Loss: 0.00068783\n","Iter: 023/100 | Train Loss: 0.00065185\n","Iter: 024/100 | Train Loss: 0.00060492\n","Iter: 025/100 | Train Loss: 0.00057093\n","Iter: 026/100 | Train Loss: 0.00055763\n","Iter: 027/100 | Train Loss: 0.00055340\n","Iter: 028/100 | Train Loss: 0.00054308\n","Iter: 029/100 | Train Loss: 0.00052270\n","Iter: 030/100 | Train Loss: 0.00049956\n","Iter: 031/100 | Train Loss: 0.00048181\n","Iter: 032/100 | Train Loss: 0.00047079\n","Iter: 033/100 | Train Loss: 0.00046209\n","Iter: 034/100 | Train Loss: 0.00045151\n","Iter: 035/100 | Train Loss: 0.00043865\n","Iter: 036/100 | Train Loss: 0.00042560\n","Iter: 037/100 | Train Loss: 0.00041406\n","Iter: 038/100 | Train Loss: 0.00040386\n","Iter: 039/100 | Train Loss: 0.00039422\n","Iter: 040/100 | Train Loss: 0.00038495\n","Iter: 041/100 | Train Loss: 0.00037624\n","Iter: 042/100 | Train Loss: 0.00036785\n","Iter: 043/100 | Train Loss: 0.00035925\n","Iter: 044/100 | Train Loss: 0.00035031\n","Iter: 045/100 | Train Loss: 0.00034173\n","Iter: 046/100 | Train Loss: 0.00033422\n","Iter: 047/100 | Train Loss: 0.00032777\n","Iter: 048/100 | Train Loss: 0.00032151\n","Iter: 049/100 | Train Loss: 0.00031464\n","Iter: 050/100 | Train Loss: 0.00030727\n","Iter: 051/100 | Train Loss: 0.00030024\n","Iter: 052/100 | Train Loss: 0.00029424\n","Iter: 053/100 | Train Loss: 0.00028909\n","Iter: 054/100 | Train Loss: 0.00028397\n","Iter: 055/100 | Train Loss: 0.00027831\n","Iter: 056/100 | Train Loss: 0.00027232\n","Iter: 057/100 | Train Loss: 0.00026667\n","Iter: 058/100 | Train Loss: 0.00026176\n","Iter: 059/100 | Train Loss: 0.00025736\n","Iter: 060/100 | Train Loss: 0.00025297\n","Iter: 061/100 | Train Loss: 0.00024827\n","Iter: 062/100 | Train Loss: 0.00024347\n","Iter: 063/100 | Train Loss: 0.00023893\n","Iter: 064/100 | Train Loss: 0.00023483\n","Iter: 065/100 | Train Loss: 0.00023100\n","Iter: 066/100 | Train Loss: 0.00022717\n","Iter: 067/100 | Train Loss: 0.00022322\n","Iter: 068/100 | Train Loss: 0.00021932\n","Iter: 069/100 | Train Loss: 0.00021563\n","Iter: 070/100 | Train Loss: 0.00021216\n","Iter: 071/100 | Train Loss: 0.00020879\n","Iter: 072/100 | Train Loss: 0.00020543\n","Iter: 073/100 | Train Loss: 0.00020210\n","Iter: 074/100 | Train Loss: 0.00019883\n","Iter: 075/100 | Train Loss: 0.00019570\n","Iter: 076/100 | Train Loss: 0.00019268\n","Iter: 077/100 | Train Loss: 0.00018972\n","Iter: 078/100 | Train Loss: 0.00018678\n","Iter: 079/100 | Train Loss: 0.00018389\n","Iter: 080/100 | Train Loss: 0.00018109\n","Iter: 081/100 | Train Loss: 0.00017838\n","Iter: 082/100 | Train Loss: 0.00017572\n","Iter: 083/100 | Train Loss: 0.00017310\n","Iter: 084/100 | Train Loss: 0.00017053\n","Iter: 085/100 | Train Loss: 0.00016802\n","Iter: 086/100 | Train Loss: 0.00016558\n","Iter: 087/100 | Train Loss: 0.00016317\n","Iter: 088/100 | Train Loss: 0.00016081\n","Iter: 089/100 | Train Loss: 0.00015850\n","Iter: 090/100 | Train Loss: 0.00015625\n","Iter: 091/100 | Train Loss: 0.00015404\n","Iter: 092/100 | Train Loss: 0.00015187\n","Iter: 093/100 | Train Loss: 0.00014973\n","Iter: 094/100 | Train Loss: 0.00014764\n","Iter: 095/100 | Train Loss: 0.00014559\n","Iter: 096/100 | Train Loss: 0.00014359\n","Iter: 097/100 | Train Loss: 0.00014161\n","Iter: 098/100 | Train Loss: 0.00013967\n","Iter: 099/100 | Train Loss: 0.00013777\n","\n","Iter: 099/100 | Test Loss: 0.00100650 | Test acc: 65.2300\n","scale:1.100000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00234423\n","Iter: 001/100 | Train Loss: 0.00194828\n","Iter: 002/100 | Train Loss: 0.00161910\n","Iter: 003/100 | Train Loss: 0.00147222\n","Iter: 004/100 | Train Loss: 0.00151292\n","Adjusting Layer 1, Kernel Nodes: 737, Adptive Nodes:63\n","Iter: 005/100 | Train Loss: 0.00167393\n","Adjusting Layer 1, Kernel Nodes: 605, Adptive Nodes:195\n","Iter: 006/100 | Train Loss: 0.00179656\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 007/100 | Train Loss: 0.00172479\n","Iter: 008/100 | Train Loss: 0.00150239\n","Iter: 009/100 | Train Loss: 0.00129493\n","Iter: 010/100 | Train Loss: 0.00121787\n","Iter: 011/100 | Train Loss: 0.00125406\n","Adjusting Layer 1, Kernel Nodes: 526, Adptive Nodes:274\n","Iter: 012/100 | Train Loss: 0.00129417\n","Adjusting Layer 1, Kernel Nodes: 379, Adptive Nodes:421\n","Iter: 013/100 | Train Loss: 0.00125173\n","Iter: 014/100 | Train Loss: 0.00112874\n","Iter: 015/100 | Train Loss: 0.00099486\n","Iter: 016/100 | Train Loss: 0.00091710\n","Iter: 017/100 | Train Loss: 0.00090468\n","Iter: 018/100 | Train Loss: 0.00091324\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 019/100 | Train Loss: 0.00087812\n","Iter: 020/100 | Train Loss: 0.00079282\n","Iter: 021/100 | Train Loss: 0.00071406\n","Iter: 022/100 | Train Loss: 0.00068334\n","Iter: 023/100 | Train Loss: 0.00068158\n","Iter: 024/100 | Train Loss: 0.00066411\n","Iter: 025/100 | Train Loss: 0.00061788\n","Iter: 026/100 | Train Loss: 0.00056957\n","Iter: 027/100 | Train Loss: 0.00054556\n","Iter: 028/100 | Train Loss: 0.00054097\n","Iter: 029/100 | Train Loss: 0.00053179\n","Iter: 030/100 | Train Loss: 0.00050659\n","Iter: 031/100 | Train Loss: 0.00047672\n","Iter: 032/100 | Train Loss: 0.00045782\n","Iter: 033/100 | Train Loss: 0.00045061\n","Iter: 034/100 | Train Loss: 0.00044324\n","Iter: 035/100 | Train Loss: 0.00042764\n","Iter: 036/100 | Train Loss: 0.00040783\n","Iter: 037/100 | Train Loss: 0.00039238\n","Iter: 038/100 | Train Loss: 0.00038356\n","Iter: 039/100 | Train Loss: 0.00037614\n","Iter: 040/100 | Train Loss: 0.00036507\n","Iter: 041/100 | Train Loss: 0.00035113\n","Iter: 042/100 | Train Loss: 0.00033868\n","Iter: 043/100 | Train Loss: 0.00032981\n","Iter: 044/100 | Train Loss: 0.00032261\n","Iter: 045/100 | Train Loss: 0.00031416\n","Iter: 046/100 | Train Loss: 0.00030409\n","Iter: 047/100 | Train Loss: 0.00029431\n","Iter: 048/100 | Train Loss: 0.00028637\n","Iter: 049/100 | Train Loss: 0.00027978\n","Iter: 050/100 | Train Loss: 0.00027304\n","Iter: 051/100 | Train Loss: 0.00026551\n","Iter: 052/100 | Train Loss: 0.00025790\n","Iter: 053/100 | Train Loss: 0.00025116\n","Iter: 054/100 | Train Loss: 0.00024536\n","Iter: 055/100 | Train Loss: 0.00023982\n","Iter: 056/100 | Train Loss: 0.00023399\n","Iter: 057/100 | Train Loss: 0.00022801\n","Iter: 058/100 | Train Loss: 0.00022243\n","Iter: 059/100 | Train Loss: 0.00021745\n","Iter: 060/100 | Train Loss: 0.00021281\n","Iter: 061/100 | Train Loss: 0.00020812\n","Iter: 062/100 | Train Loss: 0.00020335\n","Iter: 063/100 | Train Loss: 0.00019872\n","Iter: 064/100 | Train Loss: 0.00019445\n","Iter: 065/100 | Train Loss: 0.00019049\n","Iter: 066/100 | Train Loss: 0.00018661\n","Iter: 067/100 | Train Loss: 0.00018269\n","Iter: 068/100 | Train Loss: 0.00017882\n","Iter: 069/100 | Train Loss: 0.00017515\n","Iter: 070/100 | Train Loss: 0.00017170\n","Iter: 071/100 | Train Loss: 0.00016837\n","Iter: 072/100 | Train Loss: 0.00016504\n","Iter: 073/100 | Train Loss: 0.00016173\n","Iter: 074/100 | Train Loss: 0.00015855\n","Iter: 075/100 | Train Loss: 0.00015552\n","Iter: 076/100 | Train Loss: 0.00015262\n","Iter: 077/100 | Train Loss: 0.00014974\n","Iter: 078/100 | Train Loss: 0.00014688\n","Iter: 079/100 | Train Loss: 0.00014409\n","Iter: 080/100 | Train Loss: 0.00014143\n","Iter: 081/100 | Train Loss: 0.00013887\n","Iter: 082/100 | Train Loss: 0.00013634\n","Iter: 083/100 | Train Loss: 0.00013385\n","Iter: 084/100 | Train Loss: 0.00013141\n","Iter: 085/100 | Train Loss: 0.00012906\n","Iter: 086/100 | Train Loss: 0.00012678\n","Iter: 087/100 | Train Loss: 0.00012456\n","Iter: 088/100 | Train Loss: 0.00012236\n","Iter: 089/100 | Train Loss: 0.00012021\n","Iter: 090/100 | Train Loss: 0.00011813\n","Iter: 091/100 | Train Loss: 0.00011611\n","Iter: 092/100 | Train Loss: 0.00011413\n","Iter: 093/100 | Train Loss: 0.00011218\n","Iter: 094/100 | Train Loss: 0.00011028\n","Iter: 095/100 | Train Loss: 0.00010842\n","Iter: 096/100 | Train Loss: 0.00010661\n","Iter: 097/100 | Train Loss: 0.00010485\n","Iter: 098/100 | Train Loss: 0.00010312\n","Iter: 099/100 | Train Loss: 0.00010142\n","\n","Iter: 099/100 | Test Loss: 0.00102173 | Test acc: 64.8200\n","scale:1.100000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00235676\n","Iter: 001/100 | Train Loss: 0.00197287\n","Iter: 002/100 | Train Loss: 0.00164249\n","Iter: 003/100 | Train Loss: 0.00147837\n","Iter: 004/100 | Train Loss: 0.00149467\n","Adjusting Layer 1, Kernel Nodes: 783, Adptive Nodes:17\n","Iter: 005/100 | Train Loss: 0.00163384\n","Adjusting Layer 1, Kernel Nodes: 649, Adptive Nodes:151\n","Iter: 006/100 | Train Loss: 0.00177152\n","Adjusting Layer 1, Kernel Nodes: 613, Adptive Nodes:187\n","Iter: 007/100 | Train Loss: 0.00177262\n","Adjusting Layer 1, Kernel Nodes: 640, Adptive Nodes:160\n","Iter: 008/100 | Train Loss: 0.00159138\n","Iter: 009/100 | Train Loss: 0.00136527\n","Iter: 010/100 | Train Loss: 0.00123513\n","Iter: 011/100 | Train Loss: 0.00123593\n","Adjusting Layer 1, Kernel Nodes: 365, Adptive Nodes:435\n","Iter: 012/100 | Train Loss: 0.00128716\n","Adjusting Layer 1, Kernel Nodes: 719, Adptive Nodes:81\n","Iter: 013/100 | Train Loss: 0.00129017\n","Adjusting Layer 1, Kernel Nodes: 123, Adptive Nodes:677\n","Iter: 014/100 | Train Loss: 0.00120497\n","Iter: 015/100 | Train Loss: 0.00107438\n","Iter: 016/100 | Train Loss: 0.00096574\n","Iter: 017/100 | Train Loss: 0.00091787\n","Iter: 018/100 | Train Loss: 0.00091684\n","Iter: 019/100 | Train Loss: 0.00091840\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 020/100 | Train Loss: 0.00087895\n","Iter: 021/100 | Train Loss: 0.00080455\n","Iter: 022/100 | Train Loss: 0.00073466\n","Iter: 023/100 | Train Loss: 0.00069867\n","Iter: 024/100 | Train Loss: 0.00069049\n","Iter: 025/100 | Train Loss: 0.00068202\n","Iter: 026/100 | Train Loss: 0.00065406\n","Iter: 027/100 | Train Loss: 0.00061207\n","Iter: 028/100 | Train Loss: 0.00057541\n","Iter: 029/100 | Train Loss: 0.00055588\n","Iter: 030/100 | Train Loss: 0.00054832\n","Iter: 031/100 | Train Loss: 0.00053901\n","Iter: 032/100 | Train Loss: 0.00052032\n","Iter: 033/100 | Train Loss: 0.00049621\n","Iter: 034/100 | Train Loss: 0.00047569\n","Iter: 035/100 | Train Loss: 0.00046318\n","Iter: 036/100 | Train Loss: 0.00045539\n","Iter: 037/100 | Train Loss: 0.00044620\n","Iter: 038/100 | Train Loss: 0.00043286\n","Iter: 039/100 | Train Loss: 0.00041760\n","Iter: 040/100 | Train Loss: 0.00040430\n","Iter: 041/100 | Train Loss: 0.00039457\n","Iter: 042/100 | Train Loss: 0.00038682\n","Iter: 043/100 | Train Loss: 0.00037848\n","Iter: 044/100 | Train Loss: 0.00036851\n","Iter: 045/100 | Train Loss: 0.00035794\n","Iter: 046/100 | Train Loss: 0.00034839\n","Iter: 047/100 | Train Loss: 0.00034043\n","Iter: 048/100 | Train Loss: 0.00033341\n","Iter: 049/100 | Train Loss: 0.00032626\n","Iter: 050/100 | Train Loss: 0.00031859\n","Iter: 051/100 | Train Loss: 0.00031077\n","Iter: 052/100 | Train Loss: 0.00030344\n","Iter: 053/100 | Train Loss: 0.00029686\n","Iter: 054/100 | Train Loss: 0.00029081\n","Iter: 055/100 | Train Loss: 0.00028487\n","Iter: 056/100 | Train Loss: 0.00027880\n","Iter: 057/100 | Train Loss: 0.00027271\n","Iter: 058/100 | Train Loss: 0.00026687\n","Iter: 059/100 | Train Loss: 0.00026142\n","Iter: 060/100 | Train Loss: 0.00025632\n","Iter: 061/100 | Train Loss: 0.00025138\n","Iter: 062/100 | Train Loss: 0.00024644\n","Iter: 063/100 | Train Loss: 0.00024153\n","Iter: 064/100 | Train Loss: 0.00023676\n","Iter: 065/100 | Train Loss: 0.00023225\n","Iter: 066/100 | Train Loss: 0.00022798\n","Iter: 067/100 | Train Loss: 0.00022386\n","Iter: 068/100 | Train Loss: 0.00021978\n","Iter: 069/100 | Train Loss: 0.00021573\n","Iter: 070/100 | Train Loss: 0.00021178\n","Iter: 071/100 | Train Loss: 0.00020801\n","Iter: 072/100 | Train Loss: 0.00020440\n","Iter: 073/100 | Train Loss: 0.00020091\n","Iter: 074/100 | Train Loss: 0.00019745\n","Iter: 075/100 | Train Loss: 0.00019404\n","Iter: 076/100 | Train Loss: 0.00019071\n","Iter: 077/100 | Train Loss: 0.00018750\n","Iter: 078/100 | Train Loss: 0.00018440\n","Iter: 079/100 | Train Loss: 0.00018138\n","Iter: 080/100 | Train Loss: 0.00017840\n","Iter: 081/100 | Train Loss: 0.00017546\n","Iter: 082/100 | Train Loss: 0.00017259\n","Iter: 083/100 | Train Loss: 0.00016981\n","Iter: 084/100 | Train Loss: 0.00016712\n","Iter: 085/100 | Train Loss: 0.00016448\n","Iter: 086/100 | Train Loss: 0.00016188\n","Iter: 087/100 | Train Loss: 0.00015932\n","Iter: 088/100 | Train Loss: 0.00015682\n","Iter: 089/100 | Train Loss: 0.00015439\n","Iter: 090/100 | Train Loss: 0.00015203\n","Iter: 091/100 | Train Loss: 0.00014972\n","Iter: 092/100 | Train Loss: 0.00014744\n","Iter: 093/100 | Train Loss: 0.00014520\n","Iter: 094/100 | Train Loss: 0.00014301\n","Iter: 095/100 | Train Loss: 0.00014087\n","Iter: 096/100 | Train Loss: 0.00013879\n","Iter: 097/100 | Train Loss: 0.00013673\n","Iter: 098/100 | Train Loss: 0.00013472\n","Iter: 099/100 | Train Loss: 0.00013274\n","\n","Iter: 099/100 | Test Loss: 0.00100388 | Test acc: 65.2200\n","scale:1.100000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00236529\n","Iter: 001/100 | Train Loss: 0.00198993\n","Iter: 002/100 | Train Loss: 0.00165948\n","Iter: 003/100 | Train Loss: 0.00148431\n","Iter: 004/100 | Train Loss: 0.00148370\n","Iter: 005/100 | Train Loss: 0.00159777\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 006/100 | Train Loss: 0.00173235\n","Adjusting Layer 1, Kernel Nodes: 687, Adptive Nodes:113\n","Iter: 007/100 | Train Loss: 0.00178905\n","Adjusting Layer 1, Kernel Nodes: 672, Adptive Nodes:128\n","Iter: 008/100 | Train Loss: 0.00171336\n","Iter: 009/100 | Train Loss: 0.00154402\n","Iter: 010/100 | Train Loss: 0.00136508\n","Iter: 011/100 | Train Loss: 0.00124819\n","Iter: 012/100 | Train Loss: 0.00121633\n","Iter: 013/100 | Train Loss: 0.00124266\n","Adjusting Layer 1, Kernel Nodes: 573, Adptive Nodes:227\n","Iter: 014/100 | Train Loss: 0.00127576\n","Adjusting Layer 1, Kernel Nodes: 647, Adptive Nodes:153\n","Iter: 015/100 | Train Loss: 0.00126672\n","Iter: 016/100 | Train Loss: 0.00120013\n","Iter: 017/100 | Train Loss: 0.00110159\n","Iter: 018/100 | Train Loss: 0.00101206\n","Iter: 019/100 | Train Loss: 0.00095960\n","Iter: 020/100 | Train Loss: 0.00094491\n","Iter: 021/100 | Train Loss: 0.00094646\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 022/100 | Train Loss: 0.00093731\n","Iter: 023/100 | Train Loss: 0.00090390\n","Iter: 024/100 | Train Loss: 0.00085202\n","Iter: 025/100 | Train Loss: 0.00079934\n","Iter: 026/100 | Train Loss: 0.00076143\n","Iter: 027/100 | Train Loss: 0.00074203\n","Iter: 028/100 | Train Loss: 0.00073326\n","Iter: 029/100 | Train Loss: 0.00072312\n","Iter: 030/100 | Train Loss: 0.00070401\n","Iter: 031/100 | Train Loss: 0.00067665\n","Iter: 032/100 | Train Loss: 0.00064774\n","Iter: 033/100 | Train Loss: 0.00062421\n","Iter: 034/100 | Train Loss: 0.00060874\n","Iter: 035/100 | Train Loss: 0.00059893\n","Iter: 036/100 | Train Loss: 0.00058989\n","Iter: 037/100 | Train Loss: 0.00057788\n","Iter: 038/100 | Train Loss: 0.00056238\n","Iter: 039/100 | Train Loss: 0.00054566\n","Iter: 040/100 | Train Loss: 0.00053067\n","Iter: 041/100 | Train Loss: 0.00051895\n","Iter: 042/100 | Train Loss: 0.00051002\n","Iter: 043/100 | Train Loss: 0.00050206\n","Iter: 044/100 | Train Loss: 0.00049339\n","Iter: 045/100 | Train Loss: 0.00048344\n","Iter: 046/100 | Train Loss: 0.00047283\n","Iter: 047/100 | Train Loss: 0.00046270\n","Iter: 048/100 | Train Loss: 0.00045383\n","Iter: 049/100 | Train Loss: 0.00044625\n","Iter: 050/100 | Train Loss: 0.00043938\n","Iter: 051/100 | Train Loss: 0.00043251\n","Iter: 052/100 | Train Loss: 0.00042529\n","Iter: 053/100 | Train Loss: 0.00041781\n","Iter: 054/100 | Train Loss: 0.00041043\n","Iter: 055/100 | Train Loss: 0.00040349\n","Iter: 056/100 | Train Loss: 0.00039710\n","Iter: 057/100 | Train Loss: 0.00039116\n","Iter: 058/100 | Train Loss: 0.00038541\n","Iter: 059/100 | Train Loss: 0.00037966\n","Iter: 060/100 | Train Loss: 0.00037384\n","Iter: 061/100 | Train Loss: 0.00036805\n","Iter: 062/100 | Train Loss: 0.00036243\n","Iter: 063/100 | Train Loss: 0.00035707\n","Iter: 064/100 | Train Loss: 0.00035197\n","Iter: 065/100 | Train Loss: 0.00034707\n","Iter: 066/100 | Train Loss: 0.00034227\n","Iter: 067/100 | Train Loss: 0.00033750\n","Iter: 068/100 | Train Loss: 0.00033276\n","Iter: 069/100 | Train Loss: 0.00032810\n","Iter: 070/100 | Train Loss: 0.00032357\n","Iter: 071/100 | Train Loss: 0.00031920\n","Iter: 072/100 | Train Loss: 0.00031498\n","Iter: 073/100 | Train Loss: 0.00031086\n","Iter: 074/100 | Train Loss: 0.00030679\n","Iter: 075/100 | Train Loss: 0.00030277\n","Iter: 076/100 | Train Loss: 0.00029880\n","Iter: 077/100 | Train Loss: 0.00029492\n","Iter: 078/100 | Train Loss: 0.00029114\n","Iter: 079/100 | Train Loss: 0.00028747\n","Iter: 080/100 | Train Loss: 0.00028389\n","Iter: 081/100 | Train Loss: 0.00028036\n","Iter: 082/100 | Train Loss: 0.00027689\n","Iter: 083/100 | Train Loss: 0.00027346\n","Iter: 084/100 | Train Loss: 0.00027010\n","Iter: 085/100 | Train Loss: 0.00026681\n","Iter: 086/100 | Train Loss: 0.00026360\n","Iter: 087/100 | Train Loss: 0.00026046\n","Iter: 088/100 | Train Loss: 0.00025738\n","Iter: 089/100 | Train Loss: 0.00025434\n","Iter: 090/100 | Train Loss: 0.00025134\n","Iter: 091/100 | Train Loss: 0.00024839\n","Iter: 092/100 | Train Loss: 0.00024550\n","Iter: 093/100 | Train Loss: 0.00024267\n","Iter: 094/100 | Train Loss: 0.00023989\n","Iter: 095/100 | Train Loss: 0.00023716\n","Iter: 096/100 | Train Loss: 0.00023446\n","Iter: 097/100 | Train Loss: 0.00023181\n","Iter: 098/100 | Train Loss: 0.00022920\n","Iter: 099/100 | Train Loss: 0.00022664\n","\n","Iter: 099/100 | Test Loss: 0.00098512 | Test acc: 65.2400\n","scale:1.100000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00237046\n","Iter: 001/100 | Train Loss: 0.00200039\n","Iter: 002/100 | Train Loss: 0.00167020\n","Iter: 003/100 | Train Loss: 0.00148847\n","Iter: 004/100 | Train Loss: 0.00147734\n","Iter: 005/100 | Train Loss: 0.00158281\n","Adjusting Layer 1, Kernel Nodes: 756, Adptive Nodes:44\n","Iter: 006/100 | Train Loss: 0.00171376\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 007/100 | Train Loss: 0.00178239\n","Adjusting Layer 1, Kernel Nodes: 705, Adptive Nodes:95\n","Iter: 008/100 | Train Loss: 0.00174263\n","Iter: 009/100 | Train Loss: 0.00161040\n","Iter: 010/100 | Train Loss: 0.00144237\n","Iter: 011/100 | Train Loss: 0.00129981\n","Iter: 012/100 | Train Loss: 0.00121977\n","Iter: 013/100 | Train Loss: 0.00120403\n","Iter: 014/100 | Train Loss: 0.00122678\n","Adjusting Layer 1, Kernel Nodes: 690, Adptive Nodes:110\n","Iter: 015/100 | Train Loss: 0.00125218\n","Adjusting Layer 1, Kernel Nodes: 641, Adptive Nodes:159\n","Iter: 016/100 | Train Loss: 0.00124891\n","Iter: 017/100 | Train Loss: 0.00120587\n","Iter: 018/100 | Train Loss: 0.00113371\n","Iter: 019/100 | Train Loss: 0.00105504\n","Iter: 020/100 | Train Loss: 0.00099143\n","Iter: 021/100 | Train Loss: 0.00095397\n","Iter: 022/100 | Train Loss: 0.00094018\n","Iter: 023/100 | Train Loss: 0.00093798\n","Iter: 024/100 | Train Loss: 0.00093320\n","Iter: 025/100 | Train Loss: 0.00091639\n","Iter: 026/100 | Train Loss: 0.00088614\n","Iter: 027/100 | Train Loss: 0.00084803\n","Iter: 028/100 | Train Loss: 0.00081056\n","Iter: 029/100 | Train Loss: 0.00078069\n","Iter: 030/100 | Train Loss: 0.00076095\n","Iter: 031/100 | Train Loss: 0.00074933\n","Iter: 032/100 | Train Loss: 0.00074096\n","Iter: 033/100 | Train Loss: 0.00073100\n","Iter: 034/100 | Train Loss: 0.00071669\n","Iter: 035/100 | Train Loss: 0.00069818\n","Iter: 036/100 | Train Loss: 0.00067783\n","Iter: 037/100 | Train Loss: 0.00065863\n","Iter: 038/100 | Train Loss: 0.00064272\n","Iter: 039/100 | Train Loss: 0.00063064\n","Iter: 040/100 | Train Loss: 0.00062137\n","Iter: 041/100 | Train Loss: 0.00061307\n","Iter: 042/100 | Train Loss: 0.00060410\n","Iter: 043/100 | Train Loss: 0.00059366\n","Iter: 044/100 | Train Loss: 0.00058201\n","Iter: 045/100 | Train Loss: 0.00057004\n","Iter: 046/100 | Train Loss: 0.00055882\n","Iter: 047/100 | Train Loss: 0.00054899\n","Iter: 048/100 | Train Loss: 0.00054060\n","Iter: 049/100 | Train Loss: 0.00053318\n","Iter: 050/100 | Train Loss: 0.00052609\n","Iter: 051/100 | Train Loss: 0.00051880\n","Iter: 052/100 | Train Loss: 0.00051111\n","Iter: 053/100 | Train Loss: 0.00050314\n","Iter: 054/100 | Train Loss: 0.00049524\n","Iter: 055/100 | Train Loss: 0.00048774\n","Iter: 056/100 | Train Loss: 0.00048082\n","Iter: 057/100 | Train Loss: 0.00047444\n","Iter: 058/100 | Train Loss: 0.00046843\n","Iter: 059/100 | Train Loss: 0.00046258\n","Iter: 060/100 | Train Loss: 0.00045672\n","Iter: 061/100 | Train Loss: 0.00045080\n","Iter: 062/100 | Train Loss: 0.00044488\n","Iter: 063/100 | Train Loss: 0.00043908\n","Iter: 064/100 | Train Loss: 0.00043348\n","Iter: 065/100 | Train Loss: 0.00042814\n","Iter: 066/100 | Train Loss: 0.00042303\n","Iter: 067/100 | Train Loss: 0.00041810\n","Iter: 068/100 | Train Loss: 0.00041328\n","Iter: 069/100 | Train Loss: 0.00040850\n","Iter: 070/100 | Train Loss: 0.00040375\n","Iter: 071/100 | Train Loss: 0.00039906\n","Iter: 072/100 | Train Loss: 0.00039447\n","Iter: 073/100 | Train Loss: 0.00038998\n","Iter: 074/100 | Train Loss: 0.00038563\n","Iter: 075/100 | Train Loss: 0.00038140\n","Iter: 076/100 | Train Loss: 0.00037728\n","Iter: 077/100 | Train Loss: 0.00037323\n","Iter: 078/100 | Train Loss: 0.00036924\n","Iter: 079/100 | Train Loss: 0.00036531\n","Iter: 080/100 | Train Loss: 0.00036143\n","Iter: 081/100 | Train Loss: 0.00035763\n","Iter: 082/100 | Train Loss: 0.00035389\n","Iter: 083/100 | Train Loss: 0.00035023\n","Iter: 084/100 | Train Loss: 0.00034664\n","Iter: 085/100 | Train Loss: 0.00034311\n","Iter: 086/100 | Train Loss: 0.00033964\n","Iter: 087/100 | Train Loss: 0.00033622\n","Iter: 088/100 | Train Loss: 0.00033285\n","Iter: 089/100 | Train Loss: 0.00032953\n","Iter: 090/100 | Train Loss: 0.00032627\n","Iter: 091/100 | Train Loss: 0.00032306\n","Iter: 092/100 | Train Loss: 0.00031990\n","Iter: 093/100 | Train Loss: 0.00031679\n","Iter: 094/100 | Train Loss: 0.00031373\n","Iter: 095/100 | Train Loss: 0.00031073\n","Iter: 096/100 | Train Loss: 0.00030777\n","Iter: 097/100 | Train Loss: 0.00030485\n","Iter: 098/100 | Train Loss: 0.00030197\n","Iter: 099/100 | Train Loss: 0.00029913\n","\n","Iter: 099/100 | Test Loss: 0.00098737 | Test acc: 64.6200\n","scale:1.100000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00237281\n","Iter: 001/100 | Train Loss: 0.00200517\n","Iter: 002/100 | Train Loss: 0.00167512\n","Iter: 003/100 | Train Loss: 0.00149027\n","Iter: 004/100 | Train Loss: 0.00147401\n","Iter: 005/100 | Train Loss: 0.00157495\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00170266\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00177413\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 008/100 | Train Loss: 0.00174776\n","Iter: 009/100 | Train Loss: 0.00163275\n","Iter: 010/100 | Train Loss: 0.00147414\n","Iter: 011/100 | Train Loss: 0.00132576\n","Iter: 012/100 | Train Loss: 0.00122586\n","Iter: 013/100 | Train Loss: 0.00118550\n","Iter: 014/100 | Train Loss: 0.00119055\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 015/100 | Train Loss: 0.00121295\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 016/100 | Train Loss: 0.00122473\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00120870\n","Iter: 018/100 | Train Loss: 0.00116260\n","Iter: 019/100 | Train Loss: 0.00109682\n","Iter: 020/100 | Train Loss: 0.00102778\n","Iter: 021/100 | Train Loss: 0.00097037\n","Iter: 022/100 | Train Loss: 0.00093247\n","Iter: 023/100 | Train Loss: 0.00091368\n","Iter: 024/100 | Train Loss: 0.00090696\n","Iter: 025/100 | Train Loss: 0.00090270\n","Iter: 026/100 | Train Loss: 0.00089270\n","Iter: 027/100 | Train Loss: 0.00087313\n","Iter: 028/100 | Train Loss: 0.00084502\n","Iter: 029/100 | Train Loss: 0.00081274\n","Iter: 030/100 | Train Loss: 0.00078169\n","Iter: 031/100 | Train Loss: 0.00075609\n","Iter: 032/100 | Train Loss: 0.00073761\n","Iter: 033/100 | Train Loss: 0.00072524\n","Iter: 034/100 | Train Loss: 0.00071620\n","Iter: 035/100 | Train Loss: 0.00070736\n","Iter: 036/100 | Train Loss: 0.00069639\n","Iter: 037/100 | Train Loss: 0.00068243\n","Iter: 038/100 | Train Loss: 0.00066612\n","Iter: 039/100 | Train Loss: 0.00064911\n","Iter: 040/100 | Train Loss: 0.00063317\n","Iter: 041/100 | Train Loss: 0.00061953\n","Iter: 042/100 | Train Loss: 0.00060851\n","Iter: 043/100 | Train Loss: 0.00059954\n","Iter: 044/100 | Train Loss: 0.00059155\n","Iter: 045/100 | Train Loss: 0.00058349\n","Iter: 046/100 | Train Loss: 0.00057469\n","Iter: 047/100 | Train Loss: 0.00056505\n","Iter: 048/100 | Train Loss: 0.00055489\n","Iter: 049/100 | Train Loss: 0.00054480\n","Iter: 050/100 | Train Loss: 0.00053536\n","Iter: 051/100 | Train Loss: 0.00052687\n","Iter: 052/100 | Train Loss: 0.00051933\n","Iter: 053/100 | Train Loss: 0.00051250\n","Iter: 054/100 | Train Loss: 0.00050600\n","Iter: 055/100 | Train Loss: 0.00049948\n","Iter: 056/100 | Train Loss: 0.00049274\n","Iter: 057/100 | Train Loss: 0.00048579\n","Iter: 058/100 | Train Loss: 0.00047880\n","Iter: 059/100 | Train Loss: 0.00047196\n","Iter: 060/100 | Train Loss: 0.00046545\n","Iter: 061/100 | Train Loss: 0.00045937\n","Iter: 062/100 | Train Loss: 0.00045367\n","Iter: 063/100 | Train Loss: 0.00044823\n","Iter: 064/100 | Train Loss: 0.00044291\n","Iter: 065/100 | Train Loss: 0.00043762\n","Iter: 066/100 | Train Loss: 0.00043230\n","Iter: 067/100 | Train Loss: 0.00042700\n","Iter: 068/100 | Train Loss: 0.00042176\n","Iter: 069/100 | Train Loss: 0.00041668\n","Iter: 070/100 | Train Loss: 0.00041179\n","Iter: 071/100 | Train Loss: 0.00040710\n","Iter: 072/100 | Train Loss: 0.00040258\n","Iter: 073/100 | Train Loss: 0.00039819\n","Iter: 074/100 | Train Loss: 0.00039387\n","Iter: 075/100 | Train Loss: 0.00038958\n","Iter: 076/100 | Train Loss: 0.00038533\n","Iter: 077/100 | Train Loss: 0.00038114\n","Iter: 078/100 | Train Loss: 0.00037702\n","Iter: 079/100 | Train Loss: 0.00037300\n","Iter: 080/100 | Train Loss: 0.00036909\n","Iter: 081/100 | Train Loss: 0.00036528\n","Iter: 082/100 | Train Loss: 0.00036156\n","Iter: 083/100 | Train Loss: 0.00035791\n","Iter: 084/100 | Train Loss: 0.00035431\n","Iter: 085/100 | Train Loss: 0.00035076\n","Iter: 086/100 | Train Loss: 0.00034725\n","Iter: 087/100 | Train Loss: 0.00034379\n","Iter: 088/100 | Train Loss: 0.00034039\n","Iter: 089/100 | Train Loss: 0.00033706\n","Iter: 090/100 | Train Loss: 0.00033380\n","Iter: 091/100 | Train Loss: 0.00033060\n","Iter: 092/100 | Train Loss: 0.00032745\n","Iter: 093/100 | Train Loss: 0.00032434\n","Iter: 094/100 | Train Loss: 0.00032128\n","Iter: 095/100 | Train Loss: 0.00031826\n","Iter: 096/100 | Train Loss: 0.00031528\n","Iter: 097/100 | Train Loss: 0.00031235\n","Iter: 098/100 | Train Loss: 0.00030947\n","Iter: 099/100 | Train Loss: 0.00030663\n","\n","Iter: 099/100 | Test Loss: 0.00098671 | Test acc: 64.2000\n","scale:1.100000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00237513\n","Iter: 001/100 | Train Loss: 0.00200979\n","Iter: 002/100 | Train Loss: 0.00167977\n","Iter: 003/100 | Train Loss: 0.00149144\n","Iter: 004/100 | Train Loss: 0.00146956\n","Iter: 005/100 | Train Loss: 0.00156533\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00168952\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00175960\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 008/100 | Train Loss: 0.00173163\n","Iter: 009/100 | Train Loss: 0.00161543\n","Iter: 010/100 | Train Loss: 0.00145599\n","Iter: 011/100 | Train Loss: 0.00130687\n","Iter: 012/100 | Train Loss: 0.00120552\n","Iter: 013/100 | Train Loss: 0.00116211\n","Iter: 014/100 | Train Loss: 0.00116277\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00117961\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 016/100 | Train Loss: 0.00118500\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 017/100 | Train Loss: 0.00116069\n","Iter: 018/100 | Train Loss: 0.00110648\n","Iter: 019/100 | Train Loss: 0.00103504\n","Iter: 020/100 | Train Loss: 0.00096394\n","Iter: 021/100 | Train Loss: 0.00090774\n","Iter: 022/100 | Train Loss: 0.00087244\n","Iter: 023/100 | Train Loss: 0.00085476\n","Iter: 024/100 | Train Loss: 0.00084580\n","Iter: 025/100 | Train Loss: 0.00083583\n","Iter: 026/100 | Train Loss: 0.00081822\n","Iter: 027/100 | Train Loss: 0.00079154\n","Iter: 028/100 | Train Loss: 0.00075894\n","Iter: 029/100 | Train Loss: 0.00072574\n","Iter: 030/100 | Train Loss: 0.00069680\n","Iter: 031/100 | Train Loss: 0.00067467\n","Iter: 032/100 | Train Loss: 0.00065902\n","Iter: 033/100 | Train Loss: 0.00064754\n","Iter: 034/100 | Train Loss: 0.00063710\n","Iter: 035/100 | Train Loss: 0.00062521\n","Iter: 036/100 | Train Loss: 0.00061080\n","Iter: 037/100 | Train Loss: 0.00059425\n","Iter: 038/100 | Train Loss: 0.00057693\n","Iter: 039/100 | Train Loss: 0.00056052\n","Iter: 040/100 | Train Loss: 0.00054632\n","Iter: 041/100 | Train Loss: 0.00053469\n","Iter: 042/100 | Train Loss: 0.00052514\n","Iter: 043/100 | Train Loss: 0.00051665\n","Iter: 044/100 | Train Loss: 0.00050820\n","Iter: 045/100 | Train Loss: 0.00049906\n","Iter: 046/100 | Train Loss: 0.00048919\n","Iter: 047/100 | Train Loss: 0.00047893\n","Iter: 048/100 | Train Loss: 0.00046892\n","Iter: 049/100 | Train Loss: 0.00045972\n","Iter: 050/100 | Train Loss: 0.00045155\n","Iter: 051/100 | Train Loss: 0.00044432\n","Iter: 052/100 | Train Loss: 0.00043772\n","Iter: 053/100 | Train Loss: 0.00043132\n","Iter: 054/100 | Train Loss: 0.00042480\n","Iter: 055/100 | Train Loss: 0.00041803\n","Iter: 056/100 | Train Loss: 0.00041112\n","Iter: 057/100 | Train Loss: 0.00040429\n","Iter: 058/100 | Train Loss: 0.00039778\n","Iter: 059/100 | Train Loss: 0.00039172\n","Iter: 060/100 | Train Loss: 0.00038612\n","Iter: 061/100 | Train Loss: 0.00038085\n","Iter: 062/100 | Train Loss: 0.00037575\n","Iter: 063/100 | Train Loss: 0.00037064\n","Iter: 064/100 | Train Loss: 0.00036545\n","Iter: 065/100 | Train Loss: 0.00036023\n","Iter: 066/100 | Train Loss: 0.00035507\n","Iter: 067/100 | Train Loss: 0.00035007\n","Iter: 068/100 | Train Loss: 0.00034532\n","Iter: 069/100 | Train Loss: 0.00034080\n","Iter: 070/100 | Train Loss: 0.00033645\n","Iter: 071/100 | Train Loss: 0.00033220\n","Iter: 072/100 | Train Loss: 0.00032799\n","Iter: 073/100 | Train Loss: 0.00032380\n","Iter: 074/100 | Train Loss: 0.00031965\n","Iter: 075/100 | Train Loss: 0.00031560\n","Iter: 076/100 | Train Loss: 0.00031165\n","Iter: 077/100 | Train Loss: 0.00030781\n","Iter: 078/100 | Train Loss: 0.00030409\n","Iter: 079/100 | Train Loss: 0.00030048\n","Iter: 080/100 | Train Loss: 0.00029694\n","Iter: 081/100 | Train Loss: 0.00029346\n","Iter: 082/100 | Train Loss: 0.00029003\n","Iter: 083/100 | Train Loss: 0.00028665\n","Iter: 084/100 | Train Loss: 0.00028333\n","Iter: 085/100 | Train Loss: 0.00028007\n","Iter: 086/100 | Train Loss: 0.00027687\n","Iter: 087/100 | Train Loss: 0.00027375\n","Iter: 088/100 | Train Loss: 0.00027069\n","Iter: 089/100 | Train Loss: 0.00026768\n","Iter: 090/100 | Train Loss: 0.00026471\n","Iter: 091/100 | Train Loss: 0.00026177\n","Iter: 092/100 | Train Loss: 0.00025888\n","Iter: 093/100 | Train Loss: 0.00025604\n","Iter: 094/100 | Train Loss: 0.00025325\n","Iter: 095/100 | Train Loss: 0.00025051\n","Iter: 096/100 | Train Loss: 0.00024782\n","Iter: 097/100 | Train Loss: 0.00024518\n","Iter: 098/100 | Train Loss: 0.00024258\n","Iter: 099/100 | Train Loss: 0.00024001\n","\n","Iter: 099/100 | Test Loss: 0.00097254 | Test acc: 64.8800\n","scale:1.150000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00223646\n","Iter: 001/100 | Train Loss: 0.00176022\n","Iter: 002/100 | Train Loss: 0.00149087\n","Iter: 003/100 | Train Loss: 0.00152637\n","Adjusting Layer 1, Kernel Nodes: 598, Adptive Nodes:202\n","Iter: 004/100 | Train Loss: 0.00175707\n","Adjusting Layer 1, Kernel Nodes: 427, Adptive Nodes:373\n","Iter: 005/100 | Train Loss: 0.00184733\n","Adjusting Layer 1, Kernel Nodes: 343, Adptive Nodes:457\n","Iter: 006/100 | Train Loss: 0.00165020\n","Iter: 007/100 | Train Loss: 0.00136147\n","Iter: 008/100 | Train Loss: 0.00120790\n","Iter: 009/100 | Train Loss: 0.00122592\n","Adjusting Layer 1, Kernel Nodes: 447, Adptive Nodes:353\n","Iter: 010/100 | Train Loss: 0.00126139\n","Adjusting Layer 1, Kernel Nodes: 366, Adptive Nodes:434\n","Iter: 011/100 | Train Loss: 0.00124620\n","Iter: 012/100 | Train Loss: 0.00112834\n","Iter: 013/100 | Train Loss: 0.00098244\n","Iter: 014/100 | Train Loss: 0.00089832\n","Iter: 015/100 | Train Loss: 0.00089238\n","Iter: 016/100 | Train Loss: 0.00090750\n","Adjusting Layer 1, Kernel Nodes: 450, Adptive Nodes:350\n","Iter: 017/100 | Train Loss: 0.00087941\n","Iter: 018/100 | Train Loss: 0.00080765\n","Iter: 019/100 | Train Loss: 0.00072926\n","Iter: 020/100 | Train Loss: 0.00068168\n","Iter: 021/100 | Train Loss: 0.00067041\n","Iter: 022/100 | Train Loss: 0.00067099\n","Adjusting Layer 1, Kernel Nodes: 340, Adptive Nodes:460\n","Iter: 023/100 | Train Loss: 0.00065269\n","Iter: 024/100 | Train Loss: 0.00061636\n","Iter: 025/100 | Train Loss: 0.00057863\n","Iter: 026/100 | Train Loss: 0.00055528\n","Iter: 027/100 | Train Loss: 0.00054789\n","Iter: 028/100 | Train Loss: 0.00054523\n","Iter: 029/100 | Train Loss: 0.00053524\n","Iter: 030/100 | Train Loss: 0.00051529\n","Iter: 031/100 | Train Loss: 0.00049246\n","Iter: 032/100 | Train Loss: 0.00047549\n","Iter: 033/100 | Train Loss: 0.00046698\n","Iter: 034/100 | Train Loss: 0.00046236\n","Iter: 035/100 | Train Loss: 0.00045511\n","Iter: 036/100 | Train Loss: 0.00044267\n","Iter: 037/100 | Train Loss: 0.00042776\n","Iter: 038/100 | Train Loss: 0.00041505\n","Iter: 039/100 | Train Loss: 0.00040678\n","Iter: 040/100 | Train Loss: 0.00040130\n","Iter: 041/100 | Train Loss: 0.00039517\n","Iter: 042/100 | Train Loss: 0.00038646\n","Iter: 043/100 | Train Loss: 0.00037610\n","Iter: 044/100 | Train Loss: 0.00036652\n","Iter: 045/100 | Train Loss: 0.00035924\n","Iter: 046/100 | Train Loss: 0.00035373\n","Iter: 047/100 | Train Loss: 0.00034823\n","Iter: 048/100 | Train Loss: 0.00034159\n","Iter: 049/100 | Train Loss: 0.00033406\n","Iter: 050/100 | Train Loss: 0.00032676\n","Iter: 051/100 | Train Loss: 0.00032058\n","Iter: 052/100 | Train Loss: 0.00031542\n","Iter: 053/100 | Train Loss: 0.00031048\n","Iter: 054/100 | Train Loss: 0.00030513\n","Iter: 055/100 | Train Loss: 0.00029932\n","Iter: 056/100 | Train Loss: 0.00029359\n","Iter: 057/100 | Train Loss: 0.00028838\n","Iter: 058/100 | Train Loss: 0.00028376\n","Iter: 059/100 | Train Loss: 0.00027939\n","Iter: 060/100 | Train Loss: 0.00027490\n","Iter: 061/100 | Train Loss: 0.00027024\n","Iter: 062/100 | Train Loss: 0.00026560\n","Iter: 063/100 | Train Loss: 0.00026121\n","Iter: 064/100 | Train Loss: 0.00025715\n","Iter: 065/100 | Train Loss: 0.00025329\n","Iter: 066/100 | Train Loss: 0.00024947\n","Iter: 067/100 | Train Loss: 0.00024559\n","Iter: 068/100 | Train Loss: 0.00024173\n","Iter: 069/100 | Train Loss: 0.00023800\n","Iter: 070/100 | Train Loss: 0.00023446\n","Iter: 071/100 | Train Loss: 0.00023106\n","Iter: 072/100 | Train Loss: 0.00022773\n","Iter: 073/100 | Train Loss: 0.00022441\n","Iter: 074/100 | Train Loss: 0.00022110\n","Iter: 075/100 | Train Loss: 0.00021787\n","Iter: 076/100 | Train Loss: 0.00021476\n","Iter: 077/100 | Train Loss: 0.00021177\n","Iter: 078/100 | Train Loss: 0.00020884\n","Iter: 079/100 | Train Loss: 0.00020594\n","Iter: 080/100 | Train Loss: 0.00020306\n","Iter: 081/100 | Train Loss: 0.00020025\n","Iter: 082/100 | Train Loss: 0.00019751\n","Iter: 083/100 | Train Loss: 0.00019484\n","Iter: 084/100 | Train Loss: 0.00019224\n","Iter: 085/100 | Train Loss: 0.00018968\n","Iter: 086/100 | Train Loss: 0.00018715\n","Iter: 087/100 | Train Loss: 0.00018466\n","Iter: 088/100 | Train Loss: 0.00018223\n","Iter: 089/100 | Train Loss: 0.00017985\n","Iter: 090/100 | Train Loss: 0.00017752\n","Iter: 091/100 | Train Loss: 0.00017523\n","Iter: 092/100 | Train Loss: 0.00017297\n","Iter: 093/100 | Train Loss: 0.00017074\n","Iter: 094/100 | Train Loss: 0.00016857\n","Iter: 095/100 | Train Loss: 0.00016643\n","Iter: 096/100 | Train Loss: 0.00016433\n","Iter: 097/100 | Train Loss: 0.00016228\n","Iter: 098/100 | Train Loss: 0.00016025\n","Iter: 099/100 | Train Loss: 0.00015825\n","\n","Iter: 099/100 | Test Loss: 0.00100348 | Test acc: 65.1900\n","scale:1.150000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00227462\n","Iter: 001/100 | Train Loss: 0.00182185\n","Iter: 002/100 | Train Loss: 0.00152170\n","Iter: 003/100 | Train Loss: 0.00148796\n","Iter: 004/100 | Train Loss: 0.00164996\n","Adjusting Layer 1, Kernel Nodes: 629, Adptive Nodes:171\n","Iter: 005/100 | Train Loss: 0.00182813\n","Adjusting Layer 1, Kernel Nodes: 529, Adptive Nodes:271\n","Iter: 006/100 | Train Loss: 0.00175700\n","Iter: 007/100 | Train Loss: 0.00147880\n","Iter: 008/100 | Train Loss: 0.00124822\n","Iter: 009/100 | Train Loss: 0.00120712\n","Iter: 010/100 | Train Loss: 0.00127334\n","Adjusting Layer 1, Kernel Nodes: 384, Adptive Nodes:416\n","Iter: 011/100 | Train Loss: 0.00126403\n","Iter: 012/100 | Train Loss: 0.00113775\n","Iter: 013/100 | Train Loss: 0.00097836\n","Iter: 014/100 | Train Loss: 0.00088955\n","Iter: 015/100 | Train Loss: 0.00088634\n","Iter: 016/100 | Train Loss: 0.00089820\n","Adjusting Layer 1, Kernel Nodes: 540, Adptive Nodes:260\n","Iter: 017/100 | Train Loss: 0.00083476\n","Iter: 018/100 | Train Loss: 0.00073374\n","Iter: 019/100 | Train Loss: 0.00067191\n","Iter: 020/100 | Train Loss: 0.00066759\n","Iter: 021/100 | Train Loss: 0.00066917\n","Adjusting Layer 1, Kernel Nodes: 6, Adptive Nodes:794\n","Iter: 022/100 | Train Loss: 0.00064792\n","Iter: 023/100 | Train Loss: 0.00060878\n","Iter: 024/100 | Train Loss: 0.00056796\n","Iter: 025/100 | Train Loss: 0.00054070\n","Iter: 026/100 | Train Loss: 0.00053104\n","Iter: 027/100 | Train Loss: 0.00053108\n","Adjusting Layer 1, Kernel Nodes: 635, Adptive Nodes:165\n","Iter: 028/100 | Train Loss: 0.00052522\n","Iter: 029/100 | Train Loss: 0.00050510\n","Iter: 030/100 | Train Loss: 0.00047857\n","Iter: 031/100 | Train Loss: 0.00045906\n","Iter: 032/100 | Train Loss: 0.00045055\n","Iter: 033/100 | Train Loss: 0.00044546\n","Iter: 034/100 | Train Loss: 0.00043491\n","Iter: 035/100 | Train Loss: 0.00041806\n","Iter: 036/100 | Train Loss: 0.00040111\n","Iter: 037/100 | Train Loss: 0.00038945\n","Iter: 038/100 | Train Loss: 0.00038234\n","Iter: 039/100 | Train Loss: 0.00037511\n","Iter: 040/100 | Train Loss: 0.00036460\n","Iter: 041/100 | Train Loss: 0.00035213\n","Iter: 042/100 | Train Loss: 0.00034107\n","Iter: 043/100 | Train Loss: 0.00033299\n","Iter: 044/100 | Train Loss: 0.00032657\n","Iter: 045/100 | Train Loss: 0.00031933\n","Iter: 046/100 | Train Loss: 0.00031053\n","Iter: 047/100 | Train Loss: 0.00030154\n","Iter: 048/100 | Train Loss: 0.00029392\n","Iter: 049/100 | Train Loss: 0.00028776\n","Iter: 050/100 | Train Loss: 0.00028189\n","Iter: 051/100 | Train Loss: 0.00027544\n","Iter: 052/100 | Train Loss: 0.00026856\n","Iter: 053/100 | Train Loss: 0.00026206\n","Iter: 054/100 | Train Loss: 0.00025639\n","Iter: 055/100 | Train Loss: 0.00025123\n","Iter: 056/100 | Train Loss: 0.00024606\n","Iter: 057/100 | Train Loss: 0.00024071\n","Iter: 058/100 | Train Loss: 0.00023536\n","Iter: 059/100 | Train Loss: 0.00023035\n","Iter: 060/100 | Train Loss: 0.00022575\n","Iter: 061/100 | Train Loss: 0.00022136\n","Iter: 062/100 | Train Loss: 0.00021699\n","Iter: 063/100 | Train Loss: 0.00021259\n","Iter: 064/100 | Train Loss: 0.00020830\n","Iter: 065/100 | Train Loss: 0.00020427\n","Iter: 066/100 | Train Loss: 0.00020045\n","Iter: 067/100 | Train Loss: 0.00019671\n","Iter: 068/100 | Train Loss: 0.00019301\n","Iter: 069/100 | Train Loss: 0.00018938\n","Iter: 070/100 | Train Loss: 0.00018585\n","Iter: 071/100 | Train Loss: 0.00018246\n","Iter: 072/100 | Train Loss: 0.00017920\n","Iter: 073/100 | Train Loss: 0.00017600\n","Iter: 074/100 | Train Loss: 0.00017286\n","Iter: 075/100 | Train Loss: 0.00016979\n","Iter: 076/100 | Train Loss: 0.00016682\n","Iter: 077/100 | Train Loss: 0.00016392\n","Iter: 078/100 | Train Loss: 0.00016109\n","Iter: 079/100 | Train Loss: 0.00015834\n","Iter: 080/100 | Train Loss: 0.00015563\n","Iter: 081/100 | Train Loss: 0.00015298\n","Iter: 082/100 | Train Loss: 0.00015042\n","Iter: 083/100 | Train Loss: 0.00014792\n","Iter: 084/100 | Train Loss: 0.00014546\n","Iter: 085/100 | Train Loss: 0.00014307\n","Iter: 086/100 | Train Loss: 0.00014073\n","Iter: 087/100 | Train Loss: 0.00013843\n","Iter: 088/100 | Train Loss: 0.00013620\n","Iter: 089/100 | Train Loss: 0.00013401\n","Iter: 090/100 | Train Loss: 0.00013187\n","Iter: 091/100 | Train Loss: 0.00012979\n","Iter: 092/100 | Train Loss: 0.00012776\n","Iter: 093/100 | Train Loss: 0.00012575\n","Iter: 094/100 | Train Loss: 0.00012379\n","Iter: 095/100 | Train Loss: 0.00012188\n","Iter: 096/100 | Train Loss: 0.00011999\n","Iter: 097/100 | Train Loss: 0.00011817\n","Iter: 098/100 | Train Loss: 0.00011638\n","Iter: 099/100 | Train Loss: 0.00011461\n","\n","Iter: 099/100 | Test Loss: 0.00101558 | Test acc: 65.1200\n","scale:1.150000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00230396\n","Iter: 001/100 | Train Loss: 0.00187298\n","Iter: 002/100 | Train Loss: 0.00155609\n","Iter: 003/100 | Train Loss: 0.00147180\n","Iter: 004/100 | Train Loss: 0.00158666\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 005/100 | Train Loss: 0.00177503\n","Adjusting Layer 1, Kernel Nodes: 593, Adptive Nodes:207\n","Iter: 006/100 | Train Loss: 0.00179426\n","Adjusting Layer 1, Kernel Nodes: 359, Adptive Nodes:441\n","Iter: 007/100 | Train Loss: 0.00158983\n","Iter: 008/100 | Train Loss: 0.00133630\n","Iter: 009/100 | Train Loss: 0.00120837\n","Iter: 010/100 | Train Loss: 0.00122267\n","Adjusting Layer 1, Kernel Nodes: 386, Adptive Nodes:414\n","Iter: 011/100 | Train Loss: 0.00126259\n","Adjusting Layer 1, Kernel Nodes: 389, Adptive Nodes:411\n","Iter: 012/100 | Train Loss: 0.00124381\n","Iter: 013/100 | Train Loss: 0.00114473\n","Iter: 014/100 | Train Loss: 0.00101029\n","Iter: 015/100 | Train Loss: 0.00090724\n","Iter: 016/100 | Train Loss: 0.00086772\n","Iter: 017/100 | Train Loss: 0.00087144\n","Adjusting Layer 1, Kernel Nodes: 517, Adptive Nodes:283\n","Iter: 018/100 | Train Loss: 0.00086791\n","Iter: 019/100 | Train Loss: 0.00082176\n","Iter: 020/100 | Train Loss: 0.00074818\n","Iter: 021/100 | Train Loss: 0.00068750\n","Iter: 022/100 | Train Loss: 0.00066268\n","Iter: 023/100 | Train Loss: 0.00066165\n","Iter: 024/100 | Train Loss: 0.00065527\n","Iter: 025/100 | Train Loss: 0.00062738\n","Iter: 026/100 | Train Loss: 0.00058674\n","Iter: 027/100 | Train Loss: 0.00055362\n","Iter: 028/100 | Train Loss: 0.00053875\n","Iter: 029/100 | Train Loss: 0.00053528\n","Iter: 030/100 | Train Loss: 0.00052847\n","Iter: 031/100 | Train Loss: 0.00051067\n","Iter: 032/100 | Train Loss: 0.00048671\n","Iter: 033/100 | Train Loss: 0.00046681\n","Iter: 034/100 | Train Loss: 0.00045604\n","Iter: 035/100 | Train Loss: 0.00045064\n","Iter: 036/100 | Train Loss: 0.00044330\n","Iter: 037/100 | Train Loss: 0.00043051\n","Iter: 038/100 | Train Loss: 0.00041489\n","Iter: 039/100 | Train Loss: 0.00040145\n","Iter: 040/100 | Train Loss: 0.00039259\n","Iter: 041/100 | Train Loss: 0.00038639\n","Iter: 042/100 | Train Loss: 0.00037926\n","Iter: 043/100 | Train Loss: 0.00036955\n","Iter: 044/100 | Train Loss: 0.00035859\n","Iter: 045/100 | Train Loss: 0.00034884\n","Iter: 046/100 | Train Loss: 0.00034143\n","Iter: 047/100 | Train Loss: 0.00033537\n","Iter: 048/100 | Train Loss: 0.00032895\n","Iter: 049/100 | Train Loss: 0.00032141\n","Iter: 050/100 | Train Loss: 0.00031336\n","Iter: 051/100 | Train Loss: 0.00030598\n","Iter: 052/100 | Train Loss: 0.00029980\n","Iter: 053/100 | Train Loss: 0.00029436\n","Iter: 054/100 | Train Loss: 0.00028884\n","Iter: 055/100 | Train Loss: 0.00028285\n","Iter: 056/100 | Train Loss: 0.00027667\n","Iter: 057/100 | Train Loss: 0.00027085\n","Iter: 058/100 | Train Loss: 0.00026568\n","Iter: 059/100 | Train Loss: 0.00026096\n","Iter: 060/100 | Train Loss: 0.00025629\n","Iter: 061/100 | Train Loss: 0.00025143\n","Iter: 062/100 | Train Loss: 0.00024651\n","Iter: 063/100 | Train Loss: 0.00024181\n","Iter: 064/100 | Train Loss: 0.00023748\n","Iter: 065/100 | Train Loss: 0.00023343\n","Iter: 066/100 | Train Loss: 0.00022945\n","Iter: 067/100 | Train Loss: 0.00022543\n","Iter: 068/100 | Train Loss: 0.00022141\n","Iter: 069/100 | Train Loss: 0.00021752\n","Iter: 070/100 | Train Loss: 0.00021385\n","Iter: 071/100 | Train Loss: 0.00021035\n","Iter: 072/100 | Train Loss: 0.00020693\n","Iter: 073/100 | Train Loss: 0.00020353\n","Iter: 074/100 | Train Loss: 0.00020014\n","Iter: 075/100 | Train Loss: 0.00019684\n","Iter: 076/100 | Train Loss: 0.00019369\n","Iter: 077/100 | Train Loss: 0.00019066\n","Iter: 078/100 | Train Loss: 0.00018768\n","Iter: 079/100 | Train Loss: 0.00018474\n","Iter: 080/100 | Train Loss: 0.00018183\n","Iter: 081/100 | Train Loss: 0.00017900\n","Iter: 082/100 | Train Loss: 0.00017625\n","Iter: 083/100 | Train Loss: 0.00017359\n","Iter: 084/100 | Train Loss: 0.00017098\n","Iter: 085/100 | Train Loss: 0.00016841\n","Iter: 086/100 | Train Loss: 0.00016587\n","Iter: 087/100 | Train Loss: 0.00016339\n","Iter: 088/100 | Train Loss: 0.00016098\n","Iter: 089/100 | Train Loss: 0.00015862\n","Iter: 090/100 | Train Loss: 0.00015632\n","Iter: 091/100 | Train Loss: 0.00015405\n","Iter: 092/100 | Train Loss: 0.00015182\n","Iter: 093/100 | Train Loss: 0.00014963\n","Iter: 094/100 | Train Loss: 0.00014750\n","Iter: 095/100 | Train Loss: 0.00014541\n","Iter: 096/100 | Train Loss: 0.00014337\n","Iter: 097/100 | Train Loss: 0.00014136\n","Iter: 098/100 | Train Loss: 0.00013938\n","Iter: 099/100 | Train Loss: 0.00013744\n","\n","Iter: 099/100 | Test Loss: 0.00100300 | Test acc: 65.5700\n","scale:1.150000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00232705\n","Iter: 001/100 | Train Loss: 0.00191541\n","Iter: 002/100 | Train Loss: 0.00158989\n","Iter: 003/100 | Train Loss: 0.00146850\n","Iter: 004/100 | Train Loss: 0.00154182\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 005/100 | Train Loss: 0.00171979\n","Adjusting Layer 1, Kernel Nodes: 601, Adptive Nodes:199\n","Iter: 006/100 | Train Loss: 0.00180793\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00158782\n","Iter: 008/100 | Train Loss: 0.00130022\n","Iter: 009/100 | Train Loss: 0.00123107\n","Iter: 010/100 | Train Loss: 0.00132164\n","Adjusting Layer 1, Kernel Nodes: 122, Adptive Nodes:678\n","Iter: 011/100 | Train Loss: 0.00133409\n","Adjusting Layer 1, Kernel Nodes: 217, Adptive Nodes:583\n","Iter: 012/100 | Train Loss: 0.00123548\n","Iter: 013/100 | Train Loss: 0.00108237\n","Iter: 014/100 | Train Loss: 0.00095750\n","Iter: 015/100 | Train Loss: 0.00090750\n","Iter: 016/100 | Train Loss: 0.00091583\n","Adjusting Layer 1, Kernel Nodes: 617, Adptive Nodes:183\n","Iter: 017/100 | Train Loss: 0.00092144\n","Adjusting Layer 1, Kernel Nodes: 785, Adptive Nodes:15\n","Iter: 018/100 | Train Loss: 0.00085492\n","Iter: 019/100 | Train Loss: 0.00075518\n","Iter: 020/100 | Train Loss: 0.00069861\n","Iter: 021/100 | Train Loss: 0.00069357\n","Iter: 022/100 | Train Loss: 0.00068430\n","Iter: 023/100 | Train Loss: 0.00063759\n","Iter: 024/100 | Train Loss: 0.00058084\n","Iter: 025/100 | Train Loss: 0.00055236\n","Iter: 026/100 | Train Loss: 0.00054871\n","Iter: 027/100 | Train Loss: 0.00053790\n","Iter: 028/100 | Train Loss: 0.00050712\n","Iter: 029/100 | Train Loss: 0.00047471\n","Iter: 030/100 | Train Loss: 0.00045873\n","Iter: 031/100 | Train Loss: 0.00045329\n","Iter: 032/100 | Train Loss: 0.00044160\n","Iter: 033/100 | Train Loss: 0.00042007\n","Iter: 034/100 | Train Loss: 0.00039958\n","Iter: 035/100 | Train Loss: 0.00038785\n","Iter: 036/100 | Train Loss: 0.00038026\n","Iter: 037/100 | Train Loss: 0.00036875\n","Iter: 038/100 | Train Loss: 0.00035300\n","Iter: 039/100 | Train Loss: 0.00033880\n","Iter: 040/100 | Train Loss: 0.00032910\n","Iter: 041/100 | Train Loss: 0.00032104\n","Iter: 042/100 | Train Loss: 0.00031105\n","Iter: 043/100 | Train Loss: 0.00029954\n","Iter: 044/100 | Train Loss: 0.00028925\n","Iter: 045/100 | Train Loss: 0.00028120\n","Iter: 046/100 | Train Loss: 0.00027389\n","Iter: 047/100 | Train Loss: 0.00026583\n","Iter: 048/100 | Train Loss: 0.00025735\n","Iter: 049/100 | Train Loss: 0.00024960\n","Iter: 050/100 | Train Loss: 0.00024292\n","Iter: 051/100 | Train Loss: 0.00023669\n","Iter: 052/100 | Train Loss: 0.00023031\n","Iter: 053/100 | Train Loss: 0.00022387\n","Iter: 054/100 | Train Loss: 0.00021779\n","Iter: 055/100 | Train Loss: 0.00021227\n","Iter: 056/100 | Train Loss: 0.00020709\n","Iter: 057/100 | Train Loss: 0.00020199\n","Iter: 058/100 | Train Loss: 0.00019693\n","Iter: 059/100 | Train Loss: 0.00019205\n","Iter: 060/100 | Train Loss: 0.00018748\n","Iter: 061/100 | Train Loss: 0.00018320\n","Iter: 062/100 | Train Loss: 0.00017904\n","Iter: 063/100 | Train Loss: 0.00017492\n","Iter: 064/100 | Train Loss: 0.00017088\n","Iter: 065/100 | Train Loss: 0.00016704\n","Iter: 066/100 | Train Loss: 0.00016343\n","Iter: 067/100 | Train Loss: 0.00015994\n","Iter: 068/100 | Train Loss: 0.00015646\n","Iter: 069/100 | Train Loss: 0.00015304\n","Iter: 070/100 | Train Loss: 0.00014976\n","Iter: 071/100 | Train Loss: 0.00014667\n","Iter: 072/100 | Train Loss: 0.00014367\n","Iter: 073/100 | Train Loss: 0.00014067\n","Iter: 074/100 | Train Loss: 0.00013772\n","Iter: 075/100 | Train Loss: 0.00013490\n","Iter: 076/100 | Train Loss: 0.00013222\n","Iter: 077/100 | Train Loss: 0.00012960\n","Iter: 078/100 | Train Loss: 0.00012700\n","Iter: 079/100 | Train Loss: 0.00012445\n","Iter: 080/100 | Train Loss: 0.00012201\n","Iter: 081/100 | Train Loss: 0.00011968\n","Iter: 082/100 | Train Loss: 0.00011739\n","Iter: 083/100 | Train Loss: 0.00011513\n","Iter: 084/100 | Train Loss: 0.00011292\n","Iter: 085/100 | Train Loss: 0.00011080\n","Iter: 086/100 | Train Loss: 0.00010875\n","Iter: 087/100 | Train Loss: 0.00010674\n","Iter: 088/100 | Train Loss: 0.00010475\n","Iter: 089/100 | Train Loss: 0.00010282\n","Iter: 090/100 | Train Loss: 0.00010095\n","Iter: 091/100 | Train Loss: 0.00009914\n","Iter: 092/100 | Train Loss: 0.00009735\n","Iter: 093/100 | Train Loss: 0.00009560\n","Iter: 094/100 | Train Loss: 0.00009389\n","Iter: 095/100 | Train Loss: 0.00009224\n","Iter: 096/100 | Train Loss: 0.00009063\n","Iter: 097/100 | Train Loss: 0.00008904\n","Iter: 098/100 | Train Loss: 0.00008749\n","Iter: 099/100 | Train Loss: 0.00008597\n","\n","Iter: 099/100 | Test Loss: 0.00103359 | Test acc: 64.6200\n","scale:1.150000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00234425\n","Iter: 001/100 | Train Loss: 0.00194831\n","Iter: 002/100 | Train Loss: 0.00161911\n","Iter: 003/100 | Train Loss: 0.00147219\n","Iter: 004/100 | Train Loss: 0.00151286\n","Adjusting Layer 1, Kernel Nodes: 734, Adptive Nodes:66\n","Iter: 005/100 | Train Loss: 0.00167329\n","Adjusting Layer 1, Kernel Nodes: 603, Adptive Nodes:197\n","Iter: 006/100 | Train Loss: 0.00179614\n","Adjusting Layer 1, Kernel Nodes: 590, Adptive Nodes:210\n","Iter: 007/100 | Train Loss: 0.00172660\n","Iter: 008/100 | Train Loss: 0.00150733\n","Iter: 009/100 | Train Loss: 0.00129916\n","Iter: 010/100 | Train Loss: 0.00121720\n","Iter: 011/100 | Train Loss: 0.00124907\n","Adjusting Layer 1, Kernel Nodes: 495, Adptive Nodes:305\n","Iter: 012/100 | Train Loss: 0.00129016\n","Adjusting Layer 1, Kernel Nodes: 426, Adptive Nodes:374\n","Iter: 013/100 | Train Loss: 0.00125424\n","Iter: 014/100 | Train Loss: 0.00113602\n","Iter: 015/100 | Train Loss: 0.00100124\n","Iter: 016/100 | Train Loss: 0.00091842\n","Iter: 017/100 | Train Loss: 0.00090173\n","Iter: 018/100 | Train Loss: 0.00091019\n","Adjusting Layer 1, Kernel Nodes: 795, Adptive Nodes:5\n","Iter: 019/100 | Train Loss: 0.00087938\n","Iter: 020/100 | Train Loss: 0.00079647\n","Iter: 021/100 | Train Loss: 0.00071567\n","Iter: 022/100 | Train Loss: 0.00068175\n","Iter: 023/100 | Train Loss: 0.00067942\n","Iter: 024/100 | Train Loss: 0.00066388\n","Iter: 025/100 | Train Loss: 0.00061905\n","Iter: 026/100 | Train Loss: 0.00056992\n","Iter: 027/100 | Train Loss: 0.00054434\n","Iter: 028/100 | Train Loss: 0.00053937\n","Iter: 029/100 | Train Loss: 0.00053098\n","Iter: 030/100 | Train Loss: 0.00050635\n","Iter: 031/100 | Train Loss: 0.00047615\n","Iter: 032/100 | Train Loss: 0.00045662\n","Iter: 033/100 | Train Loss: 0.00044919\n","Iter: 034/100 | Train Loss: 0.00044206\n","Iter: 035/100 | Train Loss: 0.00042663\n","Iter: 036/100 | Train Loss: 0.00040665\n","Iter: 037/100 | Train Loss: 0.00039095\n","Iter: 038/100 | Train Loss: 0.00038206\n","Iter: 039/100 | Train Loss: 0.00037470\n","Iter: 040/100 | Train Loss: 0.00036363\n","Iter: 041/100 | Train Loss: 0.00034960\n","Iter: 042/100 | Train Loss: 0.00033706\n","Iter: 043/100 | Train Loss: 0.00032820\n","Iter: 044/100 | Train Loss: 0.00032101\n","Iter: 045/100 | Train Loss: 0.00031252\n","Iter: 046/100 | Train Loss: 0.00030237\n","Iter: 047/100 | Train Loss: 0.00029258\n","Iter: 048/100 | Train Loss: 0.00028469\n","Iter: 049/100 | Train Loss: 0.00027813\n","Iter: 050/100 | Train Loss: 0.00027133\n","Iter: 051/100 | Train Loss: 0.00026373\n","Iter: 052/100 | Train Loss: 0.00025611\n","Iter: 053/100 | Train Loss: 0.00024942\n","Iter: 054/100 | Train Loss: 0.00024365\n","Iter: 055/100 | Train Loss: 0.00023805\n","Iter: 056/100 | Train Loss: 0.00023216\n","Iter: 057/100 | Train Loss: 0.00022619\n","Iter: 058/100 | Train Loss: 0.00022066\n","Iter: 059/100 | Train Loss: 0.00021570\n","Iter: 060/100 | Train Loss: 0.00021102\n","Iter: 061/100 | Train Loss: 0.00020630\n","Iter: 062/100 | Train Loss: 0.00020153\n","Iter: 063/100 | Train Loss: 0.00019695\n","Iter: 064/100 | Train Loss: 0.00019272\n","Iter: 065/100 | Train Loss: 0.00018874\n","Iter: 066/100 | Train Loss: 0.00018482\n","Iter: 067/100 | Train Loss: 0.00018090\n","Iter: 068/100 | Train Loss: 0.00017707\n","Iter: 069/100 | Train Loss: 0.00017343\n","Iter: 070/100 | Train Loss: 0.00016998\n","Iter: 071/100 | Train Loss: 0.00016663\n","Iter: 072/100 | Train Loss: 0.00016331\n","Iter: 073/100 | Train Loss: 0.00016003\n","Iter: 074/100 | Train Loss: 0.00015687\n","Iter: 075/100 | Train Loss: 0.00015384\n","Iter: 076/100 | Train Loss: 0.00015092\n","Iter: 077/100 | Train Loss: 0.00014804\n","Iter: 078/100 | Train Loss: 0.00014520\n","Iter: 079/100 | Train Loss: 0.00014243\n","Iter: 080/100 | Train Loss: 0.00013977\n","Iter: 081/100 | Train Loss: 0.00013719\n","Iter: 082/100 | Train Loss: 0.00013468\n","Iter: 083/100 | Train Loss: 0.00013220\n","Iter: 084/100 | Train Loss: 0.00012977\n","Iter: 085/100 | Train Loss: 0.00012742\n","Iter: 086/100 | Train Loss: 0.00012515\n","Iter: 087/100 | Train Loss: 0.00012293\n","Iter: 088/100 | Train Loss: 0.00012075\n","Iter: 089/100 | Train Loss: 0.00011861\n","Iter: 090/100 | Train Loss: 0.00011653\n","Iter: 091/100 | Train Loss: 0.00011452\n","Iter: 092/100 | Train Loss: 0.00011255\n","Iter: 093/100 | Train Loss: 0.00011062\n","Iter: 094/100 | Train Loss: 0.00010873\n","Iter: 095/100 | Train Loss: 0.00010688\n","Iter: 096/100 | Train Loss: 0.00010508\n","Iter: 097/100 | Train Loss: 0.00010333\n","Iter: 098/100 | Train Loss: 0.00010161\n","Iter: 099/100 | Train Loss: 0.00009992\n","\n","Iter: 099/100 | Test Loss: 0.00102246 | Test acc: 64.7900\n","scale:1.150000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00235676\n","Iter: 001/100 | Train Loss: 0.00197286\n","Iter: 002/100 | Train Loss: 0.00164248\n","Iter: 003/100 | Train Loss: 0.00147835\n","Iter: 004/100 | Train Loss: 0.00149466\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 005/100 | Train Loss: 0.00163380\n","Adjusting Layer 1, Kernel Nodes: 646, Adptive Nodes:154\n","Iter: 006/100 | Train Loss: 0.00177127\n","Adjusting Layer 1, Kernel Nodes: 611, Adptive Nodes:189\n","Iter: 007/100 | Train Loss: 0.00177324\n","Adjusting Layer 1, Kernel Nodes: 635, Adptive Nodes:165\n","Iter: 008/100 | Train Loss: 0.00159480\n","Iter: 009/100 | Train Loss: 0.00136961\n","Iter: 010/100 | Train Loss: 0.00123648\n","Iter: 011/100 | Train Loss: 0.00123287\n","Iter: 012/100 | Train Loss: 0.00128815\n","Adjusting Layer 1, Kernel Nodes: 403, Adptive Nodes:397\n","Iter: 013/100 | Train Loss: 0.00129852\n","Adjusting Layer 1, Kernel Nodes: 527, Adptive Nodes:273\n","Iter: 014/100 | Train Loss: 0.00121864\n","Iter: 015/100 | Train Loss: 0.00108455\n","Iter: 016/100 | Train Loss: 0.00097064\n","Iter: 017/100 | Train Loss: 0.00092215\n","Iter: 018/100 | Train Loss: 0.00092464\n","Adjusting Layer 1, Kernel Nodes: 740, Adptive Nodes:60\n","Iter: 019/100 | Train Loss: 0.00092402\n","Iter: 020/100 | Train Loss: 0.00087698\n","Iter: 021/100 | Train Loss: 0.00079634\n","Iter: 022/100 | Train Loss: 0.00072852\n","Iter: 023/100 | Train Loss: 0.00070023\n","Iter: 024/100 | Train Loss: 0.00069653\n","Iter: 025/100 | Train Loss: 0.00068465\n","Iter: 026/100 | Train Loss: 0.00064983\n","Iter: 027/100 | Train Loss: 0.00060525\n","Iter: 028/100 | Train Loss: 0.00057278\n","Iter: 029/100 | Train Loss: 0.00055937\n","Iter: 030/100 | Train Loss: 0.00055358\n","Iter: 031/100 | Train Loss: 0.00054084\n","Iter: 032/100 | Train Loss: 0.00051796\n","Iter: 033/100 | Train Loss: 0.00049342\n","Iter: 034/100 | Train Loss: 0.00047617\n","Iter: 035/100 | Train Loss: 0.00046689\n","Iter: 036/100 | Train Loss: 0.00045929\n","Iter: 037/100 | Train Loss: 0.00044773\n","Iter: 038/100 | Train Loss: 0.00043231\n","Iter: 039/100 | Train Loss: 0.00041725\n","Iter: 040/100 | Train Loss: 0.00040582\n","Iter: 041/100 | Train Loss: 0.00039758\n","Iter: 042/100 | Train Loss: 0.00038967\n","Iter: 043/100 | Train Loss: 0.00038006\n","Iter: 044/100 | Train Loss: 0.00036913\n","Iter: 045/100 | Train Loss: 0.00035869\n","Iter: 046/100 | Train Loss: 0.00034993\n","Iter: 047/100 | Train Loss: 0.00034255\n","Iter: 048/100 | Train Loss: 0.00033541\n","Iter: 049/100 | Train Loss: 0.00032768\n","Iter: 050/100 | Train Loss: 0.00031949\n","Iter: 051/100 | Train Loss: 0.00031159\n","Iter: 052/100 | Train Loss: 0.00030456\n","Iter: 053/100 | Train Loss: 0.00029831\n","Iter: 054/100 | Train Loss: 0.00029229\n","Iter: 055/100 | Train Loss: 0.00028606\n","Iter: 056/100 | Train Loss: 0.00027965\n","Iter: 057/100 | Train Loss: 0.00027344\n","Iter: 058/100 | Train Loss: 0.00026775\n","Iter: 059/100 | Train Loss: 0.00026253\n","Iter: 060/100 | Train Loss: 0.00025749\n","Iter: 061/100 | Train Loss: 0.00025238\n","Iter: 062/100 | Train Loss: 0.00024720\n","Iter: 063/100 | Train Loss: 0.00024220\n","Iter: 064/100 | Train Loss: 0.00023754\n","Iter: 065/100 | Train Loss: 0.00023320\n","Iter: 066/100 | Train Loss: 0.00022897\n","Iter: 067/100 | Train Loss: 0.00022473\n","Iter: 068/100 | Train Loss: 0.00022049\n","Iter: 069/100 | Train Loss: 0.00021640\n","Iter: 070/100 | Train Loss: 0.00021255\n","Iter: 071/100 | Train Loss: 0.00020889\n","Iter: 072/100 | Train Loss: 0.00020529\n","Iter: 073/100 | Train Loss: 0.00020169\n","Iter: 074/100 | Train Loss: 0.00019814\n","Iter: 075/100 | Train Loss: 0.00019472\n","Iter: 076/100 | Train Loss: 0.00019146\n","Iter: 077/100 | Train Loss: 0.00018830\n","Iter: 078/100 | Train Loss: 0.00018519\n","Iter: 079/100 | Train Loss: 0.00018209\n","Iter: 080/100 | Train Loss: 0.00017906\n","Iter: 081/100 | Train Loss: 0.00017613\n","Iter: 082/100 | Train Loss: 0.00017330\n","Iter: 083/100 | Train Loss: 0.00017055\n","Iter: 084/100 | Train Loss: 0.00016782\n","Iter: 085/100 | Train Loss: 0.00016514\n","Iter: 086/100 | Train Loss: 0.00016251\n","Iter: 087/100 | Train Loss: 0.00015996\n","Iter: 088/100 | Train Loss: 0.00015749\n","Iter: 089/100 | Train Loss: 0.00015506\n","Iter: 090/100 | Train Loss: 0.00015267\n","Iter: 091/100 | Train Loss: 0.00015032\n","Iter: 092/100 | Train Loss: 0.00014803\n","Iter: 093/100 | Train Loss: 0.00014579\n","Iter: 094/100 | Train Loss: 0.00014361\n","Iter: 095/100 | Train Loss: 0.00014147\n","Iter: 096/100 | Train Loss: 0.00013936\n","Iter: 097/100 | Train Loss: 0.00013730\n","Iter: 098/100 | Train Loss: 0.00013528\n","Iter: 099/100 | Train Loss: 0.00013331\n","\n","Iter: 099/100 | Test Loss: 0.00100516 | Test acc: 65.2100\n","scale:1.150000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00236527\n","Iter: 001/100 | Train Loss: 0.00198988\n","Iter: 002/100 | Train Loss: 0.00165942\n","Iter: 003/100 | Train Loss: 0.00148426\n","Iter: 004/100 | Train Loss: 0.00148370\n","Iter: 005/100 | Train Loss: 0.00159781\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 006/100 | Train Loss: 0.00173238\n","Adjusting Layer 1, Kernel Nodes: 683, Adptive Nodes:117\n","Iter: 007/100 | Train Loss: 0.00178905\n","Adjusting Layer 1, Kernel Nodes: 668, Adptive Nodes:132\n","Iter: 008/100 | Train Loss: 0.00171373\n","Iter: 009/100 | Train Loss: 0.00154495\n","Iter: 010/100 | Train Loss: 0.00136613\n","Iter: 011/100 | Train Loss: 0.00124864\n","Iter: 012/100 | Train Loss: 0.00121574\n","Iter: 013/100 | Train Loss: 0.00124132\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 014/100 | Train Loss: 0.00127450\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 015/100 | Train Loss: 0.00126637\n","Iter: 016/100 | Train Loss: 0.00120107\n","Iter: 017/100 | Train Loss: 0.00110328\n","Iter: 018/100 | Train Loss: 0.00101344\n","Iter: 019/100 | Train Loss: 0.00095975\n","Iter: 020/100 | Train Loss: 0.00094384\n","Iter: 021/100 | Train Loss: 0.00094503\n","Adjusting Layer 1, Kernel Nodes: 560, Adptive Nodes:240\n","Iter: 022/100 | Train Loss: 0.00093667\n","Iter: 023/100 | Train Loss: 0.00090465\n","Iter: 024/100 | Train Loss: 0.00085377\n","Iter: 025/100 | Train Loss: 0.00080112\n","Iter: 026/100 | Train Loss: 0.00076231\n","Iter: 027/100 | Train Loss: 0.00074180\n","Iter: 028/100 | Train Loss: 0.00073249\n","Iter: 029/100 | Train Loss: 0.00072272\n","Iter: 030/100 | Train Loss: 0.00070453\n","Iter: 031/100 | Train Loss: 0.00067798\n","Iter: 032/100 | Train Loss: 0.00064923\n","Iter: 033/100 | Train Loss: 0.00062520\n","Iter: 034/100 | Train Loss: 0.00060898\n","Iter: 035/100 | Train Loss: 0.00059872\n","Iter: 036/100 | Train Loss: 0.00058976\n","Iter: 037/100 | Train Loss: 0.00057823\n","Iter: 038/100 | Train Loss: 0.00056325\n","Iter: 039/100 | Train Loss: 0.00054675\n","Iter: 040/100 | Train Loss: 0.00053157\n","Iter: 041/100 | Train Loss: 0.00051947\n","Iter: 042/100 | Train Loss: 0.00051023\n","Iter: 043/100 | Train Loss: 0.00050223\n","Iter: 044/100 | Train Loss: 0.00049376\n","Iter: 045/100 | Train Loss: 0.00048408\n","Iter: 046/100 | Train Loss: 0.00047361\n","Iter: 047/100 | Train Loss: 0.00046342\n","Iter: 048/100 | Train Loss: 0.00045436\n","Iter: 049/100 | Train Loss: 0.00044659\n","Iter: 050/100 | Train Loss: 0.00043963\n","Iter: 051/100 | Train Loss: 0.00043281\n","Iter: 052/100 | Train Loss: 0.00042571\n","Iter: 053/100 | Train Loss: 0.00041831\n","Iter: 054/100 | Train Loss: 0.00041093\n","Iter: 055/100 | Train Loss: 0.00040390\n","Iter: 056/100 | Train Loss: 0.00039742\n","Iter: 057/100 | Train Loss: 0.00039140\n","Iter: 058/100 | Train Loss: 0.00038565\n","Iter: 059/100 | Train Loss: 0.00037993\n","Iter: 060/100 | Train Loss: 0.00037417\n","Iter: 061/100 | Train Loss: 0.00036840\n","Iter: 062/100 | Train Loss: 0.00036276\n","Iter: 063/100 | Train Loss: 0.00035735\n","Iter: 064/100 | Train Loss: 0.00035221\n","Iter: 065/100 | Train Loss: 0.00034730\n","Iter: 066/100 | Train Loss: 0.00034251\n","Iter: 067/100 | Train Loss: 0.00033776\n","Iter: 068/100 | Train Loss: 0.00033305\n","Iter: 069/100 | Train Loss: 0.00032838\n","Iter: 070/100 | Train Loss: 0.00032383\n","Iter: 071/100 | Train Loss: 0.00031944\n","Iter: 072/100 | Train Loss: 0.00031519\n","Iter: 073/100 | Train Loss: 0.00031106\n","Iter: 074/100 | Train Loss: 0.00030701\n","Iter: 075/100 | Train Loss: 0.00030299\n","Iter: 076/100 | Train Loss: 0.00029902\n","Iter: 077/100 | Train Loss: 0.00029513\n","Iter: 078/100 | Train Loss: 0.00029134\n","Iter: 079/100 | Train Loss: 0.00028766\n","Iter: 080/100 | Train Loss: 0.00028407\n","Iter: 081/100 | Train Loss: 0.00028055\n","Iter: 082/100 | Train Loss: 0.00027709\n","Iter: 083/100 | Train Loss: 0.00027366\n","Iter: 084/100 | Train Loss: 0.00027029\n","Iter: 085/100 | Train Loss: 0.00026699\n","Iter: 086/100 | Train Loss: 0.00026377\n","Iter: 087/100 | Train Loss: 0.00026062\n","Iter: 088/100 | Train Loss: 0.00025754\n","Iter: 089/100 | Train Loss: 0.00025450\n","Iter: 090/100 | Train Loss: 0.00025151\n","Iter: 091/100 | Train Loss: 0.00024856\n","Iter: 092/100 | Train Loss: 0.00024567\n","Iter: 093/100 | Train Loss: 0.00024283\n","Iter: 094/100 | Train Loss: 0.00024005\n","Iter: 095/100 | Train Loss: 0.00023731\n","Iter: 096/100 | Train Loss: 0.00023461\n","Iter: 097/100 | Train Loss: 0.00023196\n","Iter: 098/100 | Train Loss: 0.00022934\n","Iter: 099/100 | Train Loss: 0.00022677\n","\n","Iter: 099/100 | Test Loss: 0.00098487 | Test acc: 65.2900\n","scale:1.150000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00237046\n","Iter: 001/100 | Train Loss: 0.00200039\n","Iter: 002/100 | Train Loss: 0.00167019\n","Iter: 003/100 | Train Loss: 0.00148846\n","Iter: 004/100 | Train Loss: 0.00147733\n","Iter: 005/100 | Train Loss: 0.00158280\n","Adjusting Layer 1, Kernel Nodes: 756, Adptive Nodes:44\n","Iter: 006/100 | Train Loss: 0.00171371\n","Adjusting Layer 1, Kernel Nodes: 729, Adptive Nodes:71\n","Iter: 007/100 | Train Loss: 0.00178235\n","Adjusting Layer 1, Kernel Nodes: 702, Adptive Nodes:98\n","Iter: 008/100 | Train Loss: 0.00174275\n","Iter: 009/100 | Train Loss: 0.00161081\n","Iter: 010/100 | Train Loss: 0.00144295\n","Iter: 011/100 | Train Loss: 0.00130028\n","Iter: 012/100 | Train Loss: 0.00121987\n","Iter: 013/100 | Train Loss: 0.00120368\n","Iter: 014/100 | Train Loss: 0.00122613\n","Adjusting Layer 1, Kernel Nodes: 684, Adptive Nodes:116\n","Iter: 015/100 | Train Loss: 0.00125156\n","Adjusting Layer 1, Kernel Nodes: 635, Adptive Nodes:165\n","Iter: 016/100 | Train Loss: 0.00124867\n","Iter: 017/100 | Train Loss: 0.00120627\n","Iter: 018/100 | Train Loss: 0.00113468\n","Iter: 019/100 | Train Loss: 0.00105615\n","Iter: 020/100 | Train Loss: 0.00099216\n","Iter: 021/100 | Train Loss: 0.00095402\n","Iter: 022/100 | Train Loss: 0.00093960\n","Iter: 023/100 | Train Loss: 0.00093715\n","Iter: 024/100 | Train Loss: 0.00093261\n","Iter: 025/100 | Train Loss: 0.00091637\n","Iter: 026/100 | Train Loss: 0.00088672\n","Iter: 027/100 | Train Loss: 0.00084896\n","Iter: 028/100 | Train Loss: 0.00081144\n","Iter: 029/100 | Train Loss: 0.00078119\n","Iter: 030/100 | Train Loss: 0.00076096\n","Iter: 031/100 | Train Loss: 0.00074897\n","Iter: 032/100 | Train Loss: 0.00074052\n","Iter: 033/100 | Train Loss: 0.00073076\n","Iter: 034/100 | Train Loss: 0.00071682\n","Iter: 035/100 | Train Loss: 0.00069863\n","Iter: 036/100 | Train Loss: 0.00067843\n","Iter: 037/100 | Train Loss: 0.00065916\n","Iter: 038/100 | Train Loss: 0.00064303\n","Iter: 039/100 | Train Loss: 0.00063069\n","Iter: 040/100 | Train Loss: 0.00062125\n","Iter: 041/100 | Train Loss: 0.00061296\n","Iter: 042/100 | Train Loss: 0.00060411\n","Iter: 043/100 | Train Loss: 0.00059385\n","Iter: 044/100 | Train Loss: 0.00058234\n","Iter: 045/100 | Train Loss: 0.00057042\n","Iter: 046/100 | Train Loss: 0.00055914\n","Iter: 047/100 | Train Loss: 0.00054919\n","Iter: 048/100 | Train Loss: 0.00054068\n","Iter: 049/100 | Train Loss: 0.00053320\n","Iter: 050/100 | Train Loss: 0.00052612\n","Iter: 051/100 | Train Loss: 0.00051889\n","Iter: 052/100 | Train Loss: 0.00051127\n","Iter: 053/100 | Train Loss: 0.00050336\n","Iter: 054/100 | Train Loss: 0.00049547\n","Iter: 055/100 | Train Loss: 0.00048794\n","Iter: 056/100 | Train Loss: 0.00048096\n","Iter: 057/100 | Train Loss: 0.00047453\n","Iter: 058/100 | Train Loss: 0.00046851\n","Iter: 059/100 | Train Loss: 0.00046267\n","Iter: 060/100 | Train Loss: 0.00045683\n","Iter: 061/100 | Train Loss: 0.00045094\n","Iter: 062/100 | Train Loss: 0.00044504\n","Iter: 063/100 | Train Loss: 0.00043923\n","Iter: 064/100 | Train Loss: 0.00043362\n","Iter: 065/100 | Train Loss: 0.00042826\n","Iter: 066/100 | Train Loss: 0.00042313\n","Iter: 067/100 | Train Loss: 0.00041820\n","Iter: 068/100 | Train Loss: 0.00041337\n","Iter: 069/100 | Train Loss: 0.00040860\n","Iter: 070/100 | Train Loss: 0.00040386\n","Iter: 071/100 | Train Loss: 0.00039918\n","Iter: 072/100 | Train Loss: 0.00039458\n","Iter: 073/100 | Train Loss: 0.00039009\n","Iter: 074/100 | Train Loss: 0.00038574\n","Iter: 075/100 | Train Loss: 0.00038150\n","Iter: 076/100 | Train Loss: 0.00037737\n","Iter: 077/100 | Train Loss: 0.00037333\n","Iter: 078/100 | Train Loss: 0.00036934\n","Iter: 079/100 | Train Loss: 0.00036541\n","Iter: 080/100 | Train Loss: 0.00036154\n","Iter: 081/100 | Train Loss: 0.00035773\n","Iter: 082/100 | Train Loss: 0.00035399\n","Iter: 083/100 | Train Loss: 0.00035033\n","Iter: 084/100 | Train Loss: 0.00034673\n","Iter: 085/100 | Train Loss: 0.00034320\n","Iter: 086/100 | Train Loss: 0.00033973\n","Iter: 087/100 | Train Loss: 0.00033632\n","Iter: 088/100 | Train Loss: 0.00033296\n","Iter: 089/100 | Train Loss: 0.00032964\n","Iter: 090/100 | Train Loss: 0.00032638\n","Iter: 091/100 | Train Loss: 0.00032316\n","Iter: 092/100 | Train Loss: 0.00032000\n","Iter: 093/100 | Train Loss: 0.00031689\n","Iter: 094/100 | Train Loss: 0.00031384\n","Iter: 095/100 | Train Loss: 0.00031083\n","Iter: 096/100 | Train Loss: 0.00030788\n","Iter: 097/100 | Train Loss: 0.00030496\n","Iter: 098/100 | Train Loss: 0.00030208\n","Iter: 099/100 | Train Loss: 0.00029924\n","\n","Iter: 099/100 | Test Loss: 0.00098727 | Test acc: 64.6100\n","scale:1.150000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00237281\n","Iter: 001/100 | Train Loss: 0.00200517\n","Iter: 002/100 | Train Loss: 0.00167512\n","Iter: 003/100 | Train Loss: 0.00149027\n","Iter: 004/100 | Train Loss: 0.00147401\n","Iter: 005/100 | Train Loss: 0.00157495\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00170266\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00177413\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 008/100 | Train Loss: 0.00174776\n","Iter: 009/100 | Train Loss: 0.00163275\n","Iter: 010/100 | Train Loss: 0.00147414\n","Iter: 011/100 | Train Loss: 0.00132576\n","Iter: 012/100 | Train Loss: 0.00122586\n","Iter: 013/100 | Train Loss: 0.00118550\n","Iter: 014/100 | Train Loss: 0.00119055\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 015/100 | Train Loss: 0.00121295\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 016/100 | Train Loss: 0.00122473\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00120870\n","Iter: 018/100 | Train Loss: 0.00116260\n","Iter: 019/100 | Train Loss: 0.00109682\n","Iter: 020/100 | Train Loss: 0.00102778\n","Iter: 021/100 | Train Loss: 0.00097037\n","Iter: 022/100 | Train Loss: 0.00093247\n","Iter: 023/100 | Train Loss: 0.00091368\n","Iter: 024/100 | Train Loss: 0.00090696\n","Iter: 025/100 | Train Loss: 0.00090270\n","Iter: 026/100 | Train Loss: 0.00089270\n","Iter: 027/100 | Train Loss: 0.00087313\n","Iter: 028/100 | Train Loss: 0.00084502\n","Iter: 029/100 | Train Loss: 0.00081274\n","Iter: 030/100 | Train Loss: 0.00078169\n","Iter: 031/100 | Train Loss: 0.00075609\n","Iter: 032/100 | Train Loss: 0.00073761\n","Iter: 033/100 | Train Loss: 0.00072524\n","Iter: 034/100 | Train Loss: 0.00071620\n","Iter: 035/100 | Train Loss: 0.00070736\n","Iter: 036/100 | Train Loss: 0.00069639\n","Iter: 037/100 | Train Loss: 0.00068243\n","Iter: 038/100 | Train Loss: 0.00066612\n","Iter: 039/100 | Train Loss: 0.00064911\n","Iter: 040/100 | Train Loss: 0.00063317\n","Iter: 041/100 | Train Loss: 0.00061953\n","Iter: 042/100 | Train Loss: 0.00060851\n","Iter: 043/100 | Train Loss: 0.00059954\n","Iter: 044/100 | Train Loss: 0.00059155\n","Iter: 045/100 | Train Loss: 0.00058349\n","Iter: 046/100 | Train Loss: 0.00057469\n","Iter: 047/100 | Train Loss: 0.00056505\n","Iter: 048/100 | Train Loss: 0.00055489\n","Iter: 049/100 | Train Loss: 0.00054480\n","Iter: 050/100 | Train Loss: 0.00053536\n","Iter: 051/100 | Train Loss: 0.00052687\n","Iter: 052/100 | Train Loss: 0.00051933\n","Iter: 053/100 | Train Loss: 0.00051250\n","Iter: 054/100 | Train Loss: 0.00050600\n","Iter: 055/100 | Train Loss: 0.00049948\n","Iter: 056/100 | Train Loss: 0.00049274\n","Iter: 057/100 | Train Loss: 0.00048579\n","Iter: 058/100 | Train Loss: 0.00047880\n","Iter: 059/100 | Train Loss: 0.00047196\n","Iter: 060/100 | Train Loss: 0.00046545\n","Iter: 061/100 | Train Loss: 0.00045937\n","Iter: 062/100 | Train Loss: 0.00045367\n","Iter: 063/100 | Train Loss: 0.00044823\n","Iter: 064/100 | Train Loss: 0.00044291\n","Iter: 065/100 | Train Loss: 0.00043762\n","Iter: 066/100 | Train Loss: 0.00043230\n","Iter: 067/100 | Train Loss: 0.00042700\n","Iter: 068/100 | Train Loss: 0.00042176\n","Iter: 069/100 | Train Loss: 0.00041668\n","Iter: 070/100 | Train Loss: 0.00041179\n","Iter: 071/100 | Train Loss: 0.00040710\n","Iter: 072/100 | Train Loss: 0.00040258\n","Iter: 073/100 | Train Loss: 0.00039819\n","Iter: 074/100 | Train Loss: 0.00039387\n","Iter: 075/100 | Train Loss: 0.00038958\n","Iter: 076/100 | Train Loss: 0.00038533\n","Iter: 077/100 | Train Loss: 0.00038114\n","Iter: 078/100 | Train Loss: 0.00037702\n","Iter: 079/100 | Train Loss: 0.00037300\n","Iter: 080/100 | Train Loss: 0.00036909\n","Iter: 081/100 | Train Loss: 0.00036528\n","Iter: 082/100 | Train Loss: 0.00036156\n","Iter: 083/100 | Train Loss: 0.00035791\n","Iter: 084/100 | Train Loss: 0.00035431\n","Iter: 085/100 | Train Loss: 0.00035076\n","Iter: 086/100 | Train Loss: 0.00034725\n","Iter: 087/100 | Train Loss: 0.00034379\n","Iter: 088/100 | Train Loss: 0.00034039\n","Iter: 089/100 | Train Loss: 0.00033706\n","Iter: 090/100 | Train Loss: 0.00033380\n","Iter: 091/100 | Train Loss: 0.00033060\n","Iter: 092/100 | Train Loss: 0.00032745\n","Iter: 093/100 | Train Loss: 0.00032434\n","Iter: 094/100 | Train Loss: 0.00032128\n","Iter: 095/100 | Train Loss: 0.00031826\n","Iter: 096/100 | Train Loss: 0.00031528\n","Iter: 097/100 | Train Loss: 0.00031235\n","Iter: 098/100 | Train Loss: 0.00030947\n","Iter: 099/100 | Train Loss: 0.00030663\n","\n","Iter: 099/100 | Test Loss: 0.00098671 | Test acc: 64.2000\n","scale:1.150000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00237513\n","Iter: 001/100 | Train Loss: 0.00200979\n","Iter: 002/100 | Train Loss: 0.00167977\n","Iter: 003/100 | Train Loss: 0.00149144\n","Iter: 004/100 | Train Loss: 0.00146956\n","Iter: 005/100 | Train Loss: 0.00156533\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00168952\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00175960\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 008/100 | Train Loss: 0.00173163\n","Iter: 009/100 | Train Loss: 0.00161543\n","Iter: 010/100 | Train Loss: 0.00145599\n","Iter: 011/100 | Train Loss: 0.00130687\n","Iter: 012/100 | Train Loss: 0.00120552\n","Iter: 013/100 | Train Loss: 0.00116211\n","Iter: 014/100 | Train Loss: 0.00116277\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00117961\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 016/100 | Train Loss: 0.00118500\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 017/100 | Train Loss: 0.00116069\n","Iter: 018/100 | Train Loss: 0.00110648\n","Iter: 019/100 | Train Loss: 0.00103504\n","Iter: 020/100 | Train Loss: 0.00096394\n","Iter: 021/100 | Train Loss: 0.00090774\n","Iter: 022/100 | Train Loss: 0.00087244\n","Iter: 023/100 | Train Loss: 0.00085476\n","Iter: 024/100 | Train Loss: 0.00084580\n","Iter: 025/100 | Train Loss: 0.00083583\n","Iter: 026/100 | Train Loss: 0.00081822\n","Iter: 027/100 | Train Loss: 0.00079154\n","Iter: 028/100 | Train Loss: 0.00075894\n","Iter: 029/100 | Train Loss: 0.00072574\n","Iter: 030/100 | Train Loss: 0.00069680\n","Iter: 031/100 | Train Loss: 0.00067467\n","Iter: 032/100 | Train Loss: 0.00065902\n","Iter: 033/100 | Train Loss: 0.00064754\n","Iter: 034/100 | Train Loss: 0.00063710\n","Iter: 035/100 | Train Loss: 0.00062521\n","Iter: 036/100 | Train Loss: 0.00061080\n","Iter: 037/100 | Train Loss: 0.00059425\n","Iter: 038/100 | Train Loss: 0.00057693\n","Iter: 039/100 | Train Loss: 0.00056052\n","Iter: 040/100 | Train Loss: 0.00054632\n","Iter: 041/100 | Train Loss: 0.00053469\n","Iter: 042/100 | Train Loss: 0.00052514\n","Iter: 043/100 | Train Loss: 0.00051665\n","Iter: 044/100 | Train Loss: 0.00050820\n","Iter: 045/100 | Train Loss: 0.00049906\n","Iter: 046/100 | Train Loss: 0.00048919\n","Iter: 047/100 | Train Loss: 0.00047893\n","Iter: 048/100 | Train Loss: 0.00046892\n","Iter: 049/100 | Train Loss: 0.00045972\n","Iter: 050/100 | Train Loss: 0.00045155\n","Iter: 051/100 | Train Loss: 0.00044432\n","Iter: 052/100 | Train Loss: 0.00043772\n","Iter: 053/100 | Train Loss: 0.00043132\n","Iter: 054/100 | Train Loss: 0.00042480\n","Iter: 055/100 | Train Loss: 0.00041803\n","Iter: 056/100 | Train Loss: 0.00041112\n","Iter: 057/100 | Train Loss: 0.00040429\n","Iter: 058/100 | Train Loss: 0.00039778\n","Iter: 059/100 | Train Loss: 0.00039172\n","Iter: 060/100 | Train Loss: 0.00038612\n","Iter: 061/100 | Train Loss: 0.00038085\n","Iter: 062/100 | Train Loss: 0.00037575\n","Iter: 063/100 | Train Loss: 0.00037064\n","Iter: 064/100 | Train Loss: 0.00036545\n","Iter: 065/100 | Train Loss: 0.00036023\n","Iter: 066/100 | Train Loss: 0.00035507\n","Iter: 067/100 | Train Loss: 0.00035007\n","Iter: 068/100 | Train Loss: 0.00034532\n","Iter: 069/100 | Train Loss: 0.00034080\n","Iter: 070/100 | Train Loss: 0.00033645\n","Iter: 071/100 | Train Loss: 0.00033220\n","Iter: 072/100 | Train Loss: 0.00032799\n","Iter: 073/100 | Train Loss: 0.00032380\n","Iter: 074/100 | Train Loss: 0.00031965\n","Iter: 075/100 | Train Loss: 0.00031560\n","Iter: 076/100 | Train Loss: 0.00031165\n","Iter: 077/100 | Train Loss: 0.00030781\n","Iter: 078/100 | Train Loss: 0.00030409\n","Iter: 079/100 | Train Loss: 0.00030048\n","Iter: 080/100 | Train Loss: 0.00029694\n","Iter: 081/100 | Train Loss: 0.00029346\n","Iter: 082/100 | Train Loss: 0.00029003\n","Iter: 083/100 | Train Loss: 0.00028665\n","Iter: 084/100 | Train Loss: 0.00028333\n","Iter: 085/100 | Train Loss: 0.00028007\n","Iter: 086/100 | Train Loss: 0.00027687\n","Iter: 087/100 | Train Loss: 0.00027375\n","Iter: 088/100 | Train Loss: 0.00027069\n","Iter: 089/100 | Train Loss: 0.00026768\n","Iter: 090/100 | Train Loss: 0.00026471\n","Iter: 091/100 | Train Loss: 0.00026177\n","Iter: 092/100 | Train Loss: 0.00025888\n","Iter: 093/100 | Train Loss: 0.00025604\n","Iter: 094/100 | Train Loss: 0.00025325\n","Iter: 095/100 | Train Loss: 0.00025051\n","Iter: 096/100 | Train Loss: 0.00024782\n","Iter: 097/100 | Train Loss: 0.00024518\n","Iter: 098/100 | Train Loss: 0.00024258\n","Iter: 099/100 | Train Loss: 0.00024001\n","\n","Iter: 099/100 | Test Loss: 0.00097254 | Test acc: 64.8800\n","scale:1.200000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00223708\n","Iter: 001/100 | Train Loss: 0.00176112\n","Iter: 002/100 | Train Loss: 0.00149109\n","Iter: 003/100 | Train Loss: 0.00152538\n","Adjusting Layer 1, Kernel Nodes: 594, Adptive Nodes:206\n","Iter: 004/100 | Train Loss: 0.00175382\n","Adjusting Layer 1, Kernel Nodes: 424, Adptive Nodes:376\n","Iter: 005/100 | Train Loss: 0.00184762\n","Adjusting Layer 1, Kernel Nodes: 346, Adptive Nodes:454\n","Iter: 006/100 | Train Loss: 0.00165614\n","Iter: 007/100 | Train Loss: 0.00136820\n","Iter: 008/100 | Train Loss: 0.00120916\n","Iter: 009/100 | Train Loss: 0.00122194\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 010/100 | Train Loss: 0.00125743\n","Adjusting Layer 1, Kernel Nodes: 354, Adptive Nodes:446\n","Iter: 011/100 | Train Loss: 0.00124628\n","Iter: 012/100 | Train Loss: 0.00113594\n","Iter: 013/100 | Train Loss: 0.00099253\n","Iter: 014/100 | Train Loss: 0.00090145\n","Iter: 015/100 | Train Loss: 0.00088647\n","Iter: 016/100 | Train Loss: 0.00090102\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 017/100 | Train Loss: 0.00083786\n","Iter: 018/100 | Train Loss: 0.00072335\n","Iter: 019/100 | Train Loss: 0.00068486\n","Iter: 020/100 | Train Loss: 0.00068770\n","Adjusting Layer 1, Kernel Nodes: 5, Adptive Nodes:795\n","Iter: 021/100 | Train Loss: 0.00066580\n","Iter: 022/100 | Train Loss: 0.00061982\n","Iter: 023/100 | Train Loss: 0.00057376\n","Iter: 024/100 | Train Loss: 0.00054888\n","Iter: 025/100 | Train Loss: 0.00054536\n","Iter: 026/100 | Train Loss: 0.00054632\n","Adjusting Layer 1, Kernel Nodes: 442, Adptive Nodes:358\n","Iter: 027/100 | Train Loss: 0.00053494\n","Iter: 028/100 | Train Loss: 0.00051086\n","Iter: 029/100 | Train Loss: 0.00048465\n","Iter: 030/100 | Train Loss: 0.00046686\n","Iter: 031/100 | Train Loss: 0.00045938\n","Iter: 032/100 | Train Loss: 0.00045562\n","Iter: 033/100 | Train Loss: 0.00044782\n","Iter: 034/100 | Train Loss: 0.00043356\n","Iter: 035/100 | Train Loss: 0.00041666\n","Iter: 036/100 | Train Loss: 0.00040257\n","Iter: 037/100 | Train Loss: 0.00039358\n","Iter: 038/100 | Train Loss: 0.00038757\n","Iter: 039/100 | Train Loss: 0.00038081\n","Iter: 040/100 | Train Loss: 0.00037135\n","Iter: 041/100 | Train Loss: 0.00036014\n","Iter: 042/100 | Train Loss: 0.00034967\n","Iter: 043/100 | Train Loss: 0.00034145\n","Iter: 044/100 | Train Loss: 0.00033508\n","Iter: 045/100 | Train Loss: 0.00032900\n","Iter: 046/100 | Train Loss: 0.00032200\n","Iter: 047/100 | Train Loss: 0.00031413\n","Iter: 048/100 | Train Loss: 0.00030631\n","Iter: 049/100 | Train Loss: 0.00029940\n","Iter: 050/100 | Train Loss: 0.00029349\n","Iter: 051/100 | Train Loss: 0.00028800\n","Iter: 052/100 | Train Loss: 0.00028233\n","Iter: 053/100 | Train Loss: 0.00027634\n","Iter: 054/100 | Train Loss: 0.00027034\n","Iter: 055/100 | Train Loss: 0.00026469\n","Iter: 056/100 | Train Loss: 0.00025953\n","Iter: 057/100 | Train Loss: 0.00025470\n","Iter: 058/100 | Train Loss: 0.00024994\n","Iter: 059/100 | Train Loss: 0.00024515\n","Iter: 060/100 | Train Loss: 0.00024039\n","Iter: 061/100 | Train Loss: 0.00023581\n","Iter: 062/100 | Train Loss: 0.00023147\n","Iter: 063/100 | Train Loss: 0.00022731\n","Iter: 064/100 | Train Loss: 0.00022328\n","Iter: 065/100 | Train Loss: 0.00021930\n","Iter: 066/100 | Train Loss: 0.00021539\n","Iter: 067/100 | Train Loss: 0.00021159\n","Iter: 068/100 | Train Loss: 0.00020793\n","Iter: 069/100 | Train Loss: 0.00020439\n","Iter: 070/100 | Train Loss: 0.00020094\n","Iter: 071/100 | Train Loss: 0.00019757\n","Iter: 072/100 | Train Loss: 0.00019428\n","Iter: 073/100 | Train Loss: 0.00019106\n","Iter: 074/100 | Train Loss: 0.00018793\n","Iter: 075/100 | Train Loss: 0.00018487\n","Iter: 076/100 | Train Loss: 0.00018187\n","Iter: 077/100 | Train Loss: 0.00017894\n","Iter: 078/100 | Train Loss: 0.00017608\n","Iter: 079/100 | Train Loss: 0.00017329\n","Iter: 080/100 | Train Loss: 0.00017057\n","Iter: 081/100 | Train Loss: 0.00016789\n","Iter: 082/100 | Train Loss: 0.00016527\n","Iter: 083/100 | Train Loss: 0.00016270\n","Iter: 084/100 | Train Loss: 0.00016019\n","Iter: 085/100 | Train Loss: 0.00015775\n","Iter: 086/100 | Train Loss: 0.00015535\n","Iter: 087/100 | Train Loss: 0.00015301\n","Iter: 088/100 | Train Loss: 0.00015070\n","Iter: 089/100 | Train Loss: 0.00014844\n","Iter: 090/100 | Train Loss: 0.00014623\n","Iter: 091/100 | Train Loss: 0.00014406\n","Iter: 092/100 | Train Loss: 0.00014195\n","Iter: 093/100 | Train Loss: 0.00013987\n","Iter: 094/100 | Train Loss: 0.00013783\n","Iter: 095/100 | Train Loss: 0.00013583\n","Iter: 096/100 | Train Loss: 0.00013387\n","Iter: 097/100 | Train Loss: 0.00013195\n","Iter: 098/100 | Train Loss: 0.00013006\n","Iter: 099/100 | Train Loss: 0.00012822\n","\n","Iter: 099/100 | Test Loss: 0.00100644 | Test acc: 65.4000\n","scale:1.200000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00227525\n","Iter: 001/100 | Train Loss: 0.00182287\n","Iter: 002/100 | Train Loss: 0.00152220\n","Iter: 003/100 | Train Loss: 0.00148726\n","Iter: 004/100 | Train Loss: 0.00164830\n","Adjusting Layer 1, Kernel Nodes: 623, Adptive Nodes:177\n","Iter: 005/100 | Train Loss: 0.00182639\n","Adjusting Layer 1, Kernel Nodes: 522, Adptive Nodes:278\n","Iter: 006/100 | Train Loss: 0.00176010\n","Iter: 007/100 | Train Loss: 0.00148712\n","Iter: 008/100 | Train Loss: 0.00125393\n","Iter: 009/100 | Train Loss: 0.00120353\n","Iter: 010/100 | Train Loss: 0.00126551\n","Adjusting Layer 1, Kernel Nodes: 352, Adptive Nodes:448\n","Iter: 011/100 | Train Loss: 0.00126574\n","Adjusting Layer 1, Kernel Nodes: 383, Adptive Nodes:417\n","Iter: 012/100 | Train Loss: 0.00118462\n","Iter: 013/100 | Train Loss: 0.00104335\n","Iter: 014/100 | Train Loss: 0.00092230\n","Iter: 015/100 | Train Loss: 0.00087239\n","Iter: 016/100 | Train Loss: 0.00087840\n","Adjusting Layer 1, Kernel Nodes: 458, Adptive Nodes:342\n","Iter: 017/100 | Train Loss: 0.00088140\n","Adjusting Layer 1, Kernel Nodes: 378, Adptive Nodes:422\n","Iter: 018/100 | Train Loss: 0.00084461\n","Iter: 019/100 | Train Loss: 0.00077136\n","Iter: 020/100 | Train Loss: 0.00070055\n","Iter: 021/100 | Train Loss: 0.00066379\n","Iter: 022/100 | Train Loss: 0.00065926\n","Iter: 023/100 | Train Loss: 0.00065992\n","Adjusting Layer 1, Kernel Nodes: 271, Adptive Nodes:529\n","Iter: 024/100 | Train Loss: 0.00064508\n","Iter: 025/100 | Train Loss: 0.00061391\n","Iter: 026/100 | Train Loss: 0.00057805\n","Iter: 027/100 | Train Loss: 0.00055076\n","Iter: 028/100 | Train Loss: 0.00053752\n","Iter: 029/100 | Train Loss: 0.00053361\n","Iter: 030/100 | Train Loss: 0.00052956\n","Iter: 031/100 | Train Loss: 0.00051864\n","Iter: 032/100 | Train Loss: 0.00050091\n","Iter: 033/100 | Train Loss: 0.00048156\n","Iter: 034/100 | Train Loss: 0.00046619\n","Iter: 035/100 | Train Loss: 0.00045671\n","Iter: 036/100 | Train Loss: 0.00045089\n","Iter: 037/100 | Train Loss: 0.00044478\n","Iter: 038/100 | Train Loss: 0.00043588\n","Iter: 039/100 | Train Loss: 0.00042446\n","Iter: 040/100 | Train Loss: 0.00041267\n","Iter: 041/100 | Train Loss: 0.00040260\n","Iter: 042/100 | Train Loss: 0.00039495\n","Iter: 043/100 | Train Loss: 0.00038880\n","Iter: 044/100 | Train Loss: 0.00038263\n","Iter: 045/100 | Train Loss: 0.00037552\n","Iter: 046/100 | Train Loss: 0.00036759\n","Iter: 047/100 | Train Loss: 0.00035962\n","Iter: 048/100 | Train Loss: 0.00035239\n","Iter: 049/100 | Train Loss: 0.00034615\n","Iter: 050/100 | Train Loss: 0.00034057\n","Iter: 051/100 | Train Loss: 0.00033509\n","Iter: 052/100 | Train Loss: 0.00032936\n","Iter: 053/100 | Train Loss: 0.00032338\n","Iter: 054/100 | Train Loss: 0.00031743\n","Iter: 055/100 | Train Loss: 0.00031182\n","Iter: 056/100 | Train Loss: 0.00030663\n","Iter: 057/100 | Train Loss: 0.00030176\n","Iter: 058/100 | Train Loss: 0.00029701\n","Iter: 059/100 | Train Loss: 0.00029227\n","Iter: 060/100 | Train Loss: 0.00028750\n","Iter: 061/100 | Train Loss: 0.00028278\n","Iter: 062/100 | Train Loss: 0.00027821\n","Iter: 063/100 | Train Loss: 0.00027386\n","Iter: 064/100 | Train Loss: 0.00026971\n","Iter: 065/100 | Train Loss: 0.00026568\n","Iter: 066/100 | Train Loss: 0.00026171\n","Iter: 067/100 | Train Loss: 0.00025778\n","Iter: 068/100 | Train Loss: 0.00025390\n","Iter: 069/100 | Train Loss: 0.00025010\n","Iter: 070/100 | Train Loss: 0.00024643\n","Iter: 071/100 | Train Loss: 0.00024288\n","Iter: 072/100 | Train Loss: 0.00023943\n","Iter: 073/100 | Train Loss: 0.00023604\n","Iter: 074/100 | Train Loss: 0.00023270\n","Iter: 075/100 | Train Loss: 0.00022941\n","Iter: 076/100 | Train Loss: 0.00022618\n","Iter: 077/100 | Train Loss: 0.00022304\n","Iter: 078/100 | Train Loss: 0.00021999\n","Iter: 079/100 | Train Loss: 0.00021701\n","Iter: 080/100 | Train Loss: 0.00021408\n","Iter: 081/100 | Train Loss: 0.00021121\n","Iter: 082/100 | Train Loss: 0.00020837\n","Iter: 083/100 | Train Loss: 0.00020559\n","Iter: 084/100 | Train Loss: 0.00020287\n","Iter: 085/100 | Train Loss: 0.00020021\n","Iter: 086/100 | Train Loss: 0.00019760\n","Iter: 087/100 | Train Loss: 0.00019505\n","Iter: 088/100 | Train Loss: 0.00019253\n","Iter: 089/100 | Train Loss: 0.00019005\n","Iter: 090/100 | Train Loss: 0.00018761\n","Iter: 091/100 | Train Loss: 0.00018522\n","Iter: 092/100 | Train Loss: 0.00018288\n","Iter: 093/100 | Train Loss: 0.00018058\n","Iter: 094/100 | Train Loss: 0.00017832\n","Iter: 095/100 | Train Loss: 0.00017610\n","Iter: 096/100 | Train Loss: 0.00017391\n","Iter: 097/100 | Train Loss: 0.00017176\n","Iter: 098/100 | Train Loss: 0.00016965\n","Iter: 099/100 | Train Loss: 0.00016757\n","\n","Iter: 099/100 | Test Loss: 0.00099726 | Test acc: 65.7000\n","scale:1.200000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00230451\n","Iter: 001/100 | Train Loss: 0.00187394\n","Iter: 002/100 | Train Loss: 0.00155673\n","Iter: 003/100 | Train Loss: 0.00147144\n","Iter: 004/100 | Train Loss: 0.00158529\n","Adjusting Layer 1, Kernel Nodes: 649, Adptive Nodes:151\n","Iter: 005/100 | Train Loss: 0.00177308\n","Adjusting Layer 1, Kernel Nodes: 595, Adptive Nodes:205\n","Iter: 006/100 | Train Loss: 0.00179527\n","Adjusting Layer 1, Kernel Nodes: 351, Adptive Nodes:449\n","Iter: 007/100 | Train Loss: 0.00159610\n","Iter: 008/100 | Train Loss: 0.00134359\n","Iter: 009/100 | Train Loss: 0.00121070\n","Iter: 010/100 | Train Loss: 0.00121938\n","Adjusting Layer 1, Kernel Nodes: 383, Adptive Nodes:417\n","Iter: 011/100 | Train Loss: 0.00125914\n","Adjusting Layer 1, Kernel Nodes: 413, Adptive Nodes:387\n","Iter: 012/100 | Train Loss: 0.00124542\n","Iter: 013/100 | Train Loss: 0.00114908\n","Iter: 014/100 | Train Loss: 0.00101383\n","Iter: 015/100 | Train Loss: 0.00090897\n","Iter: 016/100 | Train Loss: 0.00086887\n","Iter: 017/100 | Train Loss: 0.00087276\n","Adjusting Layer 1, Kernel Nodes: 513, Adptive Nodes:287\n","Iter: 018/100 | Train Loss: 0.00086703\n","Iter: 019/100 | Train Loss: 0.00081673\n","Iter: 020/100 | Train Loss: 0.00074024\n","Iter: 021/100 | Train Loss: 0.00068053\n","Iter: 022/100 | Train Loss: 0.00065881\n","Iter: 023/100 | Train Loss: 0.00065853\n","Iter: 024/100 | Train Loss: 0.00064859\n","Iter: 025/100 | Train Loss: 0.00061586\n","Iter: 026/100 | Train Loss: 0.00057369\n","Iter: 027/100 | Train Loss: 0.00054358\n","Iter: 028/100 | Train Loss: 0.00053271\n","Iter: 029/100 | Train Loss: 0.00052992\n","Iter: 030/100 | Train Loss: 0.00051992\n","Iter: 031/100 | Train Loss: 0.00049851\n","Iter: 032/100 | Train Loss: 0.00047414\n","Iter: 033/100 | Train Loss: 0.00045707\n","Iter: 034/100 | Train Loss: 0.00044919\n","Iter: 035/100 | Train Loss: 0.00044387\n","Iter: 036/100 | Train Loss: 0.00043402\n","Iter: 037/100 | Train Loss: 0.00041886\n","Iter: 038/100 | Train Loss: 0.00040320\n","Iter: 039/100 | Train Loss: 0.00039171\n","Iter: 040/100 | Train Loss: 0.00038451\n","Iter: 041/100 | Train Loss: 0.00037806\n","Iter: 042/100 | Train Loss: 0.00036919\n","Iter: 043/100 | Train Loss: 0.00035803\n","Iter: 044/100 | Train Loss: 0.00034712\n","Iter: 045/100 | Train Loss: 0.00033855\n","Iter: 046/100 | Train Loss: 0.00033204\n","Iter: 047/100 | Train Loss: 0.00032576\n","Iter: 048/100 | Train Loss: 0.00031833\n","Iter: 049/100 | Train Loss: 0.00031001\n","Iter: 050/100 | Train Loss: 0.00030207\n","Iter: 051/100 | Train Loss: 0.00029540\n","Iter: 052/100 | Train Loss: 0.00028975\n","Iter: 053/100 | Train Loss: 0.00028417\n","Iter: 054/100 | Train Loss: 0.00027807\n","Iter: 055/100 | Train Loss: 0.00027165\n","Iter: 056/100 | Train Loss: 0.00026557\n","Iter: 057/100 | Train Loss: 0.00026021\n","Iter: 058/100 | Train Loss: 0.00025537\n","Iter: 059/100 | Train Loss: 0.00025060\n","Iter: 060/100 | Train Loss: 0.00024563\n","Iter: 061/100 | Train Loss: 0.00024059\n","Iter: 062/100 | Train Loss: 0.00023580\n","Iter: 063/100 | Train Loss: 0.00023139\n","Iter: 064/100 | Train Loss: 0.00022727\n","Iter: 065/100 | Train Loss: 0.00022321\n","Iter: 066/100 | Train Loss: 0.00021910\n","Iter: 067/100 | Train Loss: 0.00021501\n","Iter: 068/100 | Train Loss: 0.00021107\n","Iter: 069/100 | Train Loss: 0.00020737\n","Iter: 070/100 | Train Loss: 0.00020384\n","Iter: 071/100 | Train Loss: 0.00020036\n","Iter: 072/100 | Train Loss: 0.00019689\n","Iter: 073/100 | Train Loss: 0.00019346\n","Iter: 074/100 | Train Loss: 0.00019014\n","Iter: 075/100 | Train Loss: 0.00018697\n","Iter: 076/100 | Train Loss: 0.00018390\n","Iter: 077/100 | Train Loss: 0.00018089\n","Iter: 078/100 | Train Loss: 0.00017790\n","Iter: 079/100 | Train Loss: 0.00017497\n","Iter: 080/100 | Train Loss: 0.00017212\n","Iter: 081/100 | Train Loss: 0.00016936\n","Iter: 082/100 | Train Loss: 0.00016669\n","Iter: 083/100 | Train Loss: 0.00016406\n","Iter: 084/100 | Train Loss: 0.00016146\n","Iter: 085/100 | Train Loss: 0.00015892\n","Iter: 086/100 | Train Loss: 0.00015644\n","Iter: 087/100 | Train Loss: 0.00015403\n","Iter: 088/100 | Train Loss: 0.00015168\n","Iter: 089/100 | Train Loss: 0.00014936\n","Iter: 090/100 | Train Loss: 0.00014709\n","Iter: 091/100 | Train Loss: 0.00014486\n","Iter: 092/100 | Train Loss: 0.00014268\n","Iter: 093/100 | Train Loss: 0.00014057\n","Iter: 094/100 | Train Loss: 0.00013849\n","Iter: 095/100 | Train Loss: 0.00013645\n","Iter: 096/100 | Train Loss: 0.00013444\n","Iter: 097/100 | Train Loss: 0.00013248\n","Iter: 098/100 | Train Loss: 0.00013055\n","Iter: 099/100 | Train Loss: 0.00012867\n","\n","Iter: 099/100 | Test Loss: 0.00100788 | Test acc: 65.5800\n","scale:1.200000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00232715\n","Iter: 001/100 | Train Loss: 0.00191559\n","Iter: 002/100 | Train Loss: 0.00159001\n","Iter: 003/100 | Train Loss: 0.00146844\n","Iter: 004/100 | Train Loss: 0.00154155\n","Adjusting Layer 1, Kernel Nodes: 677, Adptive Nodes:123\n","Iter: 005/100 | Train Loss: 0.00171874\n","Adjusting Layer 1, Kernel Nodes: 600, Adptive Nodes:200\n","Iter: 006/100 | Train Loss: 0.00180768\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00159273\n","Iter: 008/100 | Train Loss: 0.00130493\n","Iter: 009/100 | Train Loss: 0.00122837\n","Iter: 010/100 | Train Loss: 0.00131591\n","Adjusting Layer 1, Kernel Nodes: 128, Adptive Nodes:672\n","Iter: 011/100 | Train Loss: 0.00133302\n","Adjusting Layer 1, Kernel Nodes: 207, Adptive Nodes:593\n","Iter: 012/100 | Train Loss: 0.00124161\n","Iter: 013/100 | Train Loss: 0.00109242\n","Iter: 014/100 | Train Loss: 0.00096482\n","Iter: 015/100 | Train Loss: 0.00090797\n","Iter: 016/100 | Train Loss: 0.00091174\n","Adjusting Layer 1, Kernel Nodes: 634, Adptive Nodes:166\n","Iter: 017/100 | Train Loss: 0.00091926\n","Adjusting Layer 1, Kernel Nodes: 756, Adptive Nodes:44\n","Iter: 018/100 | Train Loss: 0.00085777\n","Iter: 019/100 | Train Loss: 0.00075972\n","Iter: 020/100 | Train Loss: 0.00070136\n","Iter: 021/100 | Train Loss: 0.00069255\n","Iter: 022/100 | Train Loss: 0.00068091\n","Iter: 023/100 | Train Loss: 0.00063640\n","Iter: 024/100 | Train Loss: 0.00058422\n","Iter: 025/100 | Train Loss: 0.00055601\n","Iter: 026/100 | Train Loss: 0.00054725\n","Iter: 027/100 | Train Loss: 0.00053342\n","Iter: 028/100 | Train Loss: 0.00050652\n","Iter: 029/100 | Train Loss: 0.00047921\n","Iter: 030/100 | Train Loss: 0.00046173\n","Iter: 031/100 | Train Loss: 0.00045088\n","Iter: 032/100 | Train Loss: 0.00043824\n","Iter: 033/100 | Train Loss: 0.00042120\n","Iter: 034/100 | Train Loss: 0.00040359\n","Iter: 035/100 | Train Loss: 0.00038902\n","Iter: 036/100 | Train Loss: 0.00037761\n","Iter: 037/100 | Train Loss: 0.00036706\n","Iter: 038/100 | Train Loss: 0.00035500\n","Iter: 039/100 | Train Loss: 0.00034158\n","Iter: 040/100 | Train Loss: 0.00032906\n","Iter: 041/100 | Train Loss: 0.00031910\n","Iter: 042/100 | Train Loss: 0.00031067\n","Iter: 043/100 | Train Loss: 0.00030145\n","Iter: 044/100 | Train Loss: 0.00029077\n","Iter: 045/100 | Train Loss: 0.00028056\n","Iter: 046/100 | Train Loss: 0.00027262\n","Iter: 047/100 | Train Loss: 0.00026602\n","Iter: 048/100 | Train Loss: 0.00025862\n","Iter: 049/100 | Train Loss: 0.00025005\n","Iter: 050/100 | Train Loss: 0.00024208\n","Iter: 051/100 | Train Loss: 0.00023591\n","Iter: 052/100 | Train Loss: 0.00023057\n","Iter: 053/100 | Train Loss: 0.00022441\n","Iter: 054/100 | Train Loss: 0.00021752\n","Iter: 055/100 | Train Loss: 0.00021137\n","Iter: 056/100 | Train Loss: 0.00020653\n","Iter: 057/100 | Train Loss: 0.00020201\n","Iter: 058/100 | Train Loss: 0.00019684\n","Iter: 059/100 | Train Loss: 0.00019138\n","Iter: 060/100 | Train Loss: 0.00018661\n","Iter: 061/100 | Train Loss: 0.00018266\n","Iter: 062/100 | Train Loss: 0.00017874\n","Iter: 063/100 | Train Loss: 0.00017441\n","Iter: 064/100 | Train Loss: 0.00017006\n","Iter: 065/100 | Train Loss: 0.00016623\n","Iter: 066/100 | Train Loss: 0.00016284\n","Iter: 067/100 | Train Loss: 0.00015939\n","Iter: 068/100 | Train Loss: 0.00015572\n","Iter: 069/100 | Train Loss: 0.00015215\n","Iter: 070/100 | Train Loss: 0.00014893\n","Iter: 071/100 | Train Loss: 0.00014592\n","Iter: 072/100 | Train Loss: 0.00014288\n","Iter: 073/100 | Train Loss: 0.00013977\n","Iter: 074/100 | Train Loss: 0.00013680\n","Iter: 075/100 | Train Loss: 0.00013403\n","Iter: 076/100 | Train Loss: 0.00013137\n","Iter: 077/100 | Train Loss: 0.00012870\n","Iter: 078/100 | Train Loss: 0.00012605\n","Iter: 079/100 | Train Loss: 0.00012350\n","Iter: 080/100 | Train Loss: 0.00012109\n","Iter: 081/100 | Train Loss: 0.00011874\n","Iter: 082/100 | Train Loss: 0.00011641\n","Iter: 083/100 | Train Loss: 0.00011413\n","Iter: 084/100 | Train Loss: 0.00011193\n","Iter: 085/100 | Train Loss: 0.00010982\n","Iter: 086/100 | Train Loss: 0.00010775\n","Iter: 087/100 | Train Loss: 0.00010572\n","Iter: 088/100 | Train Loss: 0.00010374\n","Iter: 089/100 | Train Loss: 0.00010181\n","Iter: 090/100 | Train Loss: 0.00009994\n","Iter: 091/100 | Train Loss: 0.00009812\n","Iter: 092/100 | Train Loss: 0.00009634\n","Iter: 093/100 | Train Loss: 0.00009460\n","Iter: 094/100 | Train Loss: 0.00009290\n","Iter: 095/100 | Train Loss: 0.00009124\n","Iter: 096/100 | Train Loss: 0.00008963\n","Iter: 097/100 | Train Loss: 0.00008805\n","Iter: 098/100 | Train Loss: 0.00008651\n","Iter: 099/100 | Train Loss: 0.00008499\n","\n","Iter: 099/100 | Test Loss: 0.00103326 | Test acc: 64.7400\n","scale:1.200000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00234434\n","Iter: 001/100 | Train Loss: 0.00194847\n","Iter: 002/100 | Train Loss: 0.00161923\n","Iter: 003/100 | Train Loss: 0.00147216\n","Iter: 004/100 | Train Loss: 0.00151265\n","Adjusting Layer 1, Kernel Nodes: 727, Adptive Nodes:73\n","Iter: 005/100 | Train Loss: 0.00167253\n","Adjusting Layer 1, Kernel Nodes: 595, Adptive Nodes:205\n","Iter: 006/100 | Train Loss: 0.00179550\n","Adjusting Layer 1, Kernel Nodes: 595, Adptive Nodes:205\n","Iter: 007/100 | Train Loss: 0.00172818\n","Iter: 008/100 | Train Loss: 0.00151111\n","Iter: 009/100 | Train Loss: 0.00130243\n","Iter: 010/100 | Train Loss: 0.00121731\n","Iter: 011/100 | Train Loss: 0.00124645\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 012/100 | Train Loss: 0.00128804\n","Adjusting Layer 1, Kernel Nodes: 459, Adptive Nodes:341\n","Iter: 013/100 | Train Loss: 0.00125579\n","Iter: 014/100 | Train Loss: 0.00113927\n","Iter: 015/100 | Train Loss: 0.00100353\n","Iter: 016/100 | Train Loss: 0.00091903\n","Iter: 017/100 | Train Loss: 0.00090148\n","Iter: 018/100 | Train Loss: 0.00090987\n","Adjusting Layer 1, Kernel Nodes: 793, Adptive Nodes:7\n","Iter: 019/100 | Train Loss: 0.00087856\n","Iter: 020/100 | Train Loss: 0.00079470\n","Iter: 021/100 | Train Loss: 0.00071382\n","Iter: 022/100 | Train Loss: 0.00068088\n","Iter: 023/100 | Train Loss: 0.00067888\n","Iter: 024/100 | Train Loss: 0.00066197\n","Iter: 025/100 | Train Loss: 0.00061554\n","Iter: 026/100 | Train Loss: 0.00056660\n","Iter: 027/100 | Train Loss: 0.00054267\n","Iter: 028/100 | Train Loss: 0.00053851\n","Iter: 029/100 | Train Loss: 0.00052898\n","Iter: 030/100 | Train Loss: 0.00050277\n","Iter: 031/100 | Train Loss: 0.00047250\n","Iter: 032/100 | Train Loss: 0.00045431\n","Iter: 033/100 | Train Loss: 0.00044779\n","Iter: 034/100 | Train Loss: 0.00043997\n","Iter: 035/100 | Train Loss: 0.00042324\n","Iter: 036/100 | Train Loss: 0.00040297\n","Iter: 037/100 | Train Loss: 0.00038817\n","Iter: 038/100 | Train Loss: 0.00038011\n","Iter: 039/100 | Train Loss: 0.00037249\n","Iter: 040/100 | Train Loss: 0.00036055\n","Iter: 041/100 | Train Loss: 0.00034616\n","Iter: 042/100 | Train Loss: 0.00033413\n","Iter: 043/100 | Train Loss: 0.00032594\n","Iter: 044/100 | Train Loss: 0.00031878\n","Iter: 045/100 | Train Loss: 0.00030977\n","Iter: 046/100 | Train Loss: 0.00029928\n","Iter: 047/100 | Train Loss: 0.00028973\n","Iter: 048/100 | Train Loss: 0.00028230\n","Iter: 049/100 | Train Loss: 0.00027586\n","Iter: 050/100 | Train Loss: 0.00026878\n","Iter: 051/100 | Train Loss: 0.00026090\n","Iter: 052/100 | Train Loss: 0.00025334\n","Iter: 053/100 | Train Loss: 0.00024693\n","Iter: 054/100 | Train Loss: 0.00024131\n","Iter: 055/100 | Train Loss: 0.00023560\n","Iter: 056/100 | Train Loss: 0.00022952\n","Iter: 057/100 | Train Loss: 0.00022354\n","Iter: 058/100 | Train Loss: 0.00021816\n","Iter: 059/100 | Train Loss: 0.00021333\n","Iter: 060/100 | Train Loss: 0.00020862\n","Iter: 061/100 | Train Loss: 0.00020379\n","Iter: 062/100 | Train Loss: 0.00019898\n","Iter: 063/100 | Train Loss: 0.00019450\n","Iter: 064/100 | Train Loss: 0.00019037\n","Iter: 065/100 | Train Loss: 0.00018642\n","Iter: 066/100 | Train Loss: 0.00018244\n","Iter: 067/100 | Train Loss: 0.00017849\n","Iter: 068/100 | Train Loss: 0.00017469\n","Iter: 069/100 | Train Loss: 0.00017113\n","Iter: 070/100 | Train Loss: 0.00016773\n","Iter: 071/100 | Train Loss: 0.00016436\n","Iter: 072/100 | Train Loss: 0.00016101\n","Iter: 073/100 | Train Loss: 0.00015775\n","Iter: 074/100 | Train Loss: 0.00015464\n","Iter: 075/100 | Train Loss: 0.00015165\n","Iter: 076/100 | Train Loss: 0.00014874\n","Iter: 077/100 | Train Loss: 0.00014585\n","Iter: 078/100 | Train Loss: 0.00014302\n","Iter: 079/100 | Train Loss: 0.00014029\n","Iter: 080/100 | Train Loss: 0.00013766\n","Iter: 081/100 | Train Loss: 0.00013510\n","Iter: 082/100 | Train Loss: 0.00013258\n","Iter: 083/100 | Train Loss: 0.00013011\n","Iter: 084/100 | Train Loss: 0.00012771\n","Iter: 085/100 | Train Loss: 0.00012540\n","Iter: 086/100 | Train Loss: 0.00012315\n","Iter: 087/100 | Train Loss: 0.00012094\n","Iter: 088/100 | Train Loss: 0.00011877\n","Iter: 089/100 | Train Loss: 0.00011666\n","Iter: 090/100 | Train Loss: 0.00011461\n","Iter: 091/100 | Train Loss: 0.00011261\n","Iter: 092/100 | Train Loss: 0.00011066\n","Iter: 093/100 | Train Loss: 0.00010874\n","Iter: 094/100 | Train Loss: 0.00010687\n","Iter: 095/100 | Train Loss: 0.00010504\n","Iter: 096/100 | Train Loss: 0.00010327\n","Iter: 097/100 | Train Loss: 0.00010152\n","Iter: 098/100 | Train Loss: 0.00009982\n","Iter: 099/100 | Train Loss: 0.00009814\n","\n","Iter: 099/100 | Test Loss: 0.00102190 | Test acc: 65.0000\n","scale:1.200000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00235674\n","Iter: 001/100 | Train Loss: 0.00197282\n","Iter: 002/100 | Train Loss: 0.00164243\n","Iter: 003/100 | Train Loss: 0.00147832\n","Iter: 004/100 | Train Loss: 0.00149465\n","Adjusting Layer 1, Kernel Nodes: 782, Adptive Nodes:18\n","Iter: 005/100 | Train Loss: 0.00163381\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 006/100 | Train Loss: 0.00177107\n","Adjusting Layer 1, Kernel Nodes: 604, Adptive Nodes:196\n","Iter: 007/100 | Train Loss: 0.00177398\n","Adjusting Layer 1, Kernel Nodes: 634, Adptive Nodes:166\n","Iter: 008/100 | Train Loss: 0.00159766\n","Iter: 009/100 | Train Loss: 0.00137308\n","Iter: 010/100 | Train Loss: 0.00123773\n","Iter: 011/100 | Train Loss: 0.00123090\n","Iter: 012/100 | Train Loss: 0.00128510\n","Adjusting Layer 1, Kernel Nodes: 389, Adptive Nodes:411\n","Iter: 013/100 | Train Loss: 0.00129794\n","Adjusting Layer 1, Kernel Nodes: 571, Adptive Nodes:229\n","Iter: 014/100 | Train Loss: 0.00122131\n","Iter: 015/100 | Train Loss: 0.00108764\n","Iter: 016/100 | Train Loss: 0.00097243\n","Iter: 017/100 | Train Loss: 0.00092296\n","Iter: 018/100 | Train Loss: 0.00092533\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 019/100 | Train Loss: 0.00092428\n","Iter: 020/100 | Train Loss: 0.00087377\n","Iter: 021/100 | Train Loss: 0.00079009\n","Iter: 022/100 | Train Loss: 0.00072421\n","Iter: 023/100 | Train Loss: 0.00070084\n","Iter: 024/100 | Train Loss: 0.00069858\n","Iter: 025/100 | Train Loss: 0.00068192\n","Iter: 026/100 | Train Loss: 0.00064087\n","Iter: 027/100 | Train Loss: 0.00059534\n","Iter: 028/100 | Train Loss: 0.00056783\n","Iter: 029/100 | Train Loss: 0.00055932\n","Iter: 030/100 | Train Loss: 0.00055282\n","Iter: 031/100 | Train Loss: 0.00053476\n","Iter: 032/100 | Train Loss: 0.00050789\n","Iter: 033/100 | Train Loss: 0.00048458\n","Iter: 034/100 | Train Loss: 0.00047172\n","Iter: 035/100 | Train Loss: 0.00046490\n","Iter: 036/100 | Train Loss: 0.00045544\n","Iter: 037/100 | Train Loss: 0.00044010\n","Iter: 038/100 | Train Loss: 0.00042300\n","Iter: 039/100 | Train Loss: 0.00040964\n","Iter: 040/100 | Train Loss: 0.00040096\n","Iter: 041/100 | Train Loss: 0.00039349\n","Iter: 042/100 | Train Loss: 0.00038386\n","Iter: 043/100 | Train Loss: 0.00037212\n","Iter: 044/100 | Train Loss: 0.00036074\n","Iter: 045/100 | Train Loss: 0.00035161\n","Iter: 046/100 | Train Loss: 0.00034427\n","Iter: 047/100 | Train Loss: 0.00033699\n","Iter: 048/100 | Train Loss: 0.00032871\n","Iter: 049/100 | Train Loss: 0.00031990\n","Iter: 050/100 | Train Loss: 0.00031170\n","Iter: 051/100 | Train Loss: 0.00030469\n","Iter: 052/100 | Train Loss: 0.00029839\n","Iter: 053/100 | Train Loss: 0.00029205\n","Iter: 054/100 | Train Loss: 0.00028534\n","Iter: 055/100 | Train Loss: 0.00027859\n","Iter: 056/100 | Train Loss: 0.00027231\n","Iter: 057/100 | Train Loss: 0.00026665\n","Iter: 058/100 | Train Loss: 0.00026134\n","Iter: 059/100 | Train Loss: 0.00025603\n","Iter: 060/100 | Train Loss: 0.00025060\n","Iter: 061/100 | Train Loss: 0.00024526\n","Iter: 062/100 | Train Loss: 0.00024026\n","Iter: 063/100 | Train Loss: 0.00023563\n","Iter: 064/100 | Train Loss: 0.00023120\n","Iter: 065/100 | Train Loss: 0.00022678\n","Iter: 066/100 | Train Loss: 0.00022235\n","Iter: 067/100 | Train Loss: 0.00021804\n","Iter: 068/100 | Train Loss: 0.00021398\n","Iter: 069/100 | Train Loss: 0.00021016\n","Iter: 070/100 | Train Loss: 0.00020643\n","Iter: 071/100 | Train Loss: 0.00020270\n","Iter: 072/100 | Train Loss: 0.00019901\n","Iter: 073/100 | Train Loss: 0.00019544\n","Iter: 074/100 | Train Loss: 0.00019206\n","Iter: 075/100 | Train Loss: 0.00018880\n","Iter: 076/100 | Train Loss: 0.00018559\n","Iter: 077/100 | Train Loss: 0.00018240\n","Iter: 078/100 | Train Loss: 0.00017926\n","Iter: 079/100 | Train Loss: 0.00017624\n","Iter: 080/100 | Train Loss: 0.00017333\n","Iter: 081/100 | Train Loss: 0.00017050\n","Iter: 082/100 | Train Loss: 0.00016770\n","Iter: 083/100 | Train Loss: 0.00016493\n","Iter: 084/100 | Train Loss: 0.00016224\n","Iter: 085/100 | Train Loss: 0.00015963\n","Iter: 086/100 | Train Loss: 0.00015709\n","Iter: 087/100 | Train Loss: 0.00015460\n","Iter: 088/100 | Train Loss: 0.00015215\n","Iter: 089/100 | Train Loss: 0.00014974\n","Iter: 090/100 | Train Loss: 0.00014739\n","Iter: 091/100 | Train Loss: 0.00014511\n","Iter: 092/100 | Train Loss: 0.00014288\n","Iter: 093/100 | Train Loss: 0.00014069\n","Iter: 094/100 | Train Loss: 0.00013853\n","Iter: 095/100 | Train Loss: 0.00013642\n","Iter: 096/100 | Train Loss: 0.00013437\n","Iter: 097/100 | Train Loss: 0.00013236\n","Iter: 098/100 | Train Loss: 0.00013039\n","Iter: 099/100 | Train Loss: 0.00012845\n","\n","Iter: 099/100 | Test Loss: 0.00100644 | Test acc: 65.0900\n","scale:1.200000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00236525\n","Iter: 001/100 | Train Loss: 0.00198983\n","Iter: 002/100 | Train Loss: 0.00165936\n","Iter: 003/100 | Train Loss: 0.00148422\n","Iter: 004/100 | Train Loss: 0.00148369\n","Iter: 005/100 | Train Loss: 0.00159785\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 006/100 | Train Loss: 0.00173242\n","Adjusting Layer 1, Kernel Nodes: 680, Adptive Nodes:120\n","Iter: 007/100 | Train Loss: 0.00178907\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 008/100 | Train Loss: 0.00171403\n","Iter: 009/100 | Train Loss: 0.00154569\n","Iter: 010/100 | Train Loss: 0.00136702\n","Iter: 011/100 | Train Loss: 0.00124911\n","Iter: 012/100 | Train Loss: 0.00121546\n","Iter: 013/100 | Train Loss: 0.00124041\n","Adjusting Layer 1, Kernel Nodes: 559, Adptive Nodes:241\n","Iter: 014/100 | Train Loss: 0.00127348\n","Adjusting Layer 1, Kernel Nodes: 637, Adptive Nodes:163\n","Iter: 015/100 | Train Loss: 0.00126609\n","Iter: 016/100 | Train Loss: 0.00120189\n","Iter: 017/100 | Train Loss: 0.00110483\n","Iter: 018/100 | Train Loss: 0.00101486\n","Iter: 019/100 | Train Loss: 0.00096031\n","Iter: 020/100 | Train Loss: 0.00094348\n","Iter: 021/100 | Train Loss: 0.00094434\n","Adjusting Layer 1, Kernel Nodes: 563, Adptive Nodes:237\n","Iter: 022/100 | Train Loss: 0.00093636\n","Iter: 023/100 | Train Loss: 0.00090528\n","Iter: 024/100 | Train Loss: 0.00085522\n","Iter: 025/100 | Train Loss: 0.00080269\n","Iter: 026/100 | Train Loss: 0.00076327\n","Iter: 027/100 | Train Loss: 0.00074191\n","Iter: 028/100 | Train Loss: 0.00073214\n","Iter: 029/100 | Train Loss: 0.00072253\n","Iter: 030/100 | Train Loss: 0.00070498\n","Iter: 031/100 | Train Loss: 0.00067905\n","Iter: 032/100 | Train Loss: 0.00065054\n","Iter: 033/100 | Train Loss: 0.00062623\n","Iter: 034/100 | Train Loss: 0.00060948\n","Iter: 035/100 | Train Loss: 0.00059883\n","Iter: 036/100 | Train Loss: 0.00058982\n","Iter: 037/100 | Train Loss: 0.00057860\n","Iter: 038/100 | Train Loss: 0.00056402\n","Iter: 039/100 | Train Loss: 0.00054772\n","Iter: 040/100 | Train Loss: 0.00053247\n","Iter: 041/100 | Train Loss: 0.00052010\n","Iter: 042/100 | Train Loss: 0.00051058\n","Iter: 043/100 | Train Loss: 0.00050248\n","Iter: 044/100 | Train Loss: 0.00049410\n","Iter: 045/100 | Train Loss: 0.00048461\n","Iter: 046/100 | Train Loss: 0.00047429\n","Iter: 047/100 | Train Loss: 0.00046411\n","Iter: 048/100 | Train Loss: 0.00045492\n","Iter: 049/100 | Train Loss: 0.00044698\n","Iter: 050/100 | Train Loss: 0.00043992\n","Iter: 051/100 | Train Loss: 0.00043311\n","Iter: 052/100 | Train Loss: 0.00042609\n","Iter: 053/100 | Train Loss: 0.00041879\n","Iter: 054/100 | Train Loss: 0.00041143\n","Iter: 055/100 | Train Loss: 0.00040436\n","Iter: 056/100 | Train Loss: 0.00039780\n","Iter: 057/100 | Train Loss: 0.00039172\n","Iter: 058/100 | Train Loss: 0.00038594\n","Iter: 059/100 | Train Loss: 0.00038026\n","Iter: 060/100 | Train Loss: 0.00037455\n","Iter: 061/100 | Train Loss: 0.00036881\n","Iter: 062/100 | Train Loss: 0.00036318\n","Iter: 063/100 | Train Loss: 0.00035774\n","Iter: 064/100 | Train Loss: 0.00035257\n","Iter: 065/100 | Train Loss: 0.00034762\n","Iter: 066/100 | Train Loss: 0.00034281\n","Iter: 067/100 | Train Loss: 0.00033808\n","Iter: 068/100 | Train Loss: 0.00033337\n","Iter: 069/100 | Train Loss: 0.00032871\n","Iter: 070/100 | Train Loss: 0.00032414\n","Iter: 071/100 | Train Loss: 0.00031972\n","Iter: 072/100 | Train Loss: 0.00031545\n","Iter: 073/100 | Train Loss: 0.00031132\n","Iter: 074/100 | Train Loss: 0.00030727\n","Iter: 075/100 | Train Loss: 0.00030327\n","Iter: 076/100 | Train Loss: 0.00029931\n","Iter: 077/100 | Train Loss: 0.00029542\n","Iter: 078/100 | Train Loss: 0.00029161\n","Iter: 079/100 | Train Loss: 0.00028791\n","Iter: 080/100 | Train Loss: 0.00028431\n","Iter: 081/100 | Train Loss: 0.00028079\n","Iter: 082/100 | Train Loss: 0.00027732\n","Iter: 083/100 | Train Loss: 0.00027391\n","Iter: 084/100 | Train Loss: 0.00027054\n","Iter: 085/100 | Train Loss: 0.00026724\n","Iter: 086/100 | Train Loss: 0.00026401\n","Iter: 087/100 | Train Loss: 0.00026086\n","Iter: 088/100 | Train Loss: 0.00025777\n","Iter: 089/100 | Train Loss: 0.00025473\n","Iter: 090/100 | Train Loss: 0.00025174\n","Iter: 091/100 | Train Loss: 0.00024878\n","Iter: 092/100 | Train Loss: 0.00024588\n","Iter: 093/100 | Train Loss: 0.00024304\n","Iter: 094/100 | Train Loss: 0.00024024\n","Iter: 095/100 | Train Loss: 0.00023750\n","Iter: 096/100 | Train Loss: 0.00023481\n","Iter: 097/100 | Train Loss: 0.00023215\n","Iter: 098/100 | Train Loss: 0.00022954\n","Iter: 099/100 | Train Loss: 0.00022696\n","\n","Iter: 099/100 | Test Loss: 0.00098506 | Test acc: 65.2700\n","scale:1.200000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00237044\n","Iter: 001/100 | Train Loss: 0.00200034\n","Iter: 002/100 | Train Loss: 0.00167013\n","Iter: 003/100 | Train Loss: 0.00148841\n","Iter: 004/100 | Train Loss: 0.00147732\n","Iter: 005/100 | Train Loss: 0.00158284\n","Adjusting Layer 1, Kernel Nodes: 756, Adptive Nodes:44\n","Iter: 006/100 | Train Loss: 0.00171377\n","Adjusting Layer 1, Kernel Nodes: 728, Adptive Nodes:72\n","Iter: 007/100 | Train Loss: 0.00178238\n","Adjusting Layer 1, Kernel Nodes: 702, Adptive Nodes:98\n","Iter: 008/100 | Train Loss: 0.00174273\n","Iter: 009/100 | Train Loss: 0.00161074\n","Iter: 010/100 | Train Loss: 0.00144286\n","Iter: 011/100 | Train Loss: 0.00130018\n","Iter: 012/100 | Train Loss: 0.00121978\n","Iter: 013/100 | Train Loss: 0.00120362\n","Iter: 014/100 | Train Loss: 0.00122610\n","Adjusting Layer 1, Kernel Nodes: 682, Adptive Nodes:118\n","Iter: 015/100 | Train Loss: 0.00125152\n","Adjusting Layer 1, Kernel Nodes: 634, Adptive Nodes:166\n","Iter: 016/100 | Train Loss: 0.00124865\n","Iter: 017/100 | Train Loss: 0.00120637\n","Iter: 018/100 | Train Loss: 0.00113490\n","Iter: 019/100 | Train Loss: 0.00105642\n","Iter: 020/100 | Train Loss: 0.00099234\n","Iter: 021/100 | Train Loss: 0.00095399\n","Iter: 022/100 | Train Loss: 0.00093939\n","Iter: 023/100 | Train Loss: 0.00093686\n","Iter: 024/100 | Train Loss: 0.00093241\n","Iter: 025/100 | Train Loss: 0.00091638\n","Iter: 026/100 | Train Loss: 0.00088695\n","Iter: 027/100 | Train Loss: 0.00084931\n","Iter: 028/100 | Train Loss: 0.00081176\n","Iter: 029/100 | Train Loss: 0.00078136\n","Iter: 030/100 | Train Loss: 0.00076094\n","Iter: 031/100 | Train Loss: 0.00074882\n","Iter: 032/100 | Train Loss: 0.00074037\n","Iter: 033/100 | Train Loss: 0.00073072\n","Iter: 034/100 | Train Loss: 0.00071693\n","Iter: 035/100 | Train Loss: 0.00069887\n","Iter: 036/100 | Train Loss: 0.00067872\n","Iter: 037/100 | Train Loss: 0.00065940\n","Iter: 038/100 | Train Loss: 0.00064317\n","Iter: 039/100 | Train Loss: 0.00063073\n","Iter: 040/100 | Train Loss: 0.00062123\n","Iter: 041/100 | Train Loss: 0.00061294\n","Iter: 042/100 | Train Loss: 0.00060414\n","Iter: 043/100 | Train Loss: 0.00059397\n","Iter: 044/100 | Train Loss: 0.00058250\n","Iter: 045/100 | Train Loss: 0.00057060\n","Iter: 046/100 | Train Loss: 0.00055928\n","Iter: 047/100 | Train Loss: 0.00054928\n","Iter: 048/100 | Train Loss: 0.00054073\n","Iter: 049/100 | Train Loss: 0.00053323\n","Iter: 050/100 | Train Loss: 0.00052616\n","Iter: 051/100 | Train Loss: 0.00051896\n","Iter: 052/100 | Train Loss: 0.00051138\n","Iter: 053/100 | Train Loss: 0.00050349\n","Iter: 054/100 | Train Loss: 0.00049560\n","Iter: 055/100 | Train Loss: 0.00048806\n","Iter: 056/100 | Train Loss: 0.00048106\n","Iter: 057/100 | Train Loss: 0.00047461\n","Iter: 058/100 | Train Loss: 0.00046858\n","Iter: 059/100 | Train Loss: 0.00046274\n","Iter: 060/100 | Train Loss: 0.00045692\n","Iter: 061/100 | Train Loss: 0.00045104\n","Iter: 062/100 | Train Loss: 0.00044513\n","Iter: 063/100 | Train Loss: 0.00043932\n","Iter: 064/100 | Train Loss: 0.00043370\n","Iter: 065/100 | Train Loss: 0.00042833\n","Iter: 066/100 | Train Loss: 0.00042319\n","Iter: 067/100 | Train Loss: 0.00041825\n","Iter: 068/100 | Train Loss: 0.00041343\n","Iter: 069/100 | Train Loss: 0.00040866\n","Iter: 070/100 | Train Loss: 0.00040393\n","Iter: 071/100 | Train Loss: 0.00039924\n","Iter: 072/100 | Train Loss: 0.00039464\n","Iter: 073/100 | Train Loss: 0.00039015\n","Iter: 074/100 | Train Loss: 0.00038579\n","Iter: 075/100 | Train Loss: 0.00038155\n","Iter: 076/100 | Train Loss: 0.00037742\n","Iter: 077/100 | Train Loss: 0.00037338\n","Iter: 078/100 | Train Loss: 0.00036940\n","Iter: 079/100 | Train Loss: 0.00036546\n","Iter: 080/100 | Train Loss: 0.00036159\n","Iter: 081/100 | Train Loss: 0.00035778\n","Iter: 082/100 | Train Loss: 0.00035404\n","Iter: 083/100 | Train Loss: 0.00035038\n","Iter: 084/100 | Train Loss: 0.00034679\n","Iter: 085/100 | Train Loss: 0.00034326\n","Iter: 086/100 | Train Loss: 0.00033979\n","Iter: 087/100 | Train Loss: 0.00033637\n","Iter: 088/100 | Train Loss: 0.00033300\n","Iter: 089/100 | Train Loss: 0.00032969\n","Iter: 090/100 | Train Loss: 0.00032642\n","Iter: 091/100 | Train Loss: 0.00032321\n","Iter: 092/100 | Train Loss: 0.00032005\n","Iter: 093/100 | Train Loss: 0.00031694\n","Iter: 094/100 | Train Loss: 0.00031389\n","Iter: 095/100 | Train Loss: 0.00031088\n","Iter: 096/100 | Train Loss: 0.00030793\n","Iter: 097/100 | Train Loss: 0.00030501\n","Iter: 098/100 | Train Loss: 0.00030213\n","Iter: 099/100 | Train Loss: 0.00029929\n","\n","Iter: 099/100 | Test Loss: 0.00098721 | Test acc: 64.6100\n","scale:1.200000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00237281\n","Iter: 001/100 | Train Loss: 0.00200517\n","Iter: 002/100 | Train Loss: 0.00167512\n","Iter: 003/100 | Train Loss: 0.00149027\n","Iter: 004/100 | Train Loss: 0.00147401\n","Iter: 005/100 | Train Loss: 0.00157495\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00170266\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00177413\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 008/100 | Train Loss: 0.00174776\n","Iter: 009/100 | Train Loss: 0.00163275\n","Iter: 010/100 | Train Loss: 0.00147414\n","Iter: 011/100 | Train Loss: 0.00132576\n","Iter: 012/100 | Train Loss: 0.00122586\n","Iter: 013/100 | Train Loss: 0.00118550\n","Iter: 014/100 | Train Loss: 0.00119055\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 015/100 | Train Loss: 0.00121295\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 016/100 | Train Loss: 0.00122473\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00120870\n","Iter: 018/100 | Train Loss: 0.00116260\n","Iter: 019/100 | Train Loss: 0.00109682\n","Iter: 020/100 | Train Loss: 0.00102778\n","Iter: 021/100 | Train Loss: 0.00097037\n","Iter: 022/100 | Train Loss: 0.00093247\n","Iter: 023/100 | Train Loss: 0.00091368\n","Iter: 024/100 | Train Loss: 0.00090696\n","Iter: 025/100 | Train Loss: 0.00090270\n","Iter: 026/100 | Train Loss: 0.00089270\n","Iter: 027/100 | Train Loss: 0.00087313\n","Iter: 028/100 | Train Loss: 0.00084502\n","Iter: 029/100 | Train Loss: 0.00081274\n","Iter: 030/100 | Train Loss: 0.00078169\n","Iter: 031/100 | Train Loss: 0.00075609\n","Iter: 032/100 | Train Loss: 0.00073761\n","Iter: 033/100 | Train Loss: 0.00072524\n","Iter: 034/100 | Train Loss: 0.00071620\n","Iter: 035/100 | Train Loss: 0.00070736\n","Iter: 036/100 | Train Loss: 0.00069639\n","Iter: 037/100 | Train Loss: 0.00068243\n","Iter: 038/100 | Train Loss: 0.00066612\n","Iter: 039/100 | Train Loss: 0.00064911\n","Iter: 040/100 | Train Loss: 0.00063317\n","Iter: 041/100 | Train Loss: 0.00061953\n","Iter: 042/100 | Train Loss: 0.00060851\n","Iter: 043/100 | Train Loss: 0.00059954\n","Iter: 044/100 | Train Loss: 0.00059155\n","Iter: 045/100 | Train Loss: 0.00058349\n","Iter: 046/100 | Train Loss: 0.00057469\n","Iter: 047/100 | Train Loss: 0.00056505\n","Iter: 048/100 | Train Loss: 0.00055489\n","Iter: 049/100 | Train Loss: 0.00054480\n","Iter: 050/100 | Train Loss: 0.00053536\n","Iter: 051/100 | Train Loss: 0.00052687\n","Iter: 052/100 | Train Loss: 0.00051933\n","Iter: 053/100 | Train Loss: 0.00051250\n","Iter: 054/100 | Train Loss: 0.00050600\n","Iter: 055/100 | Train Loss: 0.00049948\n","Iter: 056/100 | Train Loss: 0.00049274\n","Iter: 057/100 | Train Loss: 0.00048579\n","Iter: 058/100 | Train Loss: 0.00047880\n","Iter: 059/100 | Train Loss: 0.00047196\n","Iter: 060/100 | Train Loss: 0.00046545\n","Iter: 061/100 | Train Loss: 0.00045937\n","Iter: 062/100 | Train Loss: 0.00045367\n","Iter: 063/100 | Train Loss: 0.00044823\n","Iter: 064/100 | Train Loss: 0.00044291\n","Iter: 065/100 | Train Loss: 0.00043762\n","Iter: 066/100 | Train Loss: 0.00043230\n","Iter: 067/100 | Train Loss: 0.00042700\n","Iter: 068/100 | Train Loss: 0.00042176\n","Iter: 069/100 | Train Loss: 0.00041668\n","Iter: 070/100 | Train Loss: 0.00041179\n","Iter: 071/100 | Train Loss: 0.00040710\n","Iter: 072/100 | Train Loss: 0.00040258\n","Iter: 073/100 | Train Loss: 0.00039819\n","Iter: 074/100 | Train Loss: 0.00039387\n","Iter: 075/100 | Train Loss: 0.00038958\n","Iter: 076/100 | Train Loss: 0.00038533\n","Iter: 077/100 | Train Loss: 0.00038114\n","Iter: 078/100 | Train Loss: 0.00037702\n","Iter: 079/100 | Train Loss: 0.00037300\n","Iter: 080/100 | Train Loss: 0.00036909\n","Iter: 081/100 | Train Loss: 0.00036528\n","Iter: 082/100 | Train Loss: 0.00036156\n","Iter: 083/100 | Train Loss: 0.00035791\n","Iter: 084/100 | Train Loss: 0.00035431\n","Iter: 085/100 | Train Loss: 0.00035076\n","Iter: 086/100 | Train Loss: 0.00034725\n","Iter: 087/100 | Train Loss: 0.00034379\n","Iter: 088/100 | Train Loss: 0.00034039\n","Iter: 089/100 | Train Loss: 0.00033706\n","Iter: 090/100 | Train Loss: 0.00033380\n","Iter: 091/100 | Train Loss: 0.00033060\n","Iter: 092/100 | Train Loss: 0.00032745\n","Iter: 093/100 | Train Loss: 0.00032434\n","Iter: 094/100 | Train Loss: 0.00032128\n","Iter: 095/100 | Train Loss: 0.00031826\n","Iter: 096/100 | Train Loss: 0.00031528\n","Iter: 097/100 | Train Loss: 0.00031235\n","Iter: 098/100 | Train Loss: 0.00030947\n","Iter: 099/100 | Train Loss: 0.00030663\n","\n","Iter: 099/100 | Test Loss: 0.00098671 | Test acc: 64.2000\n","scale:1.200000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00237513\n","Iter: 001/100 | Train Loss: 0.00200979\n","Iter: 002/100 | Train Loss: 0.00167977\n","Iter: 003/100 | Train Loss: 0.00149144\n","Iter: 004/100 | Train Loss: 0.00146956\n","Iter: 005/100 | Train Loss: 0.00156533\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00168952\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00175960\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 008/100 | Train Loss: 0.00173163\n","Iter: 009/100 | Train Loss: 0.00161543\n","Iter: 010/100 | Train Loss: 0.00145599\n","Iter: 011/100 | Train Loss: 0.00130687\n","Iter: 012/100 | Train Loss: 0.00120552\n","Iter: 013/100 | Train Loss: 0.00116211\n","Iter: 014/100 | Train Loss: 0.00116277\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00117961\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 016/100 | Train Loss: 0.00118500\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 017/100 | Train Loss: 0.00116069\n","Iter: 018/100 | Train Loss: 0.00110648\n","Iter: 019/100 | Train Loss: 0.00103504\n","Iter: 020/100 | Train Loss: 0.00096394\n","Iter: 021/100 | Train Loss: 0.00090774\n","Iter: 022/100 | Train Loss: 0.00087244\n","Iter: 023/100 | Train Loss: 0.00085476\n","Iter: 024/100 | Train Loss: 0.00084580\n","Iter: 025/100 | Train Loss: 0.00083583\n","Iter: 026/100 | Train Loss: 0.00081822\n","Iter: 027/100 | Train Loss: 0.00079154\n","Iter: 028/100 | Train Loss: 0.00075894\n","Iter: 029/100 | Train Loss: 0.00072574\n","Iter: 030/100 | Train Loss: 0.00069680\n","Iter: 031/100 | Train Loss: 0.00067467\n","Iter: 032/100 | Train Loss: 0.00065902\n","Iter: 033/100 | Train Loss: 0.00064754\n","Iter: 034/100 | Train Loss: 0.00063710\n","Iter: 035/100 | Train Loss: 0.00062521\n","Iter: 036/100 | Train Loss: 0.00061080\n","Iter: 037/100 | Train Loss: 0.00059425\n","Iter: 038/100 | Train Loss: 0.00057693\n","Iter: 039/100 | Train Loss: 0.00056052\n","Iter: 040/100 | Train Loss: 0.00054632\n","Iter: 041/100 | Train Loss: 0.00053469\n","Iter: 042/100 | Train Loss: 0.00052514\n","Iter: 043/100 | Train Loss: 0.00051665\n","Iter: 044/100 | Train Loss: 0.00050820\n","Iter: 045/100 | Train Loss: 0.00049906\n","Iter: 046/100 | Train Loss: 0.00048919\n","Iter: 047/100 | Train Loss: 0.00047893\n","Iter: 048/100 | Train Loss: 0.00046892\n","Iter: 049/100 | Train Loss: 0.00045972\n","Iter: 050/100 | Train Loss: 0.00045155\n","Iter: 051/100 | Train Loss: 0.00044432\n","Iter: 052/100 | Train Loss: 0.00043772\n","Iter: 053/100 | Train Loss: 0.00043132\n","Iter: 054/100 | Train Loss: 0.00042480\n","Iter: 055/100 | Train Loss: 0.00041803\n","Iter: 056/100 | Train Loss: 0.00041112\n","Iter: 057/100 | Train Loss: 0.00040429\n","Iter: 058/100 | Train Loss: 0.00039778\n","Iter: 059/100 | Train Loss: 0.00039172\n","Iter: 060/100 | Train Loss: 0.00038612\n","Iter: 061/100 | Train Loss: 0.00038085\n","Iter: 062/100 | Train Loss: 0.00037575\n","Iter: 063/100 | Train Loss: 0.00037064\n","Iter: 064/100 | Train Loss: 0.00036545\n","Iter: 065/100 | Train Loss: 0.00036023\n","Iter: 066/100 | Train Loss: 0.00035507\n","Iter: 067/100 | Train Loss: 0.00035007\n","Iter: 068/100 | Train Loss: 0.00034532\n","Iter: 069/100 | Train Loss: 0.00034080\n","Iter: 070/100 | Train Loss: 0.00033645\n","Iter: 071/100 | Train Loss: 0.00033220\n","Iter: 072/100 | Train Loss: 0.00032799\n","Iter: 073/100 | Train Loss: 0.00032380\n","Iter: 074/100 | Train Loss: 0.00031965\n","Iter: 075/100 | Train Loss: 0.00031560\n","Iter: 076/100 | Train Loss: 0.00031165\n","Iter: 077/100 | Train Loss: 0.00030781\n","Iter: 078/100 | Train Loss: 0.00030409\n","Iter: 079/100 | Train Loss: 0.00030048\n","Iter: 080/100 | Train Loss: 0.00029694\n","Iter: 081/100 | Train Loss: 0.00029346\n","Iter: 082/100 | Train Loss: 0.00029003\n","Iter: 083/100 | Train Loss: 0.00028665\n","Iter: 084/100 | Train Loss: 0.00028333\n","Iter: 085/100 | Train Loss: 0.00028007\n","Iter: 086/100 | Train Loss: 0.00027687\n","Iter: 087/100 | Train Loss: 0.00027375\n","Iter: 088/100 | Train Loss: 0.00027069\n","Iter: 089/100 | Train Loss: 0.00026768\n","Iter: 090/100 | Train Loss: 0.00026471\n","Iter: 091/100 | Train Loss: 0.00026177\n","Iter: 092/100 | Train Loss: 0.00025888\n","Iter: 093/100 | Train Loss: 0.00025604\n","Iter: 094/100 | Train Loss: 0.00025325\n","Iter: 095/100 | Train Loss: 0.00025051\n","Iter: 096/100 | Train Loss: 0.00024782\n","Iter: 097/100 | Train Loss: 0.00024518\n","Iter: 098/100 | Train Loss: 0.00024258\n","Iter: 099/100 | Train Loss: 0.00024001\n","\n","Iter: 099/100 | Test Loss: 0.00097254 | Test acc: 64.8800\n","scale:1.250000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00223760\n","Iter: 001/100 | Train Loss: 0.00176188\n","Iter: 002/100 | Train Loss: 0.00149129\n","Iter: 003/100 | Train Loss: 0.00152456\n","Adjusting Layer 1, Kernel Nodes: 591, Adptive Nodes:209\n","Iter: 004/100 | Train Loss: 0.00175081\n","Adjusting Layer 1, Kernel Nodes: 416, Adptive Nodes:384\n","Iter: 005/100 | Train Loss: 0.00184743\n","Adjusting Layer 1, Kernel Nodes: 349, Adptive Nodes:451\n","Iter: 006/100 | Train Loss: 0.00166235\n","Iter: 007/100 | Train Loss: 0.00137648\n","Iter: 008/100 | Train Loss: 0.00121133\n","Iter: 009/100 | Train Loss: 0.00121691\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 010/100 | Train Loss: 0.00125325\n","Adjusting Layer 1, Kernel Nodes: 269, Adptive Nodes:531\n","Iter: 011/100 | Train Loss: 0.00124420\n","Iter: 012/100 | Train Loss: 0.00113946\n","Iter: 013/100 | Train Loss: 0.00099917\n","Iter: 014/100 | Train Loss: 0.00090384\n","Iter: 015/100 | Train Loss: 0.00088091\n","Iter: 016/100 | Train Loss: 0.00089277\n","Adjusting Layer 1, Kernel Nodes: 553, Adptive Nodes:247\n","Iter: 017/100 | Train Loss: 0.00087231\n","Iter: 018/100 | Train Loss: 0.00080220\n","Iter: 019/100 | Train Loss: 0.00072024\n","Iter: 020/100 | Train Loss: 0.00067236\n","Iter: 021/100 | Train Loss: 0.00066424\n","Iter: 022/100 | Train Loss: 0.00066326\n","Iter: 023/100 | Train Loss: 0.00063880\n","Iter: 024/100 | Train Loss: 0.00059273\n","Iter: 025/100 | Train Loss: 0.00055091\n","Iter: 026/100 | Train Loss: 0.00053190\n","Iter: 027/100 | Train Loss: 0.00052917\n","Iter: 028/100 | Train Loss: 0.00052253\n","Iter: 029/100 | Train Loss: 0.00050119\n","Iter: 030/100 | Train Loss: 0.00047268\n","Iter: 031/100 | Train Loss: 0.00045128\n","Iter: 032/100 | Train Loss: 0.00044197\n","Iter: 033/100 | Train Loss: 0.00043716\n","Iter: 034/100 | Train Loss: 0.00042699\n","Iter: 035/100 | Train Loss: 0.00040973\n","Iter: 036/100 | Train Loss: 0.00039170\n","Iter: 037/100 | Train Loss: 0.00037916\n","Iter: 038/100 | Train Loss: 0.00037199\n","Iter: 039/100 | Train Loss: 0.00036514\n","Iter: 040/100 | Train Loss: 0.00035477\n","Iter: 041/100 | Train Loss: 0.00034181\n","Iter: 042/100 | Train Loss: 0.00032997\n","Iter: 043/100 | Train Loss: 0.00032143\n","Iter: 044/100 | Train Loss: 0.00031500\n","Iter: 045/100 | Train Loss: 0.00030799\n","Iter: 046/100 | Train Loss: 0.00029919\n","Iter: 047/100 | Train Loss: 0.00028977\n","Iter: 048/100 | Train Loss: 0.00028158\n","Iter: 049/100 | Train Loss: 0.00027515\n","Iter: 050/100 | Train Loss: 0.00026944\n","Iter: 051/100 | Train Loss: 0.00026318\n","Iter: 052/100 | Train Loss: 0.00025618\n","Iter: 053/100 | Train Loss: 0.00024930\n","Iter: 054/100 | Train Loss: 0.00024334\n","Iter: 055/100 | Train Loss: 0.00023824\n","Iter: 056/100 | Train Loss: 0.00023332\n","Iter: 057/100 | Train Loss: 0.00022806\n","Iter: 058/100 | Train Loss: 0.00022259\n","Iter: 059/100 | Train Loss: 0.00021741\n","Iter: 060/100 | Train Loss: 0.00021281\n","Iter: 061/100 | Train Loss: 0.00020860\n","Iter: 062/100 | Train Loss: 0.00020439\n","Iter: 063/100 | Train Loss: 0.00020005\n","Iter: 064/100 | Train Loss: 0.00019573\n","Iter: 065/100 | Train Loss: 0.00019168\n","Iter: 066/100 | Train Loss: 0.00018793\n","Iter: 067/100 | Train Loss: 0.00018434\n","Iter: 068/100 | Train Loss: 0.00018075\n","Iter: 069/100 | Train Loss: 0.00017715\n","Iter: 070/100 | Train Loss: 0.00017363\n","Iter: 071/100 | Train Loss: 0.00017029\n","Iter: 072/100 | Train Loss: 0.00016712\n","Iter: 073/100 | Train Loss: 0.00016403\n","Iter: 074/100 | Train Loss: 0.00016097\n","Iter: 075/100 | Train Loss: 0.00015793\n","Iter: 076/100 | Train Loss: 0.00015499\n","Iter: 077/100 | Train Loss: 0.00015217\n","Iter: 078/100 | Train Loss: 0.00014945\n","Iter: 079/100 | Train Loss: 0.00014679\n","Iter: 080/100 | Train Loss: 0.00014417\n","Iter: 081/100 | Train Loss: 0.00014159\n","Iter: 082/100 | Train Loss: 0.00013910\n","Iter: 083/100 | Train Loss: 0.00013668\n","Iter: 084/100 | Train Loss: 0.00013433\n","Iter: 085/100 | Train Loss: 0.00013202\n","Iter: 086/100 | Train Loss: 0.00012976\n","Iter: 087/100 | Train Loss: 0.00012754\n","Iter: 088/100 | Train Loss: 0.00012538\n","Iter: 089/100 | Train Loss: 0.00012328\n","Iter: 090/100 | Train Loss: 0.00012123\n","Iter: 091/100 | Train Loss: 0.00011923\n","Iter: 092/100 | Train Loss: 0.00011726\n","Iter: 093/100 | Train Loss: 0.00011533\n","Iter: 094/100 | Train Loss: 0.00011345\n","Iter: 095/100 | Train Loss: 0.00011161\n","Iter: 096/100 | Train Loss: 0.00010982\n","Iter: 097/100 | Train Loss: 0.00010806\n","Iter: 098/100 | Train Loss: 0.00010633\n","Iter: 099/100 | Train Loss: 0.00010464\n","\n","Iter: 099/100 | Test Loss: 0.00101876 | Test acc: 65.0100\n","scale:1.250000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00227588\n","Iter: 001/100 | Train Loss: 0.00182391\n","Iter: 002/100 | Train Loss: 0.00152273\n","Iter: 003/100 | Train Loss: 0.00148658\n","Iter: 004/100 | Train Loss: 0.00164661\n","Adjusting Layer 1, Kernel Nodes: 620, Adptive Nodes:180\n","Iter: 005/100 | Train Loss: 0.00182459\n","Adjusting Layer 1, Kernel Nodes: 523, Adptive Nodes:277\n","Iter: 006/100 | Train Loss: 0.00176395\n","Iter: 007/100 | Train Loss: 0.00149626\n","Iter: 008/100 | Train Loss: 0.00125988\n","Iter: 009/100 | Train Loss: 0.00120040\n","Iter: 010/100 | Train Loss: 0.00125839\n","Adjusting Layer 1, Kernel Nodes: 332, Adptive Nodes:468\n","Iter: 011/100 | Train Loss: 0.00126608\n","Adjusting Layer 1, Kernel Nodes: 416, Adptive Nodes:384\n","Iter: 012/100 | Train Loss: 0.00119434\n","Iter: 013/100 | Train Loss: 0.00105663\n","Iter: 014/100 | Train Loss: 0.00093109\n","Iter: 015/100 | Train Loss: 0.00087328\n","Iter: 016/100 | Train Loss: 0.00087496\n","Adjusting Layer 1, Kernel Nodes: 430, Adptive Nodes:370\n","Iter: 017/100 | Train Loss: 0.00088123\n","Adjusting Layer 1, Kernel Nodes: 416, Adptive Nodes:384\n","Iter: 018/100 | Train Loss: 0.00084979\n","Iter: 019/100 | Train Loss: 0.00077950\n","Iter: 020/100 | Train Loss: 0.00070649\n","Iter: 021/100 | Train Loss: 0.00066462\n","Iter: 022/100 | Train Loss: 0.00065661\n","Iter: 023/100 | Train Loss: 0.00065781\n","Adjusting Layer 1, Kernel Nodes: 209, Adptive Nodes:591\n","Iter: 024/100 | Train Loss: 0.00064662\n","Iter: 025/100 | Train Loss: 0.00061971\n","Iter: 026/100 | Train Loss: 0.00058544\n","Iter: 027/100 | Train Loss: 0.00055598\n","Iter: 028/100 | Train Loss: 0.00053864\n","Iter: 029/100 | Train Loss: 0.00053209\n","Iter: 030/100 | Train Loss: 0.00052911\n","Iter: 031/100 | Train Loss: 0.00052221\n","Iter: 032/100 | Train Loss: 0.00050857\n","Iter: 033/100 | Train Loss: 0.00049076\n","Iter: 034/100 | Train Loss: 0.00047375\n","Iter: 035/100 | Train Loss: 0.00046119\n","Iter: 036/100 | Train Loss: 0.00045336\n","Iter: 037/100 | Train Loss: 0.00044775\n","Iter: 038/100 | Train Loss: 0.00044130\n","Iter: 039/100 | Train Loss: 0.00043244\n","Iter: 040/100 | Train Loss: 0.00042165\n","Iter: 041/100 | Train Loss: 0.00041075\n","Iter: 042/100 | Train Loss: 0.00040138\n","Iter: 043/100 | Train Loss: 0.00039404\n","Iter: 044/100 | Train Loss: 0.00038804\n","Iter: 045/100 | Train Loss: 0.00038219\n","Iter: 046/100 | Train Loss: 0.00037561\n","Iter: 047/100 | Train Loss: 0.00036830\n","Iter: 048/100 | Train Loss: 0.00036084\n","Iter: 049/100 | Train Loss: 0.00035386\n","Iter: 050/100 | Train Loss: 0.00034768\n","Iter: 051/100 | Train Loss: 0.00034216\n","Iter: 052/100 | Train Loss: 0.00033692\n","Iter: 053/100 | Train Loss: 0.00033159\n","Iter: 054/100 | Train Loss: 0.00032606\n","Iter: 055/100 | Train Loss: 0.00032046\n","Iter: 056/100 | Train Loss: 0.00031501\n","Iter: 057/100 | Train Loss: 0.00030987\n","Iter: 058/100 | Train Loss: 0.00030509\n","Iter: 059/100 | Train Loss: 0.00030055\n","Iter: 060/100 | Train Loss: 0.00029610\n","Iter: 061/100 | Train Loss: 0.00029167\n","Iter: 062/100 | Train Loss: 0.00028724\n","Iter: 063/100 | Train Loss: 0.00028287\n","Iter: 064/100 | Train Loss: 0.00027861\n","Iter: 065/100 | Train Loss: 0.00027451\n","Iter: 066/100 | Train Loss: 0.00027061\n","Iter: 067/100 | Train Loss: 0.00026682\n","Iter: 068/100 | Train Loss: 0.00026311\n","Iter: 069/100 | Train Loss: 0.00025943\n","Iter: 070/100 | Train Loss: 0.00025579\n","Iter: 071/100 | Train Loss: 0.00025221\n","Iter: 072/100 | Train Loss: 0.00024872\n","Iter: 073/100 | Train Loss: 0.00024534\n","Iter: 074/100 | Train Loss: 0.00024206\n","Iter: 075/100 | Train Loss: 0.00023886\n","Iter: 076/100 | Train Loss: 0.00023572\n","Iter: 077/100 | Train Loss: 0.00023260\n","Iter: 078/100 | Train Loss: 0.00022954\n","Iter: 079/100 | Train Loss: 0.00022653\n","Iter: 080/100 | Train Loss: 0.00022359\n","Iter: 081/100 | Train Loss: 0.00022072\n","Iter: 082/100 | Train Loss: 0.00021792\n","Iter: 083/100 | Train Loss: 0.00021516\n","Iter: 084/100 | Train Loss: 0.00021245\n","Iter: 085/100 | Train Loss: 0.00020977\n","Iter: 086/100 | Train Loss: 0.00020714\n","Iter: 087/100 | Train Loss: 0.00020457\n","Iter: 088/100 | Train Loss: 0.00020203\n","Iter: 089/100 | Train Loss: 0.00019956\n","Iter: 090/100 | Train Loss: 0.00019712\n","Iter: 091/100 | Train Loss: 0.00019472\n","Iter: 092/100 | Train Loss: 0.00019236\n","Iter: 093/100 | Train Loss: 0.00019004\n","Iter: 094/100 | Train Loss: 0.00018775\n","Iter: 095/100 | Train Loss: 0.00018551\n","Iter: 096/100 | Train Loss: 0.00018331\n","Iter: 097/100 | Train Loss: 0.00018115\n","Iter: 098/100 | Train Loss: 0.00017902\n","Iter: 099/100 | Train Loss: 0.00017692\n","\n","Iter: 099/100 | Test Loss: 0.00099550 | Test acc: 65.5600\n","scale:1.250000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00230517\n","Iter: 001/100 | Train Loss: 0.00187510\n","Iter: 002/100 | Train Loss: 0.00155751\n","Iter: 003/100 | Train Loss: 0.00147104\n","Iter: 004/100 | Train Loss: 0.00158367\n","Adjusting Layer 1, Kernel Nodes: 644, Adptive Nodes:156\n","Iter: 005/100 | Train Loss: 0.00177088\n","Adjusting Layer 1, Kernel Nodes: 591, Adptive Nodes:209\n","Iter: 006/100 | Train Loss: 0.00179606\n","Adjusting Layer 1, Kernel Nodes: 345, Adptive Nodes:455\n","Iter: 007/100 | Train Loss: 0.00160271\n","Iter: 008/100 | Train Loss: 0.00135147\n","Iter: 009/100 | Train Loss: 0.00121317\n","Iter: 010/100 | Train Loss: 0.00121535\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 011/100 | Train Loss: 0.00125474\n","Adjusting Layer 1, Kernel Nodes: 471, Adptive Nodes:329\n","Iter: 012/100 | Train Loss: 0.00124355\n","Iter: 013/100 | Train Loss: 0.00114229\n","Iter: 014/100 | Train Loss: 0.00100174\n","Iter: 015/100 | Train Loss: 0.00090061\n","Iter: 016/100 | Train Loss: 0.00087106\n","Iter: 017/100 | Train Loss: 0.00087974\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 018/100 | Train Loss: 0.00084592\n","Iter: 019/100 | Train Loss: 0.00074800\n","Iter: 020/100 | Train Loss: 0.00066962\n","Iter: 021/100 | Train Loss: 0.00065580\n","Iter: 022/100 | Train Loss: 0.00065168\n","Iter: 023/100 | Train Loss: 0.00060522\n","Iter: 024/100 | Train Loss: 0.00054524\n","Iter: 025/100 | Train Loss: 0.00052088\n","Iter: 026/100 | Train Loss: 0.00052209\n","Adjusting Layer 1, Kernel Nodes: 4, Adptive Nodes:796\n","Iter: 027/100 | Train Loss: 0.00051600\n","Iter: 028/100 | Train Loss: 0.00049354\n","Iter: 029/100 | Train Loss: 0.00046422\n","Iter: 030/100 | Train Loss: 0.00044236\n","Iter: 031/100 | Train Loss: 0.00043373\n","Iter: 032/100 | Train Loss: 0.00043221\n","Iter: 033/100 | Train Loss: 0.00042742\n","Iter: 034/100 | Train Loss: 0.00041440\n","Iter: 035/100 | Train Loss: 0.00039663\n","Iter: 036/100 | Train Loss: 0.00038120\n","Iter: 037/100 | Train Loss: 0.00037209\n","Iter: 038/100 | Train Loss: 0.00036736\n","Iter: 039/100 | Train Loss: 0.00036211\n","Iter: 040/100 | Train Loss: 0.00035316\n","Iter: 041/100 | Train Loss: 0.00034142\n","Iter: 042/100 | Train Loss: 0.00033020\n","Iter: 043/100 | Train Loss: 0.00032194\n","Iter: 044/100 | Train Loss: 0.00031631\n","Iter: 045/100 | Train Loss: 0.00031113\n","Iter: 046/100 | Train Loss: 0.00030454\n","Iter: 047/100 | Train Loss: 0.00029645\n","Iter: 048/100 | Train Loss: 0.00028833\n","Iter: 049/100 | Train Loss: 0.00028157\n","Iter: 050/100 | Train Loss: 0.00027634\n","Iter: 051/100 | Train Loss: 0.00027167\n","Iter: 052/100 | Train Loss: 0.00026652\n","Iter: 053/100 | Train Loss: 0.00026067\n","Iter: 054/100 | Train Loss: 0.00025470\n","Iter: 055/100 | Train Loss: 0.00024932\n","Iter: 056/100 | Train Loss: 0.00024474\n","Iter: 057/100 | Train Loss: 0.00024060\n","Iter: 058/100 | Train Loss: 0.00023642\n","Iter: 059/100 | Train Loss: 0.00023196\n","Iter: 060/100 | Train Loss: 0.00022739\n","Iter: 061/100 | Train Loss: 0.00022304\n","Iter: 062/100 | Train Loss: 0.00021912\n","Iter: 063/100 | Train Loss: 0.00021551\n","Iter: 064/100 | Train Loss: 0.00021199\n","Iter: 065/100 | Train Loss: 0.00020840\n","Iter: 066/100 | Train Loss: 0.00020474\n","Iter: 067/100 | Train Loss: 0.00020116\n","Iter: 068/100 | Train Loss: 0.00019780\n","Iter: 069/100 | Train Loss: 0.00019466\n","Iter: 070/100 | Train Loss: 0.00019163\n","Iter: 071/100 | Train Loss: 0.00018858\n","Iter: 072/100 | Train Loss: 0.00018552\n","Iter: 073/100 | Train Loss: 0.00018250\n","Iter: 074/100 | Train Loss: 0.00017961\n","Iter: 075/100 | Train Loss: 0.00017685\n","Iter: 076/100 | Train Loss: 0.00017417\n","Iter: 077/100 | Train Loss: 0.00017152\n","Iter: 078/100 | Train Loss: 0.00016888\n","Iter: 079/100 | Train Loss: 0.00016629\n","Iter: 080/100 | Train Loss: 0.00016376\n","Iter: 081/100 | Train Loss: 0.00016132\n","Iter: 082/100 | Train Loss: 0.00015895\n","Iter: 083/100 | Train Loss: 0.00015662\n","Iter: 084/100 | Train Loss: 0.00015432\n","Iter: 085/100 | Train Loss: 0.00015204\n","Iter: 086/100 | Train Loss: 0.00014982\n","Iter: 087/100 | Train Loss: 0.00014765\n","Iter: 088/100 | Train Loss: 0.00014554\n","Iter: 089/100 | Train Loss: 0.00014347\n","Iter: 090/100 | Train Loss: 0.00014143\n","Iter: 091/100 | Train Loss: 0.00013942\n","Iter: 092/100 | Train Loss: 0.00013744\n","Iter: 093/100 | Train Loss: 0.00013551\n","Iter: 094/100 | Train Loss: 0.00013363\n","Iter: 095/100 | Train Loss: 0.00013179\n","Iter: 096/100 | Train Loss: 0.00012997\n","Iter: 097/100 | Train Loss: 0.00012818\n","Iter: 098/100 | Train Loss: 0.00012643\n","Iter: 099/100 | Train Loss: 0.00012470\n","\n","Iter: 099/100 | Test Loss: 0.00101002 | Test acc: 64.8200\n","scale:1.250000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00232725\n","Iter: 001/100 | Train Loss: 0.00191576\n","Iter: 002/100 | Train Loss: 0.00159013\n","Iter: 003/100 | Train Loss: 0.00146838\n","Iter: 004/100 | Train Loss: 0.00154131\n","Adjusting Layer 1, Kernel Nodes: 674, Adptive Nodes:126\n","Iter: 005/100 | Train Loss: 0.00171784\n","Adjusting Layer 1, Kernel Nodes: 602, Adptive Nodes:198\n","Iter: 006/100 | Train Loss: 0.00180748\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00159701\n","Iter: 008/100 | Train Loss: 0.00130930\n","Iter: 009/100 | Train Loss: 0.00122669\n","Iter: 010/100 | Train Loss: 0.00131143\n","Adjusting Layer 1, Kernel Nodes: 134, Adptive Nodes:666\n","Iter: 011/100 | Train Loss: 0.00133195\n","Adjusting Layer 1, Kernel Nodes: 179, Adptive Nodes:621\n","Iter: 012/100 | Train Loss: 0.00124641\n","Iter: 013/100 | Train Loss: 0.00110131\n","Iter: 014/100 | Train Loss: 0.00097243\n","Iter: 015/100 | Train Loss: 0.00091018\n","Iter: 016/100 | Train Loss: 0.00090941\n","Iter: 017/100 | Train Loss: 0.00092439\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 018/100 | Train Loss: 0.00089228\n","Iter: 019/100 | Train Loss: 0.00080992\n","Iter: 020/100 | Train Loss: 0.00073023\n","Iter: 021/100 | Train Loss: 0.00069498\n","Iter: 022/100 | Train Loss: 0.00069314\n","Iter: 023/100 | Train Loss: 0.00068454\n","Iter: 024/100 | Train Loss: 0.00064832\n","Iter: 025/100 | Train Loss: 0.00060005\n","Iter: 026/100 | Train Loss: 0.00056647\n","Iter: 027/100 | Train Loss: 0.00055487\n","Iter: 028/100 | Train Loss: 0.00054985\n","Iter: 029/100 | Train Loss: 0.00053471\n","Iter: 030/100 | Train Loss: 0.00050875\n","Iter: 031/100 | Train Loss: 0.00048403\n","Iter: 032/100 | Train Loss: 0.00046918\n","Iter: 033/100 | Train Loss: 0.00046135\n","Iter: 034/100 | Train Loss: 0.00045218\n","Iter: 035/100 | Train Loss: 0.00043770\n","Iter: 036/100 | Train Loss: 0.00042099\n","Iter: 037/100 | Train Loss: 0.00040698\n","Iter: 038/100 | Train Loss: 0.00039699\n","Iter: 039/100 | Train Loss: 0.00038853\n","Iter: 040/100 | Train Loss: 0.00037888\n","Iter: 041/100 | Train Loss: 0.00036772\n","Iter: 042/100 | Train Loss: 0.00035664\n","Iter: 043/100 | Train Loss: 0.00034700\n","Iter: 044/100 | Train Loss: 0.00033873\n","Iter: 045/100 | Train Loss: 0.00033092\n","Iter: 046/100 | Train Loss: 0.00032287\n","Iter: 047/100 | Train Loss: 0.00031455\n","Iter: 048/100 | Train Loss: 0.00030640\n","Iter: 049/100 | Train Loss: 0.00029886\n","Iter: 050/100 | Train Loss: 0.00029202\n","Iter: 051/100 | Train Loss: 0.00028562\n","Iter: 052/100 | Train Loss: 0.00027930\n","Iter: 053/100 | Train Loss: 0.00027285\n","Iter: 054/100 | Train Loss: 0.00026642\n","Iter: 055/100 | Train Loss: 0.00026038\n","Iter: 056/100 | Train Loss: 0.00025488\n","Iter: 057/100 | Train Loss: 0.00024973\n","Iter: 058/100 | Train Loss: 0.00024460\n","Iter: 059/100 | Train Loss: 0.00023936\n","Iter: 060/100 | Train Loss: 0.00023420\n","Iter: 061/100 | Train Loss: 0.00022940\n","Iter: 062/100 | Train Loss: 0.00022500\n","Iter: 063/100 | Train Loss: 0.00022077\n","Iter: 064/100 | Train Loss: 0.00021647\n","Iter: 065/100 | Train Loss: 0.00021213\n","Iter: 066/100 | Train Loss: 0.00020796\n","Iter: 067/100 | Train Loss: 0.00020410\n","Iter: 068/100 | Train Loss: 0.00020045\n","Iter: 069/100 | Train Loss: 0.00019684\n","Iter: 070/100 | Train Loss: 0.00019319\n","Iter: 071/100 | Train Loss: 0.00018959\n","Iter: 072/100 | Train Loss: 0.00018616\n","Iter: 073/100 | Train Loss: 0.00018294\n","Iter: 074/100 | Train Loss: 0.00017980\n","Iter: 075/100 | Train Loss: 0.00017666\n","Iter: 076/100 | Train Loss: 0.00017353\n","Iter: 077/100 | Train Loss: 0.00017050\n","Iter: 078/100 | Train Loss: 0.00016761\n","Iter: 079/100 | Train Loss: 0.00016482\n","Iter: 080/100 | Train Loss: 0.00016207\n","Iter: 081/100 | Train Loss: 0.00015933\n","Iter: 082/100 | Train Loss: 0.00015664\n","Iter: 083/100 | Train Loss: 0.00015405\n","Iter: 084/100 | Train Loss: 0.00015156\n","Iter: 085/100 | Train Loss: 0.00014910\n","Iter: 086/100 | Train Loss: 0.00014668\n","Iter: 087/100 | Train Loss: 0.00014430\n","Iter: 088/100 | Train Loss: 0.00014199\n","Iter: 089/100 | Train Loss: 0.00013974\n","Iter: 090/100 | Train Loss: 0.00013755\n","Iter: 091/100 | Train Loss: 0.00013539\n","Iter: 092/100 | Train Loss: 0.00013326\n","Iter: 093/100 | Train Loss: 0.00013119\n","Iter: 094/100 | Train Loss: 0.00012917\n","Iter: 095/100 | Train Loss: 0.00012720\n","Iter: 096/100 | Train Loss: 0.00012526\n","Iter: 097/100 | Train Loss: 0.00012335\n","Iter: 098/100 | Train Loss: 0.00012149\n","Iter: 099/100 | Train Loss: 0.00011967\n","\n","Iter: 099/100 | Test Loss: 0.00100854 | Test acc: 65.3300\n","scale:1.250000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00234440\n","Iter: 001/100 | Train Loss: 0.00194859\n","Iter: 002/100 | Train Loss: 0.00161931\n","Iter: 003/100 | Train Loss: 0.00147212\n","Iter: 004/100 | Train Loss: 0.00151248\n","Adjusting Layer 1, Kernel Nodes: 726, Adptive Nodes:74\n","Iter: 005/100 | Train Loss: 0.00167206\n","Adjusting Layer 1, Kernel Nodes: 593, Adptive Nodes:207\n","Iter: 006/100 | Train Loss: 0.00179479\n","Adjusting Layer 1, Kernel Nodes: 599, Adptive Nodes:201\n","Iter: 007/100 | Train Loss: 0.00172933\n","Iter: 008/100 | Train Loss: 0.00151411\n","Iter: 009/100 | Train Loss: 0.00130500\n","Iter: 010/100 | Train Loss: 0.00121723\n","Iter: 011/100 | Train Loss: 0.00124405\n","Adjusting Layer 1, Kernel Nodes: 457, Adptive Nodes:343\n","Iter: 012/100 | Train Loss: 0.00128588\n","Adjusting Layer 1, Kernel Nodes: 458, Adptive Nodes:342\n","Iter: 013/100 | Train Loss: 0.00125821\n","Iter: 014/100 | Train Loss: 0.00114644\n","Iter: 015/100 | Train Loss: 0.00101136\n","Iter: 016/100 | Train Loss: 0.00092323\n","Iter: 017/100 | Train Loss: 0.00090160\n","Iter: 018/100 | Train Loss: 0.00090972\n","Adjusting Layer 1, Kernel Nodes: 793, Adptive Nodes:7\n","Iter: 019/100 | Train Loss: 0.00088382\n","Iter: 020/100 | Train Loss: 0.00080459\n","Iter: 021/100 | Train Loss: 0.00072198\n","Iter: 022/100 | Train Loss: 0.00068376\n","Iter: 023/100 | Train Loss: 0.00068008\n","Iter: 024/100 | Train Loss: 0.00066702\n","Iter: 025/100 | Train Loss: 0.00062466\n","Iter: 026/100 | Train Loss: 0.00057484\n","Iter: 027/100 | Train Loss: 0.00054662\n","Iter: 028/100 | Train Loss: 0.00054042\n","Iter: 029/100 | Train Loss: 0.00053323\n","Iter: 030/100 | Train Loss: 0.00051023\n","Iter: 031/100 | Train Loss: 0.00047998\n","Iter: 032/100 | Train Loss: 0.00045902\n","Iter: 033/100 | Train Loss: 0.00045075\n","Iter: 034/100 | Train Loss: 0.00044419\n","Iter: 035/100 | Train Loss: 0.00042980\n","Iter: 036/100 | Train Loss: 0.00041006\n","Iter: 037/100 | Train Loss: 0.00039373\n","Iter: 038/100 | Train Loss: 0.00038433\n","Iter: 039/100 | Train Loss: 0.00037722\n","Iter: 040/100 | Train Loss: 0.00036680\n","Iter: 041/100 | Train Loss: 0.00035308\n","Iter: 042/100 | Train Loss: 0.00034030\n","Iter: 043/100 | Train Loss: 0.00033116\n","Iter: 044/100 | Train Loss: 0.00032407\n","Iter: 045/100 | Train Loss: 0.00031592\n","Iter: 046/100 | Train Loss: 0.00030598\n","Iter: 047/100 | Train Loss: 0.00029609\n","Iter: 048/100 | Train Loss: 0.00028802\n","Iter: 049/100 | Train Loss: 0.00028144\n","Iter: 050/100 | Train Loss: 0.00027478\n","Iter: 051/100 | Train Loss: 0.00026728\n","Iter: 052/100 | Train Loss: 0.00025961\n","Iter: 053/100 | Train Loss: 0.00025278\n","Iter: 054/100 | Train Loss: 0.00024694\n","Iter: 055/100 | Train Loss: 0.00024137\n","Iter: 056/100 | Train Loss: 0.00023550\n","Iter: 057/100 | Train Loss: 0.00022950\n","Iter: 058/100 | Train Loss: 0.00022387\n","Iter: 059/100 | Train Loss: 0.00021883\n","Iter: 060/100 | Train Loss: 0.00021412\n","Iter: 061/100 | Train Loss: 0.00020940\n","Iter: 062/100 | Train Loss: 0.00020460\n","Iter: 063/100 | Train Loss: 0.00019996\n","Iter: 064/100 | Train Loss: 0.00019566\n","Iter: 065/100 | Train Loss: 0.00019164\n","Iter: 066/100 | Train Loss: 0.00018771\n","Iter: 067/100 | Train Loss: 0.00018377\n","Iter: 068/100 | Train Loss: 0.00017990\n","Iter: 069/100 | Train Loss: 0.00017621\n","Iter: 070/100 | Train Loss: 0.00017272\n","Iter: 071/100 | Train Loss: 0.00016935\n","Iter: 072/100 | Train Loss: 0.00016601\n","Iter: 073/100 | Train Loss: 0.00016270\n","Iter: 074/100 | Train Loss: 0.00015951\n","Iter: 075/100 | Train Loss: 0.00015646\n","Iter: 076/100 | Train Loss: 0.00015351\n","Iter: 077/100 | Train Loss: 0.00015061\n","Iter: 078/100 | Train Loss: 0.00014775\n","Iter: 079/100 | Train Loss: 0.00014496\n","Iter: 080/100 | Train Loss: 0.00014227\n","Iter: 081/100 | Train Loss: 0.00013968\n","Iter: 082/100 | Train Loss: 0.00013714\n","Iter: 083/100 | Train Loss: 0.00013464\n","Iter: 084/100 | Train Loss: 0.00013219\n","Iter: 085/100 | Train Loss: 0.00012982\n","Iter: 086/100 | Train Loss: 0.00012752\n","Iter: 087/100 | Train Loss: 0.00012528\n","Iter: 088/100 | Train Loss: 0.00012308\n","Iter: 089/100 | Train Loss: 0.00012093\n","Iter: 090/100 | Train Loss: 0.00011883\n","Iter: 091/100 | Train Loss: 0.00011679\n","Iter: 092/100 | Train Loss: 0.00011480\n","Iter: 093/100 | Train Loss: 0.00011285\n","Iter: 094/100 | Train Loss: 0.00011094\n","Iter: 095/100 | Train Loss: 0.00010907\n","Iter: 096/100 | Train Loss: 0.00010725\n","Iter: 097/100 | Train Loss: 0.00010548\n","Iter: 098/100 | Train Loss: 0.00010374\n","Iter: 099/100 | Train Loss: 0.00010203\n","\n","Iter: 099/100 | Test Loss: 0.00101985 | Test acc: 65.1300\n","scale:1.250000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00235677\n","Iter: 001/100 | Train Loss: 0.00197287\n","Iter: 002/100 | Train Loss: 0.00164245\n","Iter: 003/100 | Train Loss: 0.00147827\n","Iter: 004/100 | Train Loss: 0.00149456\n","Adjusting Layer 1, Kernel Nodes: 780, Adptive Nodes:20\n","Iter: 005/100 | Train Loss: 0.00163365\n","Adjusting Layer 1, Kernel Nodes: 639, Adptive Nodes:161\n","Iter: 006/100 | Train Loss: 0.00177082\n","Adjusting Layer 1, Kernel Nodes: 597, Adptive Nodes:203\n","Iter: 007/100 | Train Loss: 0.00177462\n","Adjusting Layer 1, Kernel Nodes: 633, Adptive Nodes:167\n","Iter: 008/100 | Train Loss: 0.00160009\n","Iter: 009/100 | Train Loss: 0.00137607\n","Iter: 010/100 | Train Loss: 0.00123876\n","Iter: 011/100 | Train Loss: 0.00122898\n","Iter: 012/100 | Train Loss: 0.00128216\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 013/100 | Train Loss: 0.00129744\n","Adjusting Layer 1, Kernel Nodes: 621, Adptive Nodes:179\n","Iter: 014/100 | Train Loss: 0.00122385\n","Iter: 015/100 | Train Loss: 0.00109097\n","Iter: 016/100 | Train Loss: 0.00097414\n","Iter: 017/100 | Train Loss: 0.00092252\n","Iter: 018/100 | Train Loss: 0.00092392\n","Adjusting Layer 1, Kernel Nodes: 790, Adptive Nodes:10\n","Iter: 019/100 | Train Loss: 0.00092357\n","Iter: 020/100 | Train Loss: 0.00087410\n","Iter: 021/100 | Train Loss: 0.00079043\n","Iter: 022/100 | Train Loss: 0.00072362\n","Iter: 023/100 | Train Loss: 0.00069948\n","Iter: 024/100 | Train Loss: 0.00069715\n","Iter: 025/100 | Train Loss: 0.00068086\n","Iter: 026/100 | Train Loss: 0.00063999\n","Iter: 027/100 | Train Loss: 0.00059426\n","Iter: 028/100 | Train Loss: 0.00056645\n","Iter: 029/100 | Train Loss: 0.00055783\n","Iter: 030/100 | Train Loss: 0.00055142\n","Iter: 031/100 | Train Loss: 0.00053346\n","Iter: 032/100 | Train Loss: 0.00050657\n","Iter: 033/100 | Train Loss: 0.00048315\n","Iter: 034/100 | Train Loss: 0.00047022\n","Iter: 035/100 | Train Loss: 0.00046342\n","Iter: 036/100 | Train Loss: 0.00045402\n","Iter: 037/100 | Train Loss: 0.00043870\n","Iter: 038/100 | Train Loss: 0.00042155\n","Iter: 039/100 | Train Loss: 0.00040812\n","Iter: 040/100 | Train Loss: 0.00039942\n","Iter: 041/100 | Train Loss: 0.00039199\n","Iter: 042/100 | Train Loss: 0.00038241\n","Iter: 043/100 | Train Loss: 0.00037063\n","Iter: 044/100 | Train Loss: 0.00035918\n","Iter: 045/100 | Train Loss: 0.00034999\n","Iter: 046/100 | Train Loss: 0.00034268\n","Iter: 047/100 | Train Loss: 0.00033546\n","Iter: 048/100 | Train Loss: 0.00032718\n","Iter: 049/100 | Train Loss: 0.00031829\n","Iter: 050/100 | Train Loss: 0.00031002\n","Iter: 051/100 | Train Loss: 0.00030300\n","Iter: 052/100 | Train Loss: 0.00029676\n","Iter: 053/100 | Train Loss: 0.00029045\n","Iter: 054/100 | Train Loss: 0.00028370\n","Iter: 055/100 | Train Loss: 0.00027689\n","Iter: 056/100 | Train Loss: 0.00027059\n","Iter: 057/100 | Train Loss: 0.00026498\n","Iter: 058/100 | Train Loss: 0.00025973\n","Iter: 059/100 | Train Loss: 0.00025441\n","Iter: 060/100 | Train Loss: 0.00024894\n","Iter: 061/100 | Train Loss: 0.00024357\n","Iter: 062/100 | Train Loss: 0.00023859\n","Iter: 063/100 | Train Loss: 0.00023401\n","Iter: 064/100 | Train Loss: 0.00022960\n","Iter: 065/100 | Train Loss: 0.00022517\n","Iter: 066/100 | Train Loss: 0.00022071\n","Iter: 067/100 | Train Loss: 0.00021640\n","Iter: 068/100 | Train Loss: 0.00021237\n","Iter: 069/100 | Train Loss: 0.00020858\n","Iter: 070/100 | Train Loss: 0.00020485\n","Iter: 071/100 | Train Loss: 0.00020110\n","Iter: 072/100 | Train Loss: 0.00019741\n","Iter: 073/100 | Train Loss: 0.00019386\n","Iter: 074/100 | Train Loss: 0.00019049\n","Iter: 075/100 | Train Loss: 0.00018725\n","Iter: 076/100 | Train Loss: 0.00018403\n","Iter: 077/100 | Train Loss: 0.00018083\n","Iter: 078/100 | Train Loss: 0.00017770\n","Iter: 079/100 | Train Loss: 0.00017469\n","Iter: 080/100 | Train Loss: 0.00017180\n","Iter: 081/100 | Train Loss: 0.00016897\n","Iter: 082/100 | Train Loss: 0.00016617\n","Iter: 083/100 | Train Loss: 0.00016340\n","Iter: 084/100 | Train Loss: 0.00016072\n","Iter: 085/100 | Train Loss: 0.00015812\n","Iter: 086/100 | Train Loss: 0.00015560\n","Iter: 087/100 | Train Loss: 0.00015311\n","Iter: 088/100 | Train Loss: 0.00015066\n","Iter: 089/100 | Train Loss: 0.00014826\n","Iter: 090/100 | Train Loss: 0.00014593\n","Iter: 091/100 | Train Loss: 0.00014366\n","Iter: 092/100 | Train Loss: 0.00014144\n","Iter: 093/100 | Train Loss: 0.00013925\n","Iter: 094/100 | Train Loss: 0.00013710\n","Iter: 095/100 | Train Loss: 0.00013501\n","Iter: 096/100 | Train Loss: 0.00013296\n","Iter: 097/100 | Train Loss: 0.00013096\n","Iter: 098/100 | Train Loss: 0.00012900\n","Iter: 099/100 | Train Loss: 0.00012707\n","\n","Iter: 099/100 | Test Loss: 0.00100527 | Test acc: 65.1800\n","scale:1.250000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00236522\n","Iter: 001/100 | Train Loss: 0.00198977\n","Iter: 002/100 | Train Loss: 0.00165929\n","Iter: 003/100 | Train Loss: 0.00148416\n","Iter: 004/100 | Train Loss: 0.00148370\n","Iter: 005/100 | Train Loss: 0.00159792\n","Adjusting Layer 1, Kernel Nodes: 742, Adptive Nodes:58\n","Iter: 006/100 | Train Loss: 0.00173250\n","Adjusting Layer 1, Kernel Nodes: 677, Adptive Nodes:123\n","Iter: 007/100 | Train Loss: 0.00178912\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 008/100 | Train Loss: 0.00171420\n","Iter: 009/100 | Train Loss: 0.00154610\n","Iter: 010/100 | Train Loss: 0.00136753\n","Iter: 011/100 | Train Loss: 0.00124943\n","Iter: 012/100 | Train Loss: 0.00121536\n","Iter: 013/100 | Train Loss: 0.00123992\n","Adjusting Layer 1, Kernel Nodes: 558, Adptive Nodes:242\n","Iter: 014/100 | Train Loss: 0.00127288\n","Adjusting Layer 1, Kernel Nodes: 626, Adptive Nodes:174\n","Iter: 015/100 | Train Loss: 0.00126606\n","Iter: 016/100 | Train Loss: 0.00120297\n","Iter: 017/100 | Train Loss: 0.00110676\n","Iter: 018/100 | Train Loss: 0.00101670\n","Iter: 019/100 | Train Loss: 0.00096113\n","Iter: 020/100 | Train Loss: 0.00094305\n","Iter: 021/100 | Train Loss: 0.00094335\n","Adjusting Layer 1, Kernel Nodes: 564, Adptive Nodes:236\n","Iter: 022/100 | Train Loss: 0.00093602\n","Iter: 023/100 | Train Loss: 0.00090620\n","Iter: 024/100 | Train Loss: 0.00085714\n","Iter: 025/100 | Train Loss: 0.00080474\n","Iter: 026/100 | Train Loss: 0.00076464\n","Iter: 027/100 | Train Loss: 0.00074233\n","Iter: 028/100 | Train Loss: 0.00073203\n","Iter: 029/100 | Train Loss: 0.00072260\n","Iter: 030/100 | Train Loss: 0.00070574\n","Iter: 031/100 | Train Loss: 0.00068052\n","Iter: 032/100 | Train Loss: 0.00065225\n","Iter: 033/100 | Train Loss: 0.00062765\n","Iter: 034/100 | Train Loss: 0.00061035\n","Iter: 035/100 | Train Loss: 0.00059924\n","Iter: 036/100 | Train Loss: 0.00059020\n","Iter: 037/100 | Train Loss: 0.00057927\n","Iter: 038/100 | Train Loss: 0.00056508\n","Iter: 039/100 | Train Loss: 0.00054903\n","Iter: 040/100 | Train Loss: 0.00053371\n","Iter: 041/100 | Train Loss: 0.00052108\n","Iter: 042/100 | Train Loss: 0.00051129\n","Iter: 043/100 | Train Loss: 0.00050309\n","Iter: 044/100 | Train Loss: 0.00049481\n","Iter: 045/100 | Train Loss: 0.00048551\n","Iter: 046/100 | Train Loss: 0.00047533\n","Iter: 047/100 | Train Loss: 0.00046516\n","Iter: 048/100 | Train Loss: 0.00045587\n","Iter: 049/100 | Train Loss: 0.00044780\n","Iter: 050/100 | Train Loss: 0.00044065\n","Iter: 051/100 | Train Loss: 0.00043384\n","Iter: 052/100 | Train Loss: 0.00042687\n","Iter: 053/100 | Train Loss: 0.00041963\n","Iter: 054/100 | Train Loss: 0.00041231\n","Iter: 055/100 | Train Loss: 0.00040524\n","Iter: 056/100 | Train Loss: 0.00039863\n","Iter: 057/100 | Train Loss: 0.00039250\n","Iter: 058/100 | Train Loss: 0.00038670\n","Iter: 059/100 | Train Loss: 0.00038103\n","Iter: 060/100 | Train Loss: 0.00037534\n","Iter: 061/100 | Train Loss: 0.00036963\n","Iter: 062/100 | Train Loss: 0.00036399\n","Iter: 063/100 | Train Loss: 0.00035854\n","Iter: 064/100 | Train Loss: 0.00035333\n","Iter: 065/100 | Train Loss: 0.00034836\n","Iter: 066/100 | Train Loss: 0.00034354\n","Iter: 067/100 | Train Loss: 0.00033881\n","Iter: 068/100 | Train Loss: 0.00033412\n","Iter: 069/100 | Train Loss: 0.00032947\n","Iter: 070/100 | Train Loss: 0.00032491\n","Iter: 071/100 | Train Loss: 0.00032048\n","Iter: 072/100 | Train Loss: 0.00031620\n","Iter: 073/100 | Train Loss: 0.00031205\n","Iter: 074/100 | Train Loss: 0.00030800\n","Iter: 075/100 | Train Loss: 0.00030401\n","Iter: 076/100 | Train Loss: 0.00030007\n","Iter: 077/100 | Train Loss: 0.00029618\n","Iter: 078/100 | Train Loss: 0.00029237\n","Iter: 079/100 | Train Loss: 0.00028866\n","Iter: 080/100 | Train Loss: 0.00028504\n","Iter: 081/100 | Train Loss: 0.00028151\n","Iter: 082/100 | Train Loss: 0.00027805\n","Iter: 083/100 | Train Loss: 0.00027463\n","Iter: 084/100 | Train Loss: 0.00027127\n","Iter: 085/100 | Train Loss: 0.00026796\n","Iter: 086/100 | Train Loss: 0.00026473\n","Iter: 087/100 | Train Loss: 0.00026157\n","Iter: 088/100 | Train Loss: 0.00025848\n","Iter: 089/100 | Train Loss: 0.00025543\n","Iter: 090/100 | Train Loss: 0.00025243\n","Iter: 091/100 | Train Loss: 0.00024948\n","Iter: 092/100 | Train Loss: 0.00024657\n","Iter: 093/100 | Train Loss: 0.00024372\n","Iter: 094/100 | Train Loss: 0.00024092\n","Iter: 095/100 | Train Loss: 0.00023818\n","Iter: 096/100 | Train Loss: 0.00023549\n","Iter: 097/100 | Train Loss: 0.00023283\n","Iter: 098/100 | Train Loss: 0.00023022\n","Iter: 099/100 | Train Loss: 0.00022764\n","\n","Iter: 099/100 | Test Loss: 0.00098531 | Test acc: 65.2200\n","scale:1.250000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00237041\n","Iter: 001/100 | Train Loss: 0.00200028\n","Iter: 002/100 | Train Loss: 0.00167006\n","Iter: 003/100 | Train Loss: 0.00148836\n","Iter: 004/100 | Train Loss: 0.00147734\n","Iter: 005/100 | Train Loss: 0.00158292\n","Adjusting Layer 1, Kernel Nodes: 755, Adptive Nodes:45\n","Iter: 006/100 | Train Loss: 0.00171385\n","Adjusting Layer 1, Kernel Nodes: 726, Adptive Nodes:74\n","Iter: 007/100 | Train Loss: 0.00178245\n","Adjusting Layer 1, Kernel Nodes: 700, Adptive Nodes:100\n","Iter: 008/100 | Train Loss: 0.00174276\n","Iter: 009/100 | Train Loss: 0.00161073\n","Iter: 010/100 | Train Loss: 0.00144282\n","Iter: 011/100 | Train Loss: 0.00130014\n","Iter: 012/100 | Train Loss: 0.00121975\n","Iter: 013/100 | Train Loss: 0.00120360\n","Iter: 014/100 | Train Loss: 0.00122609\n","Adjusting Layer 1, Kernel Nodes: 674, Adptive Nodes:126\n","Iter: 015/100 | Train Loss: 0.00125150\n","Adjusting Layer 1, Kernel Nodes: 631, Adptive Nodes:169\n","Iter: 016/100 | Train Loss: 0.00124863\n","Iter: 017/100 | Train Loss: 0.00120636\n","Iter: 018/100 | Train Loss: 0.00113492\n","Iter: 019/100 | Train Loss: 0.00105644\n","Iter: 020/100 | Train Loss: 0.00099233\n","Iter: 021/100 | Train Loss: 0.00095395\n","Iter: 022/100 | Train Loss: 0.00093931\n","Iter: 023/100 | Train Loss: 0.00093678\n","Iter: 024/100 | Train Loss: 0.00093236\n","Iter: 025/100 | Train Loss: 0.00091638\n","Iter: 026/100 | Train Loss: 0.00088700\n","Iter: 027/100 | Train Loss: 0.00084937\n","Iter: 028/100 | Train Loss: 0.00081181\n","Iter: 029/100 | Train Loss: 0.00078135\n","Iter: 030/100 | Train Loss: 0.00076090\n","Iter: 031/100 | Train Loss: 0.00074877\n","Iter: 032/100 | Train Loss: 0.00074033\n","Iter: 033/100 | Train Loss: 0.00073071\n","Iter: 034/100 | Train Loss: 0.00071696\n","Iter: 035/100 | Train Loss: 0.00069891\n","Iter: 036/100 | Train Loss: 0.00067875\n","Iter: 037/100 | Train Loss: 0.00065942\n","Iter: 038/100 | Train Loss: 0.00064316\n","Iter: 039/100 | Train Loss: 0.00063070\n","Iter: 040/100 | Train Loss: 0.00062120\n","Iter: 041/100 | Train Loss: 0.00061292\n","Iter: 042/100 | Train Loss: 0.00060415\n","Iter: 043/100 | Train Loss: 0.00059399\n","Iter: 044/100 | Train Loss: 0.00058253\n","Iter: 045/100 | Train Loss: 0.00057062\n","Iter: 046/100 | Train Loss: 0.00055929\n","Iter: 047/100 | Train Loss: 0.00054929\n","Iter: 048/100 | Train Loss: 0.00054074\n","Iter: 049/100 | Train Loss: 0.00053325\n","Iter: 050/100 | Train Loss: 0.00052619\n","Iter: 051/100 | Train Loss: 0.00051899\n","Iter: 052/100 | Train Loss: 0.00051141\n","Iter: 053/100 | Train Loss: 0.00050353\n","Iter: 054/100 | Train Loss: 0.00049563\n","Iter: 055/100 | Train Loss: 0.00048808\n","Iter: 056/100 | Train Loss: 0.00048107\n","Iter: 057/100 | Train Loss: 0.00047462\n","Iter: 058/100 | Train Loss: 0.00046858\n","Iter: 059/100 | Train Loss: 0.00046274\n","Iter: 060/100 | Train Loss: 0.00045691\n","Iter: 061/100 | Train Loss: 0.00045102\n","Iter: 062/100 | Train Loss: 0.00044512\n","Iter: 063/100 | Train Loss: 0.00043930\n","Iter: 064/100 | Train Loss: 0.00043367\n","Iter: 065/100 | Train Loss: 0.00042829\n","Iter: 066/100 | Train Loss: 0.00042315\n","Iter: 067/100 | Train Loss: 0.00041821\n","Iter: 068/100 | Train Loss: 0.00041338\n","Iter: 069/100 | Train Loss: 0.00040861\n","Iter: 070/100 | Train Loss: 0.00040387\n","Iter: 071/100 | Train Loss: 0.00039919\n","Iter: 072/100 | Train Loss: 0.00039459\n","Iter: 073/100 | Train Loss: 0.00039010\n","Iter: 074/100 | Train Loss: 0.00038574\n","Iter: 075/100 | Train Loss: 0.00038150\n","Iter: 076/100 | Train Loss: 0.00037737\n","Iter: 077/100 | Train Loss: 0.00037333\n","Iter: 078/100 | Train Loss: 0.00036934\n","Iter: 079/100 | Train Loss: 0.00036541\n","Iter: 080/100 | Train Loss: 0.00036155\n","Iter: 081/100 | Train Loss: 0.00035774\n","Iter: 082/100 | Train Loss: 0.00035400\n","Iter: 083/100 | Train Loss: 0.00035033\n","Iter: 084/100 | Train Loss: 0.00034674\n","Iter: 085/100 | Train Loss: 0.00034321\n","Iter: 086/100 | Train Loss: 0.00033974\n","Iter: 087/100 | Train Loss: 0.00033633\n","Iter: 088/100 | Train Loss: 0.00033296\n","Iter: 089/100 | Train Loss: 0.00032965\n","Iter: 090/100 | Train Loss: 0.00032638\n","Iter: 091/100 | Train Loss: 0.00032317\n","Iter: 092/100 | Train Loss: 0.00032001\n","Iter: 093/100 | Train Loss: 0.00031690\n","Iter: 094/100 | Train Loss: 0.00031385\n","Iter: 095/100 | Train Loss: 0.00031085\n","Iter: 096/100 | Train Loss: 0.00030790\n","Iter: 097/100 | Train Loss: 0.00030498\n","Iter: 098/100 | Train Loss: 0.00030211\n","Iter: 099/100 | Train Loss: 0.00029928\n","\n","Iter: 099/100 | Test Loss: 0.00098720 | Test acc: 64.6500\n","scale:1.250000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00237281\n","Iter: 001/100 | Train Loss: 0.00200517\n","Iter: 002/100 | Train Loss: 0.00167512\n","Iter: 003/100 | Train Loss: 0.00149027\n","Iter: 004/100 | Train Loss: 0.00147401\n","Iter: 005/100 | Train Loss: 0.00157495\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00170266\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00177413\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 008/100 | Train Loss: 0.00174776\n","Iter: 009/100 | Train Loss: 0.00163275\n","Iter: 010/100 | Train Loss: 0.00147414\n","Iter: 011/100 | Train Loss: 0.00132576\n","Iter: 012/100 | Train Loss: 0.00122586\n","Iter: 013/100 | Train Loss: 0.00118550\n","Iter: 014/100 | Train Loss: 0.00119055\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 015/100 | Train Loss: 0.00121295\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 016/100 | Train Loss: 0.00122473\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 017/100 | Train Loss: 0.00120870\n","Iter: 018/100 | Train Loss: 0.00116260\n","Iter: 019/100 | Train Loss: 0.00109682\n","Iter: 020/100 | Train Loss: 0.00102778\n","Iter: 021/100 | Train Loss: 0.00097037\n","Iter: 022/100 | Train Loss: 0.00093247\n","Iter: 023/100 | Train Loss: 0.00091368\n","Iter: 024/100 | Train Loss: 0.00090696\n","Iter: 025/100 | Train Loss: 0.00090270\n","Iter: 026/100 | Train Loss: 0.00089270\n","Iter: 027/100 | Train Loss: 0.00087313\n","Iter: 028/100 | Train Loss: 0.00084502\n","Iter: 029/100 | Train Loss: 0.00081274\n","Iter: 030/100 | Train Loss: 0.00078169\n","Iter: 031/100 | Train Loss: 0.00075609\n","Iter: 032/100 | Train Loss: 0.00073761\n","Iter: 033/100 | Train Loss: 0.00072524\n","Iter: 034/100 | Train Loss: 0.00071620\n","Iter: 035/100 | Train Loss: 0.00070736\n","Iter: 036/100 | Train Loss: 0.00069639\n","Iter: 037/100 | Train Loss: 0.00068243\n","Iter: 038/100 | Train Loss: 0.00066612\n","Iter: 039/100 | Train Loss: 0.00064911\n","Iter: 040/100 | Train Loss: 0.00063317\n","Iter: 041/100 | Train Loss: 0.00061953\n","Iter: 042/100 | Train Loss: 0.00060851\n","Iter: 043/100 | Train Loss: 0.00059954\n","Iter: 044/100 | Train Loss: 0.00059155\n","Iter: 045/100 | Train Loss: 0.00058349\n","Iter: 046/100 | Train Loss: 0.00057469\n","Iter: 047/100 | Train Loss: 0.00056505\n","Iter: 048/100 | Train Loss: 0.00055489\n","Iter: 049/100 | Train Loss: 0.00054480\n","Iter: 050/100 | Train Loss: 0.00053536\n","Iter: 051/100 | Train Loss: 0.00052687\n","Iter: 052/100 | Train Loss: 0.00051933\n","Iter: 053/100 | Train Loss: 0.00051250\n","Iter: 054/100 | Train Loss: 0.00050600\n","Iter: 055/100 | Train Loss: 0.00049948\n","Iter: 056/100 | Train Loss: 0.00049274\n","Iter: 057/100 | Train Loss: 0.00048579\n","Iter: 058/100 | Train Loss: 0.00047880\n","Iter: 059/100 | Train Loss: 0.00047196\n","Iter: 060/100 | Train Loss: 0.00046545\n","Iter: 061/100 | Train Loss: 0.00045937\n","Iter: 062/100 | Train Loss: 0.00045367\n","Iter: 063/100 | Train Loss: 0.00044823\n","Iter: 064/100 | Train Loss: 0.00044291\n","Iter: 065/100 | Train Loss: 0.00043762\n","Iter: 066/100 | Train Loss: 0.00043230\n","Iter: 067/100 | Train Loss: 0.00042700\n","Iter: 068/100 | Train Loss: 0.00042176\n","Iter: 069/100 | Train Loss: 0.00041668\n","Iter: 070/100 | Train Loss: 0.00041179\n","Iter: 071/100 | Train Loss: 0.00040710\n","Iter: 072/100 | Train Loss: 0.00040258\n","Iter: 073/100 | Train Loss: 0.00039819\n","Iter: 074/100 | Train Loss: 0.00039387\n","Iter: 075/100 | Train Loss: 0.00038958\n","Iter: 076/100 | Train Loss: 0.00038533\n","Iter: 077/100 | Train Loss: 0.00038114\n","Iter: 078/100 | Train Loss: 0.00037702\n","Iter: 079/100 | Train Loss: 0.00037300\n","Iter: 080/100 | Train Loss: 0.00036909\n","Iter: 081/100 | Train Loss: 0.00036528\n","Iter: 082/100 | Train Loss: 0.00036156\n","Iter: 083/100 | Train Loss: 0.00035791\n","Iter: 084/100 | Train Loss: 0.00035431\n","Iter: 085/100 | Train Loss: 0.00035076\n","Iter: 086/100 | Train Loss: 0.00034725\n","Iter: 087/100 | Train Loss: 0.00034379\n","Iter: 088/100 | Train Loss: 0.00034039\n","Iter: 089/100 | Train Loss: 0.00033706\n","Iter: 090/100 | Train Loss: 0.00033380\n","Iter: 091/100 | Train Loss: 0.00033060\n","Iter: 092/100 | Train Loss: 0.00032745\n","Iter: 093/100 | Train Loss: 0.00032434\n","Iter: 094/100 | Train Loss: 0.00032128\n","Iter: 095/100 | Train Loss: 0.00031826\n","Iter: 096/100 | Train Loss: 0.00031528\n","Iter: 097/100 | Train Loss: 0.00031235\n","Iter: 098/100 | Train Loss: 0.00030947\n","Iter: 099/100 | Train Loss: 0.00030663\n","\n","Iter: 099/100 | Test Loss: 0.00098671 | Test acc: 64.2000\n","scale:1.250000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00237513\n","Iter: 001/100 | Train Loss: 0.00200979\n","Iter: 002/100 | Train Loss: 0.00167977\n","Iter: 003/100 | Train Loss: 0.00149144\n","Iter: 004/100 | Train Loss: 0.00146956\n","Iter: 005/100 | Train Loss: 0.00156533\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00168952\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00175960\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 008/100 | Train Loss: 0.00173163\n","Iter: 009/100 | Train Loss: 0.00161543\n","Iter: 010/100 | Train Loss: 0.00145599\n","Iter: 011/100 | Train Loss: 0.00130687\n","Iter: 012/100 | Train Loss: 0.00120552\n","Iter: 013/100 | Train Loss: 0.00116211\n","Iter: 014/100 | Train Loss: 0.00116277\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 015/100 | Train Loss: 0.00117961\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 016/100 | Train Loss: 0.00118500\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 017/100 | Train Loss: 0.00116069\n","Iter: 018/100 | Train Loss: 0.00110648\n","Iter: 019/100 | Train Loss: 0.00103504\n","Iter: 020/100 | Train Loss: 0.00096394\n","Iter: 021/100 | Train Loss: 0.00090774\n","Iter: 022/100 | Train Loss: 0.00087244\n","Iter: 023/100 | Train Loss: 0.00085476\n","Iter: 024/100 | Train Loss: 0.00084580\n","Iter: 025/100 | Train Loss: 0.00083583\n","Iter: 026/100 | Train Loss: 0.00081822\n","Iter: 027/100 | Train Loss: 0.00079154\n","Iter: 028/100 | Train Loss: 0.00075894\n","Iter: 029/100 | Train Loss: 0.00072574\n","Iter: 030/100 | Train Loss: 0.00069680\n","Iter: 031/100 | Train Loss: 0.00067467\n","Iter: 032/100 | Train Loss: 0.00065902\n","Iter: 033/100 | Train Loss: 0.00064754\n","Iter: 034/100 | Train Loss: 0.00063710\n","Iter: 035/100 | Train Loss: 0.00062521\n","Iter: 036/100 | Train Loss: 0.00061080\n","Iter: 037/100 | Train Loss: 0.00059425\n","Iter: 038/100 | Train Loss: 0.00057693\n","Iter: 039/100 | Train Loss: 0.00056052\n","Iter: 040/100 | Train Loss: 0.00054632\n","Iter: 041/100 | Train Loss: 0.00053469\n","Iter: 042/100 | Train Loss: 0.00052514\n","Iter: 043/100 | Train Loss: 0.00051665\n","Iter: 044/100 | Train Loss: 0.00050820\n","Iter: 045/100 | Train Loss: 0.00049906\n","Iter: 046/100 | Train Loss: 0.00048919\n","Iter: 047/100 | Train Loss: 0.00047893\n","Iter: 048/100 | Train Loss: 0.00046892\n","Iter: 049/100 | Train Loss: 0.00045972\n","Iter: 050/100 | Train Loss: 0.00045155\n","Iter: 051/100 | Train Loss: 0.00044432\n","Iter: 052/100 | Train Loss: 0.00043772\n","Iter: 053/100 | Train Loss: 0.00043132\n","Iter: 054/100 | Train Loss: 0.00042480\n","Iter: 055/100 | Train Loss: 0.00041803\n","Iter: 056/100 | Train Loss: 0.00041112\n","Iter: 057/100 | Train Loss: 0.00040429\n","Iter: 058/100 | Train Loss: 0.00039778\n","Iter: 059/100 | Train Loss: 0.00039172\n","Iter: 060/100 | Train Loss: 0.00038612\n","Iter: 061/100 | Train Loss: 0.00038085\n","Iter: 062/100 | Train Loss: 0.00037575\n","Iter: 063/100 | Train Loss: 0.00037064\n","Iter: 064/100 | Train Loss: 0.00036545\n","Iter: 065/100 | Train Loss: 0.00036023\n","Iter: 066/100 | Train Loss: 0.00035507\n","Iter: 067/100 | Train Loss: 0.00035007\n","Iter: 068/100 | Train Loss: 0.00034532\n","Iter: 069/100 | Train Loss: 0.00034080\n","Iter: 070/100 | Train Loss: 0.00033645\n","Iter: 071/100 | Train Loss: 0.00033220\n","Iter: 072/100 | Train Loss: 0.00032799\n","Iter: 073/100 | Train Loss: 0.00032380\n","Iter: 074/100 | Train Loss: 0.00031965\n","Iter: 075/100 | Train Loss: 0.00031560\n","Iter: 076/100 | Train Loss: 0.00031165\n","Iter: 077/100 | Train Loss: 0.00030781\n","Iter: 078/100 | Train Loss: 0.00030409\n","Iter: 079/100 | Train Loss: 0.00030048\n","Iter: 080/100 | Train Loss: 0.00029694\n","Iter: 081/100 | Train Loss: 0.00029346\n","Iter: 082/100 | Train Loss: 0.00029003\n","Iter: 083/100 | Train Loss: 0.00028665\n","Iter: 084/100 | Train Loss: 0.00028333\n","Iter: 085/100 | Train Loss: 0.00028007\n","Iter: 086/100 | Train Loss: 0.00027687\n","Iter: 087/100 | Train Loss: 0.00027375\n","Iter: 088/100 | Train Loss: 0.00027069\n","Iter: 089/100 | Train Loss: 0.00026768\n","Iter: 090/100 | Train Loss: 0.00026471\n","Iter: 091/100 | Train Loss: 0.00026177\n","Iter: 092/100 | Train Loss: 0.00025888\n","Iter: 093/100 | Train Loss: 0.00025604\n","Iter: 094/100 | Train Loss: 0.00025325\n","Iter: 095/100 | Train Loss: 0.00025051\n","Iter: 096/100 | Train Loss: 0.00024782\n","Iter: 097/100 | Train Loss: 0.00024518\n","Iter: 098/100 | Train Loss: 0.00024258\n","Iter: 099/100 | Train Loss: 0.00024001\n","\n","Iter: 099/100 | Test Loss: 0.00097254 | Test acc: 64.8800\n","tensor(6.8786e-05) 0.9 0.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aKzS93GmpWjN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597611390263,"user_tz":-120,"elapsed":124529,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"d709dc5d-9e08-449b-dd3f-94a1f1eea344"},"source":["step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(0.9,0.7,2,True,initialize = 'NTK', batchnorm = False, learning_rate = 0.1 * np.sqrt(784), ** shared_model_param_dict)"],"execution_count":195,"outputs":[{"output_type":"stream","text":["Adjusting Layer 1, Kernel Nodes: 725, Adptive Nodes:75\n","Iter: 000/100 | Train Loss: 0.00230411\n","Iter: 001/100 | Train Loss: 0.00187329\n","Iter: 002/100 | Train Loss: 0.00155647\n","Iter: 003/100 | Train Loss: 0.00147197\n","Iter: 004/100 | Train Loss: 0.00158650\n","Adjusting Layer 1, Kernel Nodes: 676, Adptive Nodes:124\n","Iter: 005/100 | Train Loss: 0.00177904\n","Adjusting Layer 1, Kernel Nodes: 615, Adptive Nodes:185\n","Iter: 006/100 | Train Loss: 0.00178685\n","Adjusting Layer 1, Kernel Nodes: 383, Adptive Nodes:417\n","Iter: 007/100 | Train Loss: 0.00156161\n","Iter: 008/100 | Train Loss: 0.00130541\n","Iter: 009/100 | Train Loss: 0.00120163\n","Iter: 010/100 | Train Loss: 0.00124005\n","Adjusting Layer 1, Kernel Nodes: 411, Adptive Nodes:389\n","Iter: 011/100 | Train Loss: 0.00127333\n","Adjusting Layer 1, Kernel Nodes: 338, Adptive Nodes:462\n","Iter: 012/100 | Train Loss: 0.00122316\n","Iter: 013/100 | Train Loss: 0.00109519\n","Iter: 014/100 | Train Loss: 0.00095900\n","Iter: 015/100 | Train Loss: 0.00088005\n","Iter: 016/100 | Train Loss: 0.00086806\n","Iter: 017/100 | Train Loss: 0.00087942\n","Adjusting Layer 1, Kernel Nodes: 797, Adptive Nodes:3\n","Iter: 018/100 | Train Loss: 0.00083684\n","Iter: 019/100 | Train Loss: 0.00073875\n","Iter: 020/100 | Train Loss: 0.00066603\n","Iter: 021/100 | Train Loss: 0.00065462\n","Iter: 022/100 | Train Loss: 0.00065359\n","Iter: 023/100 | Train Loss: 0.00061327\n","Iter: 024/100 | Train Loss: 0.00055395\n","Iter: 025/100 | Train Loss: 0.00052302\n","Iter: 026/100 | Train Loss: 0.00052259\n","Iter: 027/100 | Train Loss: 0.00051487\n","Iter: 028/100 | Train Loss: 0.00048238\n","Iter: 029/100 | Train Loss: 0.00044786\n","Iter: 030/100 | Train Loss: 0.00043403\n","Iter: 031/100 | Train Loss: 0.00043150\n","Iter: 032/100 | Train Loss: 0.00041818\n","Iter: 033/100 | Train Loss: 0.00039253\n","Iter: 034/100 | Train Loss: 0.00037143\n","Iter: 035/100 | Train Loss: 0.00036288\n","Iter: 036/100 | Train Loss: 0.00035667\n","Iter: 037/100 | Train Loss: 0.00034224\n","Iter: 038/100 | Train Loss: 0.00032341\n","Iter: 039/100 | Train Loss: 0.00031018\n","Iter: 040/100 | Train Loss: 0.00030352\n","Iter: 041/100 | Train Loss: 0.00029588\n","Iter: 042/100 | Train Loss: 0.00028328\n","Iter: 043/100 | Train Loss: 0.00027016\n","Iter: 044/100 | Train Loss: 0.00026147\n","Iter: 045/100 | Train Loss: 0.00025565\n","Iter: 046/100 | Train Loss: 0.00024809\n","Iter: 047/100 | Train Loss: 0.00023811\n","Iter: 048/100 | Train Loss: 0.00022909\n","Iter: 049/100 | Train Loss: 0.00022296\n","Iter: 050/100 | Train Loss: 0.00021774\n","Iter: 051/100 | Train Loss: 0.00021108\n","Iter: 052/100 | Train Loss: 0.00020355\n","Iter: 053/100 | Train Loss: 0.00019723\n","Iter: 054/100 | Train Loss: 0.00019250\n","Iter: 055/100 | Train Loss: 0.00018782\n","Iter: 056/100 | Train Loss: 0.00018223\n","Iter: 057/100 | Train Loss: 0.00017656\n","Iter: 058/100 | Train Loss: 0.00017187\n","Iter: 059/100 | Train Loss: 0.00016792\n","Iter: 060/100 | Train Loss: 0.00016375\n","Iter: 061/100 | Train Loss: 0.00015913\n","Iter: 062/100 | Train Loss: 0.00015473\n","Iter: 063/100 | Train Loss: 0.00015098\n","Iter: 064/100 | Train Loss: 0.00014751\n","Iter: 065/100 | Train Loss: 0.00014384\n","Iter: 066/100 | Train Loss: 0.00014006\n","Iter: 067/100 | Train Loss: 0.00013657\n","Iter: 068/100 | Train Loss: 0.00013345\n","Iter: 069/100 | Train Loss: 0.00013043\n","Iter: 070/100 | Train Loss: 0.00012729\n","Iter: 071/100 | Train Loss: 0.00012416\n","Iter: 072/100 | Train Loss: 0.00012127\n","Iter: 073/100 | Train Loss: 0.00011860\n","Iter: 074/100 | Train Loss: 0.00011596\n","Iter: 075/100 | Train Loss: 0.00011328\n","Iter: 076/100 | Train Loss: 0.00011069\n","Iter: 077/100 | Train Loss: 0.00010826\n","Iter: 078/100 | Train Loss: 0.00010595\n","Iter: 079/100 | Train Loss: 0.00010366\n","Iter: 080/100 | Train Loss: 0.00010137\n","Iter: 081/100 | Train Loss: 0.00009917\n","Iter: 082/100 | Train Loss: 0.00009709\n","Iter: 083/100 | Train Loss: 0.00009507\n","Iter: 084/100 | Train Loss: 0.00009307\n","Iter: 085/100 | Train Loss: 0.00009111\n","Iter: 086/100 | Train Loss: 0.00008921\n","Iter: 087/100 | Train Loss: 0.00008740\n","Iter: 088/100 | Train Loss: 0.00008562\n","Iter: 089/100 | Train Loss: 0.00008387\n","Iter: 090/100 | Train Loss: 0.00008217\n","Iter: 091/100 | Train Loss: 0.00008052\n","Iter: 092/100 | Train Loss: 0.00007893\n","Iter: 093/100 | Train Loss: 0.00007737\n","Iter: 094/100 | Train Loss: 0.00007584\n","Iter: 095/100 | Train Loss: 0.00007435\n","Iter: 096/100 | Train Loss: 0.00007291\n","Iter: 097/100 | Train Loss: 0.00007151\n","Iter: 098/100 | Train Loss: 0.00007013\n","Iter: 099/100 | Train Loss: 0.00006879\n","\n","Iter: 099/100 | Test Loss: 0.00103940 | Test acc: 64.7400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O83ZJlzbcT1R","colab_type":"code","colab":{},"outputId":"3c973f7b-e2c1-4e1c-99e6-283d636c29ce"},"source":["title = '784-800-10; NTK scaling; no bn; full batch; train data size = 64'\n","fig = plot_loss(step_list, loss_ntk_scale_no_bn, title, fig_save_path)\n","del title"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x216 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"0n0gRKWTpfkz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1597611392020,"user_tz":-120,"elapsed":1749,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"5ca2db31-83f0-4fe2-a882-1eea48cb69d9"},"source":["plt.plot(step_list,loss_ntk_scale_no_bn['train'],'r-',label='Original_Train',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['train'],'b-',label='Modification_Train',linewidth = 2)\n","plt.plot(step_list,loss_ntk_scale_no_bn['test'],'r--',label='Original_Test',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['test'],'b--',label='Modification_Test',linewidth = 2)\n","plt.title('784-800-10; NTK scaling; no bn; full batch; train data size = 64')\n","plt.xlabel('Iter num')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.legend()\n","plt.savefig('/content/drive/My Drive/LCNN/newplots/attack3.pdf')"],"execution_count":196,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAaYAAAEZCAYAAADc7YGjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hU1daH350EEgi9iCi9CQRIIAgiIkjTi0q1YAcVvB/YESsKNuyiKFzFqyAKCiJYEK+NoiAqIL1Ib4J0CCQhdX1/rAmZhCSkDZOy3uc5z8ycc2afdc6U31l7r72WExEMwzAMo6AQ4G8DDMMwDMMbEybDMAyjQGHCZBiGYRQoTJgMwzCMAoUJk2EYhlGgMGEyDMMwChQmTIZfcc51cs7t9nq91jnXyY8m5Rjn3CTn3HM+ats55yY654445/7IiS3pr20G+4pzrkF+2nsmm/xFfn6vnHPznXN35kdbebDhJufc9/60wZcUKGFyzp1ItyQ5597y2n6dc269c+64c26dc653Ju385PnRBWVxrGDn3DvOuX3OucPOua+dc+d7ba/knJvlnIt2zu1wzt2Y7v03etZHO+e+cM5VyuJYzZxz3znnDjrnTps4dqZjZYVzbpTnXK/zWhfkWVfHOfet1/VMcM7Fe71+JwNhKOmcm+mcW+ScK5ddO/ILEQkTkfln+7gFmEuAbkANEWnjb2NScM5td851PQvHqXOm33J28Nf3ylfXSUSmiEj3/G43tzjnWjnnfvb8r+xzzt2XwT4dPZ/lGW9SCpQwiUiZlAU4F4gFPgPwiMbHwINAOWA4MNU5d453G865m4AS2TjcfUA7oAVwHnAEeMtr+zggHqgG3AT8xzkX5jlGGPAucItnewwwPotjJQDTgTsy2Z7psbLJYeBp51xg+g0i8i+vazoFeNnrOv/be1/nXDAwE6gAdBeRqBzYYPiG2sB2EYn2tyEFlbyKlpE3nHNVgP+h/4mVgQbA9+n2KQG8CfyenTYLlDClox+wH/jF87oGcFREvhXlGyAaqJ/yBudceWAk8HA22q8LfCci+0TkJDANSBGeUM/xnxSREyKyEPgKFSJQ8fhaRH4WkRPAk0Bf51zZjA4kIn+JyPvA2vTbznQs51wt59xR51ytLM7lf6iw3ZyN884Q51xp4GsgCLgysz9C51wPj7d63Dn3t3PuIa9tvZxzK5xzUc65Lc65KzzrB3p5uludc3dlYcepO0yPNzjdOTfZ8961zrnWXvu2cs4t92z7zDk3zftuzHPdLsnkOAOccwudc6867Sbb5pz7l9f285xzX3m86c3OuUFnuIRVnHM/eGxZ4Jyr7dWWOOf+7Zzb5LFpnHPOebZl+vk65+4A/gu089yJPp1id7r98tIl18PzmRx0zr3inAvwtFnfOTfXOXfIs22Kc66CZ9tHQC3ga49dD3vWX+Kc+9VzPruccwO8jlPROfeN5/r87pzz/t3Ods49mol9P3sej3qO1c5zDRY558Y45w4Bo7Ky13OMbH+v0uOc6+ac2+CcO+acextwXttyc50+c87942nvZ5fFTajnXLd67Nzm9Mb71PfX8/xhl7anKcE5N8mzrbxz7n3n3F6nv9fnXAY3sHnkQfS/dIqIxInIcRFZn26fYahYbchWiyJSIBdgLjDK63UgsADo6XneG9gNhHrtMw54AKgDCBCURfutgUWot1QamAq84dnWEohJt/9DqBgBfAk8km77CSDyDOfUQC95mnVZHisb12kU6kn2BLai3mKQ5/zrpNt3EvBcunWdgAOea/sVEHyG4+0FOnieVwRaeZ63AY6h3U4BwPlAY8+2K9EbCAd0RD3MVl7H3+3V/nagq9e5nQR6eD7zF4DfPNtKAjtQz7cE0BcV5+eyed0GoJ7sIE/b/wfsAZxn+8+oFxwCRHiuUedM2poEHAcuBYLRO8OFXtsFmI16orU8bV2RAzsXZvbaq/0G6T/j9Nc2g7YFmAdU8ti1EbjT67vazXM+VT3X442MPifP69qea3CD5/OoDER42XTI8x0JQj33T7N5/nVI91v2XINE4B5Pe6VyYm9W36sMjl/Fc17XeM7rAc+xc3WdPOtuB8p63vMGsCKTY4cCUcAFntfVgbDMvgee9TXR7/G/PK9noZ5MKHAO8AdwVybHuxE4msVSK5P3zUW/87+izsTX3vt6vhsbgTJk8B+U0VIgPSbP3WZH4MOUdSKSBExGBSTO83iXeO7sPXc87UnbHZcVm4BdwN/oh98EeMazrYxnnTfH0C9TyvZjWWzPCWc6VrYQka/QP7zcDMqWRbs1PxSRuDPsmwA0dc6VE5EjIvKnZ/0dwAci8oOIJIvI3yKywWPbNyKyRZQF6J1Th2zatlBE5ng+/4+AcM/6i9A/pbEikiAiM9EfXU7YISLvedr+EP3hV3PO1US/S4+IyEkRWYF6Lrdm0dY3oh50HPAE6uXU9Nr+oogcFZGdqBhE5NBWX/GSiBz22PUGKiyIyGbPZxknIgeA19HfZGbcCPwoIp94Po9DnuuWwiwR+UNEElFhyuv57xGRt0QkUURic2FvZt+r9PQA1orIDBFJQK/RPykbc3FcROQDUa8iDhXJcKe9PRmRDDRzzpUSkb0iclqvSwrOuVLAF8CbIvKtc66ax/77RSRaRPYDY4D+mdg1VUQqZLHszOTQNYDb0JvEWsA24BOv7WPx9AhlZnt6CqQwod1YC0VkW8oKjxv+MnoXWBL98P/rnIvwdD+MB+7zfPHT4Jx73MvNfcezehx6x1IZvZuYCXzr2XYCHcfyphx655TldqfRMinH+pYzc6Zj5YQR6J9iSA7fdxD9sn7onLv8DPv2Q7/sOzxdVu0862sCWzJ6g3PuX86535x2ix31vL9KNm37x+t5DBDidEzhPOBv8dySediVzTZPa1tEYjxPy3jaPiwi3p/BDtQLzIxTx/b8AA972jntWOh5lMmhrb7C+5rtwGOzc66ac+5TT/dPFOqVZ/WZZfr5e8jv80/zWefC3sy+V+k5j7SfrXi/zulxnXOBzrkXnXZ1R6EeFRm9x3PTfT3wb2Cvpyu0cRbn9D7wl4i85HldG/Xy9nq6V4+i3tM5mTWQS2LRG48losMiTwMXe7oRrwbKisi0nDRYUIXpVry8JQ8RwM8istRzR74EHUjriv6RtwamOef+AZZ43rPbOddBREbL6QP+EcAkz91iHOpptXE6kLcRCHLONfQ6fjipY0Rr8brDcs7VQ0Vuo2g/a8qx/sWZOdOxso2I/ABsBobk4r0z0W6tGc65y7LYb4mI9EK/3F+gQR2gP9b66fd3GlDxOfAqUE1EKgBz8OqnzyV7gfOdc97t1Mxs5xyyB6jk0o4Z1kK968w4dWznXBm0e2xPPtnjTTTa9ZxyrHPz2J73NatFqs2j0S605iJSDh2/9L7W6aNLM/z884HMyh+kX38me3PLXtJ+to601yyn1+lGoBf6v1Ue7aokM1tF5DsR6YZ68xuA9zLazzNG14i0AVa70N6lKl5eTzkRyXBMK91NdUZLZuPcq9Kdp/fzLkBrz5jaP6jQ3u+c+zKTtoACKEzOuYvRO9PP0m1aAnRwzkV49muJdgetQru+zkPFJgK9IweIJPMokCXArR5VL4H+me8RkYOeO5WZwDPOuVDnXHv0y/SR571TgKudcx2cBi88A8xMd4ftfU7OOReCeno450I8f9ic6VguNVy2zpmunYcnyF7wx2mIyCfA3cCXHjvSn0dJz5e3vKdbIwrtagC9WxvonOvinAtwzp3vubsriYr2ASDRaYBBfoS5LgaSgLudhsf3QscwvO0Vl4u5KyKyC+0vf8HzWbVAf/AfZ/G2Hk4H/0sCz6JjFmf04HLx+a4Ewjw9BSFoV1BeGO6cq+jpdrwPDQIC7d49ARxzGhE7PN379gH1vF5PAbo6ndIR5JyrnPJbPRNO5wWNymTzAfQ7Vi+T7Smcyd7c8g16vft6PKp70Yjh7B43/XUqi4rFIfQGY3RmB/Z4Y708/zFxnuMkZ7Dfvzx29RGR2JT1IrIX7TZ/zTlXzvO7rO+cy7CrMd1NdUZLZl15E4E+nu9kCTQYbKGIHPM8b0Tqf/NXqLgOzOy8oQAKE9pXedqfvGdsYhR6R38cvQsfLSLfe8Yu/klZ0C8zwD4Ric/kOA+hA6CbPPv3APp4bR+CDqruR/tL/y+lf9fz+G/0x7gf/bJl5aXURt3dFC8oFvgrO8dC7852kPXd+ilEZBE5H2vxfv+HaATNN865jObN3AJs93RD/BuNUERE/kC/bGPQG4UFQG3P53gv6lkdQe8Yv8qtfV52xqMBD3egA7M3owEGcQCeP9rjwOpcHuIG9G52DzqAPFJEfsxi/6loROhh9IYouxGSOf18N6I3Qj+i392FWb/jjHwJLANWoH/C73vWPw20Qj/Lb9CbJ29eAEZ4uoge8vxp9UC/O4c97WU2bpOemmgg0ml4ulifBxZ5jnVRJm2cyd5cISIHgWuBF1ExaZjO1hxdJ3ScPOXzXgf8lsXhA9CItz3oNe2IBumk53o08GK9O33I4lb05nAd+vubgXpf+YaIzAUeR89/PxoQcqNn2/F0/82xQLSIHM6qzZQIJKOA4pwbARwQkXf9bUtBxzn3O/COiEx0zt2MRjA95m+7sqK4f77OuRrAdBG52N+2GAUHEyaj0OLpkvgLDd64CXgHqOfpwjAMo5BiM6aNwswFaBdhKDqH6xoTJcMo/JjHZBiGYRQoCmLwg2EYhlGMKRZdeVWqVJE6der42wzDMIxCxbJlyw6KSNWzfdxiIUx16tRh6dKl/jbDMAyjUOGc2+GP41pXnmEYhlGgMGEyDMMwChQmTIZhGEaBwoTJMAzDKFCYMBmGYRgFChMmwzAMo0BhwmQYhmEUKEyYskNSkr8tMAzDKDaYMGXFzp3Qowdceqm/LTEMwyg2mDBlxZEj7PtxNUt+jYctW/xtjWEYRrHAhCkLlk/byLkJu7iJKTB9ur/NMQzDKBaYMGXB2vO7U5I4NtGIbZMW+NscwzCMYoEJUxZUrFOeBE+e2x821oING/xskWEYRtHHhCkLOnYE5xwA39Mdpk3zs0WGYRhFHxOmLChTBiLDEwH4iS4kfjrDzxYZhmEUfUyYzkDPfiUB4SgVWdrjKX+bYxiGUeQxYToD3boBOEBYM3e/n60xDMMo+pgwnYHWraFc2WTA0WXdWxAV5W+TDMMwijQmTGcgMBC6dtPLdDC+DDRsaNF5hmEYPsSEKRtodx4cpywn9kfDrFn+NcgwDKMIU6SFyTl3tXNuwrFjx/LUToowXckchvMKzJyZD9YZhmEYGVGkhUlEvhaRweXLl89TO/Xrw3nnwUlKMYceyNKl8Pff+WSlYRiG4U2RFqb85Mor9XEntVlDM/jyS/8aZBiGUUQxYcom3bunPp/NVdadZxiG4SNMmLJJt24QECCAR5jmz4cjR874vuhoHxtmGIZRxDBhyibly8Mll2jevMW048CVAzRnUSYkJUF4ONSpA127QkQEXHMNHNx6DHr2hHPOgYsvhsGD4f33rUquYRiGhyB/G1CY6NMHfv4ZhAC+X1KBm0qUyHTfl16CVav0+U8/6ePKlbBhwUlWH/waB3DgACxeDO+9B999B1OmQBZtGoZhFAfMY8oBV1+tj6Gc4Jq9Y3WibQaeTlwcjB6tz1u3Vs158K4TAKw9WI15XAaRkWk9rs8+g3794ORJX5+GYRhGgcaEKQfUrw9NmkA0ZVhJuHbJPfTQafuNHp06tvTpp9B974c8/2kDShAHwPAGs5AlS+HoUZg7F0JCdOd160DkbJ2OYRhGgcSEKYekeE0/cymyaZN2w3kFQURHwyuv6PPOnaF+zGq44w5Cju3jhdBnAfhzc3l+/BHNd3TZZTB1qr5hyxaYN+8sno1hGEbBw4Qph6QI05M8x3BeViV6551T259+GmJj9fm7Lx7RcL6kJAgM5LJJt5/a78knvZyjPn30jQA33ACbN+tiGIZRDDFhyiHt2kG5cpoFYjxDWUE4jB2rA0ugnhDQvVM8DR7sCfv26Yo33qDVNfVo3Vpf/v47/PKLV8MjRkDv3pq9vH17XfZbmQ3DMIofJkw5JDBQh5YAYilNT75i3z/J8PHHrFwJa9dCKaL5PPYKWLhQd+zeHYYOBeCee1Lb2bbNq+GAABW44GAVpP374fbbbczJMIxihwlTLkgRpuDARHZRi/YsYvn/vcv7/5pBfDzMDnuUMr97xorOOUfHkJzOgbruOqhYUXv3IiLSNVyzJtx/vz4PCoJvvoFx4/jiC3Wg7r5bVxmGYRRlTJhywdVX6+TZuKQggkhgCw1olfAH0/e2p1kzuKh3dd0xOBjmzIHKlU+9NyREJ9oCfPFFBo0/+ihUqgSJiQC8ff9m+vYVfv0Vxo2DF17w8ckZhmH4GZtgmwtCQnQsqUsXWLWqBI5kAkgmKqgy87pOoPToEbrj1Kk6XykdvXtrMN/nn0OtWnDzzV7zaitUgBEjSH5wGI8Hv85LcepBPTYsnpByJaldO4fGRkXBV1/BwYPaXVitGlx/fe5P3jAMw9eISJFfIiMjxRccOCDSokmc6ECQyCRulVMvnn8+0/fFxoqUKZO667Rp6XY4eVLuKTtRQCSQBJnIbSIvvJBml7g4kddf18cMWbhQ5LbbREqXTj0QiLRsmXa/TBswDKO4AywVP/xnm8eUB6pUgZ9+Lsm/b4+jeZlt3NawDiz39PM99lim7wsJgSuugBkz9PX48Tr2lIKUDKZer2aU+jiGzysO4l/3NoThw9O0ceON6nH984+mP0rDCy/A44+nvr70UmjZEpKT4fzzU9dv2wadOsHkydCxY66ugWEYRr7jDzU824uvPKa88PHH6sAEBurjkCEi+/Z57ZCYKHsv6Kgb33wzdf2uXSIxMfLrr/pe50R++klEEhJS99m0SaRKFZHHHhPZuDFzI15+WdsPCBAZOTJtG4ZhFHvwk8fk9NhFm9atW8vSpUv9bUYajhyBqlXViXFOHwEmToQBAzw7ffUV9OqlkX1btuhOF12kOfYGDeLpBZ0YNaUhF5Tbw+r6fSix7LdT0X/ExkKpUlkbkZioE3uff147+jp21Jx9Vav66rQNwyhEOOeWiUjrs31ci8rzExUrai+aCDz3HPTooeuHDdMs5ICG/7Vtq3Oaxo7VybqxsbBkCQwezGNTwmjAJv6KOo93lreF1atTD3AmUQINSX/2WY3kqF4dFixQ4duwIb9P1zAMI9uYMPmR3r31celSnZ80d64OTTVs6NnBudQ05S+/rGHnf/wBzzwDAwZQslsnXqn7HwBGVRjDkZotcmdI585qRGQkbN2qk6ayUQTRMAzDF1hXnh/ZuRNq14bQUC3NlKmT062bejV33qlx5l6IqK7Mnw+vvw4PPJAHg6Kj4ZZbVJiGDctDQ4ZhFAWsK68YUquWOinR0fDmm5nvJ6+PYX/JGvDf/8KsWWm2OQdvvAGffJKaNCLXhIZqqOCDD6au+/13OHw4jw0bhmFkHxMmP/Pcc/o4cqTm2fMmKUl1IvK2ZlSL38XNfETUHQ/Anj1p9gsPh/79U+Me8kRAQGpDO3fq4FfLllpdNyVCwzAMw4eYMPmZK67QHrr4eLjtNkhI0O65adMgLAyuvRaWL9d9p3AzEUfm8lvvFzMViVWr1OHJNw2pX18F6uabNbnfrFmpdT0MwzB8gI0xFQCioqB5c/3/HzQI1qyBxYt1W5068PDDcPHFMPDmBJavKUEgiXzefgy9pt8E5513qp24ONWRv/+Gx7otZXSN8Rqpt26dzga+5BLo0EGj/bwn2mZFYqJOwB01Cnbt0nXBwZqPafZs9a5E4IcfNGW6c+p1hYZC2bJaI6RaNd1mGEahwl9jTCZMBYQff9QYhxTOOUe7+QYMSM2jFxcHw67dwbiva1OfzWwIbU3QqBHQrJnmwtu9m/9N3s9V618miSBuYTKtWUojNlKFg7RmGQASWoanOy+g28MtaXexIyA7fvPJk/DuuzBpksazX3YZ/PSTbouO1rlVmfGf/8C//63Pt23TdBWRkVCyZA6vkmEYZxMTJh9SGIQJNFT87be1ZtNjj6nDkZ7ERGjaMIFN20vwAQMZyKTT9nmvxBAGJ4xLs659RDQL75wEs2ez9H8HuBC9HuFhCXz7QwmqV8+BoYcOqRBecEHq6/79tf8wOVkHx6Kj4cQJDTv/9FMNHQQdTHvmGShdWlMlXXEFXH65tpUvg2SGYeQXJkw+pLAIE2iv2Jn+nz/+WKO661aL4a+wvpQITNauuipVNNT7qqtYuj6URYs0oGLDBqhRQ+MXHMKWMV8x7rHdTIvvzR7Op0HtBOb+UoKaNTM/5rJlWpHj9981UURKwcMcn9Qbb8CECbB+fdp9zj9fy8q/8koOGjYMw5eYMPmQwiRM2SEpSQMj/vpLI8jvuCMXjezcycF+d9Ft6WhW0JI6NROZuyCIunXT7rZ9OzzxhFbwSOHrr+Gqq/T58ePq/OR4CGnfPh2X+t//4Lvv1AO77TbtKgTt8uvaVWPqy5XTpUQJjRKJj1fX8pxzdN977tHiVidP6sUJDtZMuWXKQLt2KoQprF0LDRroPoZhZIm/hMnvCVbPxlIQk7jmlSlTNP9qnTp5qFxx7JgcbtVF2vCbgEjFCkmya5duOnhQ5IEHREqW1OMEB4sMH645YWNjdZ/ERJHu3UWuuELk8OE8nExSksjq1SLr1qWumz1b0pTrSL+sWZO67803Z75ft26p+0VFpWbObdFCZOBAkXHjRH77TSQmJg8nYBhFEyyJq+8oah4TqGPQvLn2iE2YoNF8ueLwYaIuvYp+a5/maKnq/LGnJq5CeZ5/HkaM0B64m27SQIz0RQo3btRowUOHNBrw9dfVk8pWMMWZiI+HTZtg/37iD0ZxaG88oQGxlKsQoEET3bppwkHQXIJxceolBQbq85Mn4dgxNaaFJ1XTpk1q4ObNp8fTBwbC99+njoVt26bSVquW5hQ0jGKIdeVlE+dcNWAWkAAkATeJyN6s3lMUhQk028ONN0KTJtpDlevYgf37kQ6XcmTjfipd3gZmz+bEySAGDdJQ9ZYtM3/rjh3Qp0/qXKumTeG++/Q9F16o66Ki4J13UsX00ku1Zy4Lc/j0Ux0T27BB3w/aYzd2rD6PjdXzDQnJxfnGxGhk4dKluixbpgq/c2dqGP3NN6sBQUGqyDVqaKLb6tWhdWu98KATz1av1srD5cvrYkJmFBFMmLKJcy4QEBFJds4NAGqIyHNZvaeoClNCgv5n7t2rCWAvuywPjW3dqpnFDxzQ0O7x47OtdLGxGkn++uupU5169oQvv9Tnf/+t/+spBAZCmzY6/BMersklqlTRbbfcoqKUmJh2/0qV4IMPUse2Jk6EoUP1nC+7DOrVU+emRg11pHI8hBQdrYNlKec8eDB8+y3s3n36vn36wMyZ+nzXLj2wN6GhqSI1YYLOHwOd9zV/voZblimj+5UqpUvlyjo3LIXFi1ODRryXgACoWVPnhoFGPh48qAodEqJtlSxpEY5GvuAvYSp0t3YikuT1siywNrN9izolSsBdd+nc17ffzqMw1aunSnLZZereNGyYNmdeFpQqpXn6hg5VL27GDBWcFMqV0wK8SUn6f/vHH/qYMol4yZJUYTp+XP+Pr7oKbr1V/6srVjz9f3b9ehXEOXN08aZx47RBfw8+qCWmGjTQ06pTR//fExNVG4KDgdBQvvhC29q3Dw4enEBwQ6gYmUTFoOPUKnOYJzsvwv2zlzQRIgkJmhHjyBHtOoyKUpGLjtbUUd5dhnPnwpgxGV/ERo00miWFbt20jYx47bXUz+arr7Sv1ZugIBW+smU1FUiFCrr+xRfVxa1QIdXDS3msU0ddb1CbExIKfYCIiJ7GyZOpS2KirktMVOc45dIcOKA3eBm14VxqbzDo9/X4ce0xjo9P7TmOi9MegYsu0v22bNGbqcREXZKSdEmZVfH006nxOxMm6O9CJO2si5RApxEjdL+oKL0vio/X80j/+NZbcOWVuu8bb8BTT+nXsrDdp/jUY3LO3Q0MAJoDn4jIAK9tlYD3ge7AQeAxEZmaQTMZtRsBvAtUALqLyI6s9i+qHhPoj6lWLf1Cb9tGliHf2WL6dLj+ev0mT5yokXL5TFQULFwIf/4JK1bARx+lZlbfsEG9o5QfbFbs3atBfcuWaS/cjh2qBU2awM8/6z4xMSo+mfHJJzoFCzg1rpYR1aunTVEYFqaXqHp1Fb0qVdTmqlWES1rFEFb9MBw7RmLNugSUDdVxt/nzkT+WEH04jiOHkok6kkRYqa2qsOecw9RLxvPPP/onFvLftymddJzSLpZyASeoF/w3TUO2QnIyifcNI7bPjRoN+eVMTSl/8qS2Exub1t2Mjz81Q1suakfU7+s4QkWiKEcQiTRFFTzxptuY3mMSyckQuHc3AQ8PIzAASoQEUiI4gPBy2zi/bBSUKsU/L33IrjJNCAiAwK+/IPn3JSQHlSQpqCSJAcG0r7VLj1mzJpPL38PevfpHfmLBMo7HlyQ2sQRxSUFc1WwHAy/ZBAEBLK/clQfeqqeO4ckY3PEoAgLwLI4Jd6+iZvVECArixaVdmDc/gORkiD8Wo+KQEEBsXCCtIxL46N1YCA7mcHQwlc8tkelnP3WqzlAAePVVvXnKiDJl1P4UGjRQ0cmIhx5KnfEwd25aJzg9Gzemlri5/nr96WVEp04wb54+j4rS+4jM8D6nV17RrviEhNz3LhfJrjznXF8gGbgcKJVOmD5Bc/XdAUQA3wAXi8ha59y5wKcZNNlfRP7xauM6oLOI/DsrO4qyMIH+sU6bpmHdz2XZqZlNXntNf2EBATrOkvLPXQiJjlbh27RJYx42b1YRc067CCdM0HyEoM7FokXaS1a1qt4BHzmiwR2QmrwiMVGdiczyEXqXH5k6VR2agAD9c0i5CwZ9HR+fejfbsqUKdUZ4VzxZvhxatdLnwcFpewRDQmDG1HguOP8EHD/OsLG1mTZNr0NUlJCcnHrr3OHcjfx86ZNw9Cixl/Wg9GP3ZXodJ3EbtzEZgDeH7eT+1zK+AypNNNGUOXVCjWP+TOMIenMfb/AGeqEWPPAFncb0yvT469sO+DAAACAASURBVGlMY7Shm25IZuonGbsA7VnIQjoAcPLWwZSd+q72cpZIJPjYfkq6BIJcMkEBSbxy3htcWfk3KFmSSVfN4PVPPem9Dh+C4ycQ5whwQmiJeH7t9bJ+YerU4abVj7J3r/aYBm9ZR8mgJEJKJBEclMzlDbdybYu/IDCQHS2u5qM/wwgKgqADewncuonAIHdKcG+8aCsVyiZBQAA/1biNrds829avxZ2MJTDIERjkqFY5kS6RRyEwkKTqNZi35wJKloQSibGU3LudksEBlAgOoGSpQM6pKpQpHwiBgZwsX404KUm5crn3mIpkV56IzARwzrUGTo0yOOdCgX5AMxE5ASx0zn0F3AI86hGfThm16ZwrKSLxnpfHgJhM9hsMDAaolX4MoIhx990qTO+9B08+mQ89MMOGqavx1FMaBBAcrP0HhZDQ0FRBORMtWqTtssmMwEDt+tmxI6XbT18fPKiBGymiAXD0qD4mJ6sIgQ5lVaigXZTeHt2NN2pPamBgqgMUHa136xERqW3GxWkbsbH6PC4urX0nk0uq21mpElFROsanOMqU0eOWLw/1IhvBpGkAlEiE/iv1DyylKykxPpnEuCQS4pKocdvz0OohiI3lnLVVad3a0y11IpqA+DgCSCaAZEqXiIf/e01v06tWZcB+rZpStoxQZt7XlA2KpXRgHCEB8TQsewIq3gVJSYR3rszcqz0x/sv+ROZ8iyQnk5wESYlCjQrNwNWFxEQefsRxy6365x78wihKbt9IycRoSiVFUz7xECRWhLg4QsqVJCHBc+q/L0vtY0thu2cBBrz9DwMe9wjTnY/A+++n3fe/nsfISKYsfTT1Qw0MS7vfcsDj+dSeUJkRIzzb3/0KXk/3RfS69e6SfBtdUsSj5c2Z3qEEDh5M13ff1RdL10KXCzPcDyBk6VJCIiMz3V6QOSvBD86559AghQGe1y2BRSJS2mufh4COInL1GdpqA7yKRuSdBG4vrlF5KYjoH9eqVZoVIv2QQ64bHTFCK+gGBakrNnx4PsWCFz9Shm2cy78UgSIqYNHRaXvyGjZM7Rrdt0+FKyWnbrFNT5iQoAqZkJCq5imDM/HxOiiaku/xzz81GChlMCplYCopSftsU1zs5GR46aW0g0feg0h9+2qUD2jf9bRpqfuIpHWfJ05MtfWhh/T4Kdu9B6iuvjrVHV+zRgdivQexvJ//73+aRzMPFMmuvFMHOV2YOgCfici5XvsMQkO/O+X38Yu6MIF6S4MH65/SypVZVMPNCd7iBHo7P3ly2hA7wzCKLMWtgu0JIP1MlnLA8Qz2NbLBrbfqgPymTZojNV9wTiMCvvlGR/bnzdM7sKFDNWleIZtqYBhG4cBfHlMocAQIE5FNnnWTgT0i8mh+H784eEygWtGunfa2/fFH2rGOPLNvH9x+e9rY7Lp1Vajq1dNw43PPVQGrVk1dt2Lbb2QYRYMiGfzgnAvyHCMQCHTOhQCJIhLtnJsJPOOcuxONyusFXOxLe4o6bdvqfKIxYzSx6x9/pNZyyjPVqukE0ZUrNcxtyhSNT9+2LeP9S5bUSILWreGaa7Qb0ManDMPIBr4OFx8FjEy3+mkRGeWZx/QB0A04hEbjZWseUw6OfzVwdYMGDQZt2rQpP5susERH6yS/bds0+0K/flq09uhRHX9dvFidnEcfzaNOJCZqKp6tW3XZsUND0vbv12wJ6Sd61KunCf0GD9aIMcMwCjxFOvjB3xSXrrwUfvpJa++lBPxkxI03aiCQz3rbjh3TCTfz5+uBdu7U9ZUq6bjVoEFWbt0wCjgmTD6kuAkTaAaFOXNgwQL49VdNC3TJJVoo9oUXNMValy6a8i2rhKr5QlKSZu5++WUVKtD49v/85/S5JYZhFBhMmHxIcRSmrPjzT02cum+fjkv9+utZGv4Rgc8/1wm8O3fqQZ94QmcF59tgmGEY+UVxCxc3/EirVjrWVL26RvKlT4LqM5zTQIj163USoQg8+6wWdcosb41hGMUOE6ZiSt266riA5nY7q5QurRkm583TDLRLl2qiuP/8x+ZGGYZRtIXJOXe1c27CsWPH/G1KgeTOOzVNzbx5qYX+ziodO2oepZtv1lw6Q4ZovYt//jnzew3DKLIUaWESka9FZHD5rPLEF2PKl1dxAj94Td5GfPSR5hGrWFH7FZs0gXHjsg4rNAyjyFKkhck4M/feqzEIn37qnYXaD1x3nc6LuuIKnXR19906OXf+fOveM4xihglTMadOHZ2Em5io1S/9yvnnq8c0c6aOPa1YoRkj2rTRwkanahgYhlGUMWEyTgVBvPuuzov1K85p7af167X2dJUqGhxx001w3nkwcCDMmqUTsQzDKJKYMBm0batxCEeP6hzYAkHp0lqocOdOLTPbpIlW4ps0SevcVKgAkZFwzz3qTW3bZl1+hlFEsAm2BgC//aaZyUuV0jR31av726J0iMC6dfDVV/D115qhNn1wxLnnQvv20L275mSqXds/thpGEcEyP/gQE6bs0bev9pLddRe8846/rTkD0dGwZAksWqSzhRcv1gql3jRtCv37azh63br+sdMwCjEmTD6gOGYXzwsbNmixQefUOWnUyN8W5QARrZI4bx589x38+CMc96o72aGDzpPq18/SHxlGNjFh8iHmMWWfwYO1THvPnlpyqUwZf1uUSxISNM36Rx+pGxgbq+vPP18r8N51l5XfMIwzYMLkQ0yYss/ff2vx2dhYCA6Grl01YrtMGX1dv75mKXfO35bmgOPHVWXHjtVoP4DQUFXhBx6AmjX9a59hFFBMmHyICVPOmDMHnntOAyIy+nqMHg2PPXb27cozIvDDD/Daa1qGAyAoSMehHnxQ8/UZhnEKEyYfYsKUO/75RwPgVq2CuDh1PKZN0//3jz/WqUWFluXLNTZ++nRITtZ1nTppxomrr/ZhBUXDKDyYMPkQE6b848034f77NX7gu++0m69Qs327dvH997+pwRJVq8Ktt2o0X3h4Ieu3NIz8w4TJh5gw5S8PPABvvKH5V5ct03GnQs+xY/Dhhxr5sWZN6vp69TSS76qrtNqueVJGMcKEyYeYMOUvSUn6X/3llzr36fPP/W1RPiKik3cnTtRovv37U7eFhsKll0Lnzhp+3qqVhZ4bRRoTJh9g85h8x5490KCBRu8tWqRFaIscSUlad37WLA2WWLs27fbSpfXEO3bUpU0bDV00jCKCCZMPMY/JNzz5pEbvtWun4lTkh2L27tW5UT//rEv6cvClS6tApaREaty4GFwUoyhjwuRDTJh8w/HjOr504IB25/Xt62+LzjL79qlALVigi/fYFOjF6dkTevXSyV+Bgf6x0zByiQmTDzFh8h3jx2sihYYNtaerWA+57N2rqZC+/x6+/RYOHUrdVrWqlvPo109DGYv1hTIKCyZMPsSEyXckJECzZrBxo4aRv/669V4BOj61eLFmQ581CzZvTt1WqRL07g3XXKOBFDYuZRRQTJh8iAmTb/npJ/jXv1SknnhCx50ML0S0bPyMGbqkpEUCKFsWrrxShap7d6hY0X92GkY6TJh8iAmT75k5E667Th2FZ56B4cN1SCUoyDyo01i3Dj77TD2plStT1wcGaiTJv/6lSQpbtdILaBh+woTJh5gwnR0+/VTTFKVk+AEICND/165doVs3zfoTYHWTU9m6VSeEffmlhjYmJqZuK19e501deqnNmzL8ggmTDzFhOntMmQIPPaRl2hMT0/7PgkbuTZ9uAWoZEhWlwRP/+5/WlfIelwINR2/TRqv0tm+v3lWFCv6x1SgWmDD5EBMm/xEdDb/8ov+3//2vZv4ZNgxefdXflhUCdu5UgfrlF102bky73TmNPGnfXj2qjh213pRh5BMmTD7AMj8ULObN0/H9xEQt3X7XXf62qJCxf79G+i1apMvSpRAfn3afevW0v/Syy3QxoTLygAmTDzGPqeAwcSLcfrt25c2Zo0Jl5JKTJ1WcFi7Uib4LF6YtJw9wwQUakt6liwpW5cp+MdUonJgw+RATpoLF44/DCy/odJ7ly6FWLX9bVERITNQov3nzUrsAvYXKOS3j0bmzelMdOmiAhWFkQoEWJudcKBArIsnOuUZAY+BbEUnwtYH5gQlTwSI5WatIfPutjt8vWGDBZj4hIUE9qrlzdbLZr79qxccUUkImO3bUlEmXXAJVqvjPXqPAUdCFaRnQAagILAKWAPEiUihqmJowFTwOHtRK5rt365ynl1/2t0XFgNhYHaP66SeYP1/Le6QPm2zcWO8W2rXTzOlNmlh8fzGmoAvTnyLSyjl3D1BKRF52zq0QkQjfm5h3TJgKJosW6c16UpKGkF97rb8tKmZER+uH8MsvOj71++8qXt6ULQsXXqhFEtu21aVaNf/Ya5x1CrowLQeGAGOAO0RkrXNutYg097WB+YEJU8Hl5ZfhkUf0eZ8++rpBA//aVGyJj4cVK9Sr+vVXfdy16/T9atdOFamLLtLuwJCQs2+v4XMKujB1BIYBi0TkJedcPeB+EbnX1wbmByZMBZfkZA2EGD0aYmJ0rKl7d6hRQyOdL71UvSrDT+zdq57Ub79p19+SJXDiRNp9SpSAiAgVqXbt9LFOHctFVQQo0MKU5g3OBQBlRCTKNyblPyZMBZ89e2DECJg0SXOeetOjh07IbdLEL6YZ3iQlaRLa33/XZfFirXeS/kOrVi1VqNq1g8hILU1vFCoKtDA556YC/waS0MCHcsCbIvKKb83LH0yYCg+bN2si7r//hi1b4P33NeI5MFDLarz8so3FFziiojT6b/FiXX77LW0tKtAPsEWL1LGqNm10jpV9mAWagi5MK0Qkwjl3E9AKeBRYJiItfG1gfmDCVHjZvx9GjoQJE7Tb75FH4MUX/W2VkSUieoeRIlSLF+vdhnd2X4By5aB1a10uvFAfa9e2LsACREEXprVABDAVeFtEFjjnVopIuK8NzA9MmAo/332nc58SE+Htt7VqrlGIOHECli3T7r8//tAlo8CKypW12y8yUoMqIiNtvMqPFHRhuhd4BFgJXAnUAj4WkQ6+NS9/MGEqGkyaBAMHau/PrFnQs6e/LTLyxN69GkyxdGnq48GDp+9XoYKKVMoSGamhm9YN6HMKtDBl+EbngkQk8cx7+g9L4lr0ePppGDVKo5Nnz9YUcEYRQUS9qGXLdPnzT33cv//0fcuU0Rna3p7VBRdYPZV8pkALk3OuPDASuNSzagHwjIgc86Ft+YZ5TEUHEfi//4N331Vx+vprLUJoFFFENGQzRaSWL9fHv/8+fd/SpTUXoLd31bQplCx59u0uIhR0YfocWAN86Fl1CxAuIn19aFu+YcJUtEhOVnGaMEHF6csvLUt5sWPfPhWrlGXZMtix4/T9SpbUmlUtW6pQtWyp0YEWup4tCrownZZ+yFISGf4kORmGDFHPKSAArr5aAyK6dLGhh2LLoUOpHtXy5SpYGXXhOweNGql3FRGhS3g4VK9uQRbpKOjCtBgYLiILPa/bA6+KSDsf25cvmDAVTZKT4bHHYMwYTaQNOibety/07q3TZUykijlRUVoK5M8/VaxWrNAJwemT14JmVg8PT11atNBZ3cHBZ9/uAkJBF6ZwYDKQUrzlCHCbiKzyoW35hglT0WbfPi3b/s47mq08hfPP13VXXeU/24wCSFycitPKlbosXw6rVsHRo6fvGxSkGddThKpFC31+7rnFwrsq0MJ0amfnygGISJRz7n4RecNnluUjJkzFg8RETZT95Ze6bN+u6x98UPPx2Ri4kSkpEYEpYrVqlT5u2nR6uiVQ7ypFqFLEqmnTIpfMtlAIU5o3OrdTRApF7VETpuJHcjK89ppWy01M1Aw4s2bBeef52zKjUBEdnepdrVqVumTkXQUG6thV8+YqVs2b61KIJwgXRmHaJSI189ken2DCVHxZvBj694edO3Way/z52gtjGLkmxbtK8apWr9bnf/11etol0JpWzZqpSHk/FoJqwYVRmMxjMgoFBw9qtN6qVTqWPX8+nHOOv60yihyxsZp5fdWqVLFavVoHQTOiWrVUkUpZmjZVISsgFEhhcs4dBzLawaGVbIN8ZVh+YsJkHDgAnTvDmjUQFgY//miek3GWOHBABcp7WbtWuwkzonbtVKEKC9PHxo2hVKmzazcFVJiKCiZMBmhmm06d9Ka2WjWYOlXFyjDOOsnJOiE4RaTWrNFlwwatJJyegACoXz9VrFKWRo18Gs5uwuRDTJiMFPbt0zGn+fN1PHrkSHjiCY0KNgy/k5iYWpRs7VpdVq/WdUlJp+8fGAgNG6pINW2aVrDyIQzVhMmHmDAZ3iQlwTPPwLPP6jh29eowYADcfrtO0DWMAkdcnAZXpIhVipe1ZUvG4ewpgtW0qS7Dh2v9qxxiwuRDTJiMjPjhB7jnHv29p3DFFfDww9rlV0gjfI3iRGysdv+tXQvr1qWK1tatqYIVGKjjWbno8jNh8iEmTEZmiMCiRZo5Yvp0/Z2DVlF45RW47DL/2mcYuSI2Vu+41q3TulfDhuWqGRMmH2D1mIyccPAgjB+vFXIPHNDx5uef13Lu5j0ZxRF/CVORTnEpIl+LyODy5cufeWej2FOlCjz1lAZLPfFEapLYfv00F6hhGGeHIi1MhpEbSpWC556Dr76C8uU1ldFFF2m3vWEYvseEyTAy4eqrYelSjb5dv17LaPz6q7+tMoyijwmTYWRBgwYaHHH55ToG1bmzBkpklBLNMIz8wYTJMM5A+fIwe7aWc4+Lg0GD4JJLtPacYRj5jwmTYWSDoCAYNw4mT9Z0RosXQ+vWcMst+rwIB7caxlnHhMkwsolzKkQbN+q0kMBA+PhjuPhiiIjQLr6UEu+GYeQeEybDyCHlysGrr6pAPfIIVK2qFQ4GDdIcm59/bh6UYeQFEybDyCV168KLL2rNuI8/1tRkGzfCNdfoGJSFlxtG7jBhMow8EhwMN92kKcrGj9cxqF9/hZYt4bPP/G2dYRQ+TJgMI58oUUIj99avhz59NFvEddfBXXdlXhPOMIzTMWEyjHymYkUdZxo3Tr2pCRPUe/r9d39bZhiFAxMmw/ABzsGQIfDHHxoQsWkTtG8PI0bA8eP+ts4wCjYmTIbhQ1q0gCVL4KGHNFvE889DnTpapPDoUX9bZxgFExMmw/AxISFa22nBAvWaDh/WLOa1a8P996s3ZRhGKiZMhnGW6NABfvkF5s7VAoRRUfDmm9CoEfTooQljDcMwYTKMs4pzKkpz52quvdtvV4/q22/hwgs1s8TOnf620jD8iwmTYfiJli3h/fdh924YPhxKltSJuhdcAC+8YOmNjOJLkS6tnkLr1q1labp+koSEBHbv3s3Jkyf9ZJXhD0JCQqhRowYlSpTwtymnsW2bVsydNk1ft2wJH3ygefgMwx/4q7R6sRWmbdu2UbZsWSpXroxzzk+WGWcTEeHQoUMcP36cunXr+tucTPn+exg8WEu8BwXBPffAk0/q/CjDOJv4S5iKbVfeyZMnTZSKGc45KleuXOC95O7dYc0auPtuSEqCMWM0D9/bb0N8vL+tMwzfU6SFyTl3tXNuwrFjxzLbfpYtMvxNYfnMy5SBt97SSL2OHeHQIfWc6tbVuVAHD/rbQsPwHUVamETkaxEZXL58eX+bYhi5olUrmDcPZs2CsDDYs0ezR9SsqWU21q71t4WGkf8UaWEyjKKAc9C7N6xereNPV14JJ09qYcJmzeDyy3XyrmEUFUyY/Mzu3bvp1asXDRs2pH79+tx3333EZzCQsGfPHq655pozttejRw+O5jLXzahRo3j11Vcz3DZ06FAiIiJo2rQppUqVIiIigoiICGbMmJGttvNil6E4B926wezZ8NdfmouvdGkVq06dVLBWr/a3lYaRd0yY/IiI0LdvX3r37s2mTZvYuHEjJ06c4IknnkizX2JiIuedd162RGDOnDlUqFAh320dN24cK1asYM6cOdSvX58VK1awYsWKU2KZmJjoF7uKK40aafbyXbvg6aehbFmYMwfCw3XS7t69/rbQMHKPCRPoragvljMwd+5cQkJCGDhwIACBgYGMGTOGDz74gPHjx9OzZ086d+5Mly5d2L59O82aNQMgJiaG6667jqZNm9KnTx/atm1LSjh8nTp1OHjwINu3b6dJkyYMGjSIsLAwunfvTmxsLADvvfceF154IeHh4fTr14+YmJhcXbb58+fToUMHevbsSdOmTQHo3bs3kZGRhIWFMWHChFP7ZscuI+dUqqR59zZv1ii+wECYOFGj+J5/HuzSGoUREyY/snbtWiIjI9OsK1euHLVq1SIxMZE///yTGTNmsCDdAML48eOpWLEi69at49lnn2XZsmUZtr9p0yaGDh3K2rVrqVChAp9//jkAffv2ZcmSJaxcuZImTZrw/vvv5/oc/vzzT9588002btwIwAcffMCyZctYunQpY8eO5dChQ9m2y8g955yjUXzr1kGvXlqYcMQIqF9fw82tUKFRmDBhAhDxzZJHunXrRqVKlU5bv3DhQvr37w9As2bNaNGiRYbvr1u3LhGetAGRkZFs374dgDVr1tChQweaN2/OlClTWJuH0K42bdqkmaw6duxYwsPDueiii9i1axebMkidnZldRt5p2BC++EJz8bVqpV16Dz6YWmpj3z5/W2gYZ8aEyY80bdr0NG8nKiqKnTt3EhQURGhoaJ7aDw4OPvU8MDDw1DjQgAEDePvtt1m9ejUjR47M04RTbxvnz5/Pjz/+yOLFi1m5ciUtW7bMsO3M7DLyj8su0zlQX38NbdvqvKenntIw8xtvhF9/9beFhpE5Jkx+pEuXLsTExDB58mQAkpKSGDZsGAMGDKB06dKZvq99+/ZMnz4dgHXr1rE6h6FYx48fp3r16iQkJDBlypTcn0A6jh07RsWKFSldujQbNmzgt99+y7e2jZzjHFx1FSxeDD/+qF18SUnwySdaF6pdOy0Bn5Tkb0sNIy0mTH7EOcesWbP47LPPaNiwIY0aNSIkJITRo0dn+b4hQ4Zw4MABmjZtyogRIwgLCyMnk4ifffZZ2rZtS/v27WncuHFeT+MUV1xxBYmJiTRp0oRHH32Uiy66KN/aNnKPc9Cli3bxpSSKrVgRfvsNrrkGGjeGjz4ygTIKDsU2iev69etp0qSJnyzKG0lJSSQkJBASEsKWLVvo2rUrf/31FyVLlvS3aYWCwvzZ5xfR0TBpErz+OmzdqusaN4ZRo+DaayHAblkNLImrkQNiYmK45JJLCA8Pp0+fPowfP95EycgRoaEwdKhO1J00SXPwbdgA/fvrXKgZMyA52d9WGsWVIH8bYOScsmXLkt4DzE+GDh3KokWL0qy77777Ts23MooOQUFw220aEDFxIjz3nGY2v/ZaTXc0bBjccAN4xasYhs+xrjyj2GGffebExWlxwtGjtbIuQLVqmv5o8GA491z/2mecXawrzzAMvxMcDP/3f5pJYuJEaNFC5z6NHAm1aqn3tHBhvkzTM4xMMWEyDOM0goNhwABYsQJ++kmzmyclwaefQocO0KaNloC3KWiGLzBhMgwjU5yDzp21HtS2bfD441Clik7e7d8fGjTQyrqWk8/IT0yYDMPIFrVqaWLYnTvhnXc0w/mOHVpZt04deOklyKRYtGHkCBMmP+Kc4+abbz71OjExkapVq3LVVVflqJ2UzN0AF1988an1w4cPJywsjOHDh/POO++cyjCRE44ePcr48eNPvc5uXajs0rZtWyIiIqhVqxZVq1Y9VecpO/nz8tsWI3uUKgV33QXr18PMmRAZCfv3w6OPasqjhx7SchyGkWtEpMgvkZGRkp5169adtu5sExoaKuHh4RITEyMiInPmzJHw8HC58sorc9RO7dq15cCBA6etL1eunCQmJubJxm3btklYWFie2sgOEydOlKFDh562PiEhId+PVRA++6JEcrLId9+JdOqUmsE4KEjk+utFFizQ7UbhBFgqfvjPNo8Jv5VjArSy6zfffAPAJ598wg033HBq2+HDh+nduzctWrTgoosuYtWqVQAcOnSI7t27ExYWxp133ol4hUiVKVMGgJ49e3LixAkiIyOZNm1amuq0mzdvpmvXroSHh9OqVSu2bNnCiRMn6NKlC61ataJ58+Z8+eWXADz66KNs2bKFiIgIhg8fnqYu1MmTJxk4cCDNmzenZcuWzJs3D4BJkybRt29frrjiCho2bMjDDz+co89j1KhR3HLLLbRv355bbrmF7du306FDB1q1akWrVq341ZOB1NuWvB7TyD3OQffuMG+ejj3dcIPK07Rp0LEjNG+uRQ2jovxtqVFo8Icanu3lTB6Tr+penInQ0FBZuXKl9OvXT2JjYyU8PFzmzZt3ymO6++67ZdSoUSIi8tNPP0l4eLiIiNxzzz3y9NNPi4jI7NmzBTjlMYWGhqZpP4WRI0fKK6+8IiIibdq0kZkzZ4qISGxsrERHR0tCQoIcO3ZMREQOHDgg9evXl+Tk5NM8Ju/Xr776qgwcOFBERNavXy81a9aU2NhYmThxotStW1eOHj0qsbGxUqtWLdm5c2eW18LbYxo5cqS0atXqlCcZHR0tsbGxIiKyceNGSfk8vW3JyTHNY/I9u3aJPPWUyLnnpv4eQkNF7rpLZNUqf1tnZBfMY/IfvpKm7NCiRQu2b9/OJ598Qo8ePdJsW7hwIbfccgsAnTt35tChQ0RFRfHzzz+fGpu68sorqVixYrbP9fjx4/z999/06dMHgJCQEEqXLo2I8Pjjj9OiRQu6du3K33//zb4zFO9ZuHDhKTsaN25M7dq1TxUM7NKlC+XLlyckJISmTZuyY8eObNsI6vGVKlUKgISEBAYNGkTz5s259tprWbduXYbvyesxjfyjRg0t+b5jh3pOnTppfr5339W5UV27wuzZlvbIyBgTpgJAz549eeihh9J0451tpkyZwoEDB1i2bBkrVqygWrVqearTlNea+DfJSwAAF3BJREFUS951nsaMGUO1atVYuXIlS5cuJT4+3ifHNPKfkiXhuuu0m2/tWi3/Hhqqc6OuvloTx44fbxV2jbSYMBUAbr/9dkaOHEnz5s3TrO/QocOpeknz58+nSpUqlCtXjksvvZSpU6cC8O2333LkyJFsH6ts2bLUqFGDL774AoC4uDhiYmI4duwY55xzDiVKlGDevHmnvI2yZcty/PjxDNvytm/jxo3s3LmTCy64IGcnnw2OHTtG9erVCQgI4KOPPiLJ6jMUSpo21fLvu3fDa69B7dqwaZMmk61VS8tx7NzpbyuNgoAJUwGgRo0a3HvvvaetHzVqFMuWLaNFixY8+uijfPjhhwCMHDmSn3/+mbCwMGbOnEmtWrVydLyPPvqIsWPH0qJFCy6++GL++ecfbrrpJpYuXUrz5s2ZPHnyqTpNlStXpn379jRr1ozhw4enaWfIkCEkJyfTvHlzrr/+eiZNmpTGa8kvhgwZwocffkh4eDgbNmzIc2Vfw79UqKDl3jdvhunTtcLu4cPw4oua5bxvX/WorJuv+GJJXI1ih332BY/Fi9Wb+uyz1DRHDRrAoEGaGumcc/xqXrHFkrgahlFsadcOpk7Vrrynn9aJups3wyOPaCBF//4wf74ljy0umDAZZ42ULA/ey+rVq/1tllGAqF4dnnpK8/LNnq0BEklJGtl32WXQpAmMHWupj4o61pVnFDvssy9c7NoF//0vvPce7N2r60JD4eabNXAiXcyQkY9YV55hGEYG1KyZOidqxgz1nLznRF16qXpUmcwiMAohJkyGYRQKSpSAfv1g7lwt/z5kCJQpA7/8omNQNWrA8OHgmeNtFGIKrTA5525wzh3wtx2GYZx9wsI0/96ePfrYvDkcOACvvgoXXKA5+iZPhpgYf1tq5IZCKUzOuUDgWsCS6xtGMaZsWfWcVq6E336D22+H0qXh55/htts0mGLIEPjzT39bauSEQilMwA3AZ0Chn4K3e/duevXqRcOGDalfvz733Xdfhil3slt7qEePHhw9ejRXtnhnIE/P0KFDiYiIoGnTppQqVepUVN2MGTOy1Xb6uk6GkZ84pxN1339fAyQmTNDXUVHwn/9ozajISB2XsiznBR+fCpNz7m7n3FLnXJxzblK6bZWcc7Occ9HOuR3OuRuz2WYgcB0wLZ+NzXyZMCF1vwkT8l7rwoOI0LdvX3r37s2mTZvYuHEjJ06c4IknnkizX2JiIuedd162RGDOnDlUqFAhR3Zkh3HjxrFixQrmzJlD/fr1WbFiBStWrMh2oT4TJuNsUa6cTsz97TdYtQruvRcqVlSv6d//hvPO0+1Llti8qIKKrz2mPcBzwAcZbBsHxAPVgJuA/zjnwgCcc+c65+ZnsJwL3AxMF5FC7y3NnTuXkJAQBg4cCGji0TFjxvDBBx8wfvx4evbsSefOnenSpUua2kMxMTFcd911NG3alD59+tC2bVtSwuFTqtlu376dJk2aMGjQIMLCwujevTuxsbEAvPfee1x44YWEh4fTr18/YnLZER8dHc3tt99OmzZtaNmy5akaTmvXrqVNmzZERETQokULNm3adFpdJ8M4GzRvDm++qWNRH3+sEXzR0Rp+3qYNtGwJb78NOUg3aZwNzkZtDVScJnm9DkVFqZHXuo+AF7PR1kvA98D/gGPA2Ez2GwwsBZbWqlXrtDojBaEmz5tvvin333//aesjIiLkzTfflPPPP18OHTokImlrD73yyisyePBgERFZvXq1BAYGypIlS0QktZrttm3bJDAwUJYvXy4iItdee6189NFHIiJy8ODBU8d64oknZOzYsSKStmZTZnjb8dhjj51q88iRI9KwYUM5ceKE3H333fLxxx+LiEhcXJzExMSctUq42aEgfPaG/1i/XuTBB0UqV04tUhMcLNK/v8j334skJfnbwoIDxaweUyMgUUS8AztXAmFneqOIPCIi3UXkCmCTiJye/VT3myAirUWkddWqVfPH6rNMt27dqFSp0mnrFy5cSP/+/QFo1qwZLVq0yPD9devWJSIiAoDIyEi2b98OwJo1a+jQoQPNmzdnypQprF27Nlf2ff/997z44otERETQqVMnTp48yc6dO2nXrh2jR4/mpZdeYseOHafqKhlGQaBxY81u/vffOv+pa1edA/Xpp1qJt04dePJJTYlk+Ad/CVMZIP0Q5DGgbE4aET/MSM5PmjZtyrJly9Ksi4qKYufOnQQFBeU5i3Zm9YkGDBjA22+/zerVqxk5cmSu6y6JCJ9//vmp8aadO3fSpEkTbrzxRr766itKlSpFjx49mDt3bp7OwzB8QXCw1or64QdNgfT00ypKu3bBc89Bw4ba9TdxIpw44W9rixf+EqYTQLl068oBGRf+KaJ06dKFmJgYJk+eDEBSUhLDhg1jwIABlC5dOtP3tW/fnunTpwOwbt26HOebO378ONWrVychIeFUPaXccPnll/PWW2+ldJ2yfPlyALZu3Uq9evW499576dWrF6tWrcqyrpNh+JvatTVH35YtWtTwtts07PyXXzQE/dxz4Y47YNEiC5g4G/hLmDYCQc65hl7rwoHc9SkVUpxzzJo1i88++4yGDRvSqFEjQkJCGD16dJbvGzJkCAcOHKBp06aMGDGCsLAwypcvn+3jPvvss7Rt25b27dufqruUG5588kkSEhJo0aIFYWFhPPnkkwBMnz6dZs2aERERwZo1a7j11luzrOtkGAWFgAAtAz9pEvzzj4aft2+vARMffACXXKKJZF96SQMqDN/g0ySuzrkgIAgYCdQABqFjS4nOuU8BAe7k/9u7/+CqyjOB49/HEL0aCD+GyOwYfkSKsubHDWQrTjQoEJGCsiA6KEhXrBYF0WlrqGNdQdrdanW2u7YiSpEABjbSSXEVwe0gPxoZqcQxtAIypBUMKxrDz5DwM8/+8Z6Em0BiEm5ybu59PjN34J5zc+/75OTe577vec/7QCbwLpCtqmFPTtG2iOvZs2c5ffo0gUCAsrIycnNz+eyzz7j00kv9blqn0JmPvfHHZ5+5ZLV06bmFZC+5BG67zdWLGj8eAgE/W9g+onUR16eBGuBJ3DTvGm8bwEzgcuBrYCXwSLiTkojcISKvHYmyNfKrq6u56aabCAaDTJw4kQULFlhSMqYdXXst/PKXrl7U22+7Nfvi4mDtWpg82a0w8fDDsGWLDfWFg5W9MOeZNWsWH3zwQYNtjz/+eP31Vp2dHXsTDt9844obLlsGoXOYvvMd+P73XVmOlBT/2hcOfvWYLDGZmGPH3oTbX//qElRBQcNzTzff7CZS3HWXW9evs4nWoTxjjIl6aWnwq1+5ob733oMpU+Dyy2HTJjerr08f14P64x9dRV7TPEtMxhgTJnFx7iLdgoJzs/qGD4eaGrdt9Gg3NX3OHGjlVR4xxRKTMca0g8RE11vatMldH/Xss3D11W7FiRdecNV3MzNdDan9+/1ubWSxxGSMMe3s6qvdBbx79kBxMcyY4VY8Ly11VXf79nVLI+XnW1kOiPLEFOnTxUWE++67r/7+mTNnSEpK4vbbb2/V89StKA6QnZ1dvz0vL4/U1FTy8vJYuHBh/QoTrdG4XEVL60K11LBhw8jMzKRfv34kJSXV13mqW9fv29SV4jCmMxBxF+wuXOiuhyoqclPP4+Nh/XqYPt2dj5o82U1Lv0Bpttjgx8qxHX3Lyso6b9XcxitM160yfKHbq6+ee9yrrzb/2NZISEjQYDCo1dXVqqr67rvvajAY1HHjxrXqeepWFG8sMTFRz5w507pGNdJRq4IvWbJEZ82a1SE/Z6uLm0hz8KDqokWqt9zS8POkVy/VRx5RLS5Wra3t+HYRY6uLG8/YsWNZs2YNACtXruTee++t33fw4EEmTJhARkYGN9xwA9u3bwegsrKS0aNHk5qayoMPPli/Vh1A165dARg/fjxVVVVkZWVRWFjYoDrtnj17yM3NJRgMMnToUMrKyqiqqmLUqFEMHTqU9PT0+tpKjesohdaFOnHiBNOnTyc9PZ0hQ4awYcMGAPLz87nzzjsZM2YMgwYNYs6cOa36nZSVlTFmzBiysrLIyclh165dAKxatYq0tDSCwSDDhw/n1KlTPPPMMxQWFpKZmUlhYXhrRxrTUXr2hAcfdOv07d0Lzz3nZvodPOgq8N50kxsO/NnPYMcOv1vbAfzIhh19a0mPyQ8JCQlaWlqqkyZN0pqaGg0Gg7phw4b6HtOjjz6q8+bNU1XV9evXazAYVFXV2bNn67PPPquqqu+8844C9T2mhISEBs9fJ7TW0vXXX69FRUWqqlpTU6PHjx/X06dP65EjR1RVtaKiQgcOHKi1tbXn9ZhC77/44os6ffp0VVXduXOn9u3bV2tqanTJkiWakpKihw8f1pqaGu3Xr5/u27ev2d9FaM9n5MiRunv3blVV/fDDD3XEiBGqqpqWlqbl5eWq6uo/Nf65loqEY29MS5SWqublqV51VcOeVDCo+vzzqnv3tu/rYz2m2JSRkcHnn3/OypUrGTt2bIN9xcXFTJs2DYCRI0dSWVnJ0aNH2bx5c/25qXHjxtGzZ88Wv96xY8fYv38/EydOBCAQCHDFFVegqjz11FNkZGSQm5vL/v37+eqrr5p9ruLi4vp2DB48mP79+7N7tyuxNWrUKLp3704gEOC6665j7969LWpfVVUVW7Zs4e677yYzM5MZM2bwpbc42Y033sj999/PokWLOGsXg5gYkJHhro/auxfef9+tcN69u5s08dOfuqnnOTmuV+WdZo4KXfxugHHDbk888QQbN26ksrLSlzYUFBRQUVFBSUkJ8fHxDBgwoM11mqDpWlDfpra2lh49evDJJ5+ct2/hwoVs3bqVNWvWkJWVdV4tK2OiVVwcjBjhbi+/7NboW7HCTZAoLna32bPh1lvh3nthwgQ3Xb2zsh5TBHjggQeYO3cu6enpDbbn5OTU10vauHEjvXv3JjExkeHDh7NixQoA1q5dy6FDh1r8Wt26dSM5OZnVq1cDcPLkSaqrqzly5AhXXnkl8fHxbNiwob6H01wdpdD27d69m3379nHttde2LvhGEhMTSUlJYdWqVYAbai4tLQXcuadhw4Yxf/58kpKS+OKLL6zOk4k5l13mEs+bb8LXX8Py5TB2rJvxt26dWwKpTx+4+2436+8ivl/6JqoTU6RPF6+TnJzMY4+dXyF+3rx5lJSUkJGRwZNPPsnSpUsBmDt3Lps3byY1NZWioiL69evXqtdbvnw5L730EhkZGWRnZ3PgwAGmTp3Ktm3bSE9PZ9myZfV1mpqrozRz5kxqa2tJT09n8uTJ5OfnN+gptVVBQQGLFy8mGAySmppaPxEjLy+P9PR00tLSyM7OJhgMMmLECHbs2GGTH0xM6tbNLXW0Zo2bfv7KK26liRMn4Pe/d1PR+/SBFl59ETFsEVcTc+zYm2hXXg6FhW647/Bhd2GvSOufxxZxNcYYExbJyfCTn7hyHCUlbUtKfrLJD6bDDBs2jJMnTzbYtnz58vPOrRljwqdHD79b0HoxnZhUFelsXyU6sa1bt/rdBGJh6NqYzi5mh/ICgQCVlZX2QRVDVJXKykoCgYDfTTHGNCNme0zJycmUl5dTUVHhd1NMBwoEAiQnJ/vdDGNMM2I2McXHx5OSkuJ3M4wxxjQS1UN5neU6JmOMMedEdWJS1bdV9Yfdu3f3uynGGGNaKKoTkzHGmM4nJlZ+EJEKoGXLW19YbyCK1u5tkViMGWIz7liMGWIz7tbG3F9Vk9qrMU2JicR0sURkmx/LcvgpFmOG2Iw7FmOG2Iy7s8RsQ3nGGGMiiiUmY4wxEcUSU8u85ncDfBCLMUNsxh2LMUNsxt0pYrZzTMYYYyKK9ZiMMcZEFEtMxhhjIoolJmOMMRHFElMzRKSXiPxBRI6LyF4RmeJ3m8JJRC4TkcVebMdE5BMR+V7I/lEisktEqkVkg4j097O97UFEBonICRF5I2TbFO93clxEVotILz/bGE4ico+I7PRiKxORHG971B5rERkgIu+KyCEROSAivxWRLt6+TBEp8eIuEZFMv9vbFiLyqIhsE5GTIpLfaF+Tx9b7DHhdRI56v5sfd3jjL8ASU/NeBk4BfYCpwCsikupvk8KqC/AFcDPQHXgaeNN7I/cGioB/BXoB24BCvxrajl4GPqq74x3fV4FpuONeDSzwp2nhJSK3As8D04FuwHDgbzFwrBcAXwP/AGTi/t5nisilwFvAG0BPYCnwlre9s/k/4BfA66EbW3Bs5wGDgP7ACGCOiIzpgPY2T1XtdoEbkIBLSteEbFsOPOd329o57u3AJOCHwJZGv48aYLDfbQxjrPcAb+LenG942/4dWBHymIHe30E3v9sbhni3AD+4wPaoPtbATmBsyP0XcF8+RgP78WYne/v2AWP8bvNFxPoLIL+lxxaX0EaH7P858N9+x2E9pqZdA5xR1d0h20qBaOoxNSAifXBxf4qLs7Run6oeB8qIkvhFJBGYDzQeumgcdxneF5SOa134iUgc8E9AkojsEZFyb0jrcqL8WAP/CdwjIleIyFXA94B1uPi2q/eJ7NlO9MQNzRxbEemJ60WWhjw+Ij7jLDE1rStwtNG2I7ghkKgjIvFAAbBUVXfh4m9cyCqa4v85sFhVyxttj9a4+wDxwF1ADm5Iawhu+DZaY66zGfdhexQoxw1nrSb644bmY+wacr/xPl9ZYmpaFZDYaFsicMyHtrQrEbkEN0x5CnjU2xy18XsnuHOBX19gd7TGXeP9+xtV/VJVvwH+AxhL9MZc97e9DneeJQG3unZP3Lm2qI07RHMxVoXcb7zPV5aYmrYb6CIig0K2BXHDXFFDRARYjPtGPUlVT3u7PsXFW/e4BNz5lmiI/xZgALBPRA4ATwCTRORjzo/7auAy3N9Dp6Wqh3C9hdBhq7r/R/Ox7gX0A36rqidVtRJYgkvInwIZ3nugTgbREXedJo+t9zfxZeh+IuQzzhJTE7yx2CJgvogkiMiNwD/jehbR5BXgH4E7VLUmZPsfgDQRmSQiAeAZ3Hj8Lj8aGWav4d6cmd5tIbAGuA03nHmHiOR4b+L5QJGq+v4tMgyWALNF5Erv/MKPgHeI4mPt9Qz/DjwiIl1EpAfwL7hzSRuBs8Bj3rTputGC931p7EXwYgsAcUCciAS8KfHfdmyXAU+LSE8RGQw8BOT7EEJDfs++iOQb7tvWauA4brbOFL/bFOb4+uO+NZ/AdevrblO9/bnALtww0EZggN9tbqffwzy8WXne/Sne8T6Om07cy+82hinOeNzU6cPAAeAlIBDtxxr35WMjcAhXJO9NoI+3bwhQ4sX9MTDE7/a2McZ53ns59Dbv244tbjTgddz5t6+AH/sdi6raIq7GGGMiiw3lGWOMiSiWmIwxxkQUS0zGGGMiiiUmY4wxEcUSkzHGmIhiickYY0xEscRkTCuJSJX374Boq9FlTCSwxGRM2w3AXYzbYnUF6owxTbPEZEzbPQfkeJV/fyQicSLygoh8JCLbRWQGgIjcIiJ/EpH/AXY0fhIRqRKRfxORUhH50Cs/gojki8hdoY8Leb5NIvKWiPxNRJ4Tkaki8mcR+YuIDOyY8I1pH5aYjGm7J4E/qWqmqv4a+AFwRFW/C3wXeEhEUrzHDgUeV9UL1XVKAD5U1SCuRMNDLXjtIPAwbp3DabiCltcDvwNmX0xQxvjNhhWMCZ/RuNWq63o53XFlq08Bf1bVvzfxc6dwi6mCW7ft1ha81keq+iWAiJQB/+tt/wuuRLYxnZYlJmPCR4DZqvpeg40it+AWhG3KaT23aOVZzr0vz+CNanh1hS4N+ZmTIf+vDblfi72vTSdnQ3nGtN0xGlb7fA9XXiEeQESu8UpntNXnQJb3//G41cGNiXr2zcqYttsOnBWRUlwNm//CzdT72Cs+VwFMuIjnXwS85T3/OprvdRkTNazshTHGmIhiQ3nGGGMiiiUmY4wxEcUSkzHGmIhiickYY0xEscRkjDEmolhiMsYYE1EsMRljjIko/w+53UhFWDtirAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"8cBPLBMfxIQ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597606505409,"user_tz":-120,"elapsed":1363,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"0dea908a-0985-4f89-bd3b-ac250f7c302a"},"source":["loss_no_scale_no_bn['train'][-1]"],"execution_count":166,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.2898e-07)"]},"metadata":{"tags":[]},"execution_count":166}]},{"cell_type":"code","metadata":{"id":"X4r2j_F4xOs1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597606541501,"user_tz":-120,"elapsed":934,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"c6b52b17-9aa4-4842-e7f0-e2ac1cd85dc9"},"source":["loss_no_scale_no_bn_['train'][-1]"],"execution_count":167,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(8.8039e-05)"]},"metadata":{"tags":[]},"execution_count":167}]},{"cell_type":"markdown","metadata":{"id":"TDu0Vvv6cT1T","colab_type":"text"},"source":["# NTK scaling + batch norm"]},{"cell_type":"code","metadata":{"id":"j-Grp5VSDkPb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597611765836,"user_tz":-120,"elapsed":329264,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"10cbedfc-9c31-494f-d970-5c23099d2785"},"source":["# 1.2 0.53. runstep=2\n","maxstep=1000\n","for scale in range(10):\n","    scale=(scale-2)*0.05+0.9\n","    for therd in range(10):\n","        therd=(therd-2)*0.05+0.7\n","        print('scale:{:03f},therd:{:03f}'.format(scale,therd))\n","        step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(scale,therd,2,False,initialize = 'NTK', batchnorm = True, learning_rate = 0.1 * np.sqrt(784), ** shared_model_param_dict)\n","        if maxstep>train_loss:\n","            maxstep=train_loss\n","            sb=scale\n","            th=therd\n","print(maxstep,sb,th)"],"execution_count":197,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Iter: 005/100 | Train Loss: 0.00164885\n","Adjusting Layer 1, Kernel Nodes: 776, Adptive Nodes:24\n","Iter: 006/100 | Train Loss: 0.00165815\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00144338\n","Iter: 008/100 | Train Loss: 0.00123808\n","Iter: 009/100 | Train Loss: 0.00115471\n","Iter: 010/100 | Train Loss: 0.00117768\n","Adjusting Layer 1, Kernel Nodes: 345, Adptive Nodes:455\n","Iter: 011/100 | Train Loss: 0.00114400\n","Iter: 012/100 | Train Loss: 0.00106154\n","Iter: 013/100 | Train Loss: 0.00094575\n","Iter: 014/100 | Train Loss: 0.00086654\n","Iter: 015/100 | Train Loss: 0.00084170\n","Iter: 016/100 | Train Loss: 0.00083000\n","Iter: 017/100 | Train Loss: 0.00079124\n","Iter: 018/100 | Train Loss: 0.00072934\n","Iter: 019/100 | Train Loss: 0.00067556\n","Iter: 020/100 | Train Loss: 0.00064686\n","Iter: 021/100 | Train Loss: 0.00063260\n","Iter: 022/100 | Train Loss: 0.00061369\n","Iter: 023/100 | Train Loss: 0.00058450\n","Iter: 024/100 | Train Loss: 0.00055364\n","Iter: 025/100 | Train Loss: 0.00052997\n","Iter: 026/100 | Train Loss: 0.00051389\n","Iter: 027/100 | Train Loss: 0.00050016\n","Iter: 028/100 | Train Loss: 0.00048479\n","Iter: 029/100 | Train Loss: 0.00046767\n","Iter: 030/100 | Train Loss: 0.00045060\n","Iter: 031/100 | Train Loss: 0.00043519\n","Iter: 032/100 | Train Loss: 0.00042218\n","Iter: 033/100 | Train Loss: 0.00041102\n","Iter: 034/100 | Train Loss: 0.00040005\n","Iter: 035/100 | Train Loss: 0.00038791\n","Iter: 036/100 | Train Loss: 0.00037502\n","Iter: 037/100 | Train Loss: 0.00036320\n","Iter: 038/100 | Train Loss: 0.00035345\n","Iter: 039/100 | Train Loss: 0.00034484\n","Iter: 040/100 | Train Loss: 0.00033575\n","Iter: 041/100 | Train Loss: 0.00032577\n","Iter: 042/100 | Train Loss: 0.00031595\n","Iter: 043/100 | Train Loss: 0.00030735\n","Iter: 044/100 | Train Loss: 0.00029988\n","Iter: 045/100 | Train Loss: 0.00029252\n","Iter: 046/100 | Train Loss: 0.00028469\n","Iter: 047/100 | Train Loss: 0.00027680\n","Iter: 048/100 | Train Loss: 0.00026948\n","Iter: 049/100 | Train Loss: 0.00026284\n","Iter: 050/100 | Train Loss: 0.00025651\n","Iter: 051/100 | Train Loss: 0.00025015\n","Iter: 052/100 | Train Loss: 0.00024382\n","Iter: 053/100 | Train Loss: 0.00023780\n","Iter: 054/100 | Train Loss: 0.00023215\n","Iter: 055/100 | Train Loss: 0.00022672\n","Iter: 056/100 | Train Loss: 0.00022141\n","Iter: 057/100 | Train Loss: 0.00021624\n","Iter: 058/100 | Train Loss: 0.00021130\n","Iter: 059/100 | Train Loss: 0.00020655\n","Iter: 060/100 | Train Loss: 0.00020190\n","Iter: 061/100 | Train Loss: 0.00019735\n","Iter: 062/100 | Train Loss: 0.00019300\n","Iter: 063/100 | Train Loss: 0.00018888\n","Iter: 064/100 | Train Loss: 0.00018488\n","Iter: 065/100 | Train Loss: 0.00018090\n","Iter: 066/100 | Train Loss: 0.00017698\n","Iter: 067/100 | Train Loss: 0.00017324\n","Iter: 068/100 | Train Loss: 0.00016969\n","Iter: 069/100 | Train Loss: 0.00016623\n","Iter: 070/100 | Train Loss: 0.00016278\n","Iter: 071/100 | Train Loss: 0.00015937\n","Iter: 072/100 | Train Loss: 0.00015609\n","Iter: 073/100 | Train Loss: 0.00015296\n","Iter: 074/100 | Train Loss: 0.00014992\n","Iter: 075/100 | Train Loss: 0.00014691\n","Iter: 076/100 | Train Loss: 0.00014393\n","Iter: 077/100 | Train Loss: 0.00014104\n","Iter: 078/100 | Train Loss: 0.00013825\n","Iter: 079/100 | Train Loss: 0.00013555\n","Iter: 080/100 | Train Loss: 0.00013290\n","Iter: 081/100 | Train Loss: 0.00013029\n","Iter: 082/100 | Train Loss: 0.00012775\n","Iter: 083/100 | Train Loss: 0.00012529\n","Iter: 084/100 | Train Loss: 0.00012289\n","Iter: 085/100 | Train Loss: 0.00012054\n","Iter: 086/100 | Train Loss: 0.00011824\n","Iter: 087/100 | Train Loss: 0.00011599\n","Iter: 088/100 | Train Loss: 0.00011380\n","Iter: 089/100 | Train Loss: 0.00011166\n","Iter: 090/100 | Train Loss: 0.00010957\n","Iter: 091/100 | Train Loss: 0.00010754\n","Iter: 092/100 | Train Loss: 0.00010555\n","Iter: 093/100 | Train Loss: 0.00010360\n","Iter: 094/100 | Train Loss: 0.00010169\n","Iter: 095/100 | Train Loss: 0.00009983\n","Iter: 096/100 | Train Loss: 0.00009801\n","Iter: 097/100 | Train Loss: 0.00009623\n","Iter: 098/100 | Train Loss: 0.00009449\n","Iter: 099/100 | Train Loss: 0.00009278\n","\n","Iter: 099/100 | Test Loss: 0.00103701 | Test acc: 64.6900\n","scale:1.050000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00238212\n","Iter: 001/100 | Train Loss: 0.00199860\n","Iter: 002/100 | Train Loss: 0.00161786\n","Iter: 003/100 | Train Loss: 0.00145146\n","Iter: 004/100 | Train Loss: 0.00151827\n","Adjusting Layer 1, Kernel Nodes: 702, Adptive Nodes:98\n","Iter: 005/100 | Train Loss: 0.00164308\n","Adjusting Layer 1, Kernel Nodes: 714, Adptive Nodes:86\n","Iter: 006/100 | Train Loss: 0.00168092\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 007/100 | Train Loss: 0.00154919\n","Iter: 008/100 | Train Loss: 0.00135257\n","Iter: 009/100 | Train Loss: 0.00119703\n","Iter: 010/100 | Train Loss: 0.00114830\n","Iter: 011/100 | Train Loss: 0.00116791\n","Adjusting Layer 1, Kernel Nodes: 566, Adptive Nodes:234\n","Iter: 012/100 | Train Loss: 0.00117643\n","Adjusting Layer 1, Kernel Nodes: 623, Adptive Nodes:177\n","Iter: 013/100 | Train Loss: 0.00112708\n","Iter: 014/100 | Train Loss: 0.00099754\n","Iter: 015/100 | Train Loss: 0.00089970\n","Iter: 016/100 | Train Loss: 0.00086749\n","Iter: 017/100 | Train Loss: 0.00086767\n","Adjusting Layer 1, Kernel Nodes: 304, Adptive Nodes:496\n","Iter: 018/100 | Train Loss: 0.00086183\n","Iter: 019/100 | Train Loss: 0.00079992\n","Iter: 020/100 | Train Loss: 0.00072790\n","Iter: 021/100 | Train Loss: 0.00068010\n","Iter: 022/100 | Train Loss: 0.00066132\n","Iter: 023/100 | Train Loss: 0.00065131\n","Iter: 024/100 | Train Loss: 0.00063252\n","Iter: 025/100 | Train Loss: 0.00060267\n","Iter: 026/100 | Train Loss: 0.00057031\n","Iter: 027/100 | Train Loss: 0.00054403\n","Iter: 028/100 | Train Loss: 0.00052581\n","Iter: 029/100 | Train Loss: 0.00051181\n","Iter: 030/100 | Train Loss: 0.00049841\n","Iter: 031/100 | Train Loss: 0.00048379\n","Iter: 032/100 | Train Loss: 0.00046755\n","Iter: 033/100 | Train Loss: 0.00045068\n","Iter: 034/100 | Train Loss: 0.00043491\n","Iter: 035/100 | Train Loss: 0.00042188\n","Iter: 036/100 | Train Loss: 0.00041163\n","Iter: 037/100 | Train Loss: 0.00040237\n","Iter: 038/100 | Train Loss: 0.00039197\n","Iter: 039/100 | Train Loss: 0.00037989\n","Iter: 040/100 | Train Loss: 0.00036758\n","Iter: 041/100 | Train Loss: 0.00035708\n","Iter: 042/100 | Train Loss: 0.00034889\n","Iter: 043/100 | Train Loss: 0.00034175\n","Iter: 044/100 | Train Loss: 0.00033396\n","Iter: 045/100 | Train Loss: 0.00032494\n","Iter: 046/100 | Train Loss: 0.00031557\n","Iter: 047/100 | Train Loss: 0.00030713\n","Iter: 048/100 | Train Loss: 0.00030007\n","Iter: 049/100 | Train Loss: 0.00029383\n","Iter: 050/100 | Train Loss: 0.00028741\n","Iter: 051/100 | Train Loss: 0.00028044\n","Iter: 052/100 | Train Loss: 0.00027328\n","Iter: 053/100 | Train Loss: 0.00026656\n","Iter: 054/100 | Train Loss: 0.00026057\n","Iter: 055/100 | Train Loss: 0.00025507\n","Iter: 056/100 | Train Loss: 0.00024967\n","Iter: 057/100 | Train Loss: 0.00024413\n","Iter: 058/100 | Train Loss: 0.00023850\n","Iter: 059/100 | Train Loss: 0.00023303\n","Iter: 060/100 | Train Loss: 0.00022792\n","Iter: 061/100 | Train Loss: 0.00022316\n","Iter: 062/100 | Train Loss: 0.00021860\n","Iter: 063/100 | Train Loss: 0.00021406\n","Iter: 064/100 | Train Loss: 0.00020952\n","Iter: 065/100 | Train Loss: 0.00020502\n","Iter: 066/100 | Train Loss: 0.00020073\n","Iter: 067/100 | Train Loss: 0.00019667\n","Iter: 068/100 | Train Loss: 0.00019280\n","Iter: 069/100 | Train Loss: 0.00018899\n","Iter: 070/100 | Train Loss: 0.00018520\n","Iter: 071/100 | Train Loss: 0.00018147\n","Iter: 072/100 | Train Loss: 0.00017783\n","Iter: 073/100 | Train Loss: 0.00017434\n","Iter: 074/100 | Train Loss: 0.00017099\n","Iter: 075/100 | Train Loss: 0.00016771\n","Iter: 076/100 | Train Loss: 0.00016445\n","Iter: 077/100 | Train Loss: 0.00016125\n","Iter: 078/100 | Train Loss: 0.00015813\n","Iter: 079/100 | Train Loss: 0.00015511\n","Iter: 080/100 | Train Loss: 0.00015218\n","Iter: 081/100 | Train Loss: 0.00014932\n","Iter: 082/100 | Train Loss: 0.00014650\n","Iter: 083/100 | Train Loss: 0.00014374\n","Iter: 084/100 | Train Loss: 0.00014104\n","Iter: 085/100 | Train Loss: 0.00013841\n","Iter: 086/100 | Train Loss: 0.00013586\n","Iter: 087/100 | Train Loss: 0.00013337\n","Iter: 088/100 | Train Loss: 0.00013093\n","Iter: 089/100 | Train Loss: 0.00012852\n","Iter: 090/100 | Train Loss: 0.00012616\n","Iter: 091/100 | Train Loss: 0.00012386\n","Iter: 092/100 | Train Loss: 0.00012163\n","Iter: 093/100 | Train Loss: 0.00011945\n","Iter: 094/100 | Train Loss: 0.00011731\n","Iter: 095/100 | Train Loss: 0.00011521\n","Iter: 096/100 | Train Loss: 0.00011315\n","Iter: 097/100 | Train Loss: 0.00011114\n","Iter: 098/100 | Train Loss: 0.00010919\n","Iter: 099/100 | Train Loss: 0.00010727\n","\n","Iter: 099/100 | Test Loss: 0.00102372 | Test acc: 64.6700\n","scale:1.050000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00236978\n","Iter: 001/100 | Train Loss: 0.00198655\n","Iter: 002/100 | Train Loss: 0.00161076\n","Iter: 003/100 | Train Loss: 0.00144934\n","Iter: 004/100 | Train Loss: 0.00151794\n","Adjusting Layer 1, Kernel Nodes: 706, Adptive Nodes:94\n","Iter: 005/100 | Train Loss: 0.00164620\n","Adjusting Layer 1, Kernel Nodes: 716, Adptive Nodes:84\n","Iter: 006/100 | Train Loss: 0.00169069\n","Adjusting Layer 1, Kernel Nodes: 690, Adptive Nodes:110\n","Iter: 007/100 | Train Loss: 0.00158279\n","Iter: 008/100 | Train Loss: 0.00140714\n","Iter: 009/100 | Train Loss: 0.00123938\n","Iter: 010/100 | Train Loss: 0.00115250\n","Iter: 011/100 | Train Loss: 0.00114538\n","Iter: 012/100 | Train Loss: 0.00116506\n","Adjusting Layer 1, Kernel Nodes: 639, Adptive Nodes:161\n","Iter: 013/100 | Train Loss: 0.00116695\n","Adjusting Layer 1, Kernel Nodes: 674, Adptive Nodes:126\n","Iter: 014/100 | Train Loss: 0.00111580\n","Iter: 015/100 | Train Loss: 0.00100833\n","Iter: 016/100 | Train Loss: 0.00092098\n","Iter: 017/100 | Train Loss: 0.00088005\n","Iter: 018/100 | Train Loss: 0.00087286\n","Iter: 019/100 | Train Loss: 0.00086708\n","Iter: 020/100 | Train Loss: 0.00084006\n","Iter: 021/100 | Train Loss: 0.00079225\n","Iter: 022/100 | Train Loss: 0.00074076\n","Iter: 023/100 | Train Loss: 0.00070193\n","Iter: 024/100 | Train Loss: 0.00067980\n","Iter: 025/100 | Train Loss: 0.00066672\n","Iter: 026/100 | Train Loss: 0.00065240\n","Iter: 027/100 | Train Loss: 0.00063185\n","Iter: 028/100 | Train Loss: 0.00060686\n","Iter: 029/100 | Train Loss: 0.00058256\n","Iter: 030/100 | Train Loss: 0.00056244\n","Iter: 031/100 | Train Loss: 0.00054678\n","Iter: 032/100 | Train Loss: 0.00053368\n","Iter: 033/100 | Train Loss: 0.00052121\n","Iter: 034/100 | Train Loss: 0.00050843\n","Iter: 035/100 | Train Loss: 0.00049525\n","Iter: 036/100 | Train Loss: 0.00048189\n","Iter: 037/100 | Train Loss: 0.00046877\n","Iter: 038/100 | Train Loss: 0.00045649\n","Iter: 039/100 | Train Loss: 0.00044571\n","Iter: 040/100 | Train Loss: 0.00043659\n","Iter: 041/100 | Train Loss: 0.00042828\n","Iter: 042/100 | Train Loss: 0.00041965\n","Iter: 043/100 | Train Loss: 0.00041014\n","Iter: 044/100 | Train Loss: 0.00040018\n","Iter: 045/100 | Train Loss: 0.00039082\n","Iter: 046/100 | Train Loss: 0.00038280\n","Iter: 047/100 | Train Loss: 0.00037597\n","Iter: 048/100 | Train Loss: 0.00036953\n","Iter: 049/100 | Train Loss: 0.00036264\n","Iter: 050/100 | Train Loss: 0.00035510\n","Iter: 051/100 | Train Loss: 0.00034744\n","Iter: 052/100 | Train Loss: 0.00034037\n","Iter: 053/100 | Train Loss: 0.00033420\n","Iter: 054/100 | Train Loss: 0.00032864\n","Iter: 055/100 | Train Loss: 0.00032312\n","Iter: 056/100 | Train Loss: 0.00031729\n","Iter: 057/100 | Train Loss: 0.00031120\n","Iter: 058/100 | Train Loss: 0.00030522\n","Iter: 059/100 | Train Loss: 0.00029966\n","Iter: 060/100 | Train Loss: 0.00029457\n","Iter: 061/100 | Train Loss: 0.00028973\n","Iter: 062/100 | Train Loss: 0.00028486\n","Iter: 063/100 | Train Loss: 0.00027989\n","Iter: 064/100 | Train Loss: 0.00027491\n","Iter: 065/100 | Train Loss: 0.00027010\n","Iter: 066/100 | Train Loss: 0.00026555\n","Iter: 067/100 | Train Loss: 0.00026123\n","Iter: 068/100 | Train Loss: 0.00025704\n","Iter: 069/100 | Train Loss: 0.00025286\n","Iter: 070/100 | Train Loss: 0.00024869\n","Iter: 071/100 | Train Loss: 0.00024458\n","Iter: 072/100 | Train Loss: 0.00024059\n","Iter: 073/100 | Train Loss: 0.00023674\n","Iter: 074/100 | Train Loss: 0.00023302\n","Iter: 075/100 | Train Loss: 0.00022938\n","Iter: 076/100 | Train Loss: 0.00022580\n","Iter: 077/100 | Train Loss: 0.00022224\n","Iter: 078/100 | Train Loss: 0.00021874\n","Iter: 079/100 | Train Loss: 0.00021533\n","Iter: 080/100 | Train Loss: 0.00021201\n","Iter: 081/100 | Train Loss: 0.00020878\n","Iter: 082/100 | Train Loss: 0.00020561\n","Iter: 083/100 | Train Loss: 0.00020249\n","Iter: 084/100 | Train Loss: 0.00019941\n","Iter: 085/100 | Train Loss: 0.00019638\n","Iter: 086/100 | Train Loss: 0.00019341\n","Iter: 087/100 | Train Loss: 0.00019051\n","Iter: 088/100 | Train Loss: 0.00018767\n","Iter: 089/100 | Train Loss: 0.00018489\n","Iter: 090/100 | Train Loss: 0.00018214\n","Iter: 091/100 | Train Loss: 0.00017944\n","Iter: 092/100 | Train Loss: 0.00017678\n","Iter: 093/100 | Train Loss: 0.00017418\n","Iter: 094/100 | Train Loss: 0.00017164\n","Iter: 095/100 | Train Loss: 0.00016914\n","Iter: 096/100 | Train Loss: 0.00016669\n","Iter: 097/100 | Train Loss: 0.00016428\n","Iter: 098/100 | Train Loss: 0.00016191\n","Iter: 099/100 | Train Loss: 0.00015957\n","\n","Iter: 099/100 | Test Loss: 0.00100912 | Test acc: 65.1300\n","scale:1.050000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00235666\n","Iter: 001/100 | Train Loss: 0.00197082\n","Iter: 002/100 | Train Loss: 0.00159951\n","Iter: 003/100 | Train Loss: 0.00144676\n","Iter: 004/100 | Train Loss: 0.00152179\n","Adjusting Layer 1, Kernel Nodes: 712, Adptive Nodes:88\n","Iter: 005/100 | Train Loss: 0.00166042\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 006/100 | Train Loss: 0.00170756\n","Adjusting Layer 1, Kernel Nodes: 713, Adptive Nodes:87\n","Iter: 007/100 | Train Loss: 0.00160754\n","Iter: 008/100 | Train Loss: 0.00142986\n","Iter: 009/100 | Train Loss: 0.00125270\n","Iter: 010/100 | Train Loss: 0.00115040\n","Iter: 011/100 | Train Loss: 0.00113192\n","Iter: 012/100 | Train Loss: 0.00115440\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 013/100 | Train Loss: 0.00117382\n","Adjusting Layer 1, Kernel Nodes: 694, Adptive Nodes:106\n","Iter: 014/100 | Train Loss: 0.00114406\n","Iter: 015/100 | Train Loss: 0.00105995\n","Iter: 016/100 | Train Loss: 0.00096754\n","Iter: 017/100 | Train Loss: 0.00089912\n","Iter: 018/100 | Train Loss: 0.00086614\n","Iter: 019/100 | Train Loss: 0.00085784\n","Iter: 020/100 | Train Loss: 0.00085338\n","Iter: 021/100 | Train Loss: 0.00083597\n","Iter: 022/100 | Train Loss: 0.00080227\n","Iter: 023/100 | Train Loss: 0.00076010\n","Iter: 024/100 | Train Loss: 0.00072078\n","Iter: 025/100 | Train Loss: 0.00069180\n","Iter: 026/100 | Train Loss: 0.00067343\n","Iter: 027/100 | Train Loss: 0.00066104\n","Iter: 028/100 | Train Loss: 0.00064885\n","Iter: 029/100 | Train Loss: 0.00063339\n","Iter: 030/100 | Train Loss: 0.00061441\n","Iter: 031/100 | Train Loss: 0.00059399\n","Iter: 032/100 | Train Loss: 0.00057480\n","Iter: 033/100 | Train Loss: 0.00055857\n","Iter: 034/100 | Train Loss: 0.00054544\n","Iter: 035/100 | Train Loss: 0.00053452\n","Iter: 036/100 | Train Loss: 0.00052436\n","Iter: 037/100 | Train Loss: 0.00051388\n","Iter: 038/100 | Train Loss: 0.00050265\n","Iter: 039/100 | Train Loss: 0.00049089\n","Iter: 040/100 | Train Loss: 0.00047916\n","Iter: 041/100 | Train Loss: 0.00046822\n","Iter: 042/100 | Train Loss: 0.00045852\n","Iter: 043/100 | Train Loss: 0.00045002\n","Iter: 044/100 | Train Loss: 0.00044233\n","Iter: 045/100 | Train Loss: 0.00043480\n","Iter: 046/100 | Train Loss: 0.00042697\n","Iter: 047/100 | Train Loss: 0.00041873\n","Iter: 048/100 | Train Loss: 0.00041039\n","Iter: 049/100 | Train Loss: 0.00040242\n","Iter: 050/100 | Train Loss: 0.00039518\n","Iter: 051/100 | Train Loss: 0.00038871\n","Iter: 052/100 | Train Loss: 0.00038271\n","Iter: 053/100 | Train Loss: 0.00037675\n","Iter: 054/100 | Train Loss: 0.00037052\n","Iter: 055/100 | Train Loss: 0.00036403\n","Iter: 056/100 | Train Loss: 0.00035759\n","Iter: 057/100 | Train Loss: 0.00035152\n","Iter: 058/100 | Train Loss: 0.00034598\n","Iter: 059/100 | Train Loss: 0.00034083\n","Iter: 060/100 | Train Loss: 0.00033581\n","Iter: 061/100 | Train Loss: 0.00033071\n","Iter: 062/100 | Train Loss: 0.00032549\n","Iter: 063/100 | Train Loss: 0.00032024\n","Iter: 064/100 | Train Loss: 0.00031517\n","Iter: 065/100 | Train Loss: 0.00031037\n","Iter: 066/100 | Train Loss: 0.00030585\n","Iter: 067/100 | Train Loss: 0.00030151\n","Iter: 068/100 | Train Loss: 0.00029720\n","Iter: 069/100 | Train Loss: 0.00029284\n","Iter: 070/100 | Train Loss: 0.00028847\n","Iter: 071/100 | Train Loss: 0.00028418\n","Iter: 072/100 | Train Loss: 0.00028004\n","Iter: 073/100 | Train Loss: 0.00027609\n","Iter: 074/100 | Train Loss: 0.00027225\n","Iter: 075/100 | Train Loss: 0.00026846\n","Iter: 076/100 | Train Loss: 0.00026470\n","Iter: 077/100 | Train Loss: 0.00026098\n","Iter: 078/100 | Train Loss: 0.00025731\n","Iter: 079/100 | Train Loss: 0.00025374\n","Iter: 080/100 | Train Loss: 0.00025026\n","Iter: 081/100 | Train Loss: 0.00024685\n","Iter: 082/100 | Train Loss: 0.00024350\n","Iter: 083/100 | Train Loss: 0.00024019\n","Iter: 084/100 | Train Loss: 0.00023691\n","Iter: 085/100 | Train Loss: 0.00023370\n","Iter: 086/100 | Train Loss: 0.00023056\n","Iter: 087/100 | Train Loss: 0.00022749\n","Iter: 088/100 | Train Loss: 0.00022447\n","Iter: 089/100 | Train Loss: 0.00022151\n","Iter: 090/100 | Train Loss: 0.00021861\n","Iter: 091/100 | Train Loss: 0.00021574\n","Iter: 092/100 | Train Loss: 0.00021292\n","Iter: 093/100 | Train Loss: 0.00021015\n","Iter: 094/100 | Train Loss: 0.00020742\n","Iter: 095/100 | Train Loss: 0.00020474\n","Iter: 096/100 | Train Loss: 0.00020211\n","Iter: 097/100 | Train Loss: 0.00019952\n","Iter: 098/100 | Train Loss: 0.00019698\n","Iter: 099/100 | Train Loss: 0.00019448\n","\n","Iter: 099/100 | Test Loss: 0.00100665 | Test acc: 64.7700\n","scale:1.050000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00234266\n","Iter: 001/100 | Train Loss: 0.00195195\n","Iter: 002/100 | Train Loss: 0.00158488\n","Iter: 003/100 | Train Loss: 0.00144418\n","Iter: 004/100 | Train Loss: 0.00152951\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00168188\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00172915\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00161204\n","Iter: 008/100 | Train Loss: 0.00140127\n","Iter: 009/100 | Train Loss: 0.00121521\n","Iter: 010/100 | Train Loss: 0.00112615\n","Iter: 011/100 | Train Loss: 0.00112766\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 012/100 | Train Loss: 0.00116207\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 013/100 | Train Loss: 0.00116916\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00112219\n","Iter: 015/100 | Train Loss: 0.00103408\n","Iter: 016/100 | Train Loss: 0.00093964\n","Iter: 017/100 | Train Loss: 0.00087037\n","Iter: 018/100 | Train Loss: 0.00083773\n","Iter: 019/100 | Train Loss: 0.00083168\n","Iter: 020/100 | Train Loss: 0.00083146\n","Iter: 021/100 | Train Loss: 0.00081917\n","Iter: 022/100 | Train Loss: 0.00078905\n","Iter: 023/100 | Train Loss: 0.00074710\n","Iter: 024/100 | Train Loss: 0.00070507\n","Iter: 025/100 | Train Loss: 0.00067276\n","Iter: 026/100 | Train Loss: 0.00065328\n","Iter: 027/100 | Train Loss: 0.00064306\n","Iter: 028/100 | Train Loss: 0.00063541\n","Iter: 029/100 | Train Loss: 0.00062421\n","Iter: 030/100 | Train Loss: 0.00060711\n","Iter: 031/100 | Train Loss: 0.00058564\n","Iter: 032/100 | Train Loss: 0.00056381\n","Iter: 033/100 | Train Loss: 0.00054543\n","Iter: 034/100 | Train Loss: 0.00053223\n","Iter: 035/100 | Train Loss: 0.00052340\n","Iter: 036/100 | Train Loss: 0.00051622\n","Iter: 037/100 | Train Loss: 0.00050792\n","Iter: 038/100 | Train Loss: 0.00049711\n","Iter: 039/100 | Train Loss: 0.00048431\n","Iter: 040/100 | Train Loss: 0.00047133\n","Iter: 041/100 | Train Loss: 0.00045991\n","Iter: 042/100 | Train Loss: 0.00045093\n","Iter: 043/100 | Train Loss: 0.00044396\n","Iter: 044/100 | Train Loss: 0.00043765\n","Iter: 045/100 | Train Loss: 0.00043077\n","Iter: 046/100 | Train Loss: 0.00042279\n","Iter: 047/100 | Train Loss: 0.00041407\n","Iter: 048/100 | Train Loss: 0.00040549\n","Iter: 049/100 | Train Loss: 0.00039778\n","Iter: 050/100 | Train Loss: 0.00039122\n","Iter: 051/100 | Train Loss: 0.00038546\n","Iter: 052/100 | Train Loss: 0.00037992\n","Iter: 053/100 | Train Loss: 0.00037407\n","Iter: 054/100 | Train Loss: 0.00036781\n","Iter: 055/100 | Train Loss: 0.00036137\n","Iter: 056/100 | Train Loss: 0.00035514\n","Iter: 057/100 | Train Loss: 0.00034940\n","Iter: 058/100 | Train Loss: 0.00034419\n","Iter: 059/100 | Train Loss: 0.00033931\n","Iter: 060/100 | Train Loss: 0.00033447\n","Iter: 061/100 | Train Loss: 0.00032948\n","Iter: 062/100 | Train Loss: 0.00032439\n","Iter: 063/100 | Train Loss: 0.00031931\n","Iter: 064/100 | Train Loss: 0.00031444\n","Iter: 065/100 | Train Loss: 0.00030986\n","Iter: 066/100 | Train Loss: 0.00030550\n","Iter: 067/100 | Train Loss: 0.00030127\n","Iter: 068/100 | Train Loss: 0.00029704\n","Iter: 069/100 | Train Loss: 0.00029279\n","Iter: 070/100 | Train Loss: 0.00028854\n","Iter: 071/100 | Train Loss: 0.00028438\n","Iter: 072/100 | Train Loss: 0.00028038\n","Iter: 073/100 | Train Loss: 0.00027652\n","Iter: 074/100 | Train Loss: 0.00027278\n","Iter: 075/100 | Train Loss: 0.00026908\n","Iter: 076/100 | Train Loss: 0.00026541\n","Iter: 077/100 | Train Loss: 0.00026176\n","Iter: 078/100 | Train Loss: 0.00025815\n","Iter: 079/100 | Train Loss: 0.00025463\n","Iter: 080/100 | Train Loss: 0.00025119\n","Iter: 081/100 | Train Loss: 0.00024786\n","Iter: 082/100 | Train Loss: 0.00024460\n","Iter: 083/100 | Train Loss: 0.00024140\n","Iter: 084/100 | Train Loss: 0.00023824\n","Iter: 085/100 | Train Loss: 0.00023512\n","Iter: 086/100 | Train Loss: 0.00023206\n","Iter: 087/100 | Train Loss: 0.00022906\n","Iter: 088/100 | Train Loss: 0.00022614\n","Iter: 089/100 | Train Loss: 0.00022328\n","Iter: 090/100 | Train Loss: 0.00022046\n","Iter: 091/100 | Train Loss: 0.00021769\n","Iter: 092/100 | Train Loss: 0.00021496\n","Iter: 093/100 | Train Loss: 0.00021227\n","Iter: 094/100 | Train Loss: 0.00020962\n","Iter: 095/100 | Train Loss: 0.00020701\n","Iter: 096/100 | Train Loss: 0.00020444\n","Iter: 097/100 | Train Loss: 0.00020191\n","Iter: 098/100 | Train Loss: 0.00019943\n","Iter: 099/100 | Train Loss: 0.00019697\n","\n","Iter: 099/100 | Test Loss: 0.00100522 | Test acc: 64.3000\n","scale:1.050000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00232881\n","Iter: 001/100 | Train Loss: 0.00193002\n","Iter: 002/100 | Train Loss: 0.00156647\n","Iter: 003/100 | Train Loss: 0.00144146\n","Iter: 004/100 | Train Loss: 0.00154150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00171625\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00175119\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00157535\n","Iter: 008/100 | Train Loss: 0.00130236\n","Iter: 009/100 | Train Loss: 0.00113877\n","Iter: 010/100 | Train Loss: 0.00112184\n","Iter: 011/100 | Train Loss: 0.00117590\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00118716\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 013/100 | Train Loss: 0.00112095\n","Iter: 014/100 | Train Loss: 0.00101296\n","Iter: 015/100 | Train Loss: 0.00090179\n","Iter: 016/100 | Train Loss: 0.00083244\n","Iter: 017/100 | Train Loss: 0.00081256\n","Iter: 018/100 | Train Loss: 0.00081535\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 019/100 | Train Loss: 0.00081765\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 020/100 | Train Loss: 0.00078731\n","Iter: 021/100 | Train Loss: 0.00072169\n","Iter: 022/100 | Train Loss: 0.00066552\n","Iter: 023/100 | Train Loss: 0.00063582\n","Iter: 024/100 | Train Loss: 0.00062831\n","Iter: 025/100 | Train Loss: 0.00062599\n","Iter: 026/100 | Train Loss: 0.00061348\n","Iter: 027/100 | Train Loss: 0.00058657\n","Iter: 028/100 | Train Loss: 0.00055258\n","Iter: 029/100 | Train Loss: 0.00052379\n","Iter: 030/100 | Train Loss: 0.00050754\n","Iter: 031/100 | Train Loss: 0.00050125\n","Iter: 032/100 | Train Loss: 0.00049591\n","Iter: 033/100 | Train Loss: 0.00048370\n","Iter: 034/100 | Train Loss: 0.00046443\n","Iter: 035/100 | Train Loss: 0.00044402\n","Iter: 036/100 | Train Loss: 0.00042868\n","Iter: 037/100 | Train Loss: 0.00041995\n","Iter: 038/100 | Train Loss: 0.00041443\n","Iter: 039/100 | Train Loss: 0.00040743\n","Iter: 040/100 | Train Loss: 0.00039640\n","Iter: 041/100 | Train Loss: 0.00038294\n","Iter: 042/100 | Train Loss: 0.00037059\n","Iter: 043/100 | Train Loss: 0.00036187\n","Iter: 044/100 | Train Loss: 0.00035617\n","Iter: 045/100 | Train Loss: 0.00035071\n","Iter: 046/100 | Train Loss: 0.00034318\n","Iter: 047/100 | Train Loss: 0.00033365\n","Iter: 048/100 | Train Loss: 0.00032425\n","Iter: 049/100 | Train Loss: 0.00031682\n","Iter: 050/100 | Train Loss: 0.00031137\n","Iter: 051/100 | Train Loss: 0.00030637\n","Iter: 052/100 | Train Loss: 0.00030034\n","Iter: 053/100 | Train Loss: 0.00029322\n","Iter: 054/100 | Train Loss: 0.00028608\n","Iter: 055/100 | Train Loss: 0.00027997\n","Iter: 056/100 | Train Loss: 0.00027491\n","Iter: 057/100 | Train Loss: 0.00027027\n","Iter: 058/100 | Train Loss: 0.00026540\n","Iter: 059/100 | Train Loss: 0.00026018\n","Iter: 060/100 | Train Loss: 0.00025490\n","Iter: 061/100 | Train Loss: 0.00024998\n","Iter: 062/100 | Train Loss: 0.00024562\n","Iter: 063/100 | Train Loss: 0.00024158\n","Iter: 064/100 | Train Loss: 0.00023755\n","Iter: 065/100 | Train Loss: 0.00023328\n","Iter: 066/100 | Train Loss: 0.00022888\n","Iter: 067/100 | Train Loss: 0.00022463\n","Iter: 068/100 | Train Loss: 0.00022074\n","Iter: 069/100 | Train Loss: 0.00021714\n","Iter: 070/100 | Train Loss: 0.00021357\n","Iter: 071/100 | Train Loss: 0.00020985\n","Iter: 072/100 | Train Loss: 0.00020606\n","Iter: 073/100 | Train Loss: 0.00020239\n","Iter: 074/100 | Train Loss: 0.00019898\n","Iter: 075/100 | Train Loss: 0.00019579\n","Iter: 076/100 | Train Loss: 0.00019265\n","Iter: 077/100 | Train Loss: 0.00018943\n","Iter: 078/100 | Train Loss: 0.00018620\n","Iter: 079/100 | Train Loss: 0.00018308\n","Iter: 080/100 | Train Loss: 0.00018015\n","Iter: 081/100 | Train Loss: 0.00017735\n","Iter: 082/100 | Train Loss: 0.00017459\n","Iter: 083/100 | Train Loss: 0.00017181\n","Iter: 084/100 | Train Loss: 0.00016904\n","Iter: 085/100 | Train Loss: 0.00016637\n","Iter: 086/100 | Train Loss: 0.00016381\n","Iter: 087/100 | Train Loss: 0.00016132\n","Iter: 088/100 | Train Loss: 0.00015884\n","Iter: 089/100 | Train Loss: 0.00015638\n","Iter: 090/100 | Train Loss: 0.00015397\n","Iter: 091/100 | Train Loss: 0.00015163\n","Iter: 092/100 | Train Loss: 0.00014935\n","Iter: 093/100 | Train Loss: 0.00014711\n","Iter: 094/100 | Train Loss: 0.00014490\n","Iter: 095/100 | Train Loss: 0.00014273\n","Iter: 096/100 | Train Loss: 0.00014059\n","Iter: 097/100 | Train Loss: 0.00013849\n","Iter: 098/100 | Train Loss: 0.00013643\n","Iter: 099/100 | Train Loss: 0.00013441\n","\n","Iter: 099/100 | Test Loss: 0.00099937 | Test acc: 64.8300\n","scale:1.100000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00242954\n","Iter: 001/100 | Train Loss: 0.00197552\n","Iter: 002/100 | Train Loss: 0.00155924\n","Iter: 003/100 | Train Loss: 0.00146133\n","Iter: 004/100 | Train Loss: 0.00162701\n","Adjusting Layer 1, Kernel Nodes: 623, Adptive Nodes:177\n","Iter: 005/100 | Train Loss: 0.00175610\n","Adjusting Layer 1, Kernel Nodes: 502, Adptive Nodes:298\n","Iter: 006/100 | Train Loss: 0.00152825\n","Iter: 007/100 | Train Loss: 0.00123120\n","Iter: 008/100 | Train Loss: 0.00114681\n","Iter: 009/100 | Train Loss: 0.00117533\n","Adjusting Layer 1, Kernel Nodes: 332, Adptive Nodes:468\n","Iter: 010/100 | Train Loss: 0.00112789\n","Iter: 011/100 | Train Loss: 0.00100601\n","Iter: 012/100 | Train Loss: 0.00087921\n","Iter: 013/100 | Train Loss: 0.00082150\n","Iter: 014/100 | Train Loss: 0.00081695\n","Iter: 015/100 | Train Loss: 0.00079977\n","Iter: 016/100 | Train Loss: 0.00073946\n","Iter: 017/100 | Train Loss: 0.00066527\n","Iter: 018/100 | Train Loss: 0.00062157\n","Iter: 019/100 | Train Loss: 0.00061397\n","Iter: 020/100 | Train Loss: 0.00060965\n","Iter: 021/100 | Train Loss: 0.00058159\n","Iter: 022/100 | Train Loss: 0.00053838\n","Iter: 023/100 | Train Loss: 0.00050681\n","Iter: 024/100 | Train Loss: 0.00049608\n","Iter: 025/100 | Train Loss: 0.00049087\n","Iter: 026/100 | Train Loss: 0.00047630\n","Iter: 027/100 | Train Loss: 0.00045196\n","Iter: 028/100 | Train Loss: 0.00042889\n","Iter: 029/100 | Train Loss: 0.00041508\n","Iter: 030/100 | Train Loss: 0.00040755\n","Iter: 031/100 | Train Loss: 0.00039795\n","Iter: 032/100 | Train Loss: 0.00038257\n","Iter: 033/100 | Train Loss: 0.00036516\n","Iter: 034/100 | Train Loss: 0.00035137\n","Iter: 035/100 | Train Loss: 0.00034238\n","Iter: 036/100 | Train Loss: 0.00033453\n","Iter: 037/100 | Train Loss: 0.00032418\n","Iter: 038/100 | Train Loss: 0.00031156\n","Iter: 039/100 | Train Loss: 0.00029983\n","Iter: 040/100 | Train Loss: 0.00029101\n","Iter: 041/100 | Train Loss: 0.00028404\n","Iter: 042/100 | Train Loss: 0.00027650\n","Iter: 043/100 | Train Loss: 0.00026751\n","Iter: 044/100 | Train Loss: 0.00025841\n","Iter: 045/100 | Train Loss: 0.00025068\n","Iter: 046/100 | Train Loss: 0.00024444\n","Iter: 047/100 | Train Loss: 0.00023852\n","Iter: 048/100 | Train Loss: 0.00023196\n","Iter: 049/100 | Train Loss: 0.00022499\n","Iter: 050/100 | Train Loss: 0.00021853\n","Iter: 051/100 | Train Loss: 0.00021303\n","Iter: 052/100 | Train Loss: 0.00020812\n","Iter: 053/100 | Train Loss: 0.00020307\n","Iter: 054/100 | Train Loss: 0.00019767\n","Iter: 055/100 | Train Loss: 0.00019237\n","Iter: 056/100 | Train Loss: 0.00018760\n","Iter: 057/100 | Train Loss: 0.00018336\n","Iter: 058/100 | Train Loss: 0.00017921\n","Iter: 059/100 | Train Loss: 0.00017487\n","Iter: 060/100 | Train Loss: 0.00017050\n","Iter: 061/100 | Train Loss: 0.00016640\n","Iter: 062/100 | Train Loss: 0.00016265\n","Iter: 063/100 | Train Loss: 0.00015908\n","Iter: 064/100 | Train Loss: 0.00015548\n","Iter: 065/100 | Train Loss: 0.00015186\n","Iter: 066/100 | Train Loss: 0.00014835\n","Iter: 067/100 | Train Loss: 0.00014506\n","Iter: 068/100 | Train Loss: 0.00014193\n","Iter: 069/100 | Train Loss: 0.00013885\n","Iter: 070/100 | Train Loss: 0.00013576\n","Iter: 071/100 | Train Loss: 0.00013273\n","Iter: 072/100 | Train Loss: 0.00012983\n","Iter: 073/100 | Train Loss: 0.00012708\n","Iter: 074/100 | Train Loss: 0.00012440\n","Iter: 075/100 | Train Loss: 0.00012174\n","Iter: 076/100 | Train Loss: 0.00011912\n","Iter: 077/100 | Train Loss: 0.00011659\n","Iter: 078/100 | Train Loss: 0.00011417\n","Iter: 079/100 | Train Loss: 0.00011182\n","Iter: 080/100 | Train Loss: 0.00010953\n","Iter: 081/100 | Train Loss: 0.00010726\n","Iter: 082/100 | Train Loss: 0.00010506\n","Iter: 083/100 | Train Loss: 0.00010293\n","Iter: 084/100 | Train Loss: 0.00010087\n","Iter: 085/100 | Train Loss: 0.00009885\n","Iter: 086/100 | Train Loss: 0.00009686\n","Iter: 087/100 | Train Loss: 0.00009493\n","Iter: 088/100 | Train Loss: 0.00009304\n","Iter: 089/100 | Train Loss: 0.00009120\n","Iter: 090/100 | Train Loss: 0.00008942\n","Iter: 091/100 | Train Loss: 0.00008766\n","Iter: 092/100 | Train Loss: 0.00008595\n","Iter: 093/100 | Train Loss: 0.00008427\n","Iter: 094/100 | Train Loss: 0.00008265\n","Iter: 095/100 | Train Loss: 0.00008106\n","Iter: 096/100 | Train Loss: 0.00007951\n","Iter: 097/100 | Train Loss: 0.00007798\n","Iter: 098/100 | Train Loss: 0.00007650\n","Iter: 099/100 | Train Loss: 0.00007505\n","\n","Iter: 099/100 | Test Loss: 0.00105453 | Test acc: 64.1700\n","scale:1.100000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00242185\n","Iter: 001/100 | Train Loss: 0.00199428\n","Iter: 002/100 | Train Loss: 0.00158522\n","Iter: 003/100 | Train Loss: 0.00145456\n","Iter: 004/100 | Train Loss: 0.00158439\n","Adjusting Layer 1, Kernel Nodes: 680, Adptive Nodes:120\n","Iter: 005/100 | Train Loss: 0.00172638\n","Adjusting Layer 1, Kernel Nodes: 476, Adptive Nodes:324\n","Iter: 006/100 | Train Loss: 0.00162121\n","Iter: 007/100 | Train Loss: 0.00132634\n","Iter: 008/100 | Train Loss: 0.00115381\n","Iter: 009/100 | Train Loss: 0.00116428\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 010/100 | Train Loss: 0.00119608\n","Adjusting Layer 1, Kernel Nodes: 6, Adptive Nodes:794\n","Iter: 011/100 | Train Loss: 0.00102006\n","Iter: 012/100 | Train Loss: 0.00089987\n","Iter: 013/100 | Train Loss: 0.00082191\n","Iter: 014/100 | Train Loss: 0.00079464\n","Iter: 015/100 | Train Loss: 0.00077660\n","Iter: 016/100 | Train Loss: 0.00073449\n","Iter: 017/100 | Train Loss: 0.00067486\n","Iter: 018/100 | Train Loss: 0.00062472\n","Iter: 019/100 | Train Loss: 0.00059743\n","Iter: 020/100 | Train Loss: 0.00058346\n","Iter: 021/100 | Train Loss: 0.00056614\n","Iter: 022/100 | Train Loss: 0.00053941\n","Iter: 023/100 | Train Loss: 0.00050946\n","Iter: 024/100 | Train Loss: 0.00048585\n","Iter: 025/100 | Train Loss: 0.00047069\n","Iter: 026/100 | Train Loss: 0.00045884\n","Iter: 027/100 | Train Loss: 0.00044455\n","Iter: 028/100 | Train Loss: 0.00042643\n","Iter: 029/100 | Train Loss: 0.00040770\n","Iter: 030/100 | Train Loss: 0.00039204\n","Iter: 031/100 | Train Loss: 0.00037997\n","Iter: 032/100 | Train Loss: 0.00036932\n","Iter: 033/100 | Train Loss: 0.00035762\n","Iter: 034/100 | Train Loss: 0.00034442\n","Iter: 035/100 | Train Loss: 0.00033115\n","Iter: 036/100 | Train Loss: 0.00031960\n","Iter: 037/100 | Train Loss: 0.00031016\n","Iter: 038/100 | Train Loss: 0.00030169\n","Iter: 039/100 | Train Loss: 0.00029262\n","Iter: 040/100 | Train Loss: 0.00028261\n","Iter: 041/100 | Train Loss: 0.00027270\n","Iter: 042/100 | Train Loss: 0.00026418\n","Iter: 043/100 | Train Loss: 0.00025716\n","Iter: 044/100 | Train Loss: 0.00025054\n","Iter: 045/100 | Train Loss: 0.00024328\n","Iter: 046/100 | Train Loss: 0.00023555\n","Iter: 047/100 | Train Loss: 0.00022833\n","Iter: 048/100 | Train Loss: 0.00022227\n","Iter: 049/100 | Train Loss: 0.00021701\n","Iter: 050/100 | Train Loss: 0.00021168\n","Iter: 051/100 | Train Loss: 0.00020585\n","Iter: 052/100 | Train Loss: 0.00019994\n","Iter: 053/100 | Train Loss: 0.00019462\n","Iter: 054/100 | Train Loss: 0.00019004\n","Iter: 055/100 | Train Loss: 0.00018575\n","Iter: 056/100 | Train Loss: 0.00018123\n","Iter: 057/100 | Train Loss: 0.00017648\n","Iter: 058/100 | Train Loss: 0.00017191\n","Iter: 059/100 | Train Loss: 0.00016783\n","Iter: 060/100 | Train Loss: 0.00016411\n","Iter: 061/100 | Train Loss: 0.00016039\n","Iter: 062/100 | Train Loss: 0.00015651\n","Iter: 063/100 | Train Loss: 0.00015265\n","Iter: 064/100 | Train Loss: 0.00014905\n","Iter: 065/100 | Train Loss: 0.00014576\n","Iter: 066/100 | Train Loss: 0.00014257\n","Iter: 067/100 | Train Loss: 0.00013932\n","Iter: 068/100 | Train Loss: 0.00013605\n","Iter: 069/100 | Train Loss: 0.00013292\n","Iter: 070/100 | Train Loss: 0.00013000\n","Iter: 071/100 | Train Loss: 0.00012720\n","Iter: 072/100 | Train Loss: 0.00012443\n","Iter: 073/100 | Train Loss: 0.00012166\n","Iter: 074/100 | Train Loss: 0.00011897\n","Iter: 075/100 | Train Loss: 0.00011639\n","Iter: 076/100 | Train Loss: 0.00011393\n","Iter: 077/100 | Train Loss: 0.00011153\n","Iter: 078/100 | Train Loss: 0.00010915\n","Iter: 079/100 | Train Loss: 0.00010681\n","Iter: 080/100 | Train Loss: 0.00010454\n","Iter: 081/100 | Train Loss: 0.00010236\n","Iter: 082/100 | Train Loss: 0.00010025\n","Iter: 083/100 | Train Loss: 0.00009818\n","Iter: 084/100 | Train Loss: 0.00009614\n","Iter: 085/100 | Train Loss: 0.00009416\n","Iter: 086/100 | Train Loss: 0.00009224\n","Iter: 087/100 | Train Loss: 0.00009037\n","Iter: 088/100 | Train Loss: 0.00008855\n","Iter: 089/100 | Train Loss: 0.00008676\n","Iter: 090/100 | Train Loss: 0.00008502\n","Iter: 091/100 | Train Loss: 0.00008331\n","Iter: 092/100 | Train Loss: 0.00008166\n","Iter: 093/100 | Train Loss: 0.00008005\n","Iter: 094/100 | Train Loss: 0.00007848\n","Iter: 095/100 | Train Loss: 0.00007694\n","Iter: 096/100 | Train Loss: 0.00007543\n","Iter: 097/100 | Train Loss: 0.00007397\n","Iter: 098/100 | Train Loss: 0.00007253\n","Iter: 099/100 | Train Loss: 0.00007114\n","\n","Iter: 099/100 | Test Loss: 0.00105428 | Test acc: 64.2200\n","scale:1.100000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00241372\n","Iter: 001/100 | Train Loss: 0.00200532\n","Iter: 002/100 | Train Loss: 0.00160410\n","Iter: 003/100 | Train Loss: 0.00145285\n","Iter: 004/100 | Train Loss: 0.00155495\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 005/100 | Train Loss: 0.00166667\n","Adjusting Layer 1, Kernel Nodes: 465, Adptive Nodes:335\n","Iter: 006/100 | Train Loss: 0.00168576\n","Adjusting Layer 1, Kernel Nodes: 515, Adptive Nodes:285\n","Iter: 007/100 | Train Loss: 0.00138618\n","Iter: 008/100 | Train Loss: 0.00116913\n","Iter: 009/100 | Train Loss: 0.00116695\n","Iter: 010/100 | Train Loss: 0.00120524\n","Adjusting Layer 1, Kernel Nodes: 140, Adptive Nodes:660\n","Iter: 011/100 | Train Loss: 0.00110135\n","Iter: 012/100 | Train Loss: 0.00100991\n","Iter: 013/100 | Train Loss: 0.00090639\n","Iter: 014/100 | Train Loss: 0.00083456\n","Iter: 015/100 | Train Loss: 0.00080394\n","Iter: 016/100 | Train Loss: 0.00079271\n","Iter: 017/100 | Train Loss: 0.00077254\n","Iter: 018/100 | Train Loss: 0.00073226\n","Iter: 019/100 | Train Loss: 0.00068082\n","Iter: 020/100 | Train Loss: 0.00063633\n","Iter: 021/100 | Train Loss: 0.00060919\n","Iter: 022/100 | Train Loss: 0.00059646\n","Iter: 023/100 | Train Loss: 0.00058692\n","Iter: 024/100 | Train Loss: 0.00057061\n","Iter: 025/100 | Train Loss: 0.00054566\n","Iter: 026/100 | Train Loss: 0.00051858\n","Iter: 027/100 | Train Loss: 0.00049736\n","Iter: 028/100 | Train Loss: 0.00048508\n","Iter: 029/100 | Train Loss: 0.00047759\n","Iter: 030/100 | Train Loss: 0.00046802\n","Iter: 031/100 | Train Loss: 0.00045326\n","Iter: 032/100 | Train Loss: 0.00043578\n","Iter: 033/100 | Train Loss: 0.00042031\n","Iter: 034/100 | Train Loss: 0.00040941\n","Iter: 035/100 | Train Loss: 0.00040156\n","Iter: 036/100 | Train Loss: 0.00039344\n","Iter: 037/100 | Train Loss: 0.00038305\n","Iter: 038/100 | Train Loss: 0.00037099\n","Iter: 039/100 | Train Loss: 0.00035945\n","Iter: 040/100 | Train Loss: 0.00034987\n","Iter: 041/100 | Train Loss: 0.00034201\n","Iter: 042/100 | Train Loss: 0.00033467\n","Iter: 043/100 | Train Loss: 0.00032672\n","Iter: 044/100 | Train Loss: 0.00031804\n","Iter: 045/100 | Train Loss: 0.00030939\n","Iter: 046/100 | Train Loss: 0.00030158\n","Iter: 047/100 | Train Loss: 0.00029468\n","Iter: 048/100 | Train Loss: 0.00028828\n","Iter: 049/100 | Train Loss: 0.00028186\n","Iter: 050/100 | Train Loss: 0.00027527\n","Iter: 051/100 | Train Loss: 0.00026871\n","Iter: 052/100 | Train Loss: 0.00026244\n","Iter: 053/100 | Train Loss: 0.00025660\n","Iter: 054/100 | Train Loss: 0.00025124\n","Iter: 055/100 | Train Loss: 0.00024610\n","Iter: 056/100 | Train Loss: 0.00024095\n","Iter: 057/100 | Train Loss: 0.00023573\n","Iter: 058/100 | Train Loss: 0.00023057\n","Iter: 059/100 | Train Loss: 0.00022562\n","Iter: 060/100 | Train Loss: 0.00022098\n","Iter: 061/100 | Train Loss: 0.00021655\n","Iter: 062/100 | Train Loss: 0.00021215\n","Iter: 063/100 | Train Loss: 0.00020774\n","Iter: 064/100 | Train Loss: 0.00020337\n","Iter: 065/100 | Train Loss: 0.00019916\n","Iter: 066/100 | Train Loss: 0.00019518\n","Iter: 067/100 | Train Loss: 0.00019137\n","Iter: 068/100 | Train Loss: 0.00018764\n","Iter: 069/100 | Train Loss: 0.00018392\n","Iter: 070/100 | Train Loss: 0.00018027\n","Iter: 071/100 | Train Loss: 0.00017670\n","Iter: 072/100 | Train Loss: 0.00017328\n","Iter: 073/100 | Train Loss: 0.00016998\n","Iter: 074/100 | Train Loss: 0.00016676\n","Iter: 075/100 | Train Loss: 0.00016358\n","Iter: 076/100 | Train Loss: 0.00016048\n","Iter: 077/100 | Train Loss: 0.00015744\n","Iter: 078/100 | Train Loss: 0.00015448\n","Iter: 079/100 | Train Loss: 0.00015159\n","Iter: 080/100 | Train Loss: 0.00014878\n","Iter: 081/100 | Train Loss: 0.00014602\n","Iter: 082/100 | Train Loss: 0.00014333\n","Iter: 083/100 | Train Loss: 0.00014070\n","Iter: 084/100 | Train Loss: 0.00013813\n","Iter: 085/100 | Train Loss: 0.00013562\n","Iter: 086/100 | Train Loss: 0.00013317\n","Iter: 087/100 | Train Loss: 0.00013077\n","Iter: 088/100 | Train Loss: 0.00012842\n","Iter: 089/100 | Train Loss: 0.00012612\n","Iter: 090/100 | Train Loss: 0.00012387\n","Iter: 091/100 | Train Loss: 0.00012166\n","Iter: 092/100 | Train Loss: 0.00011949\n","Iter: 093/100 | Train Loss: 0.00011739\n","Iter: 094/100 | Train Loss: 0.00011533\n","Iter: 095/100 | Train Loss: 0.00011330\n","Iter: 096/100 | Train Loss: 0.00011131\n","Iter: 097/100 | Train Loss: 0.00010937\n","Iter: 098/100 | Train Loss: 0.00010747\n","Iter: 099/100 | Train Loss: 0.00010561\n","\n","Iter: 099/100 | Test Loss: 0.00102677 | Test acc: 64.8900\n","scale:1.100000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00240426\n","Iter: 001/100 | Train Loss: 0.00200890\n","Iter: 002/100 | Train Loss: 0.00161542\n","Iter: 003/100 | Train Loss: 0.00145276\n","Iter: 004/100 | Train Loss: 0.00153499\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 005/100 | Train Loss: 0.00166338\n","Adjusting Layer 1, Kernel Nodes: 692, Adptive Nodes:108\n","Iter: 006/100 | Train Loss: 0.00165314\n","Iter: 007/100 | Train Loss: 0.00146752\n","Iter: 008/100 | Train Loss: 0.00124313\n","Iter: 009/100 | Train Loss: 0.00115045\n","Iter: 010/100 | Train Loss: 0.00117854\n","Adjusting Layer 1, Kernel Nodes: 491, Adptive Nodes:309\n","Iter: 011/100 | Train Loss: 0.00118253\n","Adjusting Layer 1, Kernel Nodes: 528, Adptive Nodes:272\n","Iter: 012/100 | Train Loss: 0.00109804\n","Iter: 013/100 | Train Loss: 0.00094163\n","Iter: 014/100 | Train Loss: 0.00085452\n","Iter: 015/100 | Train Loss: 0.00084908\n","Iter: 016/100 | Train Loss: 0.00084258\n","Iter: 017/100 | Train Loss: 0.00078115\n","Iter: 018/100 | Train Loss: 0.00069538\n","Iter: 019/100 | Train Loss: 0.00064134\n","Iter: 020/100 | Train Loss: 0.00062801\n","Iter: 021/100 | Train Loss: 0.00062008\n","Iter: 022/100 | Train Loss: 0.00059051\n","Iter: 023/100 | Train Loss: 0.00054733\n","Iter: 024/100 | Train Loss: 0.00051417\n","Iter: 025/100 | Train Loss: 0.00049900\n","Iter: 026/100 | Train Loss: 0.00048999\n","Iter: 027/100 | Train Loss: 0.00047431\n","Iter: 028/100 | Train Loss: 0.00045139\n","Iter: 029/100 | Train Loss: 0.00042927\n","Iter: 030/100 | Train Loss: 0.00041362\n","Iter: 031/100 | Train Loss: 0.00040293\n","Iter: 032/100 | Train Loss: 0.00039187\n","Iter: 033/100 | Train Loss: 0.00037730\n","Iter: 034/100 | Train Loss: 0.00036123\n","Iter: 035/100 | Train Loss: 0.00034732\n","Iter: 036/100 | Train Loss: 0.00033665\n","Iter: 037/100 | Train Loss: 0.00032727\n","Iter: 038/100 | Train Loss: 0.00031673\n","Iter: 039/100 | Train Loss: 0.00030505\n","Iter: 040/100 | Train Loss: 0.00029380\n","Iter: 041/100 | Train Loss: 0.00028406\n","Iter: 042/100 | Train Loss: 0.00027560\n","Iter: 043/100 | Train Loss: 0.00026754\n","Iter: 044/100 | Train Loss: 0.00025916\n","Iter: 045/100 | Train Loss: 0.00025055\n","Iter: 046/100 | Train Loss: 0.00024233\n","Iter: 047/100 | Train Loss: 0.00023510\n","Iter: 048/100 | Train Loss: 0.00022864\n","Iter: 049/100 | Train Loss: 0.00022226\n","Iter: 050/100 | Train Loss: 0.00021559\n","Iter: 051/100 | Train Loss: 0.00020890\n","Iter: 052/100 | Train Loss: 0.00020282\n","Iter: 053/100 | Train Loss: 0.00019755\n","Iter: 054/100 | Train Loss: 0.00019257\n","Iter: 055/100 | Train Loss: 0.00018734\n","Iter: 056/100 | Train Loss: 0.00018198\n","Iter: 057/100 | Train Loss: 0.00017695\n","Iter: 058/100 | Train Loss: 0.00017250\n","Iter: 059/100 | Train Loss: 0.00016836\n","Iter: 060/100 | Train Loss: 0.00016409\n","Iter: 061/100 | Train Loss: 0.00015965\n","Iter: 062/100 | Train Loss: 0.00015540\n","Iter: 063/100 | Train Loss: 0.00015160\n","Iter: 064/100 | Train Loss: 0.00014804\n","Iter: 065/100 | Train Loss: 0.00014442\n","Iter: 066/100 | Train Loss: 0.00014070\n","Iter: 067/100 | Train Loss: 0.00013711\n","Iter: 068/100 | Train Loss: 0.00013379\n","Iter: 069/100 | Train Loss: 0.00013064\n","Iter: 070/100 | Train Loss: 0.00012752\n","Iter: 071/100 | Train Loss: 0.00012438\n","Iter: 072/100 | Train Loss: 0.00012134\n","Iter: 073/100 | Train Loss: 0.00011845\n","Iter: 074/100 | Train Loss: 0.00011569\n","Iter: 075/100 | Train Loss: 0.00011300\n","Iter: 076/100 | Train Loss: 0.00011035\n","Iter: 077/100 | Train Loss: 0.00010775\n","Iter: 078/100 | Train Loss: 0.00010525\n","Iter: 079/100 | Train Loss: 0.00010287\n","Iter: 080/100 | Train Loss: 0.00010056\n","Iter: 081/100 | Train Loss: 0.00009827\n","Iter: 082/100 | Train Loss: 0.00009602\n","Iter: 083/100 | Train Loss: 0.00009384\n","Iter: 084/100 | Train Loss: 0.00009175\n","Iter: 085/100 | Train Loss: 0.00008972\n","Iter: 086/100 | Train Loss: 0.00008773\n","Iter: 087/100 | Train Loss: 0.00008577\n","Iter: 088/100 | Train Loss: 0.00008387\n","Iter: 089/100 | Train Loss: 0.00008204\n","Iter: 090/100 | Train Loss: 0.00008026\n","Iter: 091/100 | Train Loss: 0.00007850\n","Iter: 092/100 | Train Loss: 0.00007679\n","Iter: 093/100 | Train Loss: 0.00007513\n","Iter: 094/100 | Train Loss: 0.00007352\n","Iter: 095/100 | Train Loss: 0.00007194\n","Iter: 096/100 | Train Loss: 0.00007041\n","Iter: 097/100 | Train Loss: 0.00006891\n","Iter: 098/100 | Train Loss: 0.00006745\n","Iter: 099/100 | Train Loss: 0.00006602\n","\n","Iter: 099/100 | Test Loss: 0.00104858 | Test acc: 64.2200\n","scale:1.100000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00239376\n","Iter: 001/100 | Train Loss: 0.00200639\n","Iter: 002/100 | Train Loss: 0.00161974\n","Iter: 003/100 | Train Loss: 0.00145264\n","Iter: 004/100 | Train Loss: 0.00152347\n","Adjusting Layer 1, Kernel Nodes: 698, Adptive Nodes:102\n","Iter: 005/100 | Train Loss: 0.00165013\n","Adjusting Layer 1, Kernel Nodes: 775, Adptive Nodes:25\n","Iter: 006/100 | Train Loss: 0.00165859\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00144327\n","Iter: 008/100 | Train Loss: 0.00123816\n","Iter: 009/100 | Train Loss: 0.00115470\n","Iter: 010/100 | Train Loss: 0.00117746\n","Adjusting Layer 1, Kernel Nodes: 339, Adptive Nodes:461\n","Iter: 011/100 | Train Loss: 0.00114347\n","Iter: 012/100 | Train Loss: 0.00106233\n","Iter: 013/100 | Train Loss: 0.00094731\n","Iter: 014/100 | Train Loss: 0.00086755\n","Iter: 015/100 | Train Loss: 0.00084172\n","Iter: 016/100 | Train Loss: 0.00082993\n","Iter: 017/100 | Train Loss: 0.00079216\n","Iter: 018/100 | Train Loss: 0.00073114\n","Iter: 019/100 | Train Loss: 0.00067722\n","Iter: 020/100 | Train Loss: 0.00064769\n","Iter: 021/100 | Train Loss: 0.00063292\n","Iter: 022/100 | Train Loss: 0.00061430\n","Iter: 023/100 | Train Loss: 0.00058579\n","Iter: 024/100 | Train Loss: 0.00055520\n","Iter: 025/100 | Train Loss: 0.00053125\n","Iter: 026/100 | Train Loss: 0.00051479\n","Iter: 027/100 | Train Loss: 0.00050094\n","Iter: 028/100 | Train Loss: 0.00048578\n","Iter: 029/100 | Train Loss: 0.00046897\n","Iter: 030/100 | Train Loss: 0.00045201\n","Iter: 031/100 | Train Loss: 0.00043652\n","Iter: 032/100 | Train Loss: 0.00042332\n","Iter: 033/100 | Train Loss: 0.00041204\n","Iter: 034/100 | Train Loss: 0.00040119\n","Iter: 035/100 | Train Loss: 0.00038930\n","Iter: 036/100 | Train Loss: 0.00037652\n","Iter: 037/100 | Train Loss: 0.00036454\n","Iter: 038/100 | Train Loss: 0.00035457\n","Iter: 039/100 | Train Loss: 0.00034594\n","Iter: 040/100 | Train Loss: 0.00033703\n","Iter: 041/100 | Train Loss: 0.00032719\n","Iter: 042/100 | Train Loss: 0.00031731\n","Iter: 043/100 | Train Loss: 0.00030857\n","Iter: 044/100 | Train Loss: 0.00030102\n","Iter: 045/100 | Train Loss: 0.00029374\n","Iter: 046/100 | Train Loss: 0.00028602\n","Iter: 047/100 | Train Loss: 0.00027815\n","Iter: 048/100 | Train Loss: 0.00027076\n","Iter: 049/100 | Train Loss: 0.00026406\n","Iter: 050/100 | Train Loss: 0.00025775\n","Iter: 051/100 | Train Loss: 0.00025144\n","Iter: 052/100 | Train Loss: 0.00024513\n","Iter: 053/100 | Train Loss: 0.00023906\n","Iter: 054/100 | Train Loss: 0.00023336\n","Iter: 055/100 | Train Loss: 0.00022794\n","Iter: 056/100 | Train Loss: 0.00022267\n","Iter: 057/100 | Train Loss: 0.00021752\n","Iter: 058/100 | Train Loss: 0.00021255\n","Iter: 059/100 | Train Loss: 0.00020777\n","Iter: 060/100 | Train Loss: 0.00020312\n","Iter: 061/100 | Train Loss: 0.00019857\n","Iter: 062/100 | Train Loss: 0.00019418\n","Iter: 063/100 | Train Loss: 0.00019003\n","Iter: 064/100 | Train Loss: 0.00018601\n","Iter: 065/100 | Train Loss: 0.00018203\n","Iter: 066/100 | Train Loss: 0.00017811\n","Iter: 067/100 | Train Loss: 0.00017435\n","Iter: 068/100 | Train Loss: 0.00017077\n","Iter: 069/100 | Train Loss: 0.00016729\n","Iter: 070/100 | Train Loss: 0.00016385\n","Iter: 071/100 | Train Loss: 0.00016044\n","Iter: 072/100 | Train Loss: 0.00015713\n","Iter: 073/100 | Train Loss: 0.00015398\n","Iter: 074/100 | Train Loss: 0.00015093\n","Iter: 075/100 | Train Loss: 0.00014791\n","Iter: 076/100 | Train Loss: 0.00014493\n","Iter: 077/100 | Train Loss: 0.00014203\n","Iter: 078/100 | Train Loss: 0.00013923\n","Iter: 079/100 | Train Loss: 0.00013651\n","Iter: 080/100 | Train Loss: 0.00013385\n","Iter: 081/100 | Train Loss: 0.00013124\n","Iter: 082/100 | Train Loss: 0.00012868\n","Iter: 083/100 | Train Loss: 0.00012621\n","Iter: 084/100 | Train Loss: 0.00012380\n","Iter: 085/100 | Train Loss: 0.00012144\n","Iter: 086/100 | Train Loss: 0.00011913\n","Iter: 087/100 | Train Loss: 0.00011688\n","Iter: 088/100 | Train Loss: 0.00011468\n","Iter: 089/100 | Train Loss: 0.00011253\n","Iter: 090/100 | Train Loss: 0.00011044\n","Iter: 091/100 | Train Loss: 0.00010839\n","Iter: 092/100 | Train Loss: 0.00010638\n","Iter: 093/100 | Train Loss: 0.00010443\n","Iter: 094/100 | Train Loss: 0.00010251\n","Iter: 095/100 | Train Loss: 0.00010064\n","Iter: 096/100 | Train Loss: 0.00009881\n","Iter: 097/100 | Train Loss: 0.00009702\n","Iter: 098/100 | Train Loss: 0.00009526\n","Iter: 099/100 | Train Loss: 0.00009355\n","\n","Iter: 099/100 | Test Loss: 0.00103696 | Test acc: 64.6600\n","scale:1.100000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00238225\n","Iter: 001/100 | Train Loss: 0.00199874\n","Iter: 002/100 | Train Loss: 0.00161792\n","Iter: 003/100 | Train Loss: 0.00145143\n","Iter: 004/100 | Train Loss: 0.00151825\n","Adjusting Layer 1, Kernel Nodes: 699, Adptive Nodes:101\n","Iter: 005/100 | Train Loss: 0.00164415\n","Adjusting Layer 1, Kernel Nodes: 713, Adptive Nodes:87\n","Iter: 006/100 | Train Loss: 0.00168077\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 007/100 | Train Loss: 0.00155017\n","Iter: 008/100 | Train Loss: 0.00135193\n","Iter: 009/100 | Train Loss: 0.00119670\n","Iter: 010/100 | Train Loss: 0.00114940\n","Iter: 011/100 | Train Loss: 0.00116974\n","Adjusting Layer 1, Kernel Nodes: 551, Adptive Nodes:249\n","Iter: 012/100 | Train Loss: 0.00117847\n","Adjusting Layer 1, Kernel Nodes: 609, Adptive Nodes:191\n","Iter: 013/100 | Train Loss: 0.00112429\n","Iter: 014/100 | Train Loss: 0.00099718\n","Iter: 015/100 | Train Loss: 0.00090098\n","Iter: 016/100 | Train Loss: 0.00086804\n","Iter: 017/100 | Train Loss: 0.00086617\n","Iter: 018/100 | Train Loss: 0.00084618\n","Iter: 019/100 | Train Loss: 0.00079284\n","Iter: 020/100 | Train Loss: 0.00072910\n","Iter: 021/100 | Train Loss: 0.00068317\n","Iter: 022/100 | Train Loss: 0.00066058\n","Iter: 023/100 | Train Loss: 0.00064704\n","Iter: 024/100 | Train Loss: 0.00062771\n","Iter: 025/100 | Train Loss: 0.00059999\n","Iter: 026/100 | Train Loss: 0.00057060\n","Iter: 027/100 | Train Loss: 0.00054600\n","Iter: 028/100 | Train Loss: 0.00052744\n","Iter: 029/100 | Train Loss: 0.00051259\n","Iter: 030/100 | Train Loss: 0.00049892\n","Iter: 031/100 | Train Loss: 0.00048489\n","Iter: 032/100 | Train Loss: 0.00046966\n","Iter: 033/100 | Train Loss: 0.00045337\n","Iter: 034/100 | Train Loss: 0.00043777\n","Iter: 035/100 | Train Loss: 0.00042503\n","Iter: 036/100 | Train Loss: 0.00041545\n","Iter: 037/100 | Train Loss: 0.00040675\n","Iter: 038/100 | Train Loss: 0.00039638\n","Iter: 039/100 | Train Loss: 0.00038403\n","Iter: 040/100 | Train Loss: 0.00037179\n","Iter: 041/100 | Train Loss: 0.00036181\n","Iter: 042/100 | Train Loss: 0.00035423\n","Iter: 043/100 | Train Loss: 0.00034717\n","Iter: 044/100 | Train Loss: 0.00033887\n","Iter: 045/100 | Train Loss: 0.00032931\n","Iter: 046/100 | Train Loss: 0.00031995\n","Iter: 047/100 | Train Loss: 0.00031209\n","Iter: 048/100 | Train Loss: 0.00030567\n","Iter: 049/100 | Train Loss: 0.00029953\n","Iter: 050/100 | Train Loss: 0.00029270\n","Iter: 051/100 | Train Loss: 0.00028529\n","Iter: 052/100 | Train Loss: 0.00027814\n","Iter: 053/100 | Train Loss: 0.00027183\n","Iter: 054/100 | Train Loss: 0.00026627\n","Iter: 055/100 | Train Loss: 0.00026086\n","Iter: 056/100 | Train Loss: 0.00025519\n","Iter: 057/100 | Train Loss: 0.00024935\n","Iter: 058/100 | Train Loss: 0.00024371\n","Iter: 059/100 | Train Loss: 0.00023851\n","Iter: 060/100 | Train Loss: 0.00023369\n","Iter: 061/100 | Train Loss: 0.00022900\n","Iter: 062/100 | Train Loss: 0.00022430\n","Iter: 063/100 | Train Loss: 0.00021959\n","Iter: 064/100 | Train Loss: 0.00021500\n","Iter: 065/100 | Train Loss: 0.00021062\n","Iter: 066/100 | Train Loss: 0.00020648\n","Iter: 067/100 | Train Loss: 0.00020249\n","Iter: 068/100 | Train Loss: 0.00019855\n","Iter: 069/100 | Train Loss: 0.00019463\n","Iter: 070/100 | Train Loss: 0.00019077\n","Iter: 071/100 | Train Loss: 0.00018703\n","Iter: 072/100 | Train Loss: 0.00018346\n","Iter: 073/100 | Train Loss: 0.00018002\n","Iter: 074/100 | Train Loss: 0.00017667\n","Iter: 075/100 | Train Loss: 0.00017334\n","Iter: 076/100 | Train Loss: 0.00017005\n","Iter: 077/100 | Train Loss: 0.00016685\n","Iter: 078/100 | Train Loss: 0.00016378\n","Iter: 079/100 | Train Loss: 0.00016081\n","Iter: 080/100 | Train Loss: 0.00015789\n","Iter: 081/100 | Train Loss: 0.00015501\n","Iter: 082/100 | Train Loss: 0.00015217\n","Iter: 083/100 | Train Loss: 0.00014942\n","Iter: 084/100 | Train Loss: 0.00014675\n","Iter: 085/100 | Train Loss: 0.00014415\n","Iter: 086/100 | Train Loss: 0.00014159\n","Iter: 087/100 | Train Loss: 0.00013907\n","Iter: 088/100 | Train Loss: 0.00013660\n","Iter: 089/100 | Train Loss: 0.00013418\n","Iter: 090/100 | Train Loss: 0.00013183\n","Iter: 091/100 | Train Loss: 0.00012953\n","Iter: 092/100 | Train Loss: 0.00012728\n","Iter: 093/100 | Train Loss: 0.00012507\n","Iter: 094/100 | Train Loss: 0.00012290\n","Iter: 095/100 | Train Loss: 0.00012078\n","Iter: 096/100 | Train Loss: 0.00011871\n","Iter: 097/100 | Train Loss: 0.00011669\n","Iter: 098/100 | Train Loss: 0.00011471\n","Iter: 099/100 | Train Loss: 0.00011276\n","\n","Iter: 099/100 | Test Loss: 0.00102597 | Test acc: 64.9000\n","scale:1.100000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00236988\n","Iter: 001/100 | Train Loss: 0.00198667\n","Iter: 002/100 | Train Loss: 0.00161083\n","Iter: 003/100 | Train Loss: 0.00144933\n","Iter: 004/100 | Train Loss: 0.00151790\n","Adjusting Layer 1, Kernel Nodes: 706, Adptive Nodes:94\n","Iter: 005/100 | Train Loss: 0.00164683\n","Adjusting Layer 1, Kernel Nodes: 710, Adptive Nodes:90\n","Iter: 006/100 | Train Loss: 0.00168990\n","Adjusting Layer 1, Kernel Nodes: 690, Adptive Nodes:110\n","Iter: 007/100 | Train Loss: 0.00158402\n","Iter: 008/100 | Train Loss: 0.00140786\n","Iter: 009/100 | Train Loss: 0.00123960\n","Iter: 010/100 | Train Loss: 0.00115234\n","Iter: 011/100 | Train Loss: 0.00114511\n","Iter: 012/100 | Train Loss: 0.00116494\n","Adjusting Layer 1, Kernel Nodes: 633, Adptive Nodes:167\n","Iter: 013/100 | Train Loss: 0.00116777\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 014/100 | Train Loss: 0.00111613\n","Iter: 015/100 | Train Loss: 0.00100910\n","Iter: 016/100 | Train Loss: 0.00092159\n","Iter: 017/100 | Train Loss: 0.00088000\n","Iter: 018/100 | Train Loss: 0.00087229\n","Iter: 019/100 | Train Loss: 0.00086655\n","Iter: 020/100 | Train Loss: 0.00084007\n","Iter: 021/100 | Train Loss: 0.00079307\n","Iter: 022/100 | Train Loss: 0.00074192\n","Iter: 023/100 | Train Loss: 0.00070280\n","Iter: 024/100 | Train Loss: 0.00068008\n","Iter: 025/100 | Train Loss: 0.00066651\n","Iter: 026/100 | Train Loss: 0.00065223\n","Iter: 027/100 | Train Loss: 0.00063212\n","Iter: 028/100 | Train Loss: 0.00060762\n","Iter: 029/100 | Train Loss: 0.00058343\n","Iter: 030/100 | Train Loss: 0.00056307\n","Iter: 031/100 | Train Loss: 0.00054704\n","Iter: 032/100 | Train Loss: 0.00053376\n","Iter: 033/100 | Train Loss: 0.00052134\n","Iter: 034/100 | Train Loss: 0.00050874\n","Iter: 035/100 | Train Loss: 0.00049569\n","Iter: 036/100 | Train Loss: 0.00048239\n","Iter: 037/100 | Train Loss: 0.00046922\n","Iter: 038/100 | Train Loss: 0.00045679\n","Iter: 039/100 | Train Loss: 0.00044586\n","Iter: 040/100 | Train Loss: 0.00043663\n","Iter: 041/100 | Train Loss: 0.00042834\n","Iter: 042/100 | Train Loss: 0.00041982\n","Iter: 043/100 | Train Loss: 0.00041041\n","Iter: 044/100 | Train Loss: 0.00040044\n","Iter: 045/100 | Train Loss: 0.00039096\n","Iter: 046/100 | Train Loss: 0.00038277\n","Iter: 047/100 | Train Loss: 0.00037585\n","Iter: 048/100 | Train Loss: 0.00036944\n","Iter: 049/100 | Train Loss: 0.00036267\n","Iter: 050/100 | Train Loss: 0.00035526\n","Iter: 051/100 | Train Loss: 0.00034763\n","Iter: 052/100 | Train Loss: 0.00034051\n","Iter: 053/100 | Train Loss: 0.00033424\n","Iter: 054/100 | Train Loss: 0.00032863\n","Iter: 055/100 | Train Loss: 0.00032315\n","Iter: 056/100 | Train Loss: 0.00031738\n","Iter: 057/100 | Train Loss: 0.00031135\n","Iter: 058/100 | Train Loss: 0.00030537\n","Iter: 059/100 | Train Loss: 0.00029976\n","Iter: 060/100 | Train Loss: 0.00029461\n","Iter: 061/100 | Train Loss: 0.00028974\n","Iter: 062/100 | Train Loss: 0.00028490\n","Iter: 063/100 | Train Loss: 0.00027999\n","Iter: 064/100 | Train Loss: 0.00027505\n","Iter: 065/100 | Train Loss: 0.00027025\n","Iter: 066/100 | Train Loss: 0.00026568\n","Iter: 067/100 | Train Loss: 0.00026133\n","Iter: 068/100 | Train Loss: 0.00025712\n","Iter: 069/100 | Train Loss: 0.00025296\n","Iter: 070/100 | Train Loss: 0.00024880\n","Iter: 071/100 | Train Loss: 0.00024468\n","Iter: 072/100 | Train Loss: 0.00024066\n","Iter: 073/100 | Train Loss: 0.00023679\n","Iter: 074/100 | Train Loss: 0.00023305\n","Iter: 075/100 | Train Loss: 0.00022940\n","Iter: 076/100 | Train Loss: 0.00022581\n","Iter: 077/100 | Train Loss: 0.00022227\n","Iter: 078/100 | Train Loss: 0.00021876\n","Iter: 079/100 | Train Loss: 0.00021534\n","Iter: 080/100 | Train Loss: 0.00021200\n","Iter: 081/100 | Train Loss: 0.00020875\n","Iter: 082/100 | Train Loss: 0.00020557\n","Iter: 083/100 | Train Loss: 0.00020245\n","Iter: 084/100 | Train Loss: 0.00019936\n","Iter: 085/100 | Train Loss: 0.00019633\n","Iter: 086/100 | Train Loss: 0.00019335\n","Iter: 087/100 | Train Loss: 0.00019044\n","Iter: 088/100 | Train Loss: 0.00018760\n","Iter: 089/100 | Train Loss: 0.00018482\n","Iter: 090/100 | Train Loss: 0.00018208\n","Iter: 091/100 | Train Loss: 0.00017939\n","Iter: 092/100 | Train Loss: 0.00017675\n","Iter: 093/100 | Train Loss: 0.00017416\n","Iter: 094/100 | Train Loss: 0.00017162\n","Iter: 095/100 | Train Loss: 0.00016914\n","Iter: 096/100 | Train Loss: 0.00016669\n","Iter: 097/100 | Train Loss: 0.00016429\n","Iter: 098/100 | Train Loss: 0.00016192\n","Iter: 099/100 | Train Loss: 0.00015959\n","\n","Iter: 099/100 | Test Loss: 0.00100936 | Test acc: 65.0800\n","scale:1.100000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00235667\n","Iter: 001/100 | Train Loss: 0.00197084\n","Iter: 002/100 | Train Loss: 0.00159952\n","Iter: 003/100 | Train Loss: 0.00144675\n","Iter: 004/100 | Train Loss: 0.00152178\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 005/100 | Train Loss: 0.00166060\n","Adjusting Layer 1, Kernel Nodes: 711, Adptive Nodes:89\n","Iter: 006/100 | Train Loss: 0.00170764\n","Adjusting Layer 1, Kernel Nodes: 709, Adptive Nodes:91\n","Iter: 007/100 | Train Loss: 0.00160789\n","Iter: 008/100 | Train Loss: 0.00143001\n","Iter: 009/100 | Train Loss: 0.00125280\n","Iter: 010/100 | Train Loss: 0.00115051\n","Iter: 011/100 | Train Loss: 0.00113198\n","Iter: 012/100 | Train Loss: 0.00115440\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 013/100 | Train Loss: 0.00117353\n","Adjusting Layer 1, Kernel Nodes: 691, Adptive Nodes:109\n","Iter: 014/100 | Train Loss: 0.00114412\n","Iter: 015/100 | Train Loss: 0.00106005\n","Iter: 016/100 | Train Loss: 0.00096757\n","Iter: 017/100 | Train Loss: 0.00089907\n","Iter: 018/100 | Train Loss: 0.00086607\n","Iter: 019/100 | Train Loss: 0.00085779\n","Iter: 020/100 | Train Loss: 0.00085337\n","Iter: 021/100 | Train Loss: 0.00083595\n","Iter: 022/100 | Train Loss: 0.00080217\n","Iter: 023/100 | Train Loss: 0.00075993\n","Iter: 024/100 | Train Loss: 0.00072059\n","Iter: 025/100 | Train Loss: 0.00069162\n","Iter: 026/100 | Train Loss: 0.00067332\n","Iter: 027/100 | Train Loss: 0.00066097\n","Iter: 028/100 | Train Loss: 0.00064875\n","Iter: 029/100 | Train Loss: 0.00063321\n","Iter: 030/100 | Train Loss: 0.00061411\n","Iter: 031/100 | Train Loss: 0.00059365\n","Iter: 032/100 | Train Loss: 0.00057452\n","Iter: 033/100 | Train Loss: 0.00055839\n","Iter: 034/100 | Train Loss: 0.00054534\n","Iter: 035/100 | Train Loss: 0.00053444\n","Iter: 036/100 | Train Loss: 0.00052424\n","Iter: 037/100 | Train Loss: 0.00051367\n","Iter: 038/100 | Train Loss: 0.00050238\n","Iter: 039/100 | Train Loss: 0.00049062\n","Iter: 040/100 | Train Loss: 0.00047896\n","Iter: 041/100 | Train Loss: 0.00046807\n","Iter: 042/100 | Train Loss: 0.00045838\n","Iter: 043/100 | Train Loss: 0.00044988\n","Iter: 044/100 | Train Loss: 0.00044215\n","Iter: 045/100 | Train Loss: 0.00043458\n","Iter: 046/100 | Train Loss: 0.00042673\n","Iter: 047/100 | Train Loss: 0.00041850\n","Iter: 048/100 | Train Loss: 0.00041017\n","Iter: 049/100 | Train Loss: 0.00040221\n","Iter: 050/100 | Train Loss: 0.00039499\n","Iter: 051/100 | Train Loss: 0.00038853\n","Iter: 052/100 | Train Loss: 0.00038252\n","Iter: 053/100 | Train Loss: 0.00037656\n","Iter: 054/100 | Train Loss: 0.00037033\n","Iter: 055/100 | Train Loss: 0.00036386\n","Iter: 056/100 | Train Loss: 0.00035743\n","Iter: 057/100 | Train Loss: 0.00035137\n","Iter: 058/100 | Train Loss: 0.00034583\n","Iter: 059/100 | Train Loss: 0.00034068\n","Iter: 060/100 | Train Loss: 0.00033568\n","Iter: 061/100 | Train Loss: 0.00033059\n","Iter: 062/100 | Train Loss: 0.00032537\n","Iter: 063/100 | Train Loss: 0.00032013\n","Iter: 064/100 | Train Loss: 0.00031507\n","Iter: 065/100 | Train Loss: 0.00031027\n","Iter: 066/100 | Train Loss: 0.00030576\n","Iter: 067/100 | Train Loss: 0.00030142\n","Iter: 068/100 | Train Loss: 0.00029710\n","Iter: 069/100 | Train Loss: 0.00029274\n","Iter: 070/100 | Train Loss: 0.00028837\n","Iter: 071/100 | Train Loss: 0.00028409\n","Iter: 072/100 | Train Loss: 0.00027997\n","Iter: 073/100 | Train Loss: 0.00027603\n","Iter: 074/100 | Train Loss: 0.00027220\n","Iter: 075/100 | Train Loss: 0.00026842\n","Iter: 076/100 | Train Loss: 0.00026466\n","Iter: 077/100 | Train Loss: 0.00026093\n","Iter: 078/100 | Train Loss: 0.00025727\n","Iter: 079/100 | Train Loss: 0.00025370\n","Iter: 080/100 | Train Loss: 0.00025022\n","Iter: 081/100 | Train Loss: 0.00024681\n","Iter: 082/100 | Train Loss: 0.00024346\n","Iter: 083/100 | Train Loss: 0.00024015\n","Iter: 084/100 | Train Loss: 0.00023688\n","Iter: 085/100 | Train Loss: 0.00023366\n","Iter: 086/100 | Train Loss: 0.00023052\n","Iter: 087/100 | Train Loss: 0.00022745\n","Iter: 088/100 | Train Loss: 0.00022443\n","Iter: 089/100 | Train Loss: 0.00022147\n","Iter: 090/100 | Train Loss: 0.00021856\n","Iter: 091/100 | Train Loss: 0.00021569\n","Iter: 092/100 | Train Loss: 0.00021287\n","Iter: 093/100 | Train Loss: 0.00021008\n","Iter: 094/100 | Train Loss: 0.00020735\n","Iter: 095/100 | Train Loss: 0.00020466\n","Iter: 096/100 | Train Loss: 0.00020203\n","Iter: 097/100 | Train Loss: 0.00019944\n","Iter: 098/100 | Train Loss: 0.00019689\n","Iter: 099/100 | Train Loss: 0.00019439\n","\n","Iter: 099/100 | Test Loss: 0.00100655 | Test acc: 64.8100\n","scale:1.100000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00234266\n","Iter: 001/100 | Train Loss: 0.00195195\n","Iter: 002/100 | Train Loss: 0.00158488\n","Iter: 003/100 | Train Loss: 0.00144418\n","Iter: 004/100 | Train Loss: 0.00152951\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00168188\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00172915\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00161204\n","Iter: 008/100 | Train Loss: 0.00140127\n","Iter: 009/100 | Train Loss: 0.00121521\n","Iter: 010/100 | Train Loss: 0.00112615\n","Iter: 011/100 | Train Loss: 0.00112766\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 012/100 | Train Loss: 0.00116207\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 013/100 | Train Loss: 0.00116916\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00112219\n","Iter: 015/100 | Train Loss: 0.00103408\n","Iter: 016/100 | Train Loss: 0.00093964\n","Iter: 017/100 | Train Loss: 0.00087037\n","Iter: 018/100 | Train Loss: 0.00083773\n","Iter: 019/100 | Train Loss: 0.00083168\n","Iter: 020/100 | Train Loss: 0.00083146\n","Iter: 021/100 | Train Loss: 0.00081917\n","Iter: 022/100 | Train Loss: 0.00078905\n","Iter: 023/100 | Train Loss: 0.00074710\n","Iter: 024/100 | Train Loss: 0.00070507\n","Iter: 025/100 | Train Loss: 0.00067276\n","Iter: 026/100 | Train Loss: 0.00065328\n","Iter: 027/100 | Train Loss: 0.00064306\n","Iter: 028/100 | Train Loss: 0.00063541\n","Iter: 029/100 | Train Loss: 0.00062421\n","Iter: 030/100 | Train Loss: 0.00060711\n","Iter: 031/100 | Train Loss: 0.00058564\n","Iter: 032/100 | Train Loss: 0.00056381\n","Iter: 033/100 | Train Loss: 0.00054543\n","Iter: 034/100 | Train Loss: 0.00053223\n","Iter: 035/100 | Train Loss: 0.00052340\n","Iter: 036/100 | Train Loss: 0.00051622\n","Iter: 037/100 | Train Loss: 0.00050792\n","Iter: 038/100 | Train Loss: 0.00049711\n","Iter: 039/100 | Train Loss: 0.00048431\n","Iter: 040/100 | Train Loss: 0.00047133\n","Iter: 041/100 | Train Loss: 0.00045991\n","Iter: 042/100 | Train Loss: 0.00045093\n","Iter: 043/100 | Train Loss: 0.00044396\n","Iter: 044/100 | Train Loss: 0.00043765\n","Iter: 045/100 | Train Loss: 0.00043077\n","Iter: 046/100 | Train Loss: 0.00042279\n","Iter: 047/100 | Train Loss: 0.00041407\n","Iter: 048/100 | Train Loss: 0.00040549\n","Iter: 049/100 | Train Loss: 0.00039778\n","Iter: 050/100 | Train Loss: 0.00039122\n","Iter: 051/100 | Train Loss: 0.00038546\n","Iter: 052/100 | Train Loss: 0.00037992\n","Iter: 053/100 | Train Loss: 0.00037407\n","Iter: 054/100 | Train Loss: 0.00036781\n","Iter: 055/100 | Train Loss: 0.00036137\n","Iter: 056/100 | Train Loss: 0.00035514\n","Iter: 057/100 | Train Loss: 0.00034940\n","Iter: 058/100 | Train Loss: 0.00034419\n","Iter: 059/100 | Train Loss: 0.00033931\n","Iter: 060/100 | Train Loss: 0.00033447\n","Iter: 061/100 | Train Loss: 0.00032948\n","Iter: 062/100 | Train Loss: 0.00032439\n","Iter: 063/100 | Train Loss: 0.00031931\n","Iter: 064/100 | Train Loss: 0.00031444\n","Iter: 065/100 | Train Loss: 0.00030986\n","Iter: 066/100 | Train Loss: 0.00030550\n","Iter: 067/100 | Train Loss: 0.00030127\n","Iter: 068/100 | Train Loss: 0.00029704\n","Iter: 069/100 | Train Loss: 0.00029279\n","Iter: 070/100 | Train Loss: 0.00028854\n","Iter: 071/100 | Train Loss: 0.00028438\n","Iter: 072/100 | Train Loss: 0.00028038\n","Iter: 073/100 | Train Loss: 0.00027652\n","Iter: 074/100 | Train Loss: 0.00027278\n","Iter: 075/100 | Train Loss: 0.00026908\n","Iter: 076/100 | Train Loss: 0.00026541\n","Iter: 077/100 | Train Loss: 0.00026176\n","Iter: 078/100 | Train Loss: 0.00025815\n","Iter: 079/100 | Train Loss: 0.00025463\n","Iter: 080/100 | Train Loss: 0.00025119\n","Iter: 081/100 | Train Loss: 0.00024786\n","Iter: 082/100 | Train Loss: 0.00024460\n","Iter: 083/100 | Train Loss: 0.00024140\n","Iter: 084/100 | Train Loss: 0.00023824\n","Iter: 085/100 | Train Loss: 0.00023512\n","Iter: 086/100 | Train Loss: 0.00023206\n","Iter: 087/100 | Train Loss: 0.00022906\n","Iter: 088/100 | Train Loss: 0.00022614\n","Iter: 089/100 | Train Loss: 0.00022328\n","Iter: 090/100 | Train Loss: 0.00022046\n","Iter: 091/100 | Train Loss: 0.00021769\n","Iter: 092/100 | Train Loss: 0.00021496\n","Iter: 093/100 | Train Loss: 0.00021227\n","Iter: 094/100 | Train Loss: 0.00020962\n","Iter: 095/100 | Train Loss: 0.00020701\n","Iter: 096/100 | Train Loss: 0.00020444\n","Iter: 097/100 | Train Loss: 0.00020191\n","Iter: 098/100 | Train Loss: 0.00019943\n","Iter: 099/100 | Train Loss: 0.00019697\n","\n","Iter: 099/100 | Test Loss: 0.00100522 | Test acc: 64.3000\n","scale:1.100000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00232881\n","Iter: 001/100 | Train Loss: 0.00193002\n","Iter: 002/100 | Train Loss: 0.00156647\n","Iter: 003/100 | Train Loss: 0.00144146\n","Iter: 004/100 | Train Loss: 0.00154150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00171625\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00175119\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00157535\n","Iter: 008/100 | Train Loss: 0.00130236\n","Iter: 009/100 | Train Loss: 0.00113877\n","Iter: 010/100 | Train Loss: 0.00112184\n","Iter: 011/100 | Train Loss: 0.00117590\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00118716\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 013/100 | Train Loss: 0.00112095\n","Iter: 014/100 | Train Loss: 0.00101296\n","Iter: 015/100 | Train Loss: 0.00090179\n","Iter: 016/100 | Train Loss: 0.00083244\n","Iter: 017/100 | Train Loss: 0.00081256\n","Iter: 018/100 | Train Loss: 0.00081535\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 019/100 | Train Loss: 0.00081765\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 020/100 | Train Loss: 0.00078731\n","Iter: 021/100 | Train Loss: 0.00072169\n","Iter: 022/100 | Train Loss: 0.00066552\n","Iter: 023/100 | Train Loss: 0.00063582\n","Iter: 024/100 | Train Loss: 0.00062831\n","Iter: 025/100 | Train Loss: 0.00062599\n","Iter: 026/100 | Train Loss: 0.00061348\n","Iter: 027/100 | Train Loss: 0.00058657\n","Iter: 028/100 | Train Loss: 0.00055258\n","Iter: 029/100 | Train Loss: 0.00052379\n","Iter: 030/100 | Train Loss: 0.00050754\n","Iter: 031/100 | Train Loss: 0.00050125\n","Iter: 032/100 | Train Loss: 0.00049591\n","Iter: 033/100 | Train Loss: 0.00048370\n","Iter: 034/100 | Train Loss: 0.00046443\n","Iter: 035/100 | Train Loss: 0.00044402\n","Iter: 036/100 | Train Loss: 0.00042868\n","Iter: 037/100 | Train Loss: 0.00041995\n","Iter: 038/100 | Train Loss: 0.00041443\n","Iter: 039/100 | Train Loss: 0.00040743\n","Iter: 040/100 | Train Loss: 0.00039640\n","Iter: 041/100 | Train Loss: 0.00038294\n","Iter: 042/100 | Train Loss: 0.00037059\n","Iter: 043/100 | Train Loss: 0.00036187\n","Iter: 044/100 | Train Loss: 0.00035617\n","Iter: 045/100 | Train Loss: 0.00035071\n","Iter: 046/100 | Train Loss: 0.00034318\n","Iter: 047/100 | Train Loss: 0.00033365\n","Iter: 048/100 | Train Loss: 0.00032425\n","Iter: 049/100 | Train Loss: 0.00031682\n","Iter: 050/100 | Train Loss: 0.00031137\n","Iter: 051/100 | Train Loss: 0.00030637\n","Iter: 052/100 | Train Loss: 0.00030034\n","Iter: 053/100 | Train Loss: 0.00029322\n","Iter: 054/100 | Train Loss: 0.00028608\n","Iter: 055/100 | Train Loss: 0.00027997\n","Iter: 056/100 | Train Loss: 0.00027491\n","Iter: 057/100 | Train Loss: 0.00027027\n","Iter: 058/100 | Train Loss: 0.00026540\n","Iter: 059/100 | Train Loss: 0.00026018\n","Iter: 060/100 | Train Loss: 0.00025490\n","Iter: 061/100 | Train Loss: 0.00024998\n","Iter: 062/100 | Train Loss: 0.00024562\n","Iter: 063/100 | Train Loss: 0.00024158\n","Iter: 064/100 | Train Loss: 0.00023755\n","Iter: 065/100 | Train Loss: 0.00023328\n","Iter: 066/100 | Train Loss: 0.00022888\n","Iter: 067/100 | Train Loss: 0.00022463\n","Iter: 068/100 | Train Loss: 0.00022074\n","Iter: 069/100 | Train Loss: 0.00021714\n","Iter: 070/100 | Train Loss: 0.00021357\n","Iter: 071/100 | Train Loss: 0.00020985\n","Iter: 072/100 | Train Loss: 0.00020606\n","Iter: 073/100 | Train Loss: 0.00020239\n","Iter: 074/100 | Train Loss: 0.00019898\n","Iter: 075/100 | Train Loss: 0.00019579\n","Iter: 076/100 | Train Loss: 0.00019265\n","Iter: 077/100 | Train Loss: 0.00018943\n","Iter: 078/100 | Train Loss: 0.00018620\n","Iter: 079/100 | Train Loss: 0.00018308\n","Iter: 080/100 | Train Loss: 0.00018015\n","Iter: 081/100 | Train Loss: 0.00017735\n","Iter: 082/100 | Train Loss: 0.00017459\n","Iter: 083/100 | Train Loss: 0.00017181\n","Iter: 084/100 | Train Loss: 0.00016904\n","Iter: 085/100 | Train Loss: 0.00016637\n","Iter: 086/100 | Train Loss: 0.00016381\n","Iter: 087/100 | Train Loss: 0.00016132\n","Iter: 088/100 | Train Loss: 0.00015884\n","Iter: 089/100 | Train Loss: 0.00015638\n","Iter: 090/100 | Train Loss: 0.00015397\n","Iter: 091/100 | Train Loss: 0.00015163\n","Iter: 092/100 | Train Loss: 0.00014935\n","Iter: 093/100 | Train Loss: 0.00014711\n","Iter: 094/100 | Train Loss: 0.00014490\n","Iter: 095/100 | Train Loss: 0.00014273\n","Iter: 096/100 | Train Loss: 0.00014059\n","Iter: 097/100 | Train Loss: 0.00013849\n","Iter: 098/100 | Train Loss: 0.00013643\n","Iter: 099/100 | Train Loss: 0.00013441\n","\n","Iter: 099/100 | Test Loss: 0.00099937 | Test acc: 64.8300\n","scale:1.150000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00242929\n","Iter: 001/100 | Train Loss: 0.00197553\n","Iter: 002/100 | Train Loss: 0.00155949\n","Iter: 003/100 | Train Loss: 0.00146161\n","Iter: 004/100 | Train Loss: 0.00162683\n","Adjusting Layer 1, Kernel Nodes: 619, Adptive Nodes:181\n","Iter: 005/100 | Train Loss: 0.00175646\n","Adjusting Layer 1, Kernel Nodes: 515, Adptive Nodes:285\n","Iter: 006/100 | Train Loss: 0.00152466\n","Iter: 007/100 | Train Loss: 0.00122813\n","Iter: 008/100 | Train Loss: 0.00114810\n","Iter: 009/100 | Train Loss: 0.00117763\n","Adjusting Layer 1, Kernel Nodes: 317, Adptive Nodes:483\n","Iter: 010/100 | Train Loss: 0.00112606\n","Iter: 011/100 | Train Loss: 0.00100134\n","Iter: 012/100 | Train Loss: 0.00087593\n","Iter: 013/100 | Train Loss: 0.00082118\n","Iter: 014/100 | Train Loss: 0.00081745\n","Iter: 015/100 | Train Loss: 0.00079848\n","Iter: 016/100 | Train Loss: 0.00073630\n","Iter: 017/100 | Train Loss: 0.00066242\n","Iter: 018/100 | Train Loss: 0.00062078\n","Iter: 019/100 | Train Loss: 0.00061447\n","Iter: 020/100 | Train Loss: 0.00060904\n","Iter: 021/100 | Train Loss: 0.00057903\n","Iter: 022/100 | Train Loss: 0.00053582\n","Iter: 023/100 | Train Loss: 0.00050599\n","Iter: 024/100 | Train Loss: 0.00049621\n","Iter: 025/100 | Train Loss: 0.00049116\n","Iter: 026/100 | Train Loss: 0.00047548\n","Iter: 027/100 | Train Loss: 0.00045033\n","Iter: 028/100 | Train Loss: 0.00042791\n","Iter: 029/100 | Train Loss: 0.00041558\n","Iter: 030/100 | Train Loss: 0.00040858\n","Iter: 031/100 | Train Loss: 0.00039800\n","Iter: 032/100 | Train Loss: 0.00038153\n","Iter: 033/100 | Train Loss: 0.00036425\n","Iter: 034/100 | Train Loss: 0.00035154\n","Iter: 035/100 | Train Loss: 0.00034326\n","Iter: 036/100 | Train Loss: 0.00033481\n","Iter: 037/100 | Train Loss: 0.00032355\n","Iter: 038/100 | Train Loss: 0.00031091\n","Iter: 039/100 | Train Loss: 0.00029984\n","Iter: 040/100 | Train Loss: 0.00029122\n","Iter: 041/100 | Train Loss: 0.00028391\n","Iter: 042/100 | Train Loss: 0.00027606\n","Iter: 043/100 | Train Loss: 0.00026710\n","Iter: 044/100 | Train Loss: 0.00025821\n","Iter: 045/100 | Train Loss: 0.00025072\n","Iter: 046/100 | Train Loss: 0.00024454\n","Iter: 047/100 | Train Loss: 0.00023847\n","Iter: 048/100 | Train Loss: 0.00023178\n","Iter: 049/100 | Train Loss: 0.00022486\n","Iter: 050/100 | Train Loss: 0.00021858\n","Iter: 051/100 | Train Loss: 0.00021315\n","Iter: 052/100 | Train Loss: 0.00020807\n","Iter: 053/100 | Train Loss: 0.00020276\n","Iter: 054/100 | Train Loss: 0.00019724\n","Iter: 055/100 | Train Loss: 0.00019204\n","Iter: 056/100 | Train Loss: 0.00018745\n","Iter: 057/100 | Train Loss: 0.00018326\n","Iter: 058/100 | Train Loss: 0.00017902\n","Iter: 059/100 | Train Loss: 0.00017465\n","Iter: 060/100 | Train Loss: 0.00017033\n","Iter: 061/100 | Train Loss: 0.00016628\n","Iter: 062/100 | Train Loss: 0.00016251\n","Iter: 063/100 | Train Loss: 0.00015886\n","Iter: 064/100 | Train Loss: 0.00015520\n","Iter: 065/100 | Train Loss: 0.00015155\n","Iter: 066/100 | Train Loss: 0.00014801\n","Iter: 067/100 | Train Loss: 0.00014468\n","Iter: 068/100 | Train Loss: 0.00014152\n","Iter: 069/100 | Train Loss: 0.00013839\n","Iter: 070/100 | Train Loss: 0.00013529\n","Iter: 071/100 | Train Loss: 0.00013226\n","Iter: 072/100 | Train Loss: 0.00012938\n","Iter: 073/100 | Train Loss: 0.00012661\n","Iter: 074/100 | Train Loss: 0.00012390\n","Iter: 075/100 | Train Loss: 0.00012122\n","Iter: 076/100 | Train Loss: 0.00011861\n","Iter: 077/100 | Train Loss: 0.00011608\n","Iter: 078/100 | Train Loss: 0.00011364\n","Iter: 079/100 | Train Loss: 0.00011127\n","Iter: 080/100 | Train Loss: 0.00010895\n","Iter: 081/100 | Train Loss: 0.00010667\n","Iter: 082/100 | Train Loss: 0.00010445\n","Iter: 083/100 | Train Loss: 0.00010230\n","Iter: 084/100 | Train Loss: 0.00010021\n","Iter: 085/100 | Train Loss: 0.00009818\n","Iter: 086/100 | Train Loss: 0.00009618\n","Iter: 087/100 | Train Loss: 0.00009422\n","Iter: 088/100 | Train Loss: 0.00009232\n","Iter: 089/100 | Train Loss: 0.00009049\n","Iter: 090/100 | Train Loss: 0.00008869\n","Iter: 091/100 | Train Loss: 0.00008692\n","Iter: 092/100 | Train Loss: 0.00008520\n","Iter: 093/100 | Train Loss: 0.00008352\n","Iter: 094/100 | Train Loss: 0.00008188\n","Iter: 095/100 | Train Loss: 0.00008029\n","Iter: 096/100 | Train Loss: 0.00007873\n","Iter: 097/100 | Train Loss: 0.00007720\n","Iter: 098/100 | Train Loss: 0.00007572\n","Iter: 099/100 | Train Loss: 0.00007426\n","\n","Iter: 099/100 | Test Loss: 0.00105436 | Test acc: 64.1500\n","scale:1.150000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00242207\n","Iter: 001/100 | Train Loss: 0.00199484\n","Iter: 002/100 | Train Loss: 0.00158574\n","Iter: 003/100 | Train Loss: 0.00145458\n","Iter: 004/100 | Train Loss: 0.00158385\n","Adjusting Layer 1, Kernel Nodes: 678, Adptive Nodes:122\n","Iter: 005/100 | Train Loss: 0.00172624\n","Adjusting Layer 1, Kernel Nodes: 473, Adptive Nodes:327\n","Iter: 006/100 | Train Loss: 0.00162410\n","Iter: 007/100 | Train Loss: 0.00133012\n","Iter: 008/100 | Train Loss: 0.00115538\n","Iter: 009/100 | Train Loss: 0.00116299\n","Adjusting Layer 1, Kernel Nodes: 798, Adptive Nodes:2\n","Iter: 010/100 | Train Loss: 0.00119879\n","Adjusting Layer 1, Kernel Nodes: 6, Adptive Nodes:794\n","Iter: 011/100 | Train Loss: 0.00102651\n","Iter: 012/100 | Train Loss: 0.00090570\n","Iter: 013/100 | Train Loss: 0.00082498\n","Iter: 014/100 | Train Loss: 0.00079587\n","Iter: 015/100 | Train Loss: 0.00077845\n","Iter: 016/100 | Train Loss: 0.00073804\n","Iter: 017/100 | Train Loss: 0.00067915\n","Iter: 018/100 | Train Loss: 0.00062809\n","Iter: 019/100 | Train Loss: 0.00059922\n","Iter: 020/100 | Train Loss: 0.00058425\n","Iter: 021/100 | Train Loss: 0.00056728\n","Iter: 022/100 | Train Loss: 0.00054154\n","Iter: 023/100 | Train Loss: 0.00051197\n","Iter: 024/100 | Train Loss: 0.00048770\n","Iter: 025/100 | Train Loss: 0.00047163\n","Iter: 026/100 | Train Loss: 0.00045929\n","Iter: 027/100 | Train Loss: 0.00044523\n","Iter: 028/100 | Train Loss: 0.00042788\n","Iter: 029/100 | Train Loss: 0.00040961\n","Iter: 030/100 | Train Loss: 0.00039361\n","Iter: 031/100 | Train Loss: 0.00038095\n","Iter: 032/100 | Train Loss: 0.00037010\n","Iter: 033/100 | Train Loss: 0.00035889\n","Iter: 034/100 | Train Loss: 0.00034630\n","Iter: 035/100 | Train Loss: 0.00033317\n","Iter: 036/100 | Train Loss: 0.00032115\n","Iter: 037/100 | Train Loss: 0.00031116\n","Iter: 038/100 | Train Loss: 0.00030259\n","Iter: 039/100 | Train Loss: 0.00029391\n","Iter: 040/100 | Train Loss: 0.00028424\n","Iter: 041/100 | Train Loss: 0.00027426\n","Iter: 042/100 | Train Loss: 0.00026536\n","Iter: 043/100 | Train Loss: 0.00025809\n","Iter: 044/100 | Train Loss: 0.00025163\n","Iter: 045/100 | Train Loss: 0.00024475\n","Iter: 046/100 | Train Loss: 0.00023721\n","Iter: 047/100 | Train Loss: 0.00022982\n","Iter: 048/100 | Train Loss: 0.00022348\n","Iter: 049/100 | Train Loss: 0.00021815\n","Iter: 050/100 | Train Loss: 0.00021298\n","Iter: 051/100 | Train Loss: 0.00020734\n","Iter: 052/100 | Train Loss: 0.00020142\n","Iter: 053/100 | Train Loss: 0.00019594\n","Iter: 054/100 | Train Loss: 0.00019124\n","Iter: 055/100 | Train Loss: 0.00018699\n","Iter: 056/100 | Train Loss: 0.00018260\n","Iter: 057/100 | Train Loss: 0.00017791\n","Iter: 058/100 | Train Loss: 0.00017326\n","Iter: 059/100 | Train Loss: 0.00016906\n","Iter: 060/100 | Train Loss: 0.00016531\n","Iter: 061/100 | Train Loss: 0.00016165\n","Iter: 062/100 | Train Loss: 0.00015782\n","Iter: 063/100 | Train Loss: 0.00015393\n","Iter: 064/100 | Train Loss: 0.00015026\n","Iter: 065/100 | Train Loss: 0.00014692\n","Iter: 066/100 | Train Loss: 0.00014374\n","Iter: 067/100 | Train Loss: 0.00014054\n","Iter: 068/100 | Train Loss: 0.00013727\n","Iter: 069/100 | Train Loss: 0.00013411\n","Iter: 070/100 | Train Loss: 0.00013114\n","Iter: 071/100 | Train Loss: 0.00012834\n","Iter: 072/100 | Train Loss: 0.00012558\n","Iter: 073/100 | Train Loss: 0.00012283\n","Iter: 074/100 | Train Loss: 0.00012012\n","Iter: 075/100 | Train Loss: 0.00011751\n","Iter: 076/100 | Train Loss: 0.00011503\n","Iter: 077/100 | Train Loss: 0.00011262\n","Iter: 078/100 | Train Loss: 0.00011024\n","Iter: 079/100 | Train Loss: 0.00010789\n","Iter: 080/100 | Train Loss: 0.00010561\n","Iter: 081/100 | Train Loss: 0.00010341\n","Iter: 082/100 | Train Loss: 0.00010128\n","Iter: 083/100 | Train Loss: 0.00009920\n","Iter: 084/100 | Train Loss: 0.00009716\n","Iter: 085/100 | Train Loss: 0.00009516\n","Iter: 086/100 | Train Loss: 0.00009321\n","Iter: 087/100 | Train Loss: 0.00009133\n","Iter: 088/100 | Train Loss: 0.00008948\n","Iter: 089/100 | Train Loss: 0.00008769\n","Iter: 090/100 | Train Loss: 0.00008593\n","Iter: 091/100 | Train Loss: 0.00008421\n","Iter: 092/100 | Train Loss: 0.00008254\n","Iter: 093/100 | Train Loss: 0.00008092\n","Iter: 094/100 | Train Loss: 0.00007933\n","Iter: 095/100 | Train Loss: 0.00007777\n","Iter: 096/100 | Train Loss: 0.00007625\n","Iter: 097/100 | Train Loss: 0.00007476\n","Iter: 098/100 | Train Loss: 0.00007332\n","Iter: 099/100 | Train Loss: 0.00007191\n","\n","Iter: 099/100 | Test Loss: 0.00105323 | Test acc: 64.3500\n","scale:1.150000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00241367\n","Iter: 001/100 | Train Loss: 0.00200519\n","Iter: 002/100 | Train Loss: 0.00160402\n","Iter: 003/100 | Train Loss: 0.00145298\n","Iter: 004/100 | Train Loss: 0.00155528\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 005/100 | Train Loss: 0.00166651\n","Adjusting Layer 1, Kernel Nodes: 459, Adptive Nodes:341\n","Iter: 006/100 | Train Loss: 0.00168886\n","Adjusting Layer 1, Kernel Nodes: 516, Adptive Nodes:284\n","Iter: 007/100 | Train Loss: 0.00139140\n","Iter: 008/100 | Train Loss: 0.00117181\n","Iter: 009/100 | Train Loss: 0.00116457\n","Iter: 010/100 | Train Loss: 0.00120356\n","Adjusting Layer 1, Kernel Nodes: 155, Adptive Nodes:645\n","Iter: 011/100 | Train Loss: 0.00110332\n","Iter: 012/100 | Train Loss: 0.00101503\n","Iter: 013/100 | Train Loss: 0.00091237\n","Iter: 014/100 | Train Loss: 0.00083884\n","Iter: 015/100 | Train Loss: 0.00080601\n","Iter: 016/100 | Train Loss: 0.00079406\n","Iter: 017/100 | Train Loss: 0.00077528\n","Iter: 018/100 | Train Loss: 0.00073722\n","Iter: 019/100 | Train Loss: 0.00068700\n","Iter: 020/100 | Train Loss: 0.00064185\n","Iter: 021/100 | Train Loss: 0.00061311\n","Iter: 022/100 | Train Loss: 0.00059904\n","Iter: 023/100 | Train Loss: 0.00058918\n","Iter: 024/100 | Train Loss: 0.00057354\n","Iter: 025/100 | Train Loss: 0.00054941\n","Iter: 026/100 | Train Loss: 0.00052234\n","Iter: 027/100 | Train Loss: 0.00050054\n","Iter: 028/100 | Train Loss: 0.00048754\n","Iter: 029/100 | Train Loss: 0.00047975\n","Iter: 030/100 | Train Loss: 0.00047062\n","Iter: 031/100 | Train Loss: 0.00045657\n","Iter: 032/100 | Train Loss: 0.00043914\n","Iter: 033/100 | Train Loss: 0.00042305\n","Iter: 034/100 | Train Loss: 0.00041151\n","Iter: 035/100 | Train Loss: 0.00040356\n","Iter: 036/100 | Train Loss: 0.00039575\n","Iter: 037/100 | Train Loss: 0.00038559\n","Iter: 038/100 | Train Loss: 0.00037356\n","Iter: 039/100 | Train Loss: 0.00036191\n","Iter: 040/100 | Train Loss: 0.00035219\n","Iter: 041/100 | Train Loss: 0.00034436\n","Iter: 042/100 | Train Loss: 0.00033715\n","Iter: 043/100 | Train Loss: 0.00032936\n","Iter: 044/100 | Train Loss: 0.00032083\n","Iter: 045/100 | Train Loss: 0.00031227\n","Iter: 046/100 | Train Loss: 0.00030443\n","Iter: 047/100 | Train Loss: 0.00029752\n","Iter: 048/100 | Train Loss: 0.00029113\n","Iter: 049/100 | Train Loss: 0.00028480\n","Iter: 050/100 | Train Loss: 0.00027829\n","Iter: 051/100 | Train Loss: 0.00027177\n","Iter: 052/100 | Train Loss: 0.00026548\n","Iter: 053/100 | Train Loss: 0.00025959\n","Iter: 054/100 | Train Loss: 0.00025404\n","Iter: 055/100 | Train Loss: 0.00024872\n","Iter: 056/100 | Train Loss: 0.00024354\n","Iter: 057/100 | Train Loss: 0.00023838\n","Iter: 058/100 | Train Loss: 0.00023322\n","Iter: 059/100 | Train Loss: 0.00022821\n","Iter: 060/100 | Train Loss: 0.00022347\n","Iter: 061/100 | Train Loss: 0.00021901\n","Iter: 062/100 | Train Loss: 0.00021469\n","Iter: 063/100 | Train Loss: 0.00021037\n","Iter: 064/100 | Train Loss: 0.00020603\n","Iter: 065/100 | Train Loss: 0.00020179\n","Iter: 066/100 | Train Loss: 0.00019776\n","Iter: 067/100 | Train Loss: 0.00019391\n","Iter: 068/100 | Train Loss: 0.00019016\n","Iter: 069/100 | Train Loss: 0.00018646\n","Iter: 070/100 | Train Loss: 0.00018277\n","Iter: 071/100 | Train Loss: 0.00017917\n","Iter: 072/100 | Train Loss: 0.00017572\n","Iter: 073/100 | Train Loss: 0.00017240\n","Iter: 074/100 | Train Loss: 0.00016913\n","Iter: 075/100 | Train Loss: 0.00016590\n","Iter: 076/100 | Train Loss: 0.00016274\n","Iter: 077/100 | Train Loss: 0.00015965\n","Iter: 078/100 | Train Loss: 0.00015666\n","Iter: 079/100 | Train Loss: 0.00015375\n","Iter: 080/100 | Train Loss: 0.00015089\n","Iter: 081/100 | Train Loss: 0.00014809\n","Iter: 082/100 | Train Loss: 0.00014536\n","Iter: 083/100 | Train Loss: 0.00014270\n","Iter: 084/100 | Train Loss: 0.00014008\n","Iter: 085/100 | Train Loss: 0.00013752\n","Iter: 086/100 | Train Loss: 0.00013501\n","Iter: 087/100 | Train Loss: 0.00013256\n","Iter: 088/100 | Train Loss: 0.00013017\n","Iter: 089/100 | Train Loss: 0.00012784\n","Iter: 090/100 | Train Loss: 0.00012554\n","Iter: 091/100 | Train Loss: 0.00012330\n","Iter: 092/100 | Train Loss: 0.00012111\n","Iter: 093/100 | Train Loss: 0.00011897\n","Iter: 094/100 | Train Loss: 0.00011688\n","Iter: 095/100 | Train Loss: 0.00011483\n","Iter: 096/100 | Train Loss: 0.00011281\n","Iter: 097/100 | Train Loss: 0.00011084\n","Iter: 098/100 | Train Loss: 0.00010891\n","Iter: 099/100 | Train Loss: 0.00010702\n","\n","Iter: 099/100 | Test Loss: 0.00102314 | Test acc: 64.8800\n","scale:1.150000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00240423\n","Iter: 001/100 | Train Loss: 0.00200893\n","Iter: 002/100 | Train Loss: 0.00161547\n","Iter: 003/100 | Train Loss: 0.00145279\n","Iter: 004/100 | Train Loss: 0.00153497\n","Adjusting Layer 1, Kernel Nodes: 708, Adptive Nodes:92\n","Iter: 005/100 | Train Loss: 0.00166418\n","Adjusting Layer 1, Kernel Nodes: 685, Adptive Nodes:115\n","Iter: 006/100 | Train Loss: 0.00165493\n","Iter: 007/100 | Train Loss: 0.00146913\n","Iter: 008/100 | Train Loss: 0.00124448\n","Iter: 009/100 | Train Loss: 0.00115110\n","Iter: 010/100 | Train Loss: 0.00117879\n","Adjusting Layer 1, Kernel Nodes: 497, Adptive Nodes:303\n","Iter: 011/100 | Train Loss: 0.00118677\n","Adjusting Layer 1, Kernel Nodes: 506, Adptive Nodes:294\n","Iter: 012/100 | Train Loss: 0.00109973\n","Iter: 013/100 | Train Loss: 0.00094648\n","Iter: 014/100 | Train Loss: 0.00085771\n","Iter: 015/100 | Train Loss: 0.00084909\n","Iter: 016/100 | Train Loss: 0.00084358\n","Iter: 017/100 | Train Loss: 0.00078658\n","Iter: 018/100 | Train Loss: 0.00070319\n","Iter: 019/100 | Train Loss: 0.00064693\n","Iter: 020/100 | Train Loss: 0.00063022\n","Iter: 021/100 | Train Loss: 0.00062220\n","Iter: 022/100 | Train Loss: 0.00059556\n","Iter: 023/100 | Train Loss: 0.00055449\n","Iter: 024/100 | Train Loss: 0.00052008\n","Iter: 025/100 | Train Loss: 0.00050226\n","Iter: 026/100 | Train Loss: 0.00049236\n","Iter: 027/100 | Train Loss: 0.00047798\n","Iter: 028/100 | Train Loss: 0.00045660\n","Iter: 029/100 | Train Loss: 0.00043462\n","Iter: 030/100 | Train Loss: 0.00041788\n","Iter: 031/100 | Train Loss: 0.00040628\n","Iter: 032/100 | Train Loss: 0.00039536\n","Iter: 033/100 | Train Loss: 0.00038181\n","Iter: 034/100 | Train Loss: 0.00036645\n","Iter: 035/100 | Train Loss: 0.00035226\n","Iter: 036/100 | Train Loss: 0.00034081\n","Iter: 037/100 | Train Loss: 0.00033115\n","Iter: 038/100 | Train Loss: 0.00032112\n","Iter: 039/100 | Train Loss: 0.00031015\n","Iter: 040/100 | Train Loss: 0.00029911\n","Iter: 041/100 | Train Loss: 0.00028896\n","Iter: 042/100 | Train Loss: 0.00027998\n","Iter: 043/100 | Train Loss: 0.00027179\n","Iter: 044/100 | Train Loss: 0.00026377\n","Iter: 045/100 | Train Loss: 0.00025552\n","Iter: 046/100 | Train Loss: 0.00024718\n","Iter: 047/100 | Train Loss: 0.00023943\n","Iter: 048/100 | Train Loss: 0.00023265\n","Iter: 049/100 | Train Loss: 0.00022648\n","Iter: 050/100 | Train Loss: 0.00022017\n","Iter: 051/100 | Train Loss: 0.00021351\n","Iter: 052/100 | Train Loss: 0.00020708\n","Iter: 053/100 | Train Loss: 0.00020147\n","Iter: 054/100 | Train Loss: 0.00019649\n","Iter: 055/100 | Train Loss: 0.00019147\n","Iter: 056/100 | Train Loss: 0.00018619\n","Iter: 057/100 | Train Loss: 0.00018096\n","Iter: 058/100 | Train Loss: 0.00017624\n","Iter: 059/100 | Train Loss: 0.00017201\n","Iter: 060/100 | Train Loss: 0.00016787\n","Iter: 061/100 | Train Loss: 0.00016354\n","Iter: 062/100 | Train Loss: 0.00015918\n","Iter: 063/100 | Train Loss: 0.00015515\n","Iter: 064/100 | Train Loss: 0.00015150\n","Iter: 065/100 | Train Loss: 0.00014796\n","Iter: 066/100 | Train Loss: 0.00014431\n","Iter: 067/100 | Train Loss: 0.00014066\n","Iter: 068/100 | Train Loss: 0.00013722\n","Iter: 069/100 | Train Loss: 0.00013400\n","Iter: 070/100 | Train Loss: 0.00013088\n","Iter: 071/100 | Train Loss: 0.00012777\n","Iter: 072/100 | Train Loss: 0.00012471\n","Iter: 073/100 | Train Loss: 0.00012176\n","Iter: 074/100 | Train Loss: 0.00011894\n","Iter: 075/100 | Train Loss: 0.00011621\n","Iter: 076/100 | Train Loss: 0.00011355\n","Iter: 077/100 | Train Loss: 0.00011092\n","Iter: 078/100 | Train Loss: 0.00010837\n","Iter: 079/100 | Train Loss: 0.00010591\n","Iter: 080/100 | Train Loss: 0.00010353\n","Iter: 081/100 | Train Loss: 0.00010122\n","Iter: 082/100 | Train Loss: 0.00009896\n","Iter: 083/100 | Train Loss: 0.00009675\n","Iter: 084/100 | Train Loss: 0.00009460\n","Iter: 085/100 | Train Loss: 0.00009252\n","Iter: 086/100 | Train Loss: 0.00009049\n","Iter: 087/100 | Train Loss: 0.00008852\n","Iter: 088/100 | Train Loss: 0.00008659\n","Iter: 089/100 | Train Loss: 0.00008471\n","Iter: 090/100 | Train Loss: 0.00008289\n","Iter: 091/100 | Train Loss: 0.00008112\n","Iter: 092/100 | Train Loss: 0.00007938\n","Iter: 093/100 | Train Loss: 0.00007769\n","Iter: 094/100 | Train Loss: 0.00007603\n","Iter: 095/100 | Train Loss: 0.00007443\n","Iter: 096/100 | Train Loss: 0.00007286\n","Iter: 097/100 | Train Loss: 0.00007133\n","Iter: 098/100 | Train Loss: 0.00006984\n","Iter: 099/100 | Train Loss: 0.00006839\n","\n","Iter: 099/100 | Test Loss: 0.00104702 | Test acc: 64.1600\n","scale:1.150000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00239376\n","Iter: 001/100 | Train Loss: 0.00200642\n","Iter: 002/100 | Train Loss: 0.00161973\n","Iter: 003/100 | Train Loss: 0.00145260\n","Iter: 004/100 | Train Loss: 0.00152343\n","Adjusting Layer 1, Kernel Nodes: 692, Adptive Nodes:108\n","Iter: 005/100 | Train Loss: 0.00165162\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 006/100 | Train Loss: 0.00165843\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00144363\n","Iter: 008/100 | Train Loss: 0.00123881\n","Iter: 009/100 | Train Loss: 0.00115437\n","Iter: 010/100 | Train Loss: 0.00117641\n","Adjusting Layer 1, Kernel Nodes: 335, Adptive Nodes:465\n","Iter: 011/100 | Train Loss: 0.00114289\n","Iter: 012/100 | Train Loss: 0.00106371\n","Iter: 013/100 | Train Loss: 0.00094949\n","Iter: 014/100 | Train Loss: 0.00086857\n","Iter: 015/100 | Train Loss: 0.00084126\n","Iter: 016/100 | Train Loss: 0.00082962\n","Iter: 017/100 | Train Loss: 0.00079331\n","Iter: 018/100 | Train Loss: 0.00073330\n","Iter: 019/100 | Train Loss: 0.00067895\n","Iter: 020/100 | Train Loss: 0.00064840\n","Iter: 021/100 | Train Loss: 0.00063308\n","Iter: 022/100 | Train Loss: 0.00061493\n","Iter: 023/100 | Train Loss: 0.00058719\n","Iter: 024/100 | Train Loss: 0.00055682\n","Iter: 025/100 | Train Loss: 0.00053257\n","Iter: 026/100 | Train Loss: 0.00051561\n","Iter: 027/100 | Train Loss: 0.00050154\n","Iter: 028/100 | Train Loss: 0.00048655\n","Iter: 029/100 | Train Loss: 0.00047012\n","Iter: 030/100 | Train Loss: 0.00045343\n","Iter: 031/100 | Train Loss: 0.00043775\n","Iter: 032/100 | Train Loss: 0.00042414\n","Iter: 033/100 | Train Loss: 0.00041266\n","Iter: 034/100 | Train Loss: 0.00040198\n","Iter: 035/100 | Train Loss: 0.00039042\n","Iter: 036/100 | Train Loss: 0.00037774\n","Iter: 037/100 | Train Loss: 0.00036553\n","Iter: 038/100 | Train Loss: 0.00035531\n","Iter: 039/100 | Train Loss: 0.00034670\n","Iter: 040/100 | Train Loss: 0.00033803\n","Iter: 041/100 | Train Loss: 0.00032837\n","Iter: 042/100 | Train Loss: 0.00031845\n","Iter: 043/100 | Train Loss: 0.00030954\n","Iter: 044/100 | Train Loss: 0.00030191\n","Iter: 045/100 | Train Loss: 0.00029471\n","Iter: 046/100 | Train Loss: 0.00028712\n","Iter: 047/100 | Train Loss: 0.00027927\n","Iter: 048/100 | Train Loss: 0.00027182\n","Iter: 049/100 | Train Loss: 0.00026507\n","Iter: 050/100 | Train Loss: 0.00025876\n","Iter: 051/100 | Train Loss: 0.00025251\n","Iter: 052/100 | Train Loss: 0.00024623\n","Iter: 053/100 | Train Loss: 0.00024014\n","Iter: 054/100 | Train Loss: 0.00023441\n","Iter: 055/100 | Train Loss: 0.00022898\n","Iter: 056/100 | Train Loss: 0.00022372\n","Iter: 057/100 | Train Loss: 0.00021859\n","Iter: 058/100 | Train Loss: 0.00021360\n","Iter: 059/100 | Train Loss: 0.00020879\n","Iter: 060/100 | Train Loss: 0.00020414\n","Iter: 061/100 | Train Loss: 0.00019959\n","Iter: 062/100 | Train Loss: 0.00019518\n","Iter: 063/100 | Train Loss: 0.00019100\n","Iter: 064/100 | Train Loss: 0.00018698\n","Iter: 065/100 | Train Loss: 0.00018301\n","Iter: 066/100 | Train Loss: 0.00017909\n","Iter: 067/100 | Train Loss: 0.00017531\n","Iter: 068/100 | Train Loss: 0.00017171\n","Iter: 069/100 | Train Loss: 0.00016824\n","Iter: 070/100 | Train Loss: 0.00016481\n","Iter: 071/100 | Train Loss: 0.00016140\n","Iter: 072/100 | Train Loss: 0.00015809\n","Iter: 073/100 | Train Loss: 0.00015492\n","Iter: 074/100 | Train Loss: 0.00015186\n","Iter: 075/100 | Train Loss: 0.00014886\n","Iter: 076/100 | Train Loss: 0.00014589\n","Iter: 077/100 | Train Loss: 0.00014298\n","Iter: 078/100 | Train Loss: 0.00014016\n","Iter: 079/100 | Train Loss: 0.00013744\n","Iter: 080/100 | Train Loss: 0.00013478\n","Iter: 081/100 | Train Loss: 0.00013216\n","Iter: 082/100 | Train Loss: 0.00012961\n","Iter: 083/100 | Train Loss: 0.00012712\n","Iter: 084/100 | Train Loss: 0.00012470\n","Iter: 085/100 | Train Loss: 0.00012234\n","Iter: 086/100 | Train Loss: 0.00012003\n","Iter: 087/100 | Train Loss: 0.00011776\n","Iter: 088/100 | Train Loss: 0.00011556\n","Iter: 089/100 | Train Loss: 0.00011340\n","Iter: 090/100 | Train Loss: 0.00011130\n","Iter: 091/100 | Train Loss: 0.00010924\n","Iter: 092/100 | Train Loss: 0.00010723\n","Iter: 093/100 | Train Loss: 0.00010526\n","Iter: 094/100 | Train Loss: 0.00010334\n","Iter: 095/100 | Train Loss: 0.00010146\n","Iter: 096/100 | Train Loss: 0.00009962\n","Iter: 097/100 | Train Loss: 0.00009783\n","Iter: 098/100 | Train Loss: 0.00009607\n","Iter: 099/100 | Train Loss: 0.00009435\n","\n","Iter: 099/100 | Test Loss: 0.00103653 | Test acc: 64.7400\n","scale:1.150000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00238238\n","Iter: 001/100 | Train Loss: 0.00199886\n","Iter: 002/100 | Train Loss: 0.00161794\n","Iter: 003/100 | Train Loss: 0.00145140\n","Iter: 004/100 | Train Loss: 0.00151821\n","Adjusting Layer 1, Kernel Nodes: 696, Adptive Nodes:104\n","Iter: 005/100 | Train Loss: 0.00164517\n","Adjusting Layer 1, Kernel Nodes: 714, Adptive Nodes:86\n","Iter: 006/100 | Train Loss: 0.00168089\n","Adjusting Layer 1, Kernel Nodes: 664, Adptive Nodes:136\n","Iter: 007/100 | Train Loss: 0.00155000\n","Iter: 008/100 | Train Loss: 0.00135170\n","Iter: 009/100 | Train Loss: 0.00119673\n","Iter: 010/100 | Train Loss: 0.00114912\n","Iter: 011/100 | Train Loss: 0.00116890\n","Adjusting Layer 1, Kernel Nodes: 555, Adptive Nodes:245\n","Iter: 012/100 | Train Loss: 0.00117851\n","Adjusting Layer 1, Kernel Nodes: 608, Adptive Nodes:192\n","Iter: 013/100 | Train Loss: 0.00112301\n","Iter: 014/100 | Train Loss: 0.00099759\n","Iter: 015/100 | Train Loss: 0.00090225\n","Iter: 016/100 | Train Loss: 0.00086880\n","Iter: 017/100 | Train Loss: 0.00086603\n","Iter: 018/100 | Train Loss: 0.00084579\n","Iter: 019/100 | Train Loss: 0.00079311\n","Iter: 020/100 | Train Loss: 0.00073024\n","Iter: 021/100 | Train Loss: 0.00068465\n","Iter: 022/100 | Train Loss: 0.00066183\n","Iter: 023/100 | Train Loss: 0.00064783\n","Iter: 024/100 | Train Loss: 0.00062820\n","Iter: 025/100 | Train Loss: 0.00060062\n","Iter: 026/100 | Train Loss: 0.00057160\n","Iter: 027/100 | Train Loss: 0.00054728\n","Iter: 028/100 | Train Loss: 0.00052875\n","Iter: 029/100 | Train Loss: 0.00051364\n","Iter: 030/100 | Train Loss: 0.00049966\n","Iter: 031/100 | Train Loss: 0.00048549\n","Iter: 032/100 | Train Loss: 0.00047044\n","Iter: 033/100 | Train Loss: 0.00045447\n","Iter: 034/100 | Train Loss: 0.00043909\n","Iter: 035/100 | Train Loss: 0.00042624\n","Iter: 036/100 | Train Loss: 0.00041634\n","Iter: 037/100 | Train Loss: 0.00040745\n","Iter: 038/100 | Train Loss: 0.00039723\n","Iter: 039/100 | Train Loss: 0.00038523\n","Iter: 040/100 | Train Loss: 0.00037323\n","Iter: 041/100 | Train Loss: 0.00036323\n","Iter: 042/100 | Train Loss: 0.00035548\n","Iter: 043/100 | Train Loss: 0.00034830\n","Iter: 044/100 | Train Loss: 0.00034008\n","Iter: 045/100 | Train Loss: 0.00033074\n","Iter: 046/100 | Train Loss: 0.00032155\n","Iter: 047/100 | Train Loss: 0.00031367\n","Iter: 048/100 | Train Loss: 0.00030706\n","Iter: 049/100 | Train Loss: 0.00030079\n","Iter: 050/100 | Train Loss: 0.00029398\n","Iter: 051/100 | Train Loss: 0.00028672\n","Iter: 052/100 | Train Loss: 0.00027967\n","Iter: 053/100 | Train Loss: 0.00027332\n","Iter: 054/100 | Train Loss: 0.00026761\n","Iter: 055/100 | Train Loss: 0.00026211\n","Iter: 056/100 | Train Loss: 0.00025648\n","Iter: 057/100 | Train Loss: 0.00025074\n","Iter: 058/100 | Train Loss: 0.00024515\n","Iter: 059/100 | Train Loss: 0.00023991\n","Iter: 060/100 | Train Loss: 0.00023500\n","Iter: 061/100 | Train Loss: 0.00023026\n","Iter: 062/100 | Train Loss: 0.00022557\n","Iter: 063/100 | Train Loss: 0.00022090\n","Iter: 064/100 | Train Loss: 0.00021632\n","Iter: 065/100 | Train Loss: 0.00021191\n","Iter: 066/100 | Train Loss: 0.00020771\n","Iter: 067/100 | Train Loss: 0.00020368\n","Iter: 068/100 | Train Loss: 0.00019974\n","Iter: 069/100 | Train Loss: 0.00019583\n","Iter: 070/100 | Train Loss: 0.00019197\n","Iter: 071/100 | Train Loss: 0.00018822\n","Iter: 072/100 | Train Loss: 0.00018463\n","Iter: 073/100 | Train Loss: 0.00018117\n","Iter: 074/100 | Train Loss: 0.00017780\n","Iter: 075/100 | Train Loss: 0.00017447\n","Iter: 076/100 | Train Loss: 0.00017117\n","Iter: 077/100 | Train Loss: 0.00016796\n","Iter: 078/100 | Train Loss: 0.00016487\n","Iter: 079/100 | Train Loss: 0.00016188\n","Iter: 080/100 | Train Loss: 0.00015894\n","Iter: 081/100 | Train Loss: 0.00015605\n","Iter: 082/100 | Train Loss: 0.00015322\n","Iter: 083/100 | Train Loss: 0.00015046\n","Iter: 084/100 | Train Loss: 0.00014776\n","Iter: 085/100 | Train Loss: 0.00014513\n","Iter: 086/100 | Train Loss: 0.00014257\n","Iter: 087/100 | Train Loss: 0.00014006\n","Iter: 088/100 | Train Loss: 0.00013758\n","Iter: 089/100 | Train Loss: 0.00013515\n","Iter: 090/100 | Train Loss: 0.00013278\n","Iter: 091/100 | Train Loss: 0.00013047\n","Iter: 092/100 | Train Loss: 0.00012822\n","Iter: 093/100 | Train Loss: 0.00012601\n","Iter: 094/100 | Train Loss: 0.00012384\n","Iter: 095/100 | Train Loss: 0.00012171\n","Iter: 096/100 | Train Loss: 0.00011963\n","Iter: 097/100 | Train Loss: 0.00011761\n","Iter: 098/100 | Train Loss: 0.00011562\n","Iter: 099/100 | Train Loss: 0.00011367\n","\n","Iter: 099/100 | Test Loss: 0.00102458 | Test acc: 64.8700\n","scale:1.150000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00236996\n","Iter: 001/100 | Train Loss: 0.00198674\n","Iter: 002/100 | Train Loss: 0.00161082\n","Iter: 003/100 | Train Loss: 0.00144929\n","Iter: 004/100 | Train Loss: 0.00151788\n","Adjusting Layer 1, Kernel Nodes: 702, Adptive Nodes:98\n","Iter: 005/100 | Train Loss: 0.00164734\n","Adjusting Layer 1, Kernel Nodes: 711, Adptive Nodes:89\n","Iter: 006/100 | Train Loss: 0.00168979\n","Adjusting Layer 1, Kernel Nodes: 685, Adptive Nodes:115\n","Iter: 007/100 | Train Loss: 0.00158582\n","Iter: 008/100 | Train Loss: 0.00140821\n","Iter: 009/100 | Train Loss: 0.00123936\n","Iter: 010/100 | Train Loss: 0.00115251\n","Iter: 011/100 | Train Loss: 0.00114607\n","Iter: 012/100 | Train Loss: 0.00116628\n","Adjusting Layer 1, Kernel Nodes: 627, Adptive Nodes:173\n","Iter: 013/100 | Train Loss: 0.00116956\n","Adjusting Layer 1, Kernel Nodes: 672, Adptive Nodes:128\n","Iter: 014/100 | Train Loss: 0.00111703\n","Iter: 015/100 | Train Loss: 0.00100955\n","Iter: 016/100 | Train Loss: 0.00092210\n","Iter: 017/100 | Train Loss: 0.00088080\n","Iter: 018/100 | Train Loss: 0.00087317\n","Iter: 019/100 | Train Loss: 0.00086711\n","Iter: 020/100 | Train Loss: 0.00084034\n","Iter: 021/100 | Train Loss: 0.00079341\n","Iter: 022/100 | Train Loss: 0.00074260\n","Iter: 023/100 | Train Loss: 0.00070382\n","Iter: 024/100 | Train Loss: 0.00068096\n","Iter: 025/100 | Train Loss: 0.00066695\n","Iter: 026/100 | Train Loss: 0.00065234\n","Iter: 027/100 | Train Loss: 0.00063219\n","Iter: 028/100 | Train Loss: 0.00060798\n","Iter: 029/100 | Train Loss: 0.00058412\n","Iter: 030/100 | Train Loss: 0.00056387\n","Iter: 031/100 | Train Loss: 0.00054767\n","Iter: 032/100 | Train Loss: 0.00053408\n","Iter: 033/100 | Train Loss: 0.00052150\n","Iter: 034/100 | Train Loss: 0.00050895\n","Iter: 035/100 | Train Loss: 0.00049614\n","Iter: 036/100 | Train Loss: 0.00048303\n","Iter: 037/100 | Train Loss: 0.00046980\n","Iter: 038/100 | Train Loss: 0.00045720\n","Iter: 039/100 | Train Loss: 0.00044611\n","Iter: 040/100 | Train Loss: 0.00043683\n","Iter: 041/100 | Train Loss: 0.00042861\n","Iter: 042/100 | Train Loss: 0.00042020\n","Iter: 043/100 | Train Loss: 0.00041083\n","Iter: 044/100 | Train Loss: 0.00040081\n","Iter: 045/100 | Train Loss: 0.00039125\n","Iter: 046/100 | Train Loss: 0.00038303\n","Iter: 047/100 | Train Loss: 0.00037612\n","Iter: 048/100 | Train Loss: 0.00036976\n","Iter: 049/100 | Train Loss: 0.00036304\n","Iter: 050/100 | Train Loss: 0.00035566\n","Iter: 051/100 | Train Loss: 0.00034804\n","Iter: 052/100 | Train Loss: 0.00034090\n","Iter: 053/100 | Train Loss: 0.00033460\n","Iter: 054/100 | Train Loss: 0.00032896\n","Iter: 055/100 | Train Loss: 0.00032345\n","Iter: 056/100 | Train Loss: 0.00031770\n","Iter: 057/100 | Train Loss: 0.00031170\n","Iter: 058/100 | Train Loss: 0.00030571\n","Iter: 059/100 | Train Loss: 0.00030009\n","Iter: 060/100 | Train Loss: 0.00029490\n","Iter: 061/100 | Train Loss: 0.00029002\n","Iter: 062/100 | Train Loss: 0.00028519\n","Iter: 063/100 | Train Loss: 0.00028029\n","Iter: 064/100 | Train Loss: 0.00027535\n","Iter: 065/100 | Train Loss: 0.00027052\n","Iter: 066/100 | Train Loss: 0.00026588\n","Iter: 067/100 | Train Loss: 0.00026147\n","Iter: 068/100 | Train Loss: 0.00025721\n","Iter: 069/100 | Train Loss: 0.00025303\n","Iter: 070/100 | Train Loss: 0.00024886\n","Iter: 071/100 | Train Loss: 0.00024472\n","Iter: 072/100 | Train Loss: 0.00024066\n","Iter: 073/100 | Train Loss: 0.00023674\n","Iter: 074/100 | Train Loss: 0.00023296\n","Iter: 075/100 | Train Loss: 0.00022931\n","Iter: 076/100 | Train Loss: 0.00022572\n","Iter: 077/100 | Train Loss: 0.00022218\n","Iter: 078/100 | Train Loss: 0.00021867\n","Iter: 079/100 | Train Loss: 0.00021524\n","Iter: 080/100 | Train Loss: 0.00021191\n","Iter: 081/100 | Train Loss: 0.00020867\n","Iter: 082/100 | Train Loss: 0.00020550\n","Iter: 083/100 | Train Loss: 0.00020238\n","Iter: 084/100 | Train Loss: 0.00019929\n","Iter: 085/100 | Train Loss: 0.00019626\n","Iter: 086/100 | Train Loss: 0.00019329\n","Iter: 087/100 | Train Loss: 0.00019039\n","Iter: 088/100 | Train Loss: 0.00018756\n","Iter: 089/100 | Train Loss: 0.00018478\n","Iter: 090/100 | Train Loss: 0.00018204\n","Iter: 091/100 | Train Loss: 0.00017935\n","Iter: 092/100 | Train Loss: 0.00017670\n","Iter: 093/100 | Train Loss: 0.00017410\n","Iter: 094/100 | Train Loss: 0.00017156\n","Iter: 095/100 | Train Loss: 0.00016907\n","Iter: 096/100 | Train Loss: 0.00016662\n","Iter: 097/100 | Train Loss: 0.00016422\n","Iter: 098/100 | Train Loss: 0.00016185\n","Iter: 099/100 | Train Loss: 0.00015952\n","\n","Iter: 099/100 | Test Loss: 0.00100924 | Test acc: 65.1900\n","scale:1.150000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00235669\n","Iter: 001/100 | Train Loss: 0.00197085\n","Iter: 002/100 | Train Loss: 0.00159951\n","Iter: 003/100 | Train Loss: 0.00144674\n","Iter: 004/100 | Train Loss: 0.00152176\n","Adjusting Layer 1, Kernel Nodes: 708, Adptive Nodes:92\n","Iter: 005/100 | Train Loss: 0.00166054\n","Adjusting Layer 1, Kernel Nodes: 710, Adptive Nodes:90\n","Iter: 006/100 | Train Loss: 0.00170761\n","Adjusting Layer 1, Kernel Nodes: 705, Adptive Nodes:95\n","Iter: 007/100 | Train Loss: 0.00160793\n","Iter: 008/100 | Train Loss: 0.00143011\n","Iter: 009/100 | Train Loss: 0.00125284\n","Iter: 010/100 | Train Loss: 0.00115038\n","Iter: 011/100 | Train Loss: 0.00113172\n","Iter: 012/100 | Train Loss: 0.00115420\n","Adjusting Layer 1, Kernel Nodes: 658, Adptive Nodes:142\n","Iter: 013/100 | Train Loss: 0.00117364\n","Adjusting Layer 1, Kernel Nodes: 677, Adptive Nodes:123\n","Iter: 014/100 | Train Loss: 0.00114407\n","Iter: 015/100 | Train Loss: 0.00106014\n","Iter: 016/100 | Train Loss: 0.00096766\n","Iter: 017/100 | Train Loss: 0.00089900\n","Iter: 018/100 | Train Loss: 0.00086579\n","Iter: 019/100 | Train Loss: 0.00085743\n","Iter: 020/100 | Train Loss: 0.00085309\n","Iter: 021/100 | Train Loss: 0.00083585\n","Iter: 022/100 | Train Loss: 0.00080224\n","Iter: 023/100 | Train Loss: 0.00076005\n","Iter: 024/100 | Train Loss: 0.00072063\n","Iter: 025/100 | Train Loss: 0.00069155\n","Iter: 026/100 | Train Loss: 0.00067317\n","Iter: 027/100 | Train Loss: 0.00066078\n","Iter: 028/100 | Train Loss: 0.00064860\n","Iter: 029/100 | Train Loss: 0.00063314\n","Iter: 030/100 | Train Loss: 0.00061412\n","Iter: 031/100 | Train Loss: 0.00059368\n","Iter: 032/100 | Train Loss: 0.00057452\n","Iter: 033/100 | Train Loss: 0.00055835\n","Iter: 034/100 | Train Loss: 0.00054525\n","Iter: 035/100 | Train Loss: 0.00053432\n","Iter: 036/100 | Train Loss: 0.00052413\n","Iter: 037/100 | Train Loss: 0.00051358\n","Iter: 038/100 | Train Loss: 0.00050231\n","Iter: 039/100 | Train Loss: 0.00049055\n","Iter: 040/100 | Train Loss: 0.00047888\n","Iter: 041/100 | Train Loss: 0.00046797\n","Iter: 042/100 | Train Loss: 0.00045825\n","Iter: 043/100 | Train Loss: 0.00044973\n","Iter: 044/100 | Train Loss: 0.00044199\n","Iter: 045/100 | Train Loss: 0.00043443\n","Iter: 046/100 | Train Loss: 0.00042660\n","Iter: 047/100 | Train Loss: 0.00041839\n","Iter: 048/100 | Train Loss: 0.00041008\n","Iter: 049/100 | Train Loss: 0.00040214\n","Iter: 050/100 | Train Loss: 0.00039491\n","Iter: 051/100 | Train Loss: 0.00038845\n","Iter: 052/100 | Train Loss: 0.00038244\n","Iter: 053/100 | Train Loss: 0.00037648\n","Iter: 054/100 | Train Loss: 0.00037026\n","Iter: 055/100 | Train Loss: 0.00036380\n","Iter: 056/100 | Train Loss: 0.00035737\n","Iter: 057/100 | Train Loss: 0.00035131\n","Iter: 058/100 | Train Loss: 0.00034576\n","Iter: 059/100 | Train Loss: 0.00034060\n","Iter: 060/100 | Train Loss: 0.00033559\n","Iter: 061/100 | Train Loss: 0.00033050\n","Iter: 062/100 | Train Loss: 0.00032529\n","Iter: 063/100 | Train Loss: 0.00032006\n","Iter: 064/100 | Train Loss: 0.00031499\n","Iter: 065/100 | Train Loss: 0.00031019\n","Iter: 066/100 | Train Loss: 0.00030568\n","Iter: 067/100 | Train Loss: 0.00030135\n","Iter: 068/100 | Train Loss: 0.00029704\n","Iter: 069/100 | Train Loss: 0.00029269\n","Iter: 070/100 | Train Loss: 0.00028834\n","Iter: 071/100 | Train Loss: 0.00028406\n","Iter: 072/100 | Train Loss: 0.00027995\n","Iter: 073/100 | Train Loss: 0.00027601\n","Iter: 074/100 | Train Loss: 0.00027219\n","Iter: 075/100 | Train Loss: 0.00026842\n","Iter: 076/100 | Train Loss: 0.00026467\n","Iter: 077/100 | Train Loss: 0.00026094\n","Iter: 078/100 | Train Loss: 0.00025728\n","Iter: 079/100 | Train Loss: 0.00025371\n","Iter: 080/100 | Train Loss: 0.00025023\n","Iter: 081/100 | Train Loss: 0.00024681\n","Iter: 082/100 | Train Loss: 0.00024347\n","Iter: 083/100 | Train Loss: 0.00024016\n","Iter: 084/100 | Train Loss: 0.00023690\n","Iter: 085/100 | Train Loss: 0.00023368\n","Iter: 086/100 | Train Loss: 0.00023054\n","Iter: 087/100 | Train Loss: 0.00022745\n","Iter: 088/100 | Train Loss: 0.00022444\n","Iter: 089/100 | Train Loss: 0.00022147\n","Iter: 090/100 | Train Loss: 0.00021856\n","Iter: 091/100 | Train Loss: 0.00021568\n","Iter: 092/100 | Train Loss: 0.00021285\n","Iter: 093/100 | Train Loss: 0.00021007\n","Iter: 094/100 | Train Loss: 0.00020733\n","Iter: 095/100 | Train Loss: 0.00020464\n","Iter: 096/100 | Train Loss: 0.00020201\n","Iter: 097/100 | Train Loss: 0.00019942\n","Iter: 098/100 | Train Loss: 0.00019687\n","Iter: 099/100 | Train Loss: 0.00019437\n","\n","Iter: 099/100 | Test Loss: 0.00100651 | Test acc: 64.8400\n","scale:1.150000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00234266\n","Iter: 001/100 | Train Loss: 0.00195195\n","Iter: 002/100 | Train Loss: 0.00158488\n","Iter: 003/100 | Train Loss: 0.00144418\n","Iter: 004/100 | Train Loss: 0.00152951\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00168188\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00172915\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00161204\n","Iter: 008/100 | Train Loss: 0.00140127\n","Iter: 009/100 | Train Loss: 0.00121521\n","Iter: 010/100 | Train Loss: 0.00112615\n","Iter: 011/100 | Train Loss: 0.00112766\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 012/100 | Train Loss: 0.00116207\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 013/100 | Train Loss: 0.00116916\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00112219\n","Iter: 015/100 | Train Loss: 0.00103408\n","Iter: 016/100 | Train Loss: 0.00093964\n","Iter: 017/100 | Train Loss: 0.00087037\n","Iter: 018/100 | Train Loss: 0.00083773\n","Iter: 019/100 | Train Loss: 0.00083168\n","Iter: 020/100 | Train Loss: 0.00083146\n","Iter: 021/100 | Train Loss: 0.00081917\n","Iter: 022/100 | Train Loss: 0.00078905\n","Iter: 023/100 | Train Loss: 0.00074710\n","Iter: 024/100 | Train Loss: 0.00070507\n","Iter: 025/100 | Train Loss: 0.00067276\n","Iter: 026/100 | Train Loss: 0.00065328\n","Iter: 027/100 | Train Loss: 0.00064306\n","Iter: 028/100 | Train Loss: 0.00063541\n","Iter: 029/100 | Train Loss: 0.00062421\n","Iter: 030/100 | Train Loss: 0.00060711\n","Iter: 031/100 | Train Loss: 0.00058564\n","Iter: 032/100 | Train Loss: 0.00056381\n","Iter: 033/100 | Train Loss: 0.00054543\n","Iter: 034/100 | Train Loss: 0.00053223\n","Iter: 035/100 | Train Loss: 0.00052340\n","Iter: 036/100 | Train Loss: 0.00051622\n","Iter: 037/100 | Train Loss: 0.00050792\n","Iter: 038/100 | Train Loss: 0.00049711\n","Iter: 039/100 | Train Loss: 0.00048431\n","Iter: 040/100 | Train Loss: 0.00047133\n","Iter: 041/100 | Train Loss: 0.00045991\n","Iter: 042/100 | Train Loss: 0.00045093\n","Iter: 043/100 | Train Loss: 0.00044396\n","Iter: 044/100 | Train Loss: 0.00043765\n","Iter: 045/100 | Train Loss: 0.00043077\n","Iter: 046/100 | Train Loss: 0.00042279\n","Iter: 047/100 | Train Loss: 0.00041407\n","Iter: 048/100 | Train Loss: 0.00040549\n","Iter: 049/100 | Train Loss: 0.00039778\n","Iter: 050/100 | Train Loss: 0.00039122\n","Iter: 051/100 | Train Loss: 0.00038546\n","Iter: 052/100 | Train Loss: 0.00037992\n","Iter: 053/100 | Train Loss: 0.00037407\n","Iter: 054/100 | Train Loss: 0.00036781\n","Iter: 055/100 | Train Loss: 0.00036137\n","Iter: 056/100 | Train Loss: 0.00035514\n","Iter: 057/100 | Train Loss: 0.00034940\n","Iter: 058/100 | Train Loss: 0.00034419\n","Iter: 059/100 | Train Loss: 0.00033931\n","Iter: 060/100 | Train Loss: 0.00033447\n","Iter: 061/100 | Train Loss: 0.00032948\n","Iter: 062/100 | Train Loss: 0.00032439\n","Iter: 063/100 | Train Loss: 0.00031931\n","Iter: 064/100 | Train Loss: 0.00031444\n","Iter: 065/100 | Train Loss: 0.00030986\n","Iter: 066/100 | Train Loss: 0.00030550\n","Iter: 067/100 | Train Loss: 0.00030127\n","Iter: 068/100 | Train Loss: 0.00029704\n","Iter: 069/100 | Train Loss: 0.00029279\n","Iter: 070/100 | Train Loss: 0.00028854\n","Iter: 071/100 | Train Loss: 0.00028438\n","Iter: 072/100 | Train Loss: 0.00028038\n","Iter: 073/100 | Train Loss: 0.00027652\n","Iter: 074/100 | Train Loss: 0.00027278\n","Iter: 075/100 | Train Loss: 0.00026908\n","Iter: 076/100 | Train Loss: 0.00026541\n","Iter: 077/100 | Train Loss: 0.00026176\n","Iter: 078/100 | Train Loss: 0.00025815\n","Iter: 079/100 | Train Loss: 0.00025463\n","Iter: 080/100 | Train Loss: 0.00025119\n","Iter: 081/100 | Train Loss: 0.00024786\n","Iter: 082/100 | Train Loss: 0.00024460\n","Iter: 083/100 | Train Loss: 0.00024140\n","Iter: 084/100 | Train Loss: 0.00023824\n","Iter: 085/100 | Train Loss: 0.00023512\n","Iter: 086/100 | Train Loss: 0.00023206\n","Iter: 087/100 | Train Loss: 0.00022906\n","Iter: 088/100 | Train Loss: 0.00022614\n","Iter: 089/100 | Train Loss: 0.00022328\n","Iter: 090/100 | Train Loss: 0.00022046\n","Iter: 091/100 | Train Loss: 0.00021769\n","Iter: 092/100 | Train Loss: 0.00021496\n","Iter: 093/100 | Train Loss: 0.00021227\n","Iter: 094/100 | Train Loss: 0.00020962\n","Iter: 095/100 | Train Loss: 0.00020701\n","Iter: 096/100 | Train Loss: 0.00020444\n","Iter: 097/100 | Train Loss: 0.00020191\n","Iter: 098/100 | Train Loss: 0.00019943\n","Iter: 099/100 | Train Loss: 0.00019697\n","\n","Iter: 099/100 | Test Loss: 0.00100522 | Test acc: 64.3000\n","scale:1.150000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00232881\n","Iter: 001/100 | Train Loss: 0.00193002\n","Iter: 002/100 | Train Loss: 0.00156647\n","Iter: 003/100 | Train Loss: 0.00144146\n","Iter: 004/100 | Train Loss: 0.00154150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00171625\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00175119\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00157535\n","Iter: 008/100 | Train Loss: 0.00130236\n","Iter: 009/100 | Train Loss: 0.00113877\n","Iter: 010/100 | Train Loss: 0.00112184\n","Iter: 011/100 | Train Loss: 0.00117590\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00118716\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 013/100 | Train Loss: 0.00112095\n","Iter: 014/100 | Train Loss: 0.00101296\n","Iter: 015/100 | Train Loss: 0.00090179\n","Iter: 016/100 | Train Loss: 0.00083244\n","Iter: 017/100 | Train Loss: 0.00081256\n","Iter: 018/100 | Train Loss: 0.00081535\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 019/100 | Train Loss: 0.00081765\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 020/100 | Train Loss: 0.00078731\n","Iter: 021/100 | Train Loss: 0.00072169\n","Iter: 022/100 | Train Loss: 0.00066552\n","Iter: 023/100 | Train Loss: 0.00063582\n","Iter: 024/100 | Train Loss: 0.00062831\n","Iter: 025/100 | Train Loss: 0.00062599\n","Iter: 026/100 | Train Loss: 0.00061348\n","Iter: 027/100 | Train Loss: 0.00058657\n","Iter: 028/100 | Train Loss: 0.00055258\n","Iter: 029/100 | Train Loss: 0.00052379\n","Iter: 030/100 | Train Loss: 0.00050754\n","Iter: 031/100 | Train Loss: 0.00050125\n","Iter: 032/100 | Train Loss: 0.00049591\n","Iter: 033/100 | Train Loss: 0.00048370\n","Iter: 034/100 | Train Loss: 0.00046443\n","Iter: 035/100 | Train Loss: 0.00044402\n","Iter: 036/100 | Train Loss: 0.00042868\n","Iter: 037/100 | Train Loss: 0.00041995\n","Iter: 038/100 | Train Loss: 0.00041443\n","Iter: 039/100 | Train Loss: 0.00040743\n","Iter: 040/100 | Train Loss: 0.00039640\n","Iter: 041/100 | Train Loss: 0.00038294\n","Iter: 042/100 | Train Loss: 0.00037059\n","Iter: 043/100 | Train Loss: 0.00036187\n","Iter: 044/100 | Train Loss: 0.00035617\n","Iter: 045/100 | Train Loss: 0.00035071\n","Iter: 046/100 | Train Loss: 0.00034318\n","Iter: 047/100 | Train Loss: 0.00033365\n","Iter: 048/100 | Train Loss: 0.00032425\n","Iter: 049/100 | Train Loss: 0.00031682\n","Iter: 050/100 | Train Loss: 0.00031137\n","Iter: 051/100 | Train Loss: 0.00030637\n","Iter: 052/100 | Train Loss: 0.00030034\n","Iter: 053/100 | Train Loss: 0.00029322\n","Iter: 054/100 | Train Loss: 0.00028608\n","Iter: 055/100 | Train Loss: 0.00027997\n","Iter: 056/100 | Train Loss: 0.00027491\n","Iter: 057/100 | Train Loss: 0.00027027\n","Iter: 058/100 | Train Loss: 0.00026540\n","Iter: 059/100 | Train Loss: 0.00026018\n","Iter: 060/100 | Train Loss: 0.00025490\n","Iter: 061/100 | Train Loss: 0.00024998\n","Iter: 062/100 | Train Loss: 0.00024562\n","Iter: 063/100 | Train Loss: 0.00024158\n","Iter: 064/100 | Train Loss: 0.00023755\n","Iter: 065/100 | Train Loss: 0.00023328\n","Iter: 066/100 | Train Loss: 0.00022888\n","Iter: 067/100 | Train Loss: 0.00022463\n","Iter: 068/100 | Train Loss: 0.00022074\n","Iter: 069/100 | Train Loss: 0.00021714\n","Iter: 070/100 | Train Loss: 0.00021357\n","Iter: 071/100 | Train Loss: 0.00020985\n","Iter: 072/100 | Train Loss: 0.00020606\n","Iter: 073/100 | Train Loss: 0.00020239\n","Iter: 074/100 | Train Loss: 0.00019898\n","Iter: 075/100 | Train Loss: 0.00019579\n","Iter: 076/100 | Train Loss: 0.00019265\n","Iter: 077/100 | Train Loss: 0.00018943\n","Iter: 078/100 | Train Loss: 0.00018620\n","Iter: 079/100 | Train Loss: 0.00018308\n","Iter: 080/100 | Train Loss: 0.00018015\n","Iter: 081/100 | Train Loss: 0.00017735\n","Iter: 082/100 | Train Loss: 0.00017459\n","Iter: 083/100 | Train Loss: 0.00017181\n","Iter: 084/100 | Train Loss: 0.00016904\n","Iter: 085/100 | Train Loss: 0.00016637\n","Iter: 086/100 | Train Loss: 0.00016381\n","Iter: 087/100 | Train Loss: 0.00016132\n","Iter: 088/100 | Train Loss: 0.00015884\n","Iter: 089/100 | Train Loss: 0.00015638\n","Iter: 090/100 | Train Loss: 0.00015397\n","Iter: 091/100 | Train Loss: 0.00015163\n","Iter: 092/100 | Train Loss: 0.00014935\n","Iter: 093/100 | Train Loss: 0.00014711\n","Iter: 094/100 | Train Loss: 0.00014490\n","Iter: 095/100 | Train Loss: 0.00014273\n","Iter: 096/100 | Train Loss: 0.00014059\n","Iter: 097/100 | Train Loss: 0.00013849\n","Iter: 098/100 | Train Loss: 0.00013643\n","Iter: 099/100 | Train Loss: 0.00013441\n","\n","Iter: 099/100 | Test Loss: 0.00099937 | Test acc: 64.8300\n","scale:1.200000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00242962\n","Iter: 001/100 | Train Loss: 0.00197618\n","Iter: 002/100 | Train Loss: 0.00155998\n","Iter: 003/100 | Train Loss: 0.00146172\n","Iter: 004/100 | Train Loss: 0.00162660\n","Adjusting Layer 1, Kernel Nodes: 616, Adptive Nodes:184\n","Iter: 005/100 | Train Loss: 0.00175682\n","Adjusting Layer 1, Kernel Nodes: 521, Adptive Nodes:279\n","Iter: 006/100 | Train Loss: 0.00152218\n","Iter: 007/100 | Train Loss: 0.00122477\n","Iter: 008/100 | Train Loss: 0.00114987\n","Iter: 009/100 | Train Loss: 0.00118123\n","Adjusting Layer 1, Kernel Nodes: 316, Adptive Nodes:484\n","Iter: 010/100 | Train Loss: 0.00112499\n","Iter: 011/100 | Train Loss: 0.00099560\n","Iter: 012/100 | Train Loss: 0.00087068\n","Iter: 013/100 | Train Loss: 0.00082068\n","Iter: 014/100 | Train Loss: 0.00081953\n","Iter: 015/100 | Train Loss: 0.00079695\n","Iter: 016/100 | Train Loss: 0.00072948\n","Iter: 017/100 | Train Loss: 0.00065612\n","Iter: 018/100 | Train Loss: 0.00061965\n","Iter: 019/100 | Train Loss: 0.00061625\n","Iter: 020/100 | Train Loss: 0.00060806\n","Iter: 021/100 | Train Loss: 0.00057404\n","Iter: 022/100 | Train Loss: 0.00053112\n","Iter: 023/100 | Train Loss: 0.00050500\n","Iter: 024/100 | Train Loss: 0.00049765\n","Iter: 025/100 | Train Loss: 0.00049111\n","Iter: 026/100 | Train Loss: 0.00047260\n","Iter: 027/100 | Train Loss: 0.00044698\n","Iter: 028/100 | Train Loss: 0.00042641\n","Iter: 029/100 | Train Loss: 0.00041481\n","Iter: 030/100 | Train Loss: 0.00040638\n","Iter: 031/100 | Train Loss: 0.00039427\n","Iter: 032/100 | Train Loss: 0.00037803\n","Iter: 033/100 | Train Loss: 0.00036173\n","Iter: 034/100 | Train Loss: 0.00034943\n","Iter: 035/100 | Train Loss: 0.00034084\n","Iter: 036/100 | Train Loss: 0.00033199\n","Iter: 037/100 | Train Loss: 0.00032059\n","Iter: 038/100 | Train Loss: 0.00030812\n","Iter: 039/100 | Train Loss: 0.00029737\n","Iter: 040/100 | Train Loss: 0.00028914\n","Iter: 041/100 | Train Loss: 0.00028184\n","Iter: 042/100 | Train Loss: 0.00027366\n","Iter: 043/100 | Train Loss: 0.00026456\n","Iter: 044/100 | Train Loss: 0.00025592\n","Iter: 045/100 | Train Loss: 0.00024873\n","Iter: 046/100 | Train Loss: 0.00024258\n","Iter: 047/100 | Train Loss: 0.00023636\n","Iter: 048/100 | Train Loss: 0.00022959\n","Iter: 049/100 | Train Loss: 0.00022281\n","Iter: 050/100 | Train Loss: 0.00021673\n","Iter: 051/100 | Train Loss: 0.00021145\n","Iter: 052/100 | Train Loss: 0.00020643\n","Iter: 053/100 | Train Loss: 0.00020119\n","Iter: 054/100 | Train Loss: 0.00019583\n","Iter: 055/100 | Train Loss: 0.00019079\n","Iter: 056/100 | Train Loss: 0.00018625\n","Iter: 057/100 | Train Loss: 0.00018201\n","Iter: 058/100 | Train Loss: 0.00017774\n","Iter: 059/100 | Train Loss: 0.00017336\n","Iter: 060/100 | Train Loss: 0.00016909\n","Iter: 061/100 | Train Loss: 0.00016514\n","Iter: 062/100 | Train Loss: 0.00016146\n","Iter: 063/100 | Train Loss: 0.00015785\n","Iter: 064/100 | Train Loss: 0.00015419\n","Iter: 065/100 | Train Loss: 0.00015056\n","Iter: 066/100 | Train Loss: 0.00014712\n","Iter: 067/100 | Train Loss: 0.00014389\n","Iter: 068/100 | Train Loss: 0.00014075\n","Iter: 069/100 | Train Loss: 0.00013762\n","Iter: 070/100 | Train Loss: 0.00013453\n","Iter: 071/100 | Train Loss: 0.00013155\n","Iter: 072/100 | Train Loss: 0.00012871\n","Iter: 073/100 | Train Loss: 0.00012597\n","Iter: 074/100 | Train Loss: 0.00012327\n","Iter: 075/100 | Train Loss: 0.00012061\n","Iter: 076/100 | Train Loss: 0.00011802\n","Iter: 077/100 | Train Loss: 0.00011554\n","Iter: 078/100 | Train Loss: 0.00011314\n","Iter: 079/100 | Train Loss: 0.00011079\n","Iter: 080/100 | Train Loss: 0.00010847\n","Iter: 081/100 | Train Loss: 0.00010622\n","Iter: 082/100 | Train Loss: 0.00010404\n","Iter: 083/100 | Train Loss: 0.00010194\n","Iter: 084/100 | Train Loss: 0.00009988\n","Iter: 085/100 | Train Loss: 0.00009786\n","Iter: 086/100 | Train Loss: 0.00009588\n","Iter: 087/100 | Train Loss: 0.00009396\n","Iter: 088/100 | Train Loss: 0.00009209\n","Iter: 089/100 | Train Loss: 0.00009027\n","Iter: 090/100 | Train Loss: 0.00008848\n","Iter: 091/100 | Train Loss: 0.00008673\n","Iter: 092/100 | Train Loss: 0.00008502\n","Iter: 093/100 | Train Loss: 0.00008336\n","Iter: 094/100 | Train Loss: 0.00008174\n","Iter: 095/100 | Train Loss: 0.00008016\n","Iter: 096/100 | Train Loss: 0.00007862\n","Iter: 097/100 | Train Loss: 0.00007710\n","Iter: 098/100 | Train Loss: 0.00007563\n","Iter: 099/100 | Train Loss: 0.00007419\n","\n","Iter: 099/100 | Test Loss: 0.00105419 | Test acc: 64.2000\n","scale:1.200000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00242230\n","Iter: 001/100 | Train Loss: 0.00199528\n","Iter: 002/100 | Train Loss: 0.00158610\n","Iter: 003/100 | Train Loss: 0.00145472\n","Iter: 004/100 | Train Loss: 0.00158372\n","Adjusting Layer 1, Kernel Nodes: 671, Adptive Nodes:129\n","Iter: 005/100 | Train Loss: 0.00172591\n","Adjusting Layer 1, Kernel Nodes: 468, Adptive Nodes:332\n","Iter: 006/100 | Train Loss: 0.00162643\n","Iter: 007/100 | Train Loss: 0.00133301\n","Iter: 008/100 | Train Loss: 0.00115715\n","Iter: 009/100 | Train Loss: 0.00116284\n","Adjusting Layer 1, Kernel Nodes: 791, Adptive Nodes:9\n","Iter: 010/100 | Train Loss: 0.00120404\n","Adjusting Layer 1, Kernel Nodes: 6, Adptive Nodes:794\n","Iter: 011/100 | Train Loss: 0.00104106\n","Iter: 012/100 | Train Loss: 0.00092941\n","Iter: 013/100 | Train Loss: 0.00084079\n","Iter: 014/100 | Train Loss: 0.00079847\n","Iter: 015/100 | Train Loss: 0.00078055\n","Iter: 016/100 | Train Loss: 0.00075469\n","Iter: 017/100 | Train Loss: 0.00070878\n","Iter: 018/100 | Train Loss: 0.00065616\n","Iter: 019/100 | Train Loss: 0.00061498\n","Iter: 020/100 | Train Loss: 0.00059117\n","Iter: 021/100 | Train Loss: 0.00057741\n","Iter: 022/100 | Train Loss: 0.00056209\n","Iter: 023/100 | Train Loss: 0.00053792\n","Iter: 024/100 | Train Loss: 0.00051075\n","Iter: 025/100 | Train Loss: 0.00048744\n","Iter: 026/100 | Train Loss: 0.00047099\n","Iter: 027/100 | Train Loss: 0.00045914\n","Iter: 028/100 | Train Loss: 0.00044731\n","Iter: 029/100 | Train Loss: 0.00043259\n","Iter: 030/100 | Train Loss: 0.00041559\n","Iter: 031/100 | Train Loss: 0.00039910\n","Iter: 032/100 | Train Loss: 0.00038564\n","Iter: 033/100 | Train Loss: 0.00037537\n","Iter: 034/100 | Train Loss: 0.00036621\n","Iter: 035/100 | Train Loss: 0.00035571\n","Iter: 036/100 | Train Loss: 0.00034332\n","Iter: 037/100 | Train Loss: 0.00033068\n","Iter: 038/100 | Train Loss: 0.00031989\n","Iter: 039/100 | Train Loss: 0.00031148\n","Iter: 040/100 | Train Loss: 0.00030403\n","Iter: 041/100 | Train Loss: 0.00029574\n","Iter: 042/100 | Train Loss: 0.00028625\n","Iter: 043/100 | Train Loss: 0.00027675\n","Iter: 044/100 | Train Loss: 0.00026858\n","Iter: 045/100 | Train Loss: 0.00026194\n","Iter: 046/100 | Train Loss: 0.00025589\n","Iter: 047/100 | Train Loss: 0.00024938\n","Iter: 048/100 | Train Loss: 0.00024220\n","Iter: 049/100 | Train Loss: 0.00023512\n","Iter: 050/100 | Train Loss: 0.00022892\n","Iter: 051/100 | Train Loss: 0.00022368\n","Iter: 052/100 | Train Loss: 0.00021877\n","Iter: 053/100 | Train Loss: 0.00021354\n","Iter: 054/100 | Train Loss: 0.00020797\n","Iter: 055/100 | Train Loss: 0.00020257\n","Iter: 056/100 | Train Loss: 0.00019770\n","Iter: 057/100 | Train Loss: 0.00019335\n","Iter: 058/100 | Train Loss: 0.00018915\n","Iter: 059/100 | Train Loss: 0.00018481\n","Iter: 060/100 | Train Loss: 0.00018036\n","Iter: 061/100 | Train Loss: 0.00017607\n","Iter: 062/100 | Train Loss: 0.00017211\n","Iter: 063/100 | Train Loss: 0.00016843\n","Iter: 064/100 | Train Loss: 0.00016485\n","Iter: 065/100 | Train Loss: 0.00016120\n","Iter: 066/100 | Train Loss: 0.00015753\n","Iter: 067/100 | Train Loss: 0.00015400\n","Iter: 068/100 | Train Loss: 0.00015068\n","Iter: 069/100 | Train Loss: 0.00014752\n","Iter: 070/100 | Train Loss: 0.00014443\n","Iter: 071/100 | Train Loss: 0.00014133\n","Iter: 072/100 | Train Loss: 0.00013827\n","Iter: 073/100 | Train Loss: 0.00013530\n","Iter: 074/100 | Train Loss: 0.00013248\n","Iter: 075/100 | Train Loss: 0.00012977\n","Iter: 076/100 | Train Loss: 0.00012710\n","Iter: 077/100 | Train Loss: 0.00012446\n","Iter: 078/100 | Train Loss: 0.00012186\n","Iter: 079/100 | Train Loss: 0.00011935\n","Iter: 080/100 | Train Loss: 0.00011693\n","Iter: 081/100 | Train Loss: 0.00011459\n","Iter: 082/100 | Train Loss: 0.00011228\n","Iter: 083/100 | Train Loss: 0.00011002\n","Iter: 084/100 | Train Loss: 0.00010779\n","Iter: 085/100 | Train Loss: 0.00010563\n","Iter: 086/100 | Train Loss: 0.00010354\n","Iter: 087/100 | Train Loss: 0.00010150\n","Iter: 088/100 | Train Loss: 0.00009951\n","Iter: 089/100 | Train Loss: 0.00009755\n","Iter: 090/100 | Train Loss: 0.00009563\n","Iter: 091/100 | Train Loss: 0.00009377\n","Iter: 092/100 | Train Loss: 0.00009195\n","Iter: 093/100 | Train Loss: 0.00009017\n","Iter: 094/100 | Train Loss: 0.00008844\n","Iter: 095/100 | Train Loss: 0.00008673\n","Iter: 096/100 | Train Loss: 0.00008507\n","Iter: 097/100 | Train Loss: 0.00008345\n","Iter: 098/100 | Train Loss: 0.00008187\n","Iter: 099/100 | Train Loss: 0.00008032\n","\n","Iter: 099/100 | Test Loss: 0.00104246 | Test acc: 64.5800\n","scale:1.200000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00241365\n","Iter: 001/100 | Train Loss: 0.00200551\n","Iter: 002/100 | Train Loss: 0.00160445\n","Iter: 003/100 | Train Loss: 0.00145303\n","Iter: 004/100 | Train Loss: 0.00155476\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 005/100 | Train Loss: 0.00166548\n","Adjusting Layer 1, Kernel Nodes: 449, Adptive Nodes:351\n","Iter: 006/100 | Train Loss: 0.00169242\n","Adjusting Layer 1, Kernel Nodes: 514, Adptive Nodes:286\n","Iter: 007/100 | Train Loss: 0.00139868\n","Iter: 008/100 | Train Loss: 0.00117538\n","Iter: 009/100 | Train Loss: 0.00116197\n","Iter: 010/100 | Train Loss: 0.00120166\n","Adjusting Layer 1, Kernel Nodes: 161, Adptive Nodes:639\n","Iter: 011/100 | Train Loss: 0.00110759\n","Iter: 012/100 | Train Loss: 0.00102141\n","Iter: 013/100 | Train Loss: 0.00091786\n","Iter: 014/100 | Train Loss: 0.00084265\n","Iter: 015/100 | Train Loss: 0.00080867\n","Iter: 016/100 | Train Loss: 0.00079646\n","Iter: 017/100 | Train Loss: 0.00077787\n","Iter: 018/100 | Train Loss: 0.00073975\n","Iter: 019/100 | Train Loss: 0.00068932\n","Iter: 020/100 | Train Loss: 0.00064424\n","Iter: 021/100 | Train Loss: 0.00061568\n","Iter: 022/100 | Train Loss: 0.00060179\n","Iter: 023/100 | Train Loss: 0.00059207\n","Iter: 024/100 | Train Loss: 0.00057596\n","Iter: 025/100 | Train Loss: 0.00055084\n","Iter: 026/100 | Train Loss: 0.00052281\n","Iter: 027/100 | Train Loss: 0.00050041\n","Iter: 028/100 | Train Loss: 0.00048748\n","Iter: 029/100 | Train Loss: 0.00047997\n","Iter: 030/100 | Train Loss: 0.00047076\n","Iter: 031/100 | Train Loss: 0.00045619\n","Iter: 032/100 | Train Loss: 0.00043837\n","Iter: 033/100 | Train Loss: 0.00042244\n","Iter: 034/100 | Train Loss: 0.00041150\n","Iter: 035/100 | Train Loss: 0.00040407\n","Iter: 036/100 | Train Loss: 0.00039633\n","Iter: 037/100 | Train Loss: 0.00038589\n","Iter: 038/100 | Train Loss: 0.00037365\n","Iter: 039/100 | Train Loss: 0.00036204\n","Iter: 040/100 | Train Loss: 0.00035263\n","Iter: 041/100 | Train Loss: 0.00034508\n","Iter: 042/100 | Train Loss: 0.00033792\n","Iter: 043/100 | Train Loss: 0.00032994\n","Iter: 044/100 | Train Loss: 0.00032113\n","Iter: 045/100 | Train Loss: 0.00031246\n","Iter: 046/100 | Train Loss: 0.00030473\n","Iter: 047/100 | Train Loss: 0.00029798\n","Iter: 048/100 | Train Loss: 0.00029162\n","Iter: 049/100 | Train Loss: 0.00028516\n","Iter: 050/100 | Train Loss: 0.00027850\n","Iter: 051/100 | Train Loss: 0.00027190\n","Iter: 052/100 | Train Loss: 0.00026563\n","Iter: 053/100 | Train Loss: 0.00025979\n","Iter: 054/100 | Train Loss: 0.00025427\n","Iter: 055/100 | Train Loss: 0.00024894\n","Iter: 056/100 | Train Loss: 0.00024370\n","Iter: 057/100 | Train Loss: 0.00023845\n","Iter: 058/100 | Train Loss: 0.00023326\n","Iter: 059/100 | Train Loss: 0.00022829\n","Iter: 060/100 | Train Loss: 0.00022363\n","Iter: 061/100 | Train Loss: 0.00021920\n","Iter: 062/100 | Train Loss: 0.00021483\n","Iter: 063/100 | Train Loss: 0.00021043\n","Iter: 064/100 | Train Loss: 0.00020607\n","Iter: 065/100 | Train Loss: 0.00020186\n","Iter: 066/100 | Train Loss: 0.00019786\n","Iter: 067/100 | Train Loss: 0.00019401\n","Iter: 068/100 | Train Loss: 0.00019020\n","Iter: 069/100 | Train Loss: 0.00018641\n","Iter: 070/100 | Train Loss: 0.00018270\n","Iter: 071/100 | Train Loss: 0.00017912\n","Iter: 072/100 | Train Loss: 0.00017569\n","Iter: 073/100 | Train Loss: 0.00017236\n","Iter: 074/100 | Train Loss: 0.00016909\n","Iter: 075/100 | Train Loss: 0.00016586\n","Iter: 076/100 | Train Loss: 0.00016270\n","Iter: 077/100 | Train Loss: 0.00015961\n","Iter: 078/100 | Train Loss: 0.00015663\n","Iter: 079/100 | Train Loss: 0.00015373\n","Iter: 080/100 | Train Loss: 0.00015088\n","Iter: 081/100 | Train Loss: 0.00014807\n","Iter: 082/100 | Train Loss: 0.00014534\n","Iter: 083/100 | Train Loss: 0.00014268\n","Iter: 084/100 | Train Loss: 0.00014007\n","Iter: 085/100 | Train Loss: 0.00013752\n","Iter: 086/100 | Train Loss: 0.00013502\n","Iter: 087/100 | Train Loss: 0.00013257\n","Iter: 088/100 | Train Loss: 0.00013018\n","Iter: 089/100 | Train Loss: 0.00012784\n","Iter: 090/100 | Train Loss: 0.00012555\n","Iter: 091/100 | Train Loss: 0.00012332\n","Iter: 092/100 | Train Loss: 0.00012115\n","Iter: 093/100 | Train Loss: 0.00011901\n","Iter: 094/100 | Train Loss: 0.00011693\n","Iter: 095/100 | Train Loss: 0.00011488\n","Iter: 096/100 | Train Loss: 0.00011289\n","Iter: 097/100 | Train Loss: 0.00011093\n","Iter: 098/100 | Train Loss: 0.00010900\n","Iter: 099/100 | Train Loss: 0.00010712\n","\n","Iter: 099/100 | Test Loss: 0.00102205 | Test acc: 65.0100\n","scale:1.200000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00240413\n","Iter: 001/100 | Train Loss: 0.00200889\n","Iter: 002/100 | Train Loss: 0.00161555\n","Iter: 003/100 | Train Loss: 0.00145293\n","Iter: 004/100 | Train Loss: 0.00153506\n","Adjusting Layer 1, Kernel Nodes: 703, Adptive Nodes:97\n","Iter: 005/100 | Train Loss: 0.00166493\n","Adjusting Layer 1, Kernel Nodes: 686, Adptive Nodes:114\n","Iter: 006/100 | Train Loss: 0.00165740\n","Iter: 007/100 | Train Loss: 0.00147042\n","Iter: 008/100 | Train Loss: 0.00124548\n","Iter: 009/100 | Train Loss: 0.00115224\n","Iter: 010/100 | Train Loss: 0.00117994\n","Adjusting Layer 1, Kernel Nodes: 508, Adptive Nodes:292\n","Iter: 011/100 | Train Loss: 0.00119219\n","Adjusting Layer 1, Kernel Nodes: 494, Adptive Nodes:306\n","Iter: 012/100 | Train Loss: 0.00109761\n","Iter: 013/100 | Train Loss: 0.00094694\n","Iter: 014/100 | Train Loss: 0.00085939\n","Iter: 015/100 | Train Loss: 0.00084952\n","Iter: 016/100 | Train Loss: 0.00084229\n","Iter: 017/100 | Train Loss: 0.00078520\n","Iter: 018/100 | Train Loss: 0.00070329\n","Iter: 019/100 | Train Loss: 0.00064805\n","Iter: 020/100 | Train Loss: 0.00063086\n","Iter: 021/100 | Train Loss: 0.00062175\n","Iter: 022/100 | Train Loss: 0.00059480\n","Iter: 023/100 | Train Loss: 0.00055441\n","Iter: 024/100 | Train Loss: 0.00052067\n","Iter: 025/100 | Train Loss: 0.00050285\n","Iter: 026/100 | Train Loss: 0.00049250\n","Iter: 027/100 | Train Loss: 0.00047770\n","Iter: 028/100 | Train Loss: 0.00045641\n","Iter: 029/100 | Train Loss: 0.00043494\n","Iter: 030/100 | Train Loss: 0.00041857\n","Iter: 031/100 | Train Loss: 0.00040683\n","Iter: 032/100 | Train Loss: 0.00039545\n","Iter: 033/100 | Train Loss: 0.00038181\n","Iter: 034/100 | Train Loss: 0.00036683\n","Iter: 035/100 | Train Loss: 0.00035300\n","Iter: 036/100 | Train Loss: 0.00034154\n","Iter: 037/100 | Train Loss: 0.00033158\n","Iter: 038/100 | Train Loss: 0.00032139\n","Iter: 039/100 | Train Loss: 0.00031057\n","Iter: 040/100 | Train Loss: 0.00029981\n","Iter: 041/100 | Train Loss: 0.00028977\n","Iter: 042/100 | Train Loss: 0.00028065\n","Iter: 043/100 | Train Loss: 0.00027222\n","Iter: 044/100 | Train Loss: 0.00026411\n","Iter: 045/100 | Train Loss: 0.00025602\n","Iter: 046/100 | Train Loss: 0.00024792\n","Iter: 047/100 | Train Loss: 0.00024018\n","Iter: 048/100 | Train Loss: 0.00023324\n","Iter: 049/100 | Train Loss: 0.00022699\n","Iter: 050/100 | Train Loss: 0.00022077\n","Iter: 051/100 | Train Loss: 0.00021421\n","Iter: 052/100 | Train Loss: 0.00020775\n","Iter: 053/100 | Train Loss: 0.00020200\n","Iter: 054/100 | Train Loss: 0.00019691\n","Iter: 055/100 | Train Loss: 0.00019194\n","Iter: 056/100 | Train Loss: 0.00018672\n","Iter: 057/100 | Train Loss: 0.00018151\n","Iter: 058/100 | Train Loss: 0.00017672\n","Iter: 059/100 | Train Loss: 0.00017245\n","Iter: 060/100 | Train Loss: 0.00016834\n","Iter: 061/100 | Train Loss: 0.00016406\n","Iter: 062/100 | Train Loss: 0.00015971\n","Iter: 063/100 | Train Loss: 0.00015562\n","Iter: 064/100 | Train Loss: 0.00015191\n","Iter: 065/100 | Train Loss: 0.00014838\n","Iter: 066/100 | Train Loss: 0.00014476\n","Iter: 067/100 | Train Loss: 0.00014111\n","Iter: 068/100 | Train Loss: 0.00013764\n","Iter: 069/100 | Train Loss: 0.00013439\n","Iter: 070/100 | Train Loss: 0.00013126\n","Iter: 071/100 | Train Loss: 0.00012815\n","Iter: 072/100 | Train Loss: 0.00012507\n","Iter: 073/100 | Train Loss: 0.00012209\n","Iter: 074/100 | Train Loss: 0.00011925\n","Iter: 075/100 | Train Loss: 0.00011651\n","Iter: 076/100 | Train Loss: 0.00011384\n","Iter: 077/100 | Train Loss: 0.00011122\n","Iter: 078/100 | Train Loss: 0.00010867\n","Iter: 079/100 | Train Loss: 0.00010619\n","Iter: 080/100 | Train Loss: 0.00010382\n","Iter: 081/100 | Train Loss: 0.00010151\n","Iter: 082/100 | Train Loss: 0.00009925\n","Iter: 083/100 | Train Loss: 0.00009703\n","Iter: 084/100 | Train Loss: 0.00009487\n","Iter: 085/100 | Train Loss: 0.00009278\n","Iter: 086/100 | Train Loss: 0.00009076\n","Iter: 087/100 | Train Loss: 0.00008879\n","Iter: 088/100 | Train Loss: 0.00008684\n","Iter: 089/100 | Train Loss: 0.00008496\n","Iter: 090/100 | Train Loss: 0.00008314\n","Iter: 091/100 | Train Loss: 0.00008136\n","Iter: 092/100 | Train Loss: 0.00007962\n","Iter: 093/100 | Train Loss: 0.00007792\n","Iter: 094/100 | Train Loss: 0.00007626\n","Iter: 095/100 | Train Loss: 0.00007465\n","Iter: 096/100 | Train Loss: 0.00007307\n","Iter: 097/100 | Train Loss: 0.00007154\n","Iter: 098/100 | Train Loss: 0.00007004\n","Iter: 099/100 | Train Loss: 0.00006858\n","\n","Iter: 099/100 | Test Loss: 0.00104733 | Test acc: 64.1100\n","scale:1.200000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00239379\n","Iter: 001/100 | Train Loss: 0.00200648\n","Iter: 002/100 | Train Loss: 0.00161979\n","Iter: 003/100 | Train Loss: 0.00145262\n","Iter: 004/100 | Train Loss: 0.00152341\n","Adjusting Layer 1, Kernel Nodes: 692, Adptive Nodes:108\n","Iter: 005/100 | Train Loss: 0.00165327\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 006/100 | Train Loss: 0.00165814\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00144342\n","Iter: 008/100 | Train Loss: 0.00123907\n","Iter: 009/100 | Train Loss: 0.00115434\n","Iter: 010/100 | Train Loss: 0.00117582\n","Adjusting Layer 1, Kernel Nodes: 332, Adptive Nodes:468\n","Iter: 011/100 | Train Loss: 0.00114230\n","Iter: 012/100 | Train Loss: 0.00106460\n","Iter: 013/100 | Train Loss: 0.00095125\n","Iter: 014/100 | Train Loss: 0.00086956\n","Iter: 015/100 | Train Loss: 0.00084094\n","Iter: 016/100 | Train Loss: 0.00082921\n","Iter: 017/100 | Train Loss: 0.00079415\n","Iter: 018/100 | Train Loss: 0.00073522\n","Iter: 019/100 | Train Loss: 0.00068064\n","Iter: 020/100 | Train Loss: 0.00064901\n","Iter: 021/100 | Train Loss: 0.00063312\n","Iter: 022/100 | Train Loss: 0.00061538\n","Iter: 023/100 | Train Loss: 0.00058842\n","Iter: 024/100 | Train Loss: 0.00055843\n","Iter: 025/100 | Train Loss: 0.00053383\n","Iter: 026/100 | Train Loss: 0.00051635\n","Iter: 027/100 | Train Loss: 0.00050213\n","Iter: 028/100 | Train Loss: 0.00048744\n","Iter: 029/100 | Train Loss: 0.00047139\n","Iter: 030/100 | Train Loss: 0.00045480\n","Iter: 031/100 | Train Loss: 0.00043896\n","Iter: 032/100 | Train Loss: 0.00042505\n","Iter: 033/100 | Train Loss: 0.00041341\n","Iter: 034/100 | Train Loss: 0.00040287\n","Iter: 035/100 | Train Loss: 0.00039159\n","Iter: 036/100 | Train Loss: 0.00037907\n","Iter: 037/100 | Train Loss: 0.00036675\n","Iter: 038/100 | Train Loss: 0.00035627\n","Iter: 039/100 | Train Loss: 0.00034758\n","Iter: 040/100 | Train Loss: 0.00033906\n","Iter: 041/100 | Train Loss: 0.00032959\n","Iter: 042/100 | Train Loss: 0.00031967\n","Iter: 043/100 | Train Loss: 0.00031059\n","Iter: 044/100 | Train Loss: 0.00030281\n","Iter: 045/100 | Train Loss: 0.00029562\n","Iter: 046/100 | Train Loss: 0.00028816\n","Iter: 047/100 | Train Loss: 0.00028038\n","Iter: 048/100 | Train Loss: 0.00027287\n","Iter: 049/100 | Train Loss: 0.00026603\n","Iter: 050/100 | Train Loss: 0.00025968\n","Iter: 051/100 | Train Loss: 0.00025347\n","Iter: 052/100 | Train Loss: 0.00024724\n","Iter: 053/100 | Train Loss: 0.00024115\n","Iter: 054/100 | Train Loss: 0.00023539\n","Iter: 055/100 | Train Loss: 0.00022993\n","Iter: 056/100 | Train Loss: 0.00022467\n","Iter: 057/100 | Train Loss: 0.00021955\n","Iter: 058/100 | Train Loss: 0.00021456\n","Iter: 059/100 | Train Loss: 0.00020975\n","Iter: 060/100 | Train Loss: 0.00020509\n","Iter: 061/100 | Train Loss: 0.00020054\n","Iter: 062/100 | Train Loss: 0.00019613\n","Iter: 063/100 | Train Loss: 0.00019195\n","Iter: 064/100 | Train Loss: 0.00018794\n","Iter: 065/100 | Train Loss: 0.00018398\n","Iter: 066/100 | Train Loss: 0.00018005\n","Iter: 067/100 | Train Loss: 0.00017623\n","Iter: 068/100 | Train Loss: 0.00017259\n","Iter: 069/100 | Train Loss: 0.00016909\n","Iter: 070/100 | Train Loss: 0.00016566\n","Iter: 071/100 | Train Loss: 0.00016225\n","Iter: 072/100 | Train Loss: 0.00015891\n","Iter: 073/100 | Train Loss: 0.00015571\n","Iter: 074/100 | Train Loss: 0.00015264\n","Iter: 075/100 | Train Loss: 0.00014963\n","Iter: 076/100 | Train Loss: 0.00014665\n","Iter: 077/100 | Train Loss: 0.00014372\n","Iter: 078/100 | Train Loss: 0.00014088\n","Iter: 079/100 | Train Loss: 0.00013814\n","Iter: 080/100 | Train Loss: 0.00013547\n","Iter: 081/100 | Train Loss: 0.00013285\n","Iter: 082/100 | Train Loss: 0.00013028\n","Iter: 083/100 | Train Loss: 0.00012778\n","Iter: 084/100 | Train Loss: 0.00012535\n","Iter: 085/100 | Train Loss: 0.00012297\n","Iter: 086/100 | Train Loss: 0.00012064\n","Iter: 087/100 | Train Loss: 0.00011837\n","Iter: 088/100 | Train Loss: 0.00011616\n","Iter: 089/100 | Train Loss: 0.00011399\n","Iter: 090/100 | Train Loss: 0.00011188\n","Iter: 091/100 | Train Loss: 0.00010981\n","Iter: 092/100 | Train Loss: 0.00010779\n","Iter: 093/100 | Train Loss: 0.00010582\n","Iter: 094/100 | Train Loss: 0.00010389\n","Iter: 095/100 | Train Loss: 0.00010200\n","Iter: 096/100 | Train Loss: 0.00010015\n","Iter: 097/100 | Train Loss: 0.00009835\n","Iter: 098/100 | Train Loss: 0.00009659\n","Iter: 099/100 | Train Loss: 0.00009486\n","\n","Iter: 099/100 | Test Loss: 0.00103649 | Test acc: 64.8000\n","scale:1.200000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00238241\n","Iter: 001/100 | Train Loss: 0.00199885\n","Iter: 002/100 | Train Loss: 0.00161787\n","Iter: 003/100 | Train Loss: 0.00145134\n","Iter: 004/100 | Train Loss: 0.00151822\n","Adjusting Layer 1, Kernel Nodes: 695, Adptive Nodes:105\n","Iter: 005/100 | Train Loss: 0.00164597\n","Adjusting Layer 1, Kernel Nodes: 710, Adptive Nodes:90\n","Iter: 006/100 | Train Loss: 0.00168135\n","Adjusting Layer 1, Kernel Nodes: 667, Adptive Nodes:133\n","Iter: 007/100 | Train Loss: 0.00154979\n","Iter: 008/100 | Train Loss: 0.00135171\n","Iter: 009/100 | Train Loss: 0.00119734\n","Iter: 010/100 | Train Loss: 0.00114957\n","Iter: 011/100 | Train Loss: 0.00116855\n","Adjusting Layer 1, Kernel Nodes: 548, Adptive Nodes:252\n","Iter: 012/100 | Train Loss: 0.00117664\n","Adjusting Layer 1, Kernel Nodes: 602, Adptive Nodes:198\n","Iter: 013/100 | Train Loss: 0.00111914\n","Iter: 014/100 | Train Loss: 0.00099846\n","Iter: 015/100 | Train Loss: 0.00090326\n","Iter: 016/100 | Train Loss: 0.00086634\n","Iter: 017/100 | Train Loss: 0.00086117\n","Iter: 018/100 | Train Loss: 0.00084236\n","Iter: 019/100 | Train Loss: 0.00079309\n","Iter: 020/100 | Train Loss: 0.00073194\n","Iter: 021/100 | Train Loss: 0.00068534\n","Iter: 022/100 | Train Loss: 0.00066084\n","Iter: 023/100 | Train Loss: 0.00064624\n","Iter: 024/100 | Train Loss: 0.00062747\n","Iter: 025/100 | Train Loss: 0.00060111\n","Iter: 026/100 | Train Loss: 0.00057267\n","Iter: 027/100 | Train Loss: 0.00054821\n","Iter: 028/100 | Train Loss: 0.00052930\n","Iter: 029/100 | Train Loss: 0.00051389\n","Iter: 030/100 | Train Loss: 0.00049978\n","Iter: 031/100 | Train Loss: 0.00048573\n","Iter: 032/100 | Train Loss: 0.00047116\n","Iter: 033/100 | Train Loss: 0.00045572\n","Iter: 034/100 | Train Loss: 0.00044044\n","Iter: 035/100 | Train Loss: 0.00042711\n","Iter: 036/100 | Train Loss: 0.00041668\n","Iter: 037/100 | Train Loss: 0.00040774\n","Iter: 038/100 | Train Loss: 0.00039800\n","Iter: 039/100 | Train Loss: 0.00038650\n","Iter: 040/100 | Train Loss: 0.00037456\n","Iter: 041/100 | Train Loss: 0.00036413\n","Iter: 042/100 | Train Loss: 0.00035588\n","Iter: 043/100 | Train Loss: 0.00034866\n","Iter: 044/100 | Train Loss: 0.00034086\n","Iter: 045/100 | Train Loss: 0.00033199\n","Iter: 046/100 | Train Loss: 0.00032289\n","Iter: 047/100 | Train Loss: 0.00031468\n","Iter: 048/100 | Train Loss: 0.00030774\n","Iter: 049/100 | Train Loss: 0.00030143\n","Iter: 050/100 | Train Loss: 0.00029492\n","Iter: 051/100 | Train Loss: 0.00028793\n","Iter: 052/100 | Train Loss: 0.00028088\n","Iter: 053/100 | Train Loss: 0.00027431\n","Iter: 054/100 | Train Loss: 0.00026840\n","Iter: 055/100 | Train Loss: 0.00026289\n","Iter: 056/100 | Train Loss: 0.00025745\n","Iter: 057/100 | Train Loss: 0.00025190\n","Iter: 058/100 | Train Loss: 0.00024633\n","Iter: 059/100 | Train Loss: 0.00024097\n","Iter: 060/100 | Train Loss: 0.00023596\n","Iter: 061/100 | Train Loss: 0.00023124\n","Iter: 062/100 | Train Loss: 0.00022667\n","Iter: 063/100 | Train Loss: 0.00022208\n","Iter: 064/100 | Train Loss: 0.00021748\n","Iter: 065/100 | Train Loss: 0.00021301\n","Iter: 066/100 | Train Loss: 0.00020876\n","Iter: 067/100 | Train Loss: 0.00020474\n","Iter: 068/100 | Train Loss: 0.00020085\n","Iter: 069/100 | Train Loss: 0.00019698\n","Iter: 070/100 | Train Loss: 0.00019312\n","Iter: 071/100 | Train Loss: 0.00018934\n","Iter: 072/100 | Train Loss: 0.00018572\n","Iter: 073/100 | Train Loss: 0.00018225\n","Iter: 074/100 | Train Loss: 0.00017889\n","Iter: 075/100 | Train Loss: 0.00017557\n","Iter: 076/100 | Train Loss: 0.00017228\n","Iter: 077/100 | Train Loss: 0.00016906\n","Iter: 078/100 | Train Loss: 0.00016595\n","Iter: 079/100 | Train Loss: 0.00016294\n","Iter: 080/100 | Train Loss: 0.00016000\n","Iter: 081/100 | Train Loss: 0.00015713\n","Iter: 082/100 | Train Loss: 0.00015431\n","Iter: 083/100 | Train Loss: 0.00015154\n","Iter: 084/100 | Train Loss: 0.00014883\n","Iter: 085/100 | Train Loss: 0.00014620\n","Iter: 086/100 | Train Loss: 0.00014363\n","Iter: 087/100 | Train Loss: 0.00014111\n","Iter: 088/100 | Train Loss: 0.00013865\n","Iter: 089/100 | Train Loss: 0.00013622\n","Iter: 090/100 | Train Loss: 0.00013384\n","Iter: 091/100 | Train Loss: 0.00013152\n","Iter: 092/100 | Train Loss: 0.00012927\n","Iter: 093/100 | Train Loss: 0.00012706\n","Iter: 094/100 | Train Loss: 0.00012488\n","Iter: 095/100 | Train Loss: 0.00012274\n","Iter: 096/100 | Train Loss: 0.00012065\n","Iter: 097/100 | Train Loss: 0.00011862\n","Iter: 098/100 | Train Loss: 0.00011663\n","Iter: 099/100 | Train Loss: 0.00011467\n","\n","Iter: 099/100 | Test Loss: 0.00102421 | Test acc: 64.9000\n","scale:1.200000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00237003\n","Iter: 001/100 | Train Loss: 0.00198677\n","Iter: 002/100 | Train Loss: 0.00161078\n","Iter: 003/100 | Train Loss: 0.00144923\n","Iter: 004/100 | Train Loss: 0.00151785\n","Adjusting Layer 1, Kernel Nodes: 701, Adptive Nodes:99\n","Iter: 005/100 | Train Loss: 0.00164787\n","Adjusting Layer 1, Kernel Nodes: 704, Adptive Nodes:96\n","Iter: 006/100 | Train Loss: 0.00169092\n","Adjusting Layer 1, Kernel Nodes: 687, Adptive Nodes:113\n","Iter: 007/100 | Train Loss: 0.00158588\n","Iter: 008/100 | Train Loss: 0.00140766\n","Iter: 009/100 | Train Loss: 0.00123880\n","Iter: 010/100 | Train Loss: 0.00115238\n","Iter: 011/100 | Train Loss: 0.00114641\n","Iter: 012/100 | Train Loss: 0.00116700\n","Adjusting Layer 1, Kernel Nodes: 622, Adptive Nodes:178\n","Iter: 013/100 | Train Loss: 0.00117047\n","Adjusting Layer 1, Kernel Nodes: 663, Adptive Nodes:137\n","Iter: 014/100 | Train Loss: 0.00111628\n","Iter: 015/100 | Train Loss: 0.00100931\n","Iter: 016/100 | Train Loss: 0.00092224\n","Iter: 017/100 | Train Loss: 0.00088108\n","Iter: 018/100 | Train Loss: 0.00087319\n","Iter: 019/100 | Train Loss: 0.00086673\n","Iter: 020/100 | Train Loss: 0.00083964\n","Iter: 021/100 | Train Loss: 0.00079278\n","Iter: 022/100 | Train Loss: 0.00074246\n","Iter: 023/100 | Train Loss: 0.00070411\n","Iter: 024/100 | Train Loss: 0.00068132\n","Iter: 025/100 | Train Loss: 0.00066702\n","Iter: 026/100 | Train Loss: 0.00065194\n","Iter: 027/100 | Train Loss: 0.00063157\n","Iter: 028/100 | Train Loss: 0.00060759\n","Iter: 029/100 | Train Loss: 0.00058417\n","Iter: 030/100 | Train Loss: 0.00056420\n","Iter: 031/100 | Train Loss: 0.00054790\n","Iter: 032/100 | Train Loss: 0.00053395\n","Iter: 033/100 | Train Loss: 0.00052099\n","Iter: 034/100 | Train Loss: 0.00050832\n","Iter: 035/100 | Train Loss: 0.00049569\n","Iter: 036/100 | Train Loss: 0.00048281\n","Iter: 037/100 | Train Loss: 0.00046971\n","Iter: 038/100 | Train Loss: 0.00045698\n","Iter: 039/100 | Train Loss: 0.00044563\n","Iter: 040/100 | Train Loss: 0.00043621\n","Iter: 041/100 | Train Loss: 0.00042807\n","Iter: 042/100 | Train Loss: 0.00041986\n","Iter: 043/100 | Train Loss: 0.00041062\n","Iter: 044/100 | Train Loss: 0.00040062\n","Iter: 045/100 | Train Loss: 0.00039101\n","Iter: 046/100 | Train Loss: 0.00038273\n","Iter: 047/100 | Train Loss: 0.00037587\n","Iter: 048/100 | Train Loss: 0.00036960\n","Iter: 049/100 | Train Loss: 0.00036298\n","Iter: 050/100 | Train Loss: 0.00035563\n","Iter: 051/100 | Train Loss: 0.00034800\n","Iter: 052/100 | Train Loss: 0.00034082\n","Iter: 053/100 | Train Loss: 0.00033451\n","Iter: 054/100 | Train Loss: 0.00032889\n","Iter: 055/100 | Train Loss: 0.00032341\n","Iter: 056/100 | Train Loss: 0.00031767\n","Iter: 057/100 | Train Loss: 0.00031165\n","Iter: 058/100 | Train Loss: 0.00030565\n","Iter: 059/100 | Train Loss: 0.00030002\n","Iter: 060/100 | Train Loss: 0.00029484\n","Iter: 061/100 | Train Loss: 0.00028994\n","Iter: 062/100 | Train Loss: 0.00028509\n","Iter: 063/100 | Train Loss: 0.00028017\n","Iter: 064/100 | Train Loss: 0.00027524\n","Iter: 065/100 | Train Loss: 0.00027041\n","Iter: 066/100 | Train Loss: 0.00026578\n","Iter: 067/100 | Train Loss: 0.00026137\n","Iter: 068/100 | Train Loss: 0.00025712\n","Iter: 069/100 | Train Loss: 0.00025295\n","Iter: 070/100 | Train Loss: 0.00024880\n","Iter: 071/100 | Train Loss: 0.00024468\n","Iter: 072/100 | Train Loss: 0.00024064\n","Iter: 073/100 | Train Loss: 0.00023672\n","Iter: 074/100 | Train Loss: 0.00023295\n","Iter: 075/100 | Train Loss: 0.00022928\n","Iter: 076/100 | Train Loss: 0.00022568\n","Iter: 077/100 | Train Loss: 0.00022211\n","Iter: 078/100 | Train Loss: 0.00021858\n","Iter: 079/100 | Train Loss: 0.00021512\n","Iter: 080/100 | Train Loss: 0.00021175\n","Iter: 081/100 | Train Loss: 0.00020848\n","Iter: 082/100 | Train Loss: 0.00020530\n","Iter: 083/100 | Train Loss: 0.00020219\n","Iter: 084/100 | Train Loss: 0.00019911\n","Iter: 085/100 | Train Loss: 0.00019608\n","Iter: 086/100 | Train Loss: 0.00019311\n","Iter: 087/100 | Train Loss: 0.00019021\n","Iter: 088/100 | Train Loss: 0.00018737\n","Iter: 089/100 | Train Loss: 0.00018459\n","Iter: 090/100 | Train Loss: 0.00018186\n","Iter: 091/100 | Train Loss: 0.00017917\n","Iter: 092/100 | Train Loss: 0.00017652\n","Iter: 093/100 | Train Loss: 0.00017393\n","Iter: 094/100 | Train Loss: 0.00017138\n","Iter: 095/100 | Train Loss: 0.00016889\n","Iter: 096/100 | Train Loss: 0.00016644\n","Iter: 097/100 | Train Loss: 0.00016404\n","Iter: 098/100 | Train Loss: 0.00016168\n","Iter: 099/100 | Train Loss: 0.00015935\n","\n","Iter: 099/100 | Test Loss: 0.00100946 | Test acc: 65.0600\n","scale:1.200000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 718, Adptive Nodes:82\n","Iter: 000/100 | Train Loss: 0.00235676\n","Iter: 001/100 | Train Loss: 0.00197090\n","Iter: 002/100 | Train Loss: 0.00159949\n","Iter: 003/100 | Train Loss: 0.00144669\n","Iter: 004/100 | Train Loss: 0.00152175\n","Adjusting Layer 1, Kernel Nodes: 707, Adptive Nodes:93\n","Iter: 005/100 | Train Loss: 0.00166035\n","Adjusting Layer 1, Kernel Nodes: 706, Adptive Nodes:94\n","Iter: 006/100 | Train Loss: 0.00170799\n","Adjusting Layer 1, Kernel Nodes: 703, Adptive Nodes:97\n","Iter: 007/100 | Train Loss: 0.00160818\n","Iter: 008/100 | Train Loss: 0.00143017\n","Iter: 009/100 | Train Loss: 0.00125280\n","Iter: 010/100 | Train Loss: 0.00115031\n","Iter: 011/100 | Train Loss: 0.00113165\n","Iter: 012/100 | Train Loss: 0.00115415\n","Adjusting Layer 1, Kernel Nodes: 657, Adptive Nodes:143\n","Iter: 013/100 | Train Loss: 0.00117326\n","Adjusting Layer 1, Kernel Nodes: 675, Adptive Nodes:125\n","Iter: 014/100 | Train Loss: 0.00114369\n","Iter: 015/100 | Train Loss: 0.00106010\n","Iter: 016/100 | Train Loss: 0.00096780\n","Iter: 017/100 | Train Loss: 0.00089906\n","Iter: 018/100 | Train Loss: 0.00086559\n","Iter: 019/100 | Train Loss: 0.00085697\n","Iter: 020/100 | Train Loss: 0.00085255\n","Iter: 021/100 | Train Loss: 0.00083547\n","Iter: 022/100 | Train Loss: 0.00080211\n","Iter: 023/100 | Train Loss: 0.00076009\n","Iter: 024/100 | Train Loss: 0.00072071\n","Iter: 025/100 | Train Loss: 0.00069153\n","Iter: 026/100 | Train Loss: 0.00067298\n","Iter: 027/100 | Train Loss: 0.00066049\n","Iter: 028/100 | Train Loss: 0.00064830\n","Iter: 029/100 | Train Loss: 0.00063292\n","Iter: 030/100 | Train Loss: 0.00061402\n","Iter: 031/100 | Train Loss: 0.00059364\n","Iter: 032/100 | Train Loss: 0.00057449\n","Iter: 033/100 | Train Loss: 0.00055825\n","Iter: 034/100 | Train Loss: 0.00054509\n","Iter: 035/100 | Train Loss: 0.00053412\n","Iter: 036/100 | Train Loss: 0.00052392\n","Iter: 037/100 | Train Loss: 0.00051341\n","Iter: 038/100 | Train Loss: 0.00050219\n","Iter: 039/100 | Train Loss: 0.00049045\n","Iter: 040/100 | Train Loss: 0.00047876\n","Iter: 041/100 | Train Loss: 0.00046781\n","Iter: 042/100 | Train Loss: 0.00045804\n","Iter: 043/100 | Train Loss: 0.00044949\n","Iter: 044/100 | Train Loss: 0.00044175\n","Iter: 045/100 | Train Loss: 0.00043420\n","Iter: 046/100 | Train Loss: 0.00042638\n","Iter: 047/100 | Train Loss: 0.00041817\n","Iter: 048/100 | Train Loss: 0.00040986\n","Iter: 049/100 | Train Loss: 0.00040189\n","Iter: 050/100 | Train Loss: 0.00039465\n","Iter: 051/100 | Train Loss: 0.00038816\n","Iter: 052/100 | Train Loss: 0.00038214\n","Iter: 053/100 | Train Loss: 0.00037619\n","Iter: 054/100 | Train Loss: 0.00037000\n","Iter: 055/100 | Train Loss: 0.00036355\n","Iter: 056/100 | Train Loss: 0.00035714\n","Iter: 057/100 | Train Loss: 0.00035107\n","Iter: 058/100 | Train Loss: 0.00034551\n","Iter: 059/100 | Train Loss: 0.00034035\n","Iter: 060/100 | Train Loss: 0.00033535\n","Iter: 061/100 | Train Loss: 0.00033028\n","Iter: 062/100 | Train Loss: 0.00032510\n","Iter: 063/100 | Train Loss: 0.00031989\n","Iter: 064/100 | Train Loss: 0.00031482\n","Iter: 065/100 | Train Loss: 0.00031003\n","Iter: 066/100 | Train Loss: 0.00030553\n","Iter: 067/100 | Train Loss: 0.00030120\n","Iter: 068/100 | Train Loss: 0.00029691\n","Iter: 069/100 | Train Loss: 0.00029258\n","Iter: 070/100 | Train Loss: 0.00028825\n","Iter: 071/100 | Train Loss: 0.00028398\n","Iter: 072/100 | Train Loss: 0.00027987\n","Iter: 073/100 | Train Loss: 0.00027593\n","Iter: 074/100 | Train Loss: 0.00027211\n","Iter: 075/100 | Train Loss: 0.00026834\n","Iter: 076/100 | Train Loss: 0.00026459\n","Iter: 077/100 | Train Loss: 0.00026086\n","Iter: 078/100 | Train Loss: 0.00025718\n","Iter: 079/100 | Train Loss: 0.00025360\n","Iter: 080/100 | Train Loss: 0.00025012\n","Iter: 081/100 | Train Loss: 0.00024671\n","Iter: 082/100 | Train Loss: 0.00024336\n","Iter: 083/100 | Train Loss: 0.00024005\n","Iter: 084/100 | Train Loss: 0.00023678\n","Iter: 085/100 | Train Loss: 0.00023357\n","Iter: 086/100 | Train Loss: 0.00023042\n","Iter: 087/100 | Train Loss: 0.00022734\n","Iter: 088/100 | Train Loss: 0.00022433\n","Iter: 089/100 | Train Loss: 0.00022136\n","Iter: 090/100 | Train Loss: 0.00021845\n","Iter: 091/100 | Train Loss: 0.00021557\n","Iter: 092/100 | Train Loss: 0.00021274\n","Iter: 093/100 | Train Loss: 0.00020996\n","Iter: 094/100 | Train Loss: 0.00020723\n","Iter: 095/100 | Train Loss: 0.00020455\n","Iter: 096/100 | Train Loss: 0.00020191\n","Iter: 097/100 | Train Loss: 0.00019932\n","Iter: 098/100 | Train Loss: 0.00019677\n","Iter: 099/100 | Train Loss: 0.00019425\n","\n","Iter: 099/100 | Test Loss: 0.00100633 | Test acc: 64.8800\n","scale:1.200000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00234266\n","Iter: 001/100 | Train Loss: 0.00195195\n","Iter: 002/100 | Train Loss: 0.00158488\n","Iter: 003/100 | Train Loss: 0.00144418\n","Iter: 004/100 | Train Loss: 0.00152951\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00168188\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00172915\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00161204\n","Iter: 008/100 | Train Loss: 0.00140127\n","Iter: 009/100 | Train Loss: 0.00121521\n","Iter: 010/100 | Train Loss: 0.00112615\n","Iter: 011/100 | Train Loss: 0.00112766\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 012/100 | Train Loss: 0.00116207\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 013/100 | Train Loss: 0.00116916\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00112219\n","Iter: 015/100 | Train Loss: 0.00103408\n","Iter: 016/100 | Train Loss: 0.00093964\n","Iter: 017/100 | Train Loss: 0.00087037\n","Iter: 018/100 | Train Loss: 0.00083773\n","Iter: 019/100 | Train Loss: 0.00083168\n","Iter: 020/100 | Train Loss: 0.00083146\n","Iter: 021/100 | Train Loss: 0.00081917\n","Iter: 022/100 | Train Loss: 0.00078905\n","Iter: 023/100 | Train Loss: 0.00074710\n","Iter: 024/100 | Train Loss: 0.00070507\n","Iter: 025/100 | Train Loss: 0.00067276\n","Iter: 026/100 | Train Loss: 0.00065328\n","Iter: 027/100 | Train Loss: 0.00064306\n","Iter: 028/100 | Train Loss: 0.00063541\n","Iter: 029/100 | Train Loss: 0.00062421\n","Iter: 030/100 | Train Loss: 0.00060711\n","Iter: 031/100 | Train Loss: 0.00058564\n","Iter: 032/100 | Train Loss: 0.00056381\n","Iter: 033/100 | Train Loss: 0.00054543\n","Iter: 034/100 | Train Loss: 0.00053223\n","Iter: 035/100 | Train Loss: 0.00052340\n","Iter: 036/100 | Train Loss: 0.00051622\n","Iter: 037/100 | Train Loss: 0.00050792\n","Iter: 038/100 | Train Loss: 0.00049711\n","Iter: 039/100 | Train Loss: 0.00048431\n","Iter: 040/100 | Train Loss: 0.00047133\n","Iter: 041/100 | Train Loss: 0.00045991\n","Iter: 042/100 | Train Loss: 0.00045093\n","Iter: 043/100 | Train Loss: 0.00044396\n","Iter: 044/100 | Train Loss: 0.00043765\n","Iter: 045/100 | Train Loss: 0.00043077\n","Iter: 046/100 | Train Loss: 0.00042279\n","Iter: 047/100 | Train Loss: 0.00041407\n","Iter: 048/100 | Train Loss: 0.00040549\n","Iter: 049/100 | Train Loss: 0.00039778\n","Iter: 050/100 | Train Loss: 0.00039122\n","Iter: 051/100 | Train Loss: 0.00038546\n","Iter: 052/100 | Train Loss: 0.00037992\n","Iter: 053/100 | Train Loss: 0.00037407\n","Iter: 054/100 | Train Loss: 0.00036781\n","Iter: 055/100 | Train Loss: 0.00036137\n","Iter: 056/100 | Train Loss: 0.00035514\n","Iter: 057/100 | Train Loss: 0.00034940\n","Iter: 058/100 | Train Loss: 0.00034419\n","Iter: 059/100 | Train Loss: 0.00033931\n","Iter: 060/100 | Train Loss: 0.00033447\n","Iter: 061/100 | Train Loss: 0.00032948\n","Iter: 062/100 | Train Loss: 0.00032439\n","Iter: 063/100 | Train Loss: 0.00031931\n","Iter: 064/100 | Train Loss: 0.00031444\n","Iter: 065/100 | Train Loss: 0.00030986\n","Iter: 066/100 | Train Loss: 0.00030550\n","Iter: 067/100 | Train Loss: 0.00030127\n","Iter: 068/100 | Train Loss: 0.00029704\n","Iter: 069/100 | Train Loss: 0.00029279\n","Iter: 070/100 | Train Loss: 0.00028854\n","Iter: 071/100 | Train Loss: 0.00028438\n","Iter: 072/100 | Train Loss: 0.00028038\n","Iter: 073/100 | Train Loss: 0.00027652\n","Iter: 074/100 | Train Loss: 0.00027278\n","Iter: 075/100 | Train Loss: 0.00026908\n","Iter: 076/100 | Train Loss: 0.00026541\n","Iter: 077/100 | Train Loss: 0.00026176\n","Iter: 078/100 | Train Loss: 0.00025815\n","Iter: 079/100 | Train Loss: 0.00025463\n","Iter: 080/100 | Train Loss: 0.00025119\n","Iter: 081/100 | Train Loss: 0.00024786\n","Iter: 082/100 | Train Loss: 0.00024460\n","Iter: 083/100 | Train Loss: 0.00024140\n","Iter: 084/100 | Train Loss: 0.00023824\n","Iter: 085/100 | Train Loss: 0.00023512\n","Iter: 086/100 | Train Loss: 0.00023206\n","Iter: 087/100 | Train Loss: 0.00022906\n","Iter: 088/100 | Train Loss: 0.00022614\n","Iter: 089/100 | Train Loss: 0.00022328\n","Iter: 090/100 | Train Loss: 0.00022046\n","Iter: 091/100 | Train Loss: 0.00021769\n","Iter: 092/100 | Train Loss: 0.00021496\n","Iter: 093/100 | Train Loss: 0.00021227\n","Iter: 094/100 | Train Loss: 0.00020962\n","Iter: 095/100 | Train Loss: 0.00020701\n","Iter: 096/100 | Train Loss: 0.00020444\n","Iter: 097/100 | Train Loss: 0.00020191\n","Iter: 098/100 | Train Loss: 0.00019943\n","Iter: 099/100 | Train Loss: 0.00019697\n","\n","Iter: 099/100 | Test Loss: 0.00100522 | Test acc: 64.3000\n","scale:1.200000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00232881\n","Iter: 001/100 | Train Loss: 0.00193002\n","Iter: 002/100 | Train Loss: 0.00156647\n","Iter: 003/100 | Train Loss: 0.00144146\n","Iter: 004/100 | Train Loss: 0.00154150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00171625\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00175119\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00157535\n","Iter: 008/100 | Train Loss: 0.00130236\n","Iter: 009/100 | Train Loss: 0.00113877\n","Iter: 010/100 | Train Loss: 0.00112184\n","Iter: 011/100 | Train Loss: 0.00117590\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00118716\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 013/100 | Train Loss: 0.00112095\n","Iter: 014/100 | Train Loss: 0.00101296\n","Iter: 015/100 | Train Loss: 0.00090179\n","Iter: 016/100 | Train Loss: 0.00083244\n","Iter: 017/100 | Train Loss: 0.00081256\n","Iter: 018/100 | Train Loss: 0.00081535\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 019/100 | Train Loss: 0.00081765\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 020/100 | Train Loss: 0.00078731\n","Iter: 021/100 | Train Loss: 0.00072169\n","Iter: 022/100 | Train Loss: 0.00066552\n","Iter: 023/100 | Train Loss: 0.00063582\n","Iter: 024/100 | Train Loss: 0.00062831\n","Iter: 025/100 | Train Loss: 0.00062599\n","Iter: 026/100 | Train Loss: 0.00061348\n","Iter: 027/100 | Train Loss: 0.00058657\n","Iter: 028/100 | Train Loss: 0.00055258\n","Iter: 029/100 | Train Loss: 0.00052379\n","Iter: 030/100 | Train Loss: 0.00050754\n","Iter: 031/100 | Train Loss: 0.00050125\n","Iter: 032/100 | Train Loss: 0.00049591\n","Iter: 033/100 | Train Loss: 0.00048370\n","Iter: 034/100 | Train Loss: 0.00046443\n","Iter: 035/100 | Train Loss: 0.00044402\n","Iter: 036/100 | Train Loss: 0.00042868\n","Iter: 037/100 | Train Loss: 0.00041995\n","Iter: 038/100 | Train Loss: 0.00041443\n","Iter: 039/100 | Train Loss: 0.00040743\n","Iter: 040/100 | Train Loss: 0.00039640\n","Iter: 041/100 | Train Loss: 0.00038294\n","Iter: 042/100 | Train Loss: 0.00037059\n","Iter: 043/100 | Train Loss: 0.00036187\n","Iter: 044/100 | Train Loss: 0.00035617\n","Iter: 045/100 | Train Loss: 0.00035071\n","Iter: 046/100 | Train Loss: 0.00034318\n","Iter: 047/100 | Train Loss: 0.00033365\n","Iter: 048/100 | Train Loss: 0.00032425\n","Iter: 049/100 | Train Loss: 0.00031682\n","Iter: 050/100 | Train Loss: 0.00031137\n","Iter: 051/100 | Train Loss: 0.00030637\n","Iter: 052/100 | Train Loss: 0.00030034\n","Iter: 053/100 | Train Loss: 0.00029322\n","Iter: 054/100 | Train Loss: 0.00028608\n","Iter: 055/100 | Train Loss: 0.00027997\n","Iter: 056/100 | Train Loss: 0.00027491\n","Iter: 057/100 | Train Loss: 0.00027027\n","Iter: 058/100 | Train Loss: 0.00026540\n","Iter: 059/100 | Train Loss: 0.00026018\n","Iter: 060/100 | Train Loss: 0.00025490\n","Iter: 061/100 | Train Loss: 0.00024998\n","Iter: 062/100 | Train Loss: 0.00024562\n","Iter: 063/100 | Train Loss: 0.00024158\n","Iter: 064/100 | Train Loss: 0.00023755\n","Iter: 065/100 | Train Loss: 0.00023328\n","Iter: 066/100 | Train Loss: 0.00022888\n","Iter: 067/100 | Train Loss: 0.00022463\n","Iter: 068/100 | Train Loss: 0.00022074\n","Iter: 069/100 | Train Loss: 0.00021714\n","Iter: 070/100 | Train Loss: 0.00021357\n","Iter: 071/100 | Train Loss: 0.00020985\n","Iter: 072/100 | Train Loss: 0.00020606\n","Iter: 073/100 | Train Loss: 0.00020239\n","Iter: 074/100 | Train Loss: 0.00019898\n","Iter: 075/100 | Train Loss: 0.00019579\n","Iter: 076/100 | Train Loss: 0.00019265\n","Iter: 077/100 | Train Loss: 0.00018943\n","Iter: 078/100 | Train Loss: 0.00018620\n","Iter: 079/100 | Train Loss: 0.00018308\n","Iter: 080/100 | Train Loss: 0.00018015\n","Iter: 081/100 | Train Loss: 0.00017735\n","Iter: 082/100 | Train Loss: 0.00017459\n","Iter: 083/100 | Train Loss: 0.00017181\n","Iter: 084/100 | Train Loss: 0.00016904\n","Iter: 085/100 | Train Loss: 0.00016637\n","Iter: 086/100 | Train Loss: 0.00016381\n","Iter: 087/100 | Train Loss: 0.00016132\n","Iter: 088/100 | Train Loss: 0.00015884\n","Iter: 089/100 | Train Loss: 0.00015638\n","Iter: 090/100 | Train Loss: 0.00015397\n","Iter: 091/100 | Train Loss: 0.00015163\n","Iter: 092/100 | Train Loss: 0.00014935\n","Iter: 093/100 | Train Loss: 0.00014711\n","Iter: 094/100 | Train Loss: 0.00014490\n","Iter: 095/100 | Train Loss: 0.00014273\n","Iter: 096/100 | Train Loss: 0.00014059\n","Iter: 097/100 | Train Loss: 0.00013849\n","Iter: 098/100 | Train Loss: 0.00013643\n","Iter: 099/100 | Train Loss: 0.00013441\n","\n","Iter: 099/100 | Test Loss: 0.00099937 | Test acc: 64.8300\n","scale:1.250000,therd:0.600000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00242996\n","Iter: 001/100 | Train Loss: 0.00197671\n","Iter: 002/100 | Train Loss: 0.00156035\n","Iter: 003/100 | Train Loss: 0.00146180\n","Iter: 004/100 | Train Loss: 0.00162643\n","Adjusting Layer 1, Kernel Nodes: 610, Adptive Nodes:190\n","Iter: 005/100 | Train Loss: 0.00175664\n","Adjusting Layer 1, Kernel Nodes: 533, Adptive Nodes:267\n","Iter: 006/100 | Train Loss: 0.00152025\n","Iter: 007/100 | Train Loss: 0.00122332\n","Iter: 008/100 | Train Loss: 0.00115032\n","Iter: 009/100 | Train Loss: 0.00118155\n","Adjusting Layer 1, Kernel Nodes: 375, Adptive Nodes:425\n","Iter: 010/100 | Train Loss: 0.00112318\n","Iter: 011/100 | Train Loss: 0.00098055\n","Iter: 012/100 | Train Loss: 0.00085669\n","Iter: 013/100 | Train Loss: 0.00081847\n","Iter: 014/100 | Train Loss: 0.00081968\n","Adjusting Layer 1, Kernel Nodes: 416, Adptive Nodes:384\n","Iter: 015/100 | Train Loss: 0.00074552\n","Iter: 016/100 | Train Loss: 0.00067047\n","Iter: 017/100 | Train Loss: 0.00062410\n","Iter: 018/100 | Train Loss: 0.00060859\n","Iter: 019/100 | Train Loss: 0.00059303\n","Iter: 020/100 | Train Loss: 0.00055838\n","Iter: 021/100 | Train Loss: 0.00051849\n","Iter: 022/100 | Train Loss: 0.00049255\n","Iter: 023/100 | Train Loss: 0.00048026\n","Iter: 024/100 | Train Loss: 0.00046708\n","Iter: 025/100 | Train Loss: 0.00044460\n","Iter: 026/100 | Train Loss: 0.00041924\n","Iter: 027/100 | Train Loss: 0.00040059\n","Iter: 028/100 | Train Loss: 0.00038845\n","Iter: 029/100 | Train Loss: 0.00037551\n","Iter: 030/100 | Train Loss: 0.00035808\n","Iter: 031/100 | Train Loss: 0.00034026\n","Iter: 032/100 | Train Loss: 0.00032622\n","Iter: 033/100 | Train Loss: 0.00031485\n","Iter: 034/100 | Train Loss: 0.00030295\n","Iter: 035/100 | Train Loss: 0.00029001\n","Iter: 036/100 | Train Loss: 0.00027772\n","Iter: 037/100 | Train Loss: 0.00026698\n","Iter: 038/100 | Train Loss: 0.00025727\n","Iter: 039/100 | Train Loss: 0.00024802\n","Iter: 040/100 | Train Loss: 0.00023898\n","Iter: 041/100 | Train Loss: 0.00023008\n","Iter: 042/100 | Train Loss: 0.00022158\n","Iter: 043/100 | Train Loss: 0.00021381\n","Iter: 044/100 | Train Loss: 0.00020664\n","Iter: 045/100 | Train Loss: 0.00019972\n","Iter: 046/100 | Train Loss: 0.00019301\n","Iter: 047/100 | Train Loss: 0.00018663\n","Iter: 048/100 | Train Loss: 0.00018057\n","Iter: 049/100 | Train Loss: 0.00017480\n","Iter: 050/100 | Train Loss: 0.00016938\n","Iter: 051/100 | Train Loss: 0.00016419\n","Iter: 052/100 | Train Loss: 0.00015907\n","Iter: 053/100 | Train Loss: 0.00015410\n","Iter: 054/100 | Train Loss: 0.00014946\n","Iter: 055/100 | Train Loss: 0.00014514\n","Iter: 056/100 | Train Loss: 0.00014091\n","Iter: 057/100 | Train Loss: 0.00013676\n","Iter: 058/100 | Train Loss: 0.00013272\n","Iter: 059/100 | Train Loss: 0.00012884\n","Iter: 060/100 | Train Loss: 0.00012518\n","Iter: 061/100 | Train Loss: 0.00012171\n","Iter: 062/100 | Train Loss: 0.00011822\n","Iter: 063/100 | Train Loss: 0.00011478\n","Iter: 064/100 | Train Loss: 0.00011157\n","Iter: 065/100 | Train Loss: 0.00010855\n","Iter: 066/100 | Train Loss: 0.00010561\n","Iter: 067/100 | Train Loss: 0.00010270\n","Iter: 068/100 | Train Loss: 0.00009988\n","Iter: 069/100 | Train Loss: 0.00009719\n","Iter: 070/100 | Train Loss: 0.00009463\n","Iter: 071/100 | Train Loss: 0.00009217\n","Iter: 072/100 | Train Loss: 0.00008973\n","Iter: 073/100 | Train Loss: 0.00008736\n","Iter: 074/100 | Train Loss: 0.00008510\n","Iter: 075/100 | Train Loss: 0.00008294\n","Iter: 076/100 | Train Loss: 0.00008083\n","Iter: 077/100 | Train Loss: 0.00007876\n","Iter: 078/100 | Train Loss: 0.00007674\n","Iter: 079/100 | Train Loss: 0.00007480\n","Iter: 080/100 | Train Loss: 0.00007294\n","Iter: 081/100 | Train Loss: 0.00007113\n","Iter: 082/100 | Train Loss: 0.00006936\n","Iter: 083/100 | Train Loss: 0.00006763\n","Iter: 084/100 | Train Loss: 0.00006597\n","Iter: 085/100 | Train Loss: 0.00006437\n","Iter: 086/100 | Train Loss: 0.00006281\n","Iter: 087/100 | Train Loss: 0.00006129\n","Iter: 088/100 | Train Loss: 0.00005980\n","Iter: 089/100 | Train Loss: 0.00005838\n","Iter: 090/100 | Train Loss: 0.00005700\n","Iter: 091/100 | Train Loss: 0.00005565\n","Iter: 092/100 | Train Loss: 0.00005434\n","Iter: 093/100 | Train Loss: 0.00005306\n","Iter: 094/100 | Train Loss: 0.00005182\n","Iter: 095/100 | Train Loss: 0.00005063\n","Iter: 096/100 | Train Loss: 0.00004946\n","Iter: 097/100 | Train Loss: 0.00004832\n","Iter: 098/100 | Train Loss: 0.00004721\n","Iter: 099/100 | Train Loss: 0.00004613\n","\n","Iter: 099/100 | Test Loss: 0.00107329 | Test acc: 63.4200\n","scale:1.250000,therd:0.650000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00242240\n","Iter: 001/100 | Train Loss: 0.00199576\n","Iter: 002/100 | Train Loss: 0.00158661\n","Iter: 003/100 | Train Loss: 0.00145469\n","Iter: 004/100 | Train Loss: 0.00158306\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 005/100 | Train Loss: 0.00172533\n","Adjusting Layer 1, Kernel Nodes: 467, Adptive Nodes:333\n","Iter: 006/100 | Train Loss: 0.00162907\n","Iter: 007/100 | Train Loss: 0.00133564\n","Iter: 008/100 | Train Loss: 0.00115851\n","Iter: 009/100 | Train Loss: 0.00116256\n","Adjusting Layer 1, Kernel Nodes: 700, Adptive Nodes:100\n","Iter: 010/100 | Train Loss: 0.00120200\n","Adjusting Layer 1, Kernel Nodes: 9, Adptive Nodes:791\n","Iter: 011/100 | Train Loss: 0.00105284\n","Iter: 012/100 | Train Loss: 0.00095291\n","Iter: 013/100 | Train Loss: 0.00086139\n","Iter: 014/100 | Train Loss: 0.00080659\n","Iter: 015/100 | Train Loss: 0.00078148\n","Iter: 016/100 | Train Loss: 0.00076148\n","Iter: 017/100 | Train Loss: 0.00072935\n","Iter: 018/100 | Train Loss: 0.00068438\n","Iter: 019/100 | Train Loss: 0.00063835\n","Iter: 020/100 | Train Loss: 0.00060278\n","Iter: 021/100 | Train Loss: 0.00058335\n","Iter: 022/100 | Train Loss: 0.00057207\n","Iter: 023/100 | Train Loss: 0.00055567\n","Iter: 024/100 | Train Loss: 0.00053128\n","Iter: 025/100 | Train Loss: 0.00050658\n","Iter: 026/100 | Train Loss: 0.00048504\n","Iter: 027/100 | Train Loss: 0.00046939\n","Iter: 028/100 | Train Loss: 0.00045803\n","Iter: 029/100 | Train Loss: 0.00044724\n","Iter: 030/100 | Train Loss: 0.00043414\n","Iter: 031/100 | Train Loss: 0.00041870\n","Iter: 032/100 | Train Loss: 0.00040320\n","Iter: 033/100 | Train Loss: 0.00038993\n","Iter: 034/100 | Train Loss: 0.00037962\n","Iter: 035/100 | Train Loss: 0.00037088\n","Iter: 036/100 | Train Loss: 0.00036163\n","Iter: 037/100 | Train Loss: 0.00035080\n","Iter: 038/100 | Train Loss: 0.00033914\n","Iter: 039/100 | Train Loss: 0.00032837\n","Iter: 040/100 | Train Loss: 0.00031953\n","Iter: 041/100 | Train Loss: 0.00031220\n","Iter: 042/100 | Train Loss: 0.00030493\n","Iter: 043/100 | Train Loss: 0.00029676\n","Iter: 044/100 | Train Loss: 0.00028800\n","Iter: 045/100 | Train Loss: 0.00027968\n","Iter: 046/100 | Train Loss: 0.00027250\n","Iter: 047/100 | Train Loss: 0.00026628\n","Iter: 048/100 | Train Loss: 0.00026030\n","Iter: 049/100 | Train Loss: 0.00025395\n","Iter: 050/100 | Train Loss: 0.00024728\n","Iter: 051/100 | Train Loss: 0.00024078\n","Iter: 052/100 | Train Loss: 0.00023491\n","Iter: 053/100 | Train Loss: 0.00022966\n","Iter: 054/100 | Train Loss: 0.00022466\n","Iter: 055/100 | Train Loss: 0.00021961\n","Iter: 056/100 | Train Loss: 0.00021445\n","Iter: 057/100 | Train Loss: 0.00020938\n","Iter: 058/100 | Train Loss: 0.00020461\n","Iter: 059/100 | Train Loss: 0.00020018\n","Iter: 060/100 | Train Loss: 0.00019597\n","Iter: 061/100 | Train Loss: 0.00019180\n","Iter: 062/100 | Train Loss: 0.00018763\n","Iter: 063/100 | Train Loss: 0.00018351\n","Iter: 064/100 | Train Loss: 0.00017954\n","Iter: 065/100 | Train Loss: 0.00017578\n","Iter: 066/100 | Train Loss: 0.00017217\n","Iter: 067/100 | Train Loss: 0.00016863\n","Iter: 068/100 | Train Loss: 0.00016510\n","Iter: 069/100 | Train Loss: 0.00016163\n","Iter: 070/100 | Train Loss: 0.00015826\n","Iter: 071/100 | Train Loss: 0.00015501\n","Iter: 072/100 | Train Loss: 0.00015188\n","Iter: 073/100 | Train Loss: 0.00014883\n","Iter: 074/100 | Train Loss: 0.00014583\n","Iter: 075/100 | Train Loss: 0.00014287\n","Iter: 076/100 | Train Loss: 0.00013998\n","Iter: 077/100 | Train Loss: 0.00013718\n","Iter: 078/100 | Train Loss: 0.00013448\n","Iter: 079/100 | Train Loss: 0.00013184\n","Iter: 080/100 | Train Loss: 0.00012925\n","Iter: 081/100 | Train Loss: 0.00012669\n","Iter: 082/100 | Train Loss: 0.00012420\n","Iter: 083/100 | Train Loss: 0.00012178\n","Iter: 084/100 | Train Loss: 0.00011943\n","Iter: 085/100 | Train Loss: 0.00011714\n","Iter: 086/100 | Train Loss: 0.00011489\n","Iter: 087/100 | Train Loss: 0.00011268\n","Iter: 088/100 | Train Loss: 0.00011052\n","Iter: 089/100 | Train Loss: 0.00010842\n","Iter: 090/100 | Train Loss: 0.00010637\n","Iter: 091/100 | Train Loss: 0.00010437\n","Iter: 092/100 | Train Loss: 0.00010240\n","Iter: 093/100 | Train Loss: 0.00010047\n","Iter: 094/100 | Train Loss: 0.00009859\n","Iter: 095/100 | Train Loss: 0.00009674\n","Iter: 096/100 | Train Loss: 0.00009495\n","Iter: 097/100 | Train Loss: 0.00009319\n","Iter: 098/100 | Train Loss: 0.00009147\n","Iter: 099/100 | Train Loss: 0.00008978\n","\n","Iter: 099/100 | Test Loss: 0.00103340 | Test acc: 64.6100\n","scale:1.250000,therd:0.700000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00241368\n","Iter: 001/100 | Train Loss: 0.00200596\n","Iter: 002/100 | Train Loss: 0.00160502\n","Iter: 003/100 | Train Loss: 0.00145309\n","Iter: 004/100 | Train Loss: 0.00155410\n","Adjusting Layer 1, Kernel Nodes: 789, Adptive Nodes:11\n","Iter: 005/100 | Train Loss: 0.00166421\n","Adjusting Layer 1, Kernel Nodes: 439, Adptive Nodes:361\n","Iter: 006/100 | Train Loss: 0.00169559\n","Adjusting Layer 1, Kernel Nodes: 496, Adptive Nodes:304\n","Iter: 007/100 | Train Loss: 0.00140533\n","Iter: 008/100 | Train Loss: 0.00117878\n","Iter: 009/100 | Train Loss: 0.00115947\n","Iter: 010/100 | Train Loss: 0.00119919\n","Adjusting Layer 1, Kernel Nodes: 219, Adptive Nodes:581\n","Iter: 011/100 | Train Loss: 0.00111518\n","Iter: 012/100 | Train Loss: 0.00102685\n","Iter: 013/100 | Train Loss: 0.00091907\n","Iter: 014/100 | Train Loss: 0.00084173\n","Iter: 015/100 | Train Loss: 0.00080830\n","Iter: 016/100 | Train Loss: 0.00079680\n","Iter: 017/100 | Train Loss: 0.00077728\n","Iter: 018/100 | Train Loss: 0.00073686\n","Iter: 019/100 | Train Loss: 0.00068471\n","Iter: 020/100 | Train Loss: 0.00063912\n","Iter: 021/100 | Train Loss: 0.00061125\n","Iter: 022/100 | Train Loss: 0.00059832\n","Iter: 023/100 | Train Loss: 0.00058840\n","Iter: 024/100 | Train Loss: 0.00057121\n","Iter: 025/100 | Train Loss: 0.00054476\n","Iter: 026/100 | Train Loss: 0.00051632\n","Iter: 027/100 | Train Loss: 0.00049489\n","Iter: 028/100 | Train Loss: 0.00048293\n","Iter: 029/100 | Train Loss: 0.00047559\n","Iter: 030/100 | Train Loss: 0.00046560\n","Iter: 031/100 | Train Loss: 0.00044996\n","Iter: 032/100 | Train Loss: 0.00043166\n","Iter: 033/100 | Train Loss: 0.00041583\n","Iter: 034/100 | Train Loss: 0.00040508\n","Iter: 035/100 | Train Loss: 0.00039768\n","Iter: 036/100 | Train Loss: 0.00038964\n","Iter: 037/100 | Train Loss: 0.00037882\n","Iter: 038/100 | Train Loss: 0.00036630\n","Iter: 039/100 | Train Loss: 0.00035473\n","Iter: 040/100 | Train Loss: 0.00034532\n","Iter: 041/100 | Train Loss: 0.00033771\n","Iter: 042/100 | Train Loss: 0.00033034\n","Iter: 043/100 | Train Loss: 0.00032250\n","Iter: 044/100 | Train Loss: 0.00031368\n","Iter: 045/100 | Train Loss: 0.00030484\n","Iter: 046/100 | Train Loss: 0.00029695\n","Iter: 047/100 | Train Loss: 0.00029018\n","Iter: 048/100 | Train Loss: 0.00028391\n","Iter: 049/100 | Train Loss: 0.00027754\n","Iter: 050/100 | Train Loss: 0.00027084\n","Iter: 051/100 | Train Loss: 0.00026406\n","Iter: 052/100 | Train Loss: 0.00025765\n","Iter: 053/100 | Train Loss: 0.00025174\n","Iter: 054/100 | Train Loss: 0.00024626\n","Iter: 055/100 | Train Loss: 0.00024103\n","Iter: 056/100 | Train Loss: 0.00023583\n","Iter: 057/100 | Train Loss: 0.00023053\n","Iter: 058/100 | Train Loss: 0.00022525\n","Iter: 059/100 | Train Loss: 0.00022026\n","Iter: 060/100 | Train Loss: 0.00021568\n","Iter: 061/100 | Train Loss: 0.00021132\n","Iter: 062/100 | Train Loss: 0.00020697\n","Iter: 063/100 | Train Loss: 0.00020257\n","Iter: 064/100 | Train Loss: 0.00019825\n","Iter: 065/100 | Train Loss: 0.00019413\n","Iter: 066/100 | Train Loss: 0.00019019\n","Iter: 067/100 | Train Loss: 0.00018645\n","Iter: 068/100 | Train Loss: 0.00018276\n","Iter: 069/100 | Train Loss: 0.00017905\n","Iter: 070/100 | Train Loss: 0.00017541\n","Iter: 071/100 | Train Loss: 0.00017191\n","Iter: 072/100 | Train Loss: 0.00016856\n","Iter: 073/100 | Train Loss: 0.00016529\n","Iter: 074/100 | Train Loss: 0.00016206\n","Iter: 075/100 | Train Loss: 0.00015887\n","Iter: 076/100 | Train Loss: 0.00015578\n","Iter: 077/100 | Train Loss: 0.00015277\n","Iter: 078/100 | Train Loss: 0.00014984\n","Iter: 079/100 | Train Loss: 0.00014701\n","Iter: 080/100 | Train Loss: 0.00014423\n","Iter: 081/100 | Train Loss: 0.00014150\n","Iter: 082/100 | Train Loss: 0.00013884\n","Iter: 083/100 | Train Loss: 0.00013623\n","Iter: 084/100 | Train Loss: 0.00013369\n","Iter: 085/100 | Train Loss: 0.00013120\n","Iter: 086/100 | Train Loss: 0.00012878\n","Iter: 087/100 | Train Loss: 0.00012639\n","Iter: 088/100 | Train Loss: 0.00012406\n","Iter: 089/100 | Train Loss: 0.00012179\n","Iter: 090/100 | Train Loss: 0.00011957\n","Iter: 091/100 | Train Loss: 0.00011740\n","Iter: 092/100 | Train Loss: 0.00011527\n","Iter: 093/100 | Train Loss: 0.00011320\n","Iter: 094/100 | Train Loss: 0.00011117\n","Iter: 095/100 | Train Loss: 0.00010918\n","Iter: 096/100 | Train Loss: 0.00010724\n","Iter: 097/100 | Train Loss: 0.00010535\n","Iter: 098/100 | Train Loss: 0.00010350\n","Iter: 099/100 | Train Loss: 0.00010168\n","\n","Iter: 099/100 | Test Loss: 0.00102454 | Test acc: 64.8700\n","scale:1.250000,therd:0.750000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00240408\n","Iter: 001/100 | Train Loss: 0.00200890\n","Iter: 002/100 | Train Loss: 0.00161564\n","Iter: 003/100 | Train Loss: 0.00145305\n","Iter: 004/100 | Train Loss: 0.00153516\n","Adjusting Layer 1, Kernel Nodes: 697, Adptive Nodes:103\n","Iter: 005/100 | Train Loss: 0.00166537\n","Adjusting Layer 1, Kernel Nodes: 687, Adptive Nodes:113\n","Iter: 006/100 | Train Loss: 0.00166056\n","Iter: 007/100 | Train Loss: 0.00147191\n","Iter: 008/100 | Train Loss: 0.00124646\n","Iter: 009/100 | Train Loss: 0.00115355\n","Iter: 010/100 | Train Loss: 0.00118165\n","Adjusting Layer 1, Kernel Nodes: 512, Adptive Nodes:288\n","Iter: 011/100 | Train Loss: 0.00119462\n","Adjusting Layer 1, Kernel Nodes: 492, Adptive Nodes:308\n","Iter: 012/100 | Train Loss: 0.00109827\n","Iter: 013/100 | Train Loss: 0.00094841\n","Iter: 014/100 | Train Loss: 0.00086070\n","Iter: 015/100 | Train Loss: 0.00084996\n","Iter: 016/100 | Train Loss: 0.00084236\n","Iter: 017/100 | Train Loss: 0.00078572\n","Iter: 018/100 | Train Loss: 0.00070410\n","Iter: 019/100 | Train Loss: 0.00064848\n","Iter: 020/100 | Train Loss: 0.00063076\n","Iter: 021/100 | Train Loss: 0.00062163\n","Iter: 022/100 | Train Loss: 0.00059510\n","Iter: 023/100 | Train Loss: 0.00055486\n","Iter: 024/100 | Train Loss: 0.00052080\n","Iter: 025/100 | Train Loss: 0.00050262\n","Iter: 026/100 | Train Loss: 0.00049248\n","Iter: 027/100 | Train Loss: 0.00047825\n","Iter: 028/100 | Train Loss: 0.00045709\n","Iter: 029/100 | Train Loss: 0.00043502\n","Iter: 030/100 | Train Loss: 0.00041819\n","Iter: 031/100 | Train Loss: 0.00040671\n","Iter: 032/100 | Train Loss: 0.00039595\n","Iter: 033/100 | Train Loss: 0.00038236\n","Iter: 034/100 | Train Loss: 0.00036674\n","Iter: 035/100 | Train Loss: 0.00035247\n","Iter: 036/100 | Train Loss: 0.00034122\n","Iter: 037/100 | Train Loss: 0.00033174\n","Iter: 038/100 | Train Loss: 0.00032157\n","Iter: 039/100 | Train Loss: 0.00031026\n","Iter: 040/100 | Train Loss: 0.00029913\n","Iter: 041/100 | Train Loss: 0.00028927\n","Iter: 042/100 | Train Loss: 0.00028060\n","Iter: 043/100 | Train Loss: 0.00027235\n","Iter: 044/100 | Train Loss: 0.00026410\n","Iter: 045/100 | Train Loss: 0.00025584\n","Iter: 046/100 | Train Loss: 0.00024779\n","Iter: 047/100 | Train Loss: 0.00024028\n","Iter: 048/100 | Train Loss: 0.00023349\n","Iter: 049/100 | Train Loss: 0.00022710\n","Iter: 050/100 | Train Loss: 0.00022073\n","Iter: 051/100 | Train Loss: 0.00021425\n","Iter: 052/100 | Train Loss: 0.00020795\n","Iter: 053/100 | Train Loss: 0.00020226\n","Iter: 054/100 | Train Loss: 0.00019712\n","Iter: 055/100 | Train Loss: 0.00019208\n","Iter: 056/100 | Train Loss: 0.00018689\n","Iter: 057/100 | Train Loss: 0.00018174\n","Iter: 058/100 | Train Loss: 0.00017696\n","Iter: 059/100 | Train Loss: 0.00017262\n","Iter: 060/100 | Train Loss: 0.00016845\n","Iter: 061/100 | Train Loss: 0.00016418\n","Iter: 062/100 | Train Loss: 0.00015988\n","Iter: 063/100 | Train Loss: 0.00015583\n","Iter: 064/100 | Train Loss: 0.00015210\n","Iter: 065/100 | Train Loss: 0.00014851\n","Iter: 066/100 | Train Loss: 0.00014492\n","Iter: 067/100 | Train Loss: 0.00014131\n","Iter: 068/100 | Train Loss: 0.00013783\n","Iter: 069/100 | Train Loss: 0.00013454\n","Iter: 070/100 | Train Loss: 0.00013139\n","Iter: 071/100 | Train Loss: 0.00012830\n","Iter: 072/100 | Train Loss: 0.00012524\n","Iter: 073/100 | Train Loss: 0.00012226\n","Iter: 074/100 | Train Loss: 0.00011939\n","Iter: 075/100 | Train Loss: 0.00011666\n","Iter: 076/100 | Train Loss: 0.00011400\n","Iter: 077/100 | Train Loss: 0.00011138\n","Iter: 078/100 | Train Loss: 0.00010880\n","Iter: 079/100 | Train Loss: 0.00010631\n","Iter: 080/100 | Train Loss: 0.00010393\n","Iter: 081/100 | Train Loss: 0.00010164\n","Iter: 082/100 | Train Loss: 0.00009937\n","Iter: 083/100 | Train Loss: 0.00009714\n","Iter: 084/100 | Train Loss: 0.00009497\n","Iter: 085/100 | Train Loss: 0.00009290\n","Iter: 086/100 | Train Loss: 0.00009088\n","Iter: 087/100 | Train Loss: 0.00008889\n","Iter: 088/100 | Train Loss: 0.00008695\n","Iter: 089/100 | Train Loss: 0.00008506\n","Iter: 090/100 | Train Loss: 0.00008324\n","Iter: 091/100 | Train Loss: 0.00008145\n","Iter: 092/100 | Train Loss: 0.00007971\n","Iter: 093/100 | Train Loss: 0.00007801\n","Iter: 094/100 | Train Loss: 0.00007636\n","Iter: 095/100 | Train Loss: 0.00007474\n","Iter: 096/100 | Train Loss: 0.00007317\n","Iter: 097/100 | Train Loss: 0.00007164\n","Iter: 098/100 | Train Loss: 0.00007014\n","Iter: 099/100 | Train Loss: 0.00006867\n","\n","Iter: 099/100 | Test Loss: 0.00104784 | Test acc: 63.9900\n","scale:1.250000,therd:0.800000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00239373\n","Iter: 001/100 | Train Loss: 0.00200644\n","Iter: 002/100 | Train Loss: 0.00161980\n","Iter: 003/100 | Train Loss: 0.00145269\n","Iter: 004/100 | Train Loss: 0.00152350\n","Adjusting Layer 1, Kernel Nodes: 688, Adptive Nodes:112\n","Iter: 005/100 | Train Loss: 0.00165485\n","Adjusting Layer 1, Kernel Nodes: 773, Adptive Nodes:27\n","Iter: 006/100 | Train Loss: 0.00165774\n","Adjusting Layer 1, Kernel Nodes: 788, Adptive Nodes:12\n","Iter: 007/100 | Train Loss: 0.00144287\n","Iter: 008/100 | Train Loss: 0.00123910\n","Iter: 009/100 | Train Loss: 0.00115465\n","Iter: 010/100 | Train Loss: 0.00117588\n","Adjusting Layer 1, Kernel Nodes: 335, Adptive Nodes:465\n","Iter: 011/100 | Train Loss: 0.00114255\n","Iter: 012/100 | Train Loss: 0.00106410\n","Iter: 013/100 | Train Loss: 0.00095043\n","Iter: 014/100 | Train Loss: 0.00086921\n","Iter: 015/100 | Train Loss: 0.00084104\n","Iter: 016/100 | Train Loss: 0.00082903\n","Iter: 017/100 | Train Loss: 0.00079317\n","Iter: 018/100 | Train Loss: 0.00073376\n","Iter: 019/100 | Train Loss: 0.00067947\n","Iter: 020/100 | Train Loss: 0.00064830\n","Iter: 021/100 | Train Loss: 0.00063257\n","Iter: 022/100 | Train Loss: 0.00061448\n","Iter: 023/100 | Train Loss: 0.00058717\n","Iter: 024/100 | Train Loss: 0.00055724\n","Iter: 025/100 | Train Loss: 0.00053293\n","Iter: 026/100 | Train Loss: 0.00051563\n","Iter: 027/100 | Train Loss: 0.00050134\n","Iter: 028/100 | Train Loss: 0.00048651\n","Iter: 029/100 | Train Loss: 0.00047038\n","Iter: 030/100 | Train Loss: 0.00045386\n","Iter: 031/100 | Train Loss: 0.00043811\n","Iter: 032/100 | Train Loss: 0.00042425\n","Iter: 033/100 | Train Loss: 0.00041261\n","Iter: 034/100 | Train Loss: 0.00040206\n","Iter: 035/100 | Train Loss: 0.00039077\n","Iter: 036/100 | Train Loss: 0.00037822\n","Iter: 037/100 | Train Loss: 0.00036586\n","Iter: 038/100 | Train Loss: 0.00035538\n","Iter: 039/100 | Train Loss: 0.00034669\n","Iter: 040/100 | Train Loss: 0.00033817\n","Iter: 041/100 | Train Loss: 0.00032867\n","Iter: 042/100 | Train Loss: 0.00031872\n","Iter: 043/100 | Train Loss: 0.00030963\n","Iter: 044/100 | Train Loss: 0.00030188\n","Iter: 045/100 | Train Loss: 0.00029472\n","Iter: 046/100 | Train Loss: 0.00028728\n","Iter: 047/100 | Train Loss: 0.00027949\n","Iter: 048/100 | Train Loss: 0.00027197\n","Iter: 049/100 | Train Loss: 0.00026512\n","Iter: 050/100 | Train Loss: 0.00025878\n","Iter: 051/100 | Train Loss: 0.00025257\n","Iter: 052/100 | Train Loss: 0.00024634\n","Iter: 053/100 | Train Loss: 0.00024024\n","Iter: 054/100 | Train Loss: 0.00023448\n","Iter: 055/100 | Train Loss: 0.00022903\n","Iter: 056/100 | Train Loss: 0.00022378\n","Iter: 057/100 | Train Loss: 0.00021864\n","Iter: 058/100 | Train Loss: 0.00021367\n","Iter: 059/100 | Train Loss: 0.00020890\n","Iter: 060/100 | Train Loss: 0.00020426\n","Iter: 061/100 | Train Loss: 0.00019971\n","Iter: 062/100 | Train Loss: 0.00019530\n","Iter: 063/100 | Train Loss: 0.00019114\n","Iter: 064/100 | Train Loss: 0.00018713\n","Iter: 065/100 | Train Loss: 0.00018317\n","Iter: 066/100 | Train Loss: 0.00017924\n","Iter: 067/100 | Train Loss: 0.00017542\n","Iter: 068/100 | Train Loss: 0.00017179\n","Iter: 069/100 | Train Loss: 0.00016831\n","Iter: 070/100 | Train Loss: 0.00016489\n","Iter: 071/100 | Train Loss: 0.00016148\n","Iter: 072/100 | Train Loss: 0.00015815\n","Iter: 073/100 | Train Loss: 0.00015497\n","Iter: 074/100 | Train Loss: 0.00015191\n","Iter: 075/100 | Train Loss: 0.00014891\n","Iter: 076/100 | Train Loss: 0.00014593\n","Iter: 077/100 | Train Loss: 0.00014301\n","Iter: 078/100 | Train Loss: 0.00014019\n","Iter: 079/100 | Train Loss: 0.00013747\n","Iter: 080/100 | Train Loss: 0.00013482\n","Iter: 081/100 | Train Loss: 0.00013221\n","Iter: 082/100 | Train Loss: 0.00012964\n","Iter: 083/100 | Train Loss: 0.00012715\n","Iter: 084/100 | Train Loss: 0.00012473\n","Iter: 085/100 | Train Loss: 0.00012238\n","Iter: 086/100 | Train Loss: 0.00012006\n","Iter: 087/100 | Train Loss: 0.00011780\n","Iter: 088/100 | Train Loss: 0.00011559\n","Iter: 089/100 | Train Loss: 0.00011344\n","Iter: 090/100 | Train Loss: 0.00011133\n","Iter: 091/100 | Train Loss: 0.00010927\n","Iter: 092/100 | Train Loss: 0.00010726\n","Iter: 093/100 | Train Loss: 0.00010529\n","Iter: 094/100 | Train Loss: 0.00010337\n","Iter: 095/100 | Train Loss: 0.00010148\n","Iter: 096/100 | Train Loss: 0.00009964\n","Iter: 097/100 | Train Loss: 0.00009784\n","Iter: 098/100 | Train Loss: 0.00009609\n","Iter: 099/100 | Train Loss: 0.00009436\n","\n","Iter: 099/100 | Test Loss: 0.00103711 | Test acc: 64.7900\n","scale:1.250000,therd:0.850000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00238234\n","Iter: 001/100 | Train Loss: 0.00199876\n","Iter: 002/100 | Train Loss: 0.00161780\n","Iter: 003/100 | Train Loss: 0.00145133\n","Iter: 004/100 | Train Loss: 0.00151826\n","Adjusting Layer 1, Kernel Nodes: 694, Adptive Nodes:106\n","Iter: 005/100 | Train Loss: 0.00164690\n","Adjusting Layer 1, Kernel Nodes: 704, Adptive Nodes:96\n","Iter: 006/100 | Train Loss: 0.00168266\n","Adjusting Layer 1, Kernel Nodes: 673, Adptive Nodes:127\n","Iter: 007/100 | Train Loss: 0.00154924\n","Iter: 008/100 | Train Loss: 0.00135129\n","Iter: 009/100 | Train Loss: 0.00119757\n","Iter: 010/100 | Train Loss: 0.00114968\n","Iter: 011/100 | Train Loss: 0.00116814\n","Adjusting Layer 1, Kernel Nodes: 549, Adptive Nodes:251\n","Iter: 012/100 | Train Loss: 0.00117446\n","Adjusting Layer 1, Kernel Nodes: 599, Adptive Nodes:201\n","Iter: 013/100 | Train Loss: 0.00112039\n","Iter: 014/100 | Train Loss: 0.00099979\n","Iter: 015/100 | Train Loss: 0.00090411\n","Iter: 016/100 | Train Loss: 0.00086671\n","Iter: 017/100 | Train Loss: 0.00086173\n","Iter: 018/100 | Train Loss: 0.00084367\n","Iter: 019/100 | Train Loss: 0.00079484\n","Iter: 020/100 | Train Loss: 0.00073347\n","Iter: 021/100 | Train Loss: 0.00068628\n","Iter: 022/100 | Train Loss: 0.00066148\n","Iter: 023/100 | Train Loss: 0.00064709\n","Iter: 024/100 | Train Loss: 0.00062875\n","Iter: 025/100 | Train Loss: 0.00060252\n","Iter: 026/100 | Train Loss: 0.00057381\n","Iter: 027/100 | Train Loss: 0.00054895\n","Iter: 028/100 | Train Loss: 0.00052981\n","Iter: 029/100 | Train Loss: 0.00051444\n","Iter: 030/100 | Train Loss: 0.00050046\n","Iter: 031/100 | Train Loss: 0.00048644\n","Iter: 032/100 | Train Loss: 0.00047179\n","Iter: 033/100 | Train Loss: 0.00045621\n","Iter: 034/100 | Train Loss: 0.00044085\n","Iter: 035/100 | Train Loss: 0.00042755\n","Iter: 036/100 | Train Loss: 0.00041714\n","Iter: 037/100 | Train Loss: 0.00040824\n","Iter: 038/100 | Train Loss: 0.00039855\n","Iter: 039/100 | Train Loss: 0.00038709\n","Iter: 040/100 | Train Loss: 0.00037513\n","Iter: 041/100 | Train Loss: 0.00036461\n","Iter: 042/100 | Train Loss: 0.00035630\n","Iter: 043/100 | Train Loss: 0.00034909\n","Iter: 044/100 | Train Loss: 0.00034132\n","Iter: 045/100 | Train Loss: 0.00033245\n","Iter: 046/100 | Train Loss: 0.00032327\n","Iter: 047/100 | Train Loss: 0.00031498\n","Iter: 048/100 | Train Loss: 0.00030799\n","Iter: 049/100 | Train Loss: 0.00030173\n","Iter: 050/100 | Train Loss: 0.00029527\n","Iter: 051/100 | Train Loss: 0.00028829\n","Iter: 052/100 | Train Loss: 0.00028119\n","Iter: 053/100 | Train Loss: 0.00027456\n","Iter: 054/100 | Train Loss: 0.00026863\n","Iter: 055/100 | Train Loss: 0.00026315\n","Iter: 056/100 | Train Loss: 0.00025770\n","Iter: 057/100 | Train Loss: 0.00025210\n","Iter: 058/100 | Train Loss: 0.00024647\n","Iter: 059/100 | Train Loss: 0.00024106\n","Iter: 060/100 | Train Loss: 0.00023603\n","Iter: 061/100 | Train Loss: 0.00023132\n","Iter: 062/100 | Train Loss: 0.00022671\n","Iter: 063/100 | Train Loss: 0.00022209\n","Iter: 064/100 | Train Loss: 0.00021748\n","Iter: 065/100 | Train Loss: 0.00021298\n","Iter: 066/100 | Train Loss: 0.00020872\n","Iter: 067/100 | Train Loss: 0.00020470\n","Iter: 068/100 | Train Loss: 0.00020081\n","Iter: 069/100 | Train Loss: 0.00019693\n","Iter: 070/100 | Train Loss: 0.00019308\n","Iter: 071/100 | Train Loss: 0.00018931\n","Iter: 072/100 | Train Loss: 0.00018570\n","Iter: 073/100 | Train Loss: 0.00018224\n","Iter: 074/100 | Train Loss: 0.00017888\n","Iter: 075/100 | Train Loss: 0.00017558\n","Iter: 076/100 | Train Loss: 0.00017229\n","Iter: 077/100 | Train Loss: 0.00016907\n","Iter: 078/100 | Train Loss: 0.00016596\n","Iter: 079/100 | Train Loss: 0.00016295\n","Iter: 080/100 | Train Loss: 0.00016002\n","Iter: 081/100 | Train Loss: 0.00015714\n","Iter: 082/100 | Train Loss: 0.00015430\n","Iter: 083/100 | Train Loss: 0.00015153\n","Iter: 084/100 | Train Loss: 0.00014882\n","Iter: 085/100 | Train Loss: 0.00014619\n","Iter: 086/100 | Train Loss: 0.00014362\n","Iter: 087/100 | Train Loss: 0.00014110\n","Iter: 088/100 | Train Loss: 0.00013863\n","Iter: 089/100 | Train Loss: 0.00013621\n","Iter: 090/100 | Train Loss: 0.00013383\n","Iter: 091/100 | Train Loss: 0.00013151\n","Iter: 092/100 | Train Loss: 0.00012925\n","Iter: 093/100 | Train Loss: 0.00012704\n","Iter: 094/100 | Train Loss: 0.00012487\n","Iter: 095/100 | Train Loss: 0.00012273\n","Iter: 096/100 | Train Loss: 0.00012064\n","Iter: 097/100 | Train Loss: 0.00011860\n","Iter: 098/100 | Train Loss: 0.00011660\n","Iter: 099/100 | Train Loss: 0.00011465\n","\n","Iter: 099/100 | Test Loss: 0.00102440 | Test acc: 64.8500\n","scale:1.250000,therd:0.900000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00237006\n","Iter: 001/100 | Train Loss: 0.00198675\n","Iter: 002/100 | Train Loss: 0.00161070\n","Iter: 003/100 | Train Loss: 0.00144916\n","Iter: 004/100 | Train Loss: 0.00151784\n","Adjusting Layer 1, Kernel Nodes: 701, Adptive Nodes:99\n","Iter: 005/100 | Train Loss: 0.00164830\n","Adjusting Layer 1, Kernel Nodes: 697, Adptive Nodes:103\n","Iter: 006/100 | Train Loss: 0.00169236\n","Adjusting Layer 1, Kernel Nodes: 684, Adptive Nodes:116\n","Iter: 007/100 | Train Loss: 0.00158467\n","Iter: 008/100 | Train Loss: 0.00140672\n","Iter: 009/100 | Train Loss: 0.00123836\n","Iter: 010/100 | Train Loss: 0.00115231\n","Iter: 011/100 | Train Loss: 0.00114627\n","Iter: 012/100 | Train Loss: 0.00116672\n","Adjusting Layer 1, Kernel Nodes: 619, Adptive Nodes:181\n","Iter: 013/100 | Train Loss: 0.00116894\n","Adjusting Layer 1, Kernel Nodes: 660, Adptive Nodes:140\n","Iter: 014/100 | Train Loss: 0.00111485\n","Iter: 015/100 | Train Loss: 0.00100853\n","Iter: 016/100 | Train Loss: 0.00092181\n","Iter: 017/100 | Train Loss: 0.00088063\n","Iter: 018/100 | Train Loss: 0.00087264\n","Iter: 019/100 | Train Loss: 0.00086624\n","Iter: 020/100 | Train Loss: 0.00083910\n","Iter: 021/100 | Train Loss: 0.00079210\n","Iter: 022/100 | Train Loss: 0.00074181\n","Iter: 023/100 | Train Loss: 0.00070370\n","Iter: 024/100 | Train Loss: 0.00068111\n","Iter: 025/100 | Train Loss: 0.00066699\n","Iter: 026/100 | Train Loss: 0.00065189\n","Iter: 027/100 | Train Loss: 0.00063133\n","Iter: 028/100 | Train Loss: 0.00060709\n","Iter: 029/100 | Train Loss: 0.00058363\n","Iter: 030/100 | Train Loss: 0.00056387\n","Iter: 031/100 | Train Loss: 0.00054789\n","Iter: 032/100 | Train Loss: 0.00053417\n","Iter: 033/100 | Train Loss: 0.00052123\n","Iter: 034/100 | Train Loss: 0.00050839\n","Iter: 035/100 | Train Loss: 0.00049560\n","Iter: 036/100 | Train Loss: 0.00048276\n","Iter: 037/100 | Train Loss: 0.00046984\n","Iter: 038/100 | Train Loss: 0.00045732\n","Iter: 039/100 | Train Loss: 0.00044609\n","Iter: 040/100 | Train Loss: 0.00043658\n","Iter: 041/100 | Train Loss: 0.00042828\n","Iter: 042/100 | Train Loss: 0.00041995\n","Iter: 043/100 | Train Loss: 0.00041073\n","Iter: 044/100 | Train Loss: 0.00040080\n","Iter: 045/100 | Train Loss: 0.00039124\n","Iter: 046/100 | Train Loss: 0.00038292\n","Iter: 047/100 | Train Loss: 0.00037594\n","Iter: 048/100 | Train Loss: 0.00036955\n","Iter: 049/100 | Train Loss: 0.00036286\n","Iter: 050/100 | Train Loss: 0.00035550\n","Iter: 051/100 | Train Loss: 0.00034790\n","Iter: 052/100 | Train Loss: 0.00034072\n","Iter: 053/100 | Train Loss: 0.00033437\n","Iter: 054/100 | Train Loss: 0.00032868\n","Iter: 055/100 | Train Loss: 0.00032315\n","Iter: 056/100 | Train Loss: 0.00031740\n","Iter: 057/100 | Train Loss: 0.00031140\n","Iter: 058/100 | Train Loss: 0.00030543\n","Iter: 059/100 | Train Loss: 0.00029982\n","Iter: 060/100 | Train Loss: 0.00029463\n","Iter: 061/100 | Train Loss: 0.00028973\n","Iter: 062/100 | Train Loss: 0.00028490\n","Iter: 063/100 | Train Loss: 0.00028002\n","Iter: 064/100 | Train Loss: 0.00027511\n","Iter: 065/100 | Train Loss: 0.00027030\n","Iter: 066/100 | Train Loss: 0.00026568\n","Iter: 067/100 | Train Loss: 0.00026128\n","Iter: 068/100 | Train Loss: 0.00025703\n","Iter: 069/100 | Train Loss: 0.00025285\n","Iter: 070/100 | Train Loss: 0.00024869\n","Iter: 071/100 | Train Loss: 0.00024457\n","Iter: 072/100 | Train Loss: 0.00024054\n","Iter: 073/100 | Train Loss: 0.00023663\n","Iter: 074/100 | Train Loss: 0.00023286\n","Iter: 075/100 | Train Loss: 0.00022919\n","Iter: 076/100 | Train Loss: 0.00022559\n","Iter: 077/100 | Train Loss: 0.00022205\n","Iter: 078/100 | Train Loss: 0.00021856\n","Iter: 079/100 | Train Loss: 0.00021513\n","Iter: 080/100 | Train Loss: 0.00021178\n","Iter: 081/100 | Train Loss: 0.00020852\n","Iter: 082/100 | Train Loss: 0.00020534\n","Iter: 083/100 | Train Loss: 0.00020221\n","Iter: 084/100 | Train Loss: 0.00019913\n","Iter: 085/100 | Train Loss: 0.00019609\n","Iter: 086/100 | Train Loss: 0.00019312\n","Iter: 087/100 | Train Loss: 0.00019021\n","Iter: 088/100 | Train Loss: 0.00018736\n","Iter: 089/100 | Train Loss: 0.00018457\n","Iter: 090/100 | Train Loss: 0.00018183\n","Iter: 091/100 | Train Loss: 0.00017913\n","Iter: 092/100 | Train Loss: 0.00017648\n","Iter: 093/100 | Train Loss: 0.00017388\n","Iter: 094/100 | Train Loss: 0.00017134\n","Iter: 095/100 | Train Loss: 0.00016886\n","Iter: 096/100 | Train Loss: 0.00016642\n","Iter: 097/100 | Train Loss: 0.00016402\n","Iter: 098/100 | Train Loss: 0.00016166\n","Iter: 099/100 | Train Loss: 0.00015934\n","\n","Iter: 099/100 | Test Loss: 0.00101032 | Test acc: 65.0700\n","scale:1.250000,therd:0.950000\n","Adjusting Layer 1, Kernel Nodes: 717, Adptive Nodes:83\n","Iter: 000/100 | Train Loss: 0.00235685\n","Iter: 001/100 | Train Loss: 0.00197096\n","Iter: 002/100 | Train Loss: 0.00159947\n","Iter: 003/100 | Train Loss: 0.00144667\n","Iter: 004/100 | Train Loss: 0.00152178\n","Adjusting Layer 1, Kernel Nodes: 706, Adptive Nodes:94\n","Iter: 005/100 | Train Loss: 0.00166023\n","Adjusting Layer 1, Kernel Nodes: 702, Adptive Nodes:98\n","Iter: 006/100 | Train Loss: 0.00170834\n","Adjusting Layer 1, Kernel Nodes: 706, Adptive Nodes:94\n","Iter: 007/100 | Train Loss: 0.00160825\n","Iter: 008/100 | Train Loss: 0.00143014\n","Iter: 009/100 | Train Loss: 0.00125271\n","Iter: 010/100 | Train Loss: 0.00115018\n","Iter: 011/100 | Train Loss: 0.00113152\n","Iter: 012/100 | Train Loss: 0.00115405\n","Adjusting Layer 1, Kernel Nodes: 653, Adptive Nodes:147\n","Iter: 013/100 | Train Loss: 0.00117292\n","Adjusting Layer 1, Kernel Nodes: 666, Adptive Nodes:134\n","Iter: 014/100 | Train Loss: 0.00114368\n","Iter: 015/100 | Train Loss: 0.00106001\n","Iter: 016/100 | Train Loss: 0.00096763\n","Iter: 017/100 | Train Loss: 0.00089886\n","Iter: 018/100 | Train Loss: 0.00086545\n","Iter: 019/100 | Train Loss: 0.00085689\n","Iter: 020/100 | Train Loss: 0.00085252\n","Iter: 021/100 | Train Loss: 0.00083543\n","Iter: 022/100 | Train Loss: 0.00080200\n","Iter: 023/100 | Train Loss: 0.00075994\n","Iter: 024/100 | Train Loss: 0.00072057\n","Iter: 025/100 | Train Loss: 0.00069142\n","Iter: 026/100 | Train Loss: 0.00067293\n","Iter: 027/100 | Train Loss: 0.00066047\n","Iter: 028/100 | Train Loss: 0.00064823\n","Iter: 029/100 | Train Loss: 0.00063272\n","Iter: 030/100 | Train Loss: 0.00061372\n","Iter: 031/100 | Train Loss: 0.00059332\n","Iter: 032/100 | Train Loss: 0.00057423\n","Iter: 033/100 | Train Loss: 0.00055808\n","Iter: 034/100 | Train Loss: 0.00054498\n","Iter: 035/100 | Train Loss: 0.00053400\n","Iter: 036/100 | Train Loss: 0.00052375\n","Iter: 037/100 | Train Loss: 0.00051318\n","Iter: 038/100 | Train Loss: 0.00050193\n","Iter: 039/100 | Train Loss: 0.00049022\n","Iter: 040/100 | Train Loss: 0.00047859\n","Iter: 041/100 | Train Loss: 0.00046769\n","Iter: 042/100 | Train Loss: 0.00045793\n","Iter: 043/100 | Train Loss: 0.00044935\n","Iter: 044/100 | Train Loss: 0.00044158\n","Iter: 045/100 | Train Loss: 0.00043401\n","Iter: 046/100 | Train Loss: 0.00042618\n","Iter: 047/100 | Train Loss: 0.00041801\n","Iter: 048/100 | Train Loss: 0.00040974\n","Iter: 049/100 | Train Loss: 0.00040179\n","Iter: 050/100 | Train Loss: 0.00039454\n","Iter: 051/100 | Train Loss: 0.00038804\n","Iter: 052/100 | Train Loss: 0.00038203\n","Iter: 053/100 | Train Loss: 0.00037609\n","Iter: 054/100 | Train Loss: 0.00036990\n","Iter: 055/100 | Train Loss: 0.00036347\n","Iter: 056/100 | Train Loss: 0.00035706\n","Iter: 057/100 | Train Loss: 0.00035099\n","Iter: 058/100 | Train Loss: 0.00034543\n","Iter: 059/100 | Train Loss: 0.00034027\n","Iter: 060/100 | Train Loss: 0.00033528\n","Iter: 061/100 | Train Loss: 0.00033022\n","Iter: 062/100 | Train Loss: 0.00032504\n","Iter: 063/100 | Train Loss: 0.00031983\n","Iter: 064/100 | Train Loss: 0.00031476\n","Iter: 065/100 | Train Loss: 0.00030998\n","Iter: 066/100 | Train Loss: 0.00030548\n","Iter: 067/100 | Train Loss: 0.00030116\n","Iter: 068/100 | Train Loss: 0.00029688\n","Iter: 069/100 | Train Loss: 0.00029256\n","Iter: 070/100 | Train Loss: 0.00028821\n","Iter: 071/100 | Train Loss: 0.00028394\n","Iter: 072/100 | Train Loss: 0.00027984\n","Iter: 073/100 | Train Loss: 0.00027591\n","Iter: 074/100 | Train Loss: 0.00027209\n","Iter: 075/100 | Train Loss: 0.00026832\n","Iter: 076/100 | Train Loss: 0.00026456\n","Iter: 077/100 | Train Loss: 0.00026083\n","Iter: 078/100 | Train Loss: 0.00025715\n","Iter: 079/100 | Train Loss: 0.00025358\n","Iter: 080/100 | Train Loss: 0.00025010\n","Iter: 081/100 | Train Loss: 0.00024670\n","Iter: 082/100 | Train Loss: 0.00024335\n","Iter: 083/100 | Train Loss: 0.00024004\n","Iter: 084/100 | Train Loss: 0.00023677\n","Iter: 085/100 | Train Loss: 0.00023356\n","Iter: 086/100 | Train Loss: 0.00023041\n","Iter: 087/100 | Train Loss: 0.00022733\n","Iter: 088/100 | Train Loss: 0.00022431\n","Iter: 089/100 | Train Loss: 0.00022134\n","Iter: 090/100 | Train Loss: 0.00021842\n","Iter: 091/100 | Train Loss: 0.00021555\n","Iter: 092/100 | Train Loss: 0.00021272\n","Iter: 093/100 | Train Loss: 0.00020994\n","Iter: 094/100 | Train Loss: 0.00020721\n","Iter: 095/100 | Train Loss: 0.00020453\n","Iter: 096/100 | Train Loss: 0.00020190\n","Iter: 097/100 | Train Loss: 0.00019930\n","Iter: 098/100 | Train Loss: 0.00019675\n","Iter: 099/100 | Train Loss: 0.00019425\n","\n","Iter: 099/100 | Test Loss: 0.00100621 | Test acc: 64.8900\n","scale:1.250000,therd:1.000000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 000/100 | Train Loss: 0.00234266\n","Iter: 001/100 | Train Loss: 0.00195195\n","Iter: 002/100 | Train Loss: 0.00158488\n","Iter: 003/100 | Train Loss: 0.00144418\n","Iter: 004/100 | Train Loss: 0.00152951\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 005/100 | Train Loss: 0.00168188\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 006/100 | Train Loss: 0.00172915\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 007/100 | Train Loss: 0.00161204\n","Iter: 008/100 | Train Loss: 0.00140127\n","Iter: 009/100 | Train Loss: 0.00121521\n","Iter: 010/100 | Train Loss: 0.00112615\n","Iter: 011/100 | Train Loss: 0.00112766\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 012/100 | Train Loss: 0.00116207\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 013/100 | Train Loss: 0.00116916\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:0\n","Iter: 014/100 | Train Loss: 0.00112219\n","Iter: 015/100 | Train Loss: 0.00103408\n","Iter: 016/100 | Train Loss: 0.00093964\n","Iter: 017/100 | Train Loss: 0.00087037\n","Iter: 018/100 | Train Loss: 0.00083773\n","Iter: 019/100 | Train Loss: 0.00083168\n","Iter: 020/100 | Train Loss: 0.00083146\n","Iter: 021/100 | Train Loss: 0.00081917\n","Iter: 022/100 | Train Loss: 0.00078905\n","Iter: 023/100 | Train Loss: 0.00074710\n","Iter: 024/100 | Train Loss: 0.00070507\n","Iter: 025/100 | Train Loss: 0.00067276\n","Iter: 026/100 | Train Loss: 0.00065328\n","Iter: 027/100 | Train Loss: 0.00064306\n","Iter: 028/100 | Train Loss: 0.00063541\n","Iter: 029/100 | Train Loss: 0.00062421\n","Iter: 030/100 | Train Loss: 0.00060711\n","Iter: 031/100 | Train Loss: 0.00058564\n","Iter: 032/100 | Train Loss: 0.00056381\n","Iter: 033/100 | Train Loss: 0.00054543\n","Iter: 034/100 | Train Loss: 0.00053223\n","Iter: 035/100 | Train Loss: 0.00052340\n","Iter: 036/100 | Train Loss: 0.00051622\n","Iter: 037/100 | Train Loss: 0.00050792\n","Iter: 038/100 | Train Loss: 0.00049711\n","Iter: 039/100 | Train Loss: 0.00048431\n","Iter: 040/100 | Train Loss: 0.00047133\n","Iter: 041/100 | Train Loss: 0.00045991\n","Iter: 042/100 | Train Loss: 0.00045093\n","Iter: 043/100 | Train Loss: 0.00044396\n","Iter: 044/100 | Train Loss: 0.00043765\n","Iter: 045/100 | Train Loss: 0.00043077\n","Iter: 046/100 | Train Loss: 0.00042279\n","Iter: 047/100 | Train Loss: 0.00041407\n","Iter: 048/100 | Train Loss: 0.00040549\n","Iter: 049/100 | Train Loss: 0.00039778\n","Iter: 050/100 | Train Loss: 0.00039122\n","Iter: 051/100 | Train Loss: 0.00038546\n","Iter: 052/100 | Train Loss: 0.00037992\n","Iter: 053/100 | Train Loss: 0.00037407\n","Iter: 054/100 | Train Loss: 0.00036781\n","Iter: 055/100 | Train Loss: 0.00036137\n","Iter: 056/100 | Train Loss: 0.00035514\n","Iter: 057/100 | Train Loss: 0.00034940\n","Iter: 058/100 | Train Loss: 0.00034419\n","Iter: 059/100 | Train Loss: 0.00033931\n","Iter: 060/100 | Train Loss: 0.00033447\n","Iter: 061/100 | Train Loss: 0.00032948\n","Iter: 062/100 | Train Loss: 0.00032439\n","Iter: 063/100 | Train Loss: 0.00031931\n","Iter: 064/100 | Train Loss: 0.00031444\n","Iter: 065/100 | Train Loss: 0.00030986\n","Iter: 066/100 | Train Loss: 0.00030550\n","Iter: 067/100 | Train Loss: 0.00030127\n","Iter: 068/100 | Train Loss: 0.00029704\n","Iter: 069/100 | Train Loss: 0.00029279\n","Iter: 070/100 | Train Loss: 0.00028854\n","Iter: 071/100 | Train Loss: 0.00028438\n","Iter: 072/100 | Train Loss: 0.00028038\n","Iter: 073/100 | Train Loss: 0.00027652\n","Iter: 074/100 | Train Loss: 0.00027278\n","Iter: 075/100 | Train Loss: 0.00026908\n","Iter: 076/100 | Train Loss: 0.00026541\n","Iter: 077/100 | Train Loss: 0.00026176\n","Iter: 078/100 | Train Loss: 0.00025815\n","Iter: 079/100 | Train Loss: 0.00025463\n","Iter: 080/100 | Train Loss: 0.00025119\n","Iter: 081/100 | Train Loss: 0.00024786\n","Iter: 082/100 | Train Loss: 0.00024460\n","Iter: 083/100 | Train Loss: 0.00024140\n","Iter: 084/100 | Train Loss: 0.00023824\n","Iter: 085/100 | Train Loss: 0.00023512\n","Iter: 086/100 | Train Loss: 0.00023206\n","Iter: 087/100 | Train Loss: 0.00022906\n","Iter: 088/100 | Train Loss: 0.00022614\n","Iter: 089/100 | Train Loss: 0.00022328\n","Iter: 090/100 | Train Loss: 0.00022046\n","Iter: 091/100 | Train Loss: 0.00021769\n","Iter: 092/100 | Train Loss: 0.00021496\n","Iter: 093/100 | Train Loss: 0.00021227\n","Iter: 094/100 | Train Loss: 0.00020962\n","Iter: 095/100 | Train Loss: 0.00020701\n","Iter: 096/100 | Train Loss: 0.00020444\n","Iter: 097/100 | Train Loss: 0.00020191\n","Iter: 098/100 | Train Loss: 0.00019943\n","Iter: 099/100 | Train Loss: 0.00019697\n","\n","Iter: 099/100 | Test Loss: 0.00100522 | Test acc: 64.3000\n","scale:1.250000,therd:1.050000\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 000/100 | Train Loss: 0.00232881\n","Iter: 001/100 | Train Loss: 0.00193002\n","Iter: 002/100 | Train Loss: 0.00156647\n","Iter: 003/100 | Train Loss: 0.00144146\n","Iter: 004/100 | Train Loss: 0.00154150\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 005/100 | Train Loss: 0.00171625\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 006/100 | Train Loss: 0.00175119\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 007/100 | Train Loss: 0.00157535\n","Iter: 008/100 | Train Loss: 0.00130236\n","Iter: 009/100 | Train Loss: 0.00113877\n","Iter: 010/100 | Train Loss: 0.00112184\n","Iter: 011/100 | Train Loss: 0.00117590\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 012/100 | Train Loss: 0.00118716\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 013/100 | Train Loss: 0.00112095\n","Iter: 014/100 | Train Loss: 0.00101296\n","Iter: 015/100 | Train Loss: 0.00090179\n","Iter: 016/100 | Train Loss: 0.00083244\n","Iter: 017/100 | Train Loss: 0.00081256\n","Iter: 018/100 | Train Loss: 0.00081535\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 019/100 | Train Loss: 0.00081765\n","Adjusting Layer 1, Kernel Nodes: 0, Adptive Nodes:800\n","Iter: 020/100 | Train Loss: 0.00078731\n","Iter: 021/100 | Train Loss: 0.00072169\n","Iter: 022/100 | Train Loss: 0.00066552\n","Iter: 023/100 | Train Loss: 0.00063582\n","Iter: 024/100 | Train Loss: 0.00062831\n","Iter: 025/100 | Train Loss: 0.00062599\n","Iter: 026/100 | Train Loss: 0.00061348\n","Iter: 027/100 | Train Loss: 0.00058657\n","Iter: 028/100 | Train Loss: 0.00055258\n","Iter: 029/100 | Train Loss: 0.00052379\n","Iter: 030/100 | Train Loss: 0.00050754\n","Iter: 031/100 | Train Loss: 0.00050125\n","Iter: 032/100 | Train Loss: 0.00049591\n","Iter: 033/100 | Train Loss: 0.00048370\n","Iter: 034/100 | Train Loss: 0.00046443\n","Iter: 035/100 | Train Loss: 0.00044402\n","Iter: 036/100 | Train Loss: 0.00042868\n","Iter: 037/100 | Train Loss: 0.00041995\n","Iter: 038/100 | Train Loss: 0.00041443\n","Iter: 039/100 | Train Loss: 0.00040743\n","Iter: 040/100 | Train Loss: 0.00039640\n","Iter: 041/100 | Train Loss: 0.00038294\n","Iter: 042/100 | Train Loss: 0.00037059\n","Iter: 043/100 | Train Loss: 0.00036187\n","Iter: 044/100 | Train Loss: 0.00035617\n","Iter: 045/100 | Train Loss: 0.00035071\n","Iter: 046/100 | Train Loss: 0.00034318\n","Iter: 047/100 | Train Loss: 0.00033365\n","Iter: 048/100 | Train Loss: 0.00032425\n","Iter: 049/100 | Train Loss: 0.00031682\n","Iter: 050/100 | Train Loss: 0.00031137\n","Iter: 051/100 | Train Loss: 0.00030637\n","Iter: 052/100 | Train Loss: 0.00030034\n","Iter: 053/100 | Train Loss: 0.00029322\n","Iter: 054/100 | Train Loss: 0.00028608\n","Iter: 055/100 | Train Loss: 0.00027997\n","Iter: 056/100 | Train Loss: 0.00027491\n","Iter: 057/100 | Train Loss: 0.00027027\n","Iter: 058/100 | Train Loss: 0.00026540\n","Iter: 059/100 | Train Loss: 0.00026018\n","Iter: 060/100 | Train Loss: 0.00025490\n","Iter: 061/100 | Train Loss: 0.00024998\n","Iter: 062/100 | Train Loss: 0.00024562\n","Iter: 063/100 | Train Loss: 0.00024158\n","Iter: 064/100 | Train Loss: 0.00023755\n","Iter: 065/100 | Train Loss: 0.00023328\n","Iter: 066/100 | Train Loss: 0.00022888\n","Iter: 067/100 | Train Loss: 0.00022463\n","Iter: 068/100 | Train Loss: 0.00022074\n","Iter: 069/100 | Train Loss: 0.00021714\n","Iter: 070/100 | Train Loss: 0.00021357\n","Iter: 071/100 | Train Loss: 0.00020985\n","Iter: 072/100 | Train Loss: 0.00020606\n","Iter: 073/100 | Train Loss: 0.00020239\n","Iter: 074/100 | Train Loss: 0.00019898\n","Iter: 075/100 | Train Loss: 0.00019579\n","Iter: 076/100 | Train Loss: 0.00019265\n","Iter: 077/100 | Train Loss: 0.00018943\n","Iter: 078/100 | Train Loss: 0.00018620\n","Iter: 079/100 | Train Loss: 0.00018308\n","Iter: 080/100 | Train Loss: 0.00018015\n","Iter: 081/100 | Train Loss: 0.00017735\n","Iter: 082/100 | Train Loss: 0.00017459\n","Iter: 083/100 | Train Loss: 0.00017181\n","Iter: 084/100 | Train Loss: 0.00016904\n","Iter: 085/100 | Train Loss: 0.00016637\n","Iter: 086/100 | Train Loss: 0.00016381\n","Iter: 087/100 | Train Loss: 0.00016132\n","Iter: 088/100 | Train Loss: 0.00015884\n","Iter: 089/100 | Train Loss: 0.00015638\n","Iter: 090/100 | Train Loss: 0.00015397\n","Iter: 091/100 | Train Loss: 0.00015163\n","Iter: 092/100 | Train Loss: 0.00014935\n","Iter: 093/100 | Train Loss: 0.00014711\n","Iter: 094/100 | Train Loss: 0.00014490\n","Iter: 095/100 | Train Loss: 0.00014273\n","Iter: 096/100 | Train Loss: 0.00014059\n","Iter: 097/100 | Train Loss: 0.00013849\n","Iter: 098/100 | Train Loss: 0.00013643\n","Iter: 099/100 | Train Loss: 0.00013441\n","\n","Iter: 099/100 | Test Loss: 0.00099937 | Test acc: 64.8300\n","tensor(2.5316e-05) 1.05 0.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1LOeSR-DEK4B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597612203093,"user_tz":-120,"elapsed":121972,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"2e4b6892-1cd9-4c6a-f899-e0258582cf4e"},"source":["step_list_, loss_no_scale_no_bn_,train_loss = full_batch_train_(1.05,0.6,2,True,initialize = 'NTK', batchnorm = True, learning_rate = 0.1 * np.sqrt(784), ** shared_model_param_dict)\n"],"execution_count":200,"outputs":[{"output_type":"stream","text":["Adjusting Layer 1, Kernel Nodes: 721, Adptive Nodes:79\n","Iter: 000/100 | Train Loss: 0.00243025\n","Iter: 001/100 | Train Loss: 0.00197599\n","Iter: 002/100 | Train Loss: 0.00155922\n","Iter: 003/100 | Train Loss: 0.00146102\n","Iter: 004/100 | Train Loss: 0.00162715\n","Adjusting Layer 1, Kernel Nodes: 628, Adptive Nodes:172\n","Iter: 005/100 | Train Loss: 0.00175585\n","Adjusting Layer 1, Kernel Nodes: 493, Adptive Nodes:307\n","Iter: 006/100 | Train Loss: 0.00153182\n","Iter: 007/100 | Train Loss: 0.00123259\n","Iter: 008/100 | Train Loss: 0.00114627\n","Iter: 009/100 | Train Loss: 0.00117522\n","Adjusting Layer 1, Kernel Nodes: 414, Adptive Nodes:386\n","Iter: 010/100 | Train Loss: 0.00113716\n","Iter: 011/100 | Train Loss: 0.00098196\n","Iter: 012/100 | Train Loss: 0.00085145\n","Iter: 013/100 | Train Loss: 0.00082034\n","Iter: 014/100 | Train Loss: 0.00082536\n","Adjusting Layer 1, Kernel Nodes: 642, Adptive Nodes:158\n","Iter: 015/100 | Train Loss: 0.00072763\n","Iter: 016/100 | Train Loss: 0.00064108\n","Iter: 017/100 | Train Loss: 0.00060837\n","Iter: 018/100 | Train Loss: 0.00059662\n","Iter: 019/100 | Train Loss: 0.00055204\n","Iter: 020/100 | Train Loss: 0.00050223\n","Iter: 021/100 | Train Loss: 0.00048616\n","Iter: 022/100 | Train Loss: 0.00047770\n","Iter: 023/100 | Train Loss: 0.00044609\n","Iter: 024/100 | Train Loss: 0.00041293\n","Iter: 025/100 | Train Loss: 0.00040025\n","Iter: 026/100 | Train Loss: 0.00038907\n","Iter: 027/100 | Train Loss: 0.00036304\n","Iter: 028/100 | Train Loss: 0.00033790\n","Iter: 029/100 | Train Loss: 0.00032556\n","Iter: 030/100 | Train Loss: 0.00031330\n","Iter: 031/100 | Train Loss: 0.00029251\n","Iter: 032/100 | Train Loss: 0.00027361\n","Iter: 033/100 | Train Loss: 0.00026266\n","Iter: 034/100 | Train Loss: 0.00025156\n","Iter: 035/100 | Train Loss: 0.00023595\n","Iter: 036/100 | Train Loss: 0.00022220\n","Iter: 037/100 | Train Loss: 0.00021346\n","Iter: 038/100 | Train Loss: 0.00020481\n","Iter: 039/100 | Train Loss: 0.00019370\n","Iter: 040/100 | Train Loss: 0.00018382\n","Iter: 041/100 | Train Loss: 0.00017693\n","Iter: 042/100 | Train Loss: 0.00016999\n","Iter: 043/100 | Train Loss: 0.00016162\n","Iter: 044/100 | Train Loss: 0.00015406\n","Iter: 045/100 | Train Loss: 0.00014839\n","Iter: 046/100 | Train Loss: 0.00014288\n","Iter: 047/100 | Train Loss: 0.00013656\n","Iter: 048/100 | Train Loss: 0.00013072\n","Iter: 049/100 | Train Loss: 0.00012606\n","Iter: 050/100 | Train Loss: 0.00012150\n","Iter: 051/100 | Train Loss: 0.00011645\n","Iter: 052/100 | Train Loss: 0.00011175\n","Iter: 053/100 | Train Loss: 0.00010782\n","Iter: 054/100 | Train Loss: 0.00010397\n","Iter: 055/100 | Train Loss: 0.00009985\n","Iter: 056/100 | Train Loss: 0.00009599\n","Iter: 057/100 | Train Loss: 0.00009267\n","Iter: 058/100 | Train Loss: 0.00008944\n","Iter: 059/100 | Train Loss: 0.00008607\n","Iter: 060/100 | Train Loss: 0.00008291\n","Iter: 061/100 | Train Loss: 0.00008014\n","Iter: 062/100 | Train Loss: 0.00007744\n","Iter: 063/100 | Train Loss: 0.00007468\n","Iter: 064/100 | Train Loss: 0.00007209\n","Iter: 065/100 | Train Loss: 0.00006976\n","Iter: 066/100 | Train Loss: 0.00006748\n","Iter: 067/100 | Train Loss: 0.00006519\n","Iter: 068/100 | Train Loss: 0.00006302\n","Iter: 069/100 | Train Loss: 0.00006103\n","Iter: 070/100 | Train Loss: 0.00005908\n","Iter: 071/100 | Train Loss: 0.00005715\n","Iter: 072/100 | Train Loss: 0.00005532\n","Iter: 073/100 | Train Loss: 0.00005362\n","Iter: 074/100 | Train Loss: 0.00005197\n","Iter: 075/100 | Train Loss: 0.00005034\n","Iter: 076/100 | Train Loss: 0.00004879\n","Iter: 077/100 | Train Loss: 0.00004734\n","Iter: 078/100 | Train Loss: 0.00004592\n","Iter: 079/100 | Train Loss: 0.00004453\n","Iter: 080/100 | Train Loss: 0.00004321\n","Iter: 081/100 | Train Loss: 0.00004195\n","Iter: 082/100 | Train Loss: 0.00004073\n","Iter: 083/100 | Train Loss: 0.00003953\n","Iter: 084/100 | Train Loss: 0.00003839\n","Iter: 085/100 | Train Loss: 0.00003730\n","Iter: 086/100 | Train Loss: 0.00003624\n","Iter: 087/100 | Train Loss: 0.00003520\n","Iter: 088/100 | Train Loss: 0.00003421\n","Iter: 089/100 | Train Loss: 0.00003326\n","Iter: 090/100 | Train Loss: 0.00003233\n","Iter: 091/100 | Train Loss: 0.00003143\n","Iter: 092/100 | Train Loss: 0.00003057\n","Iter: 093/100 | Train Loss: 0.00002975\n","Iter: 094/100 | Train Loss: 0.00002894\n","Iter: 095/100 | Train Loss: 0.00002817\n","Iter: 096/100 | Train Loss: 0.00002742\n","Iter: 097/100 | Train Loss: 0.00002670\n","Iter: 098/100 | Train Loss: 0.00002600\n","Iter: 099/100 | Train Loss: 0.00002532\n","\n","Iter: 099/100 | Test Loss: 0.00110231 | Test acc: 63.0100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gOZR9R0JcT1T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597612081110,"user_tz":-120,"elapsed":120269,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"1b05852e-f9c7-482e-8c7d-30d899121602"},"source":["step_list,  loss_ntk_scale_bn,train_loss =  full_batch_train(initialize = 'NTK', batchnorm = True, learning_rate =  0.1 * np.sqrt(784), ** shared_model_param_dict)"],"execution_count":199,"outputs":[{"output_type":"stream","text":["Iter: 000/100 | Train Loss: 0.00240460\n","Iter: 001/100 | Train Loss: 0.00197716\n","Iter: 002/100 | Train Loss: 0.00157913\n","Iter: 003/100 | Train Loss: 0.00143947\n","Iter: 004/100 | Train Loss: 0.00155030\n","Iter: 005/100 | Train Loss: 0.00172613\n","Iter: 006/100 | Train Loss: 0.00177632\n","Iter: 007/100 | Train Loss: 0.00164352\n","Iter: 008/100 | Train Loss: 0.00141492\n","Iter: 009/100 | Train Loss: 0.00122395\n","Iter: 010/100 | Train Loss: 0.00114596\n","Iter: 011/100 | Train Loss: 0.00116446\n","Iter: 012/100 | Train Loss: 0.00120952\n","Iter: 013/100 | Train Loss: 0.00121559\n","Iter: 014/100 | Train Loss: 0.00115919\n","Iter: 015/100 | Train Loss: 0.00106156\n","Iter: 016/100 | Train Loss: 0.00096427\n","Iter: 017/100 | Train Loss: 0.00089941\n","Iter: 018/100 | Train Loss: 0.00087482\n","Iter: 019/100 | Train Loss: 0.00087514\n","Iter: 020/100 | Train Loss: 0.00087645\n","Iter: 021/100 | Train Loss: 0.00086111\n","Iter: 022/100 | Train Loss: 0.00082608\n","Iter: 023/100 | Train Loss: 0.00078047\n","Iter: 024/100 | Train Loss: 0.00073774\n","Iter: 025/100 | Train Loss: 0.00070754\n","Iter: 026/100 | Train Loss: 0.00069135\n","Iter: 027/100 | Train Loss: 0.00068368\n","Iter: 028/100 | Train Loss: 0.00067666\n","Iter: 029/100 | Train Loss: 0.00066414\n","Iter: 030/100 | Train Loss: 0.00064474\n","Iter: 031/100 | Train Loss: 0.00062134\n","Iter: 032/100 | Train Loss: 0.00059869\n","Iter: 033/100 | Train Loss: 0.00058070\n","Iter: 034/100 | Train Loss: 0.00056850\n","Iter: 035/100 | Train Loss: 0.00056033\n","Iter: 036/100 | Train Loss: 0.00055288\n","Iter: 037/100 | Train Loss: 0.00054345\n","Iter: 038/100 | Train Loss: 0.00053118\n","Iter: 039/100 | Train Loss: 0.00051721\n","Iter: 040/100 | Train Loss: 0.00050357\n","Iter: 041/100 | Train Loss: 0.00049208\n","Iter: 042/100 | Train Loss: 0.00048331\n","Iter: 043/100 | Train Loss: 0.00047638\n","Iter: 044/100 | Train Loss: 0.00046978\n","Iter: 045/100 | Train Loss: 0.00046227\n","Iter: 046/100 | Train Loss: 0.00045348\n","Iter: 047/100 | Train Loss: 0.00044403\n","Iter: 048/100 | Train Loss: 0.00043498\n","Iter: 049/100 | Train Loss: 0.00042717\n","Iter: 050/100 | Train Loss: 0.00042071\n","Iter: 051/100 | Train Loss: 0.00041501\n","Iter: 052/100 | Train Loss: 0.00040927\n","Iter: 053/100 | Train Loss: 0.00040302\n","Iter: 054/100 | Train Loss: 0.00039626\n","Iter: 055/100 | Train Loss: 0.00038943\n","Iter: 056/100 | Train Loss: 0.00038304\n","Iter: 057/100 | Train Loss: 0.00037734\n","Iter: 058/100 | Train Loss: 0.00037223\n","Iter: 059/100 | Train Loss: 0.00036739\n","Iter: 060/100 | Train Loss: 0.00036244\n","Iter: 061/100 | Train Loss: 0.00035724\n","Iter: 062/100 | Train Loss: 0.00035193\n","Iter: 063/100 | Train Loss: 0.00034671\n","Iter: 064/100 | Train Loss: 0.00034182\n","Iter: 065/100 | Train Loss: 0.00033729\n","Iter: 066/100 | Train Loss: 0.00033297\n","Iter: 067/100 | Train Loss: 0.00032868\n","Iter: 068/100 | Train Loss: 0.00032434\n","Iter: 069/100 | Train Loss: 0.00031995\n","Iter: 070/100 | Train Loss: 0.00031562\n","Iter: 071/100 | Train Loss: 0.00031142\n","Iter: 072/100 | Train Loss: 0.00030741\n","Iter: 073/100 | Train Loss: 0.00030356\n","Iter: 074/100 | Train Loss: 0.00029979\n","Iter: 075/100 | Train Loss: 0.00029605\n","Iter: 076/100 | Train Loss: 0.00029232\n","Iter: 077/100 | Train Loss: 0.00028862\n","Iter: 078/100 | Train Loss: 0.00028500\n","Iter: 079/100 | Train Loss: 0.00028147\n","Iter: 080/100 | Train Loss: 0.00027804\n","Iter: 081/100 | Train Loss: 0.00027468\n","Iter: 082/100 | Train Loss: 0.00027138\n","Iter: 083/100 | Train Loss: 0.00026811\n","Iter: 084/100 | Train Loss: 0.00026489\n","Iter: 085/100 | Train Loss: 0.00026171\n","Iter: 086/100 | Train Loss: 0.00025859\n","Iter: 087/100 | Train Loss: 0.00025553\n","Iter: 088/100 | Train Loss: 0.00025253\n","Iter: 089/100 | Train Loss: 0.00024958\n","Iter: 090/100 | Train Loss: 0.00024666\n","Iter: 091/100 | Train Loss: 0.00024377\n","Iter: 092/100 | Train Loss: 0.00024092\n","Iter: 093/100 | Train Loss: 0.00023813\n","Iter: 094/100 | Train Loss: 0.00023540\n","Iter: 095/100 | Train Loss: 0.00023273\n","Iter: 096/100 | Train Loss: 0.00023009\n","Iter: 097/100 | Train Loss: 0.00022748\n","Iter: 098/100 | Train Loss: 0.00022489\n","Iter: 099/100 | Train Loss: 0.00022233\n","\n","Iter: 099/100 | Test Loss: 0.00099668 | Test acc: 64.4800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fwv9x4L-cT1V","colab_type":"code","colab":{},"outputId":"c629bfcc-35dd-453f-c4a5-43ef84dc803f"},"source":["title = '784-800-10; NTK scaling; with bn; full batch; train data size = 64'\n","fig = plot_loss(step_list, loss_ntk_scale_bn, title, fig_save_path)\n","del title"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x216 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"rb8r9ZBvEBwD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1597612203685,"user_tz":-120,"elapsed":122561,"user":{"displayName":"施成","photoUrl":"","userId":"14707086256107758378"}},"outputId":"43d24878-22fd-49e8-e986-b2a1f7f0e989"},"source":["plt.plot(step_list,loss_ntk_scale_bn['train'],'r-',label='Original_Train',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['train'],'b-',label='Modification_Train',linewidth = 2)\n","plt.plot(step_list,loss_ntk_scale_bn['test'],'r--',label='Original_Test',linewidth = 2)\n","plt.plot(step_list_,loss_no_scale_no_bn_['test'],'b--',label='Modification_Test',linewidth = 2)\n","plt.title('784-800-10; NTK scaling; with bn; full batch; train data size = 64')\n","plt.xlabel('Iter num')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.legend()\n","plt.savefig('/content/drive/My Drive/LCNN/newplots/attack4.pdf')"],"execution_count":201,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAawAAAEZCAYAAADLzxFqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3gV1daH351GCB3pvZcEkkAoAqIIigqXai/cC3bBehHFcgVFvXauCsjVq4gIWAA7+lkoipWAoZcgBgREeg0lZX1/rHPISUhC+klZ7/PMk5yZPXvWzJwzv1l7r722ExEMwzAMo7gT4G8DDMMwDCMnmGAZhmEYJQITLMMwDKNEYIJlGIZhlAhMsAzDMIwSgQmWYRiGUSIwwTKKDOdcL+fcNp/Pa5xzvfxoUjqccw865/6XzfbhzrkluagvwTl3QcFYd1rdtZ1z3zrnDjvnns+NLc658c65t7Mol+4eFSaFeX1yePxGzrkjzrnAAqpPnHMtCqKufNgw1Tn3L3/aUJj4TbA8XxTfJcU597LP9iucc+s8P8i1zrnBWdTzjeeLEpTNscp5buRfzrl9zrlPnHP1fbZXd8594Jw76pzb4py7JsP+13jWH3XOfeicq57Nsdo55/7PObfHOXfaILczHSs7PA8acc5d4bMuyLOuiXPuc5/rmeScO+nzeWomghHinJvnnPveOVc5p3YUFCISISKLivq4WSEiT4rIjQCe65nt98rP3AzsASqLyGh/GwNFe81y+/KQGSKyVUQqikhKQdmVEwrzOonIrSIyoaDrzSvOuas8z/GjzrnfnHM9MynziOd6nPHlxW+C5fmiVBSRikAd4BjwPoBHTN4G/glUBsYAs5xztXzrcM5dCwTn4HB3Ad2ASKAesB942Wf7ZOAkUBu4FnjFORfhOUYE8F9gmGd7IjAlm2MlAe8BN2SxPctj5ZB9wKOZvRWKyCU+13Qm8IzPdb7Vt6xzrhwwD6gK9BWRQ7mwwfA/jYG1YiP/s6SgPCcjbzjnLgSeBkYAlYBzgc0ZyjQHLgf+zFGlIuL3BfiH50Sc53NXYFeGMruBbj6fqwAbgbMBAYKyqf8V9OHt/dwf2OD5vwIqIK18ts8AnvL8/yQwy2dbc0/5Smc4pxZ6edOtO9OxGgEHgEZZ1DkeFaIVwD8864I8598kQ9k3gcczrOsFbAPCgC+BL4Dy2ZxDP2AtcBjYDtzrs20QEAccAn4DLvasHwGs8+yzGbgl4/F9PicAF/ic23vAW5591wCdfMp2BH71bHsfeNf3/DzX7ZwszmMLEOP5/1rP9YrwfL4B+NDHhrc9/2/1lDviWboBw4ElwHPoS8/vwCXZXL8E4AHPNdwPTANCM9yL0cAu9Ac7wmffa4CVWdT7JvpidNJj2wUZ73cOrvXbWdTttetB1INLAK7N8Nv51XPf/wDG+2w77Zp51t/k851YC3T0seleYCVw0HNPQ890T4G2wHEgxXOcAz7X5RVgPnDUc12ys7cJPs8OYBEwAfjeY+uXQI1s7u8Yz33bAVzvqatFXq4T+lxZAOz1XPeZQNUsjuuAiZ7vzSFgFdAu4+8e+MTnGEeAVGC4Z1sb4Cv0BXgDcEV2z7O8LMAPwA1nKPMF+pxJwPP9zG4pLn1Y/wDeEs8ZALHAOufcQOdcoKc58AT6xfbyJPrl3JmD+l8Hejjn6jnnwtCH1ueeba2AZBHZ6FN+BeD1eiI8nwEQkd/wiE5uTjAnxxJtoqgqIluzqUOAfwHjnHM58S4zUg499+PAIBE5lk3Z11HBqQS0Q39QOOe6oMIyBvXQzkW/cKA/or+hnvEIYKJzrmMObRsIvOOp82Ngkud4IcAH6I+xOjAbGOK7o+e6ZdVEtBh9EAOchwrpuT6fF2eyj3d7VVEP9UfP567oD7wG8AzwunPOZXNO1wIXoQ+kVsDDPtvqoC9e9VHhnOycq+Y5n1kiEplZhSIynPQe9NfZHD8v1EHPrz7623zVOdfas+0o8Hf0HvUHbvNprj/tmjnnLkcF8u/od2Ig+lD2cgVwMdAUbQEZ7t2Q1T0VkXXArcCPnuNU9dl8DfAE+ka/5Az2ZsY16Pe2FhCCCuppOOcu9my7EGiJiqMvubpOqAj9G20Bags0RK9bZvT11NEK/f5cQfprCoCIDJC0FpfL0WflN865CqhYzfKc51XAFOdceBbnOsU5dyCLZWUW+wQCnYCazrlNzrltzrlJzrnyPmUuB06IyPwszvM0/C5YzrnG6ENjunedaJvyW+gFPeH5e4uIHPXs0wnoQfpmveyIR99ytqNvJG2BxzzbKnrW+XIQ/cJ7tx/MZntuONOxcoSIfIx6nDfmwYZK6BvddBE5cYaySUC4c66yiOwXkeWe9TcAb4jIVyKSKiLbRWS9x7bPROQ3URajb6mntVtnwRIRme+5/zOAKM/6s1FP8iURSRKRecAvOT5jFaTzPP/3RB8M3s9ZCVZWbBGR1zw2Tgfqos27WTFJRP4QkX3og/Rqn21JwGOec5qPvgW3zqwSP/AvETnhuYefoQ9FRGSRiKzy3PeV6MvDednUcyMqrEs934lNIrLFZ/tLIrLDc30+AaLzafdHIvK9x77jebB3mohs9LzIvZeNPVd4yq72PJfG+27M7XE91+UrzzXfDbyQTfkk9HfcBm2VWiciWTapOedaod/VK0TkD/SFMkFEpolIsoj8CsxFRS0z20Z6Xh4yWzJ9qUJ/E8HAZehvLhrogOeFzTlXCXU67srK7szwu2ChfUNLROR37wpP59sz6FtxCHrj/ueci3bOBaB9SHeJSHLGypxGep0KNPCsnox6FmehzXLzSPOwjqBvfr5URpsEst3unLvW51ifc2bOdKzc8DDwEBCay/32oG9U051zF52h7KWou77FObfYOdfNs74h2gx4Gs65S5xzPzkNbjng2b9GDm3z9ZYTgVBPx3Q9YLuPBw76ApJTFgM9nXN1gUD0QdTDOdcEfUONy0Vdp2wUkUTPvxWzKe9r5xb0XLzszfAdTjxDXUXFfu/LoYdTdjvnujrnFjrndjvnDqKeTnb3N8vvioeM9zy/55/ue5EHe3NqTz1Ov7d5Pq7TqM93nHPbnXOH0D78TMuLyAK09WEysMs592pWQVPOuSrAR8DDPt5qY6Crr6eEtgTUycq+POBtuXlZRP4UkT2oCPfzrB8PzBCRhNxUWhwE6+/4eFceooFvRSTW84ayFPgZdbsro67mu865ncBSzz7bnHM9RSO9MgYaRANvisg+j1fxMtDFOVcD7QcLcs619Dl+FNqHguev900f51wzVPw2ishMn2NdkoNzPdOxcoyIfAVsAkbmYd95aL/CHOfc+dmUWyoig9Bmgw/RBz3oD7V5xvKeQI65aB9PbU9TzXy0uSM//AnUz9D01jCnO4vIJvThcwf6vTqEPphuRl+WUjPbLR/2+uJrZyO0v6MwOIr2TXrJz8OnmqfZyIuv3bPQ5tqGIlIFmEra/c3smmX6XSkAsro/GddnZ29++JPT721Oj5uZ7U961rcXkcrAddnZKSIviUgMEI42DY7JWMbzcj8LWCgir/ps+gNYnMFTqigit2V2LKcRxhmjur1Lps8uEdmP9oX6nqvv/32AO51zOz3P8YbAe865+7M6Z/CzYDnnuqPt5O9n2LQUfSOO9pTrgLqV3s7ZeqgIRZOm2DGoqGXGUuDvzrkqnn6fkcAOEdnjeZOcBzzmnKvgnOuBBhTM8Ow7ExjgnOvp+RE/BswTkUy9IqeEop4hzrlQz4OcMx3LpYW7NjnTtfPwEHBfDsumQ0RmA7cDH3nsyHgeIR4PsoqIJKFNmd4H++vACOdcH+dcgHOuvnOujeecy6HNlcnOuUvQ9vb88iPawX670zD+QUCXDPaKy35M12L0fL3Nf4syfM7IbvR8m+XdbABGOecaOB0K8RAaWHBGnIZtJ+TiOHFAP6fDJuoAd+fe1HQ86vkO9ESbkLy/0UrAPhE57rQv03dYRmbX7H/Avc65GM9vo4WnG+CMnOGe/gU0cNq/mR3Z2Zsf3gOGO+fCnfaLj8vFcTO7TpXQFpiDTqOkTxMgL865zh4PLhh9UTlO2m/TlyfQFqWMzW6fAq2cc8Occ8GepbNzrm1mxxMNla+YxZJdhPM04A7nXC2nfbP3eI4NKljtSHuO7wBuQb3GLPG3h/UPMnn4e9rNx6MewGH0rf1JEfnS0w6+07ugNx/gLxE5mcVx7kVvarynfD/Sd9qPBMqjAQOzgdtEZI3HljWoOz/Ts70S2Xs1jVF32PvmcQztpD/jsdC3jC1oX9sZEZHvyV1fTsb9p6NRap95flQZGQYkeJoobkWbDRCRX/AEVKAvEIuBxp77eCf6Y96P/kg/zqt9PnaeBIaifWcH0LfPT9H+TZxzDdFm1VXZVLMYvXffZvE54zET0R/8955mk7PzaP4stB9vM9o09ngO92uIRqvllBloAE+C53g5EsYs2Inevx3o9/5W8fRRot/fxzy/y0dI87ozvWYi8r5n3Sz0Hn2IBs5kSw7u6QL0N7bTObcnm6qytDc/iMjnwH88dmzy/M3RcbP4bj2KRsIeRPsM52Vz+MrAa+g92oIGXDybSbmr0f7f/T4e0bWe32lftGtgB3q/n0ZfNguSCaizsBGNEv0VPW9EZG+G53gK2hR9JLsKvWHkRjHAOfcwsFtE/utvW4o7zrmfgakiMs05dx0apv6Av+0qKJxzX6L9tOv8bYs/KI331Mg/JlhGicA5dx7qqe5BPb2pQLPsoqMMwyhdFNe0M4aRkdZos0oFtHntMhMrwyhbmIdlGIZhlAj8HXRhGIZhGDmiTDQJ1qhRQ5o0aeJvMwzDMEoUy5Yt2yMiNf1th5cyIVhNmjQhNjbW32YYhmGUKJxzW85cquiwJkHDMAyjRFCqBcs5N8A59+rBgxlz1xqGYRgljVItWCLyiYjcXKVKFX+bYhiGYeSTUi1YhmEYRunBBMswDMMoEZhgGYZhGCUCEyzDMAyjRGCClQOST5vX2DAMwyhqTLCyYdsfwsVnH6Bby91nLmwYhmEUKiZY2bDgmV/48WdHbEJN1sUe9bc5hmEYZRoTrGxoErKTnnwHwIxx8X62xjAMo2xjgpUN5zzQk/KoZzXzq1qkpvrZIMMwjDKMCVY2BNSoTvNm0IA/2JpUj+/f+cPfJhmGYZRZTLDOwGUj6xDJSgDeenqHn60xDMMou5hgnYGY27shnsv0waoWJJ9I8bNFhmEYZZMyMR9WfnDlQoiJgZhlE4isspWgE89Ducr+NsswDKPMYYKVA66+tz4RV19MtYMHGBQURoi/DTIMwyiDWJNgDgi/sj3tQzayX6rxxVNxFi1oGIbhB0q1YBXYBI7OcXWvHYRxlBueaMb859cVjIGGYRhGjinVglWQEzhePrYFHVjOntTqfPjKnwVgnWEYhpEbSrVgFSQtzm9ISlA5AD75vR0piSf8bJFhGEbZwgQrF/TskkQ9trOLWvw8OfaM5bdvh61bgQMH4D//gWeftdTvhmEYecQEKxcMvKUu9dkOwIfT92dbdvFiaNkilfAWJ4iv3wvuuQfuuw/+9jfIb5+aYRhGGcQEKxd0u6Yph6kIwMdrW8HJk1mWjVuwj2PHAziaVI4RiZNI8V7q//s/uPvuojDXMAyjVGGClQsCgxydwo9RmYNskFasf+uXTMvt3w9LZvzOK9xCNfbyPefwnwbPg3NaoHXrIrTaMAyjdGCClUsGXVeZ81nAK9xK84bpPaytW2HNGujfX5jzewxPcz9JBAPw0O67Wf/ou1rwwQdh9uyiNt0wDKNEY4KVSy4a2ZzPuYRRTOZAQPVT60+cgEsvhQ7Rqfz4o6MhW1lcdQjtYkJPbR/+2eWkPvkUiMDf/w633AILFvjrVAzDMEoUJli5pFKVAHo1/J1UAvnsxU0883QqO347xujREBsLtZO38zKj+IbeNJr6IE8+q4mcAgNh8GBIufd+GDZMowVffRVGjlQ1MwzDMLLFBCsPDBisl+35+W24f2wAndodZ/JkCOEEHzCEa5hNyyGRcMUVnH8+XHghpKRodHtwMPDII2n9WRs2wPPP++9kDMMwSggmWHng0rGtqBx0lEQJpR2r+PN4NQAmcg8nCaZ6dQevvHJKlJ54Qvd76SUNyKBFC7jqqrQKJ0yATZtyZ4QIbNwIb78Na9cWwFkZhmEUb0yw8kDdeo5FX6dwNLAKEazmb3zC/TzFfqrStvxW+PRTqF37VPnOneH88+HYMe26uu02YOxY3RgQAMePw7XXQlISoOV+/131KEvmzdNow2HDICICOnVSRTx0qPBO3DAMw4+YYOWRDudV5vt5u1hKF8JI5Ht6EBmykWqfz4Ju3U4rf8UV+vezz7TrKqFyJAwYAKmpULky/PIL/+n/FdWqQVgYNGumejR5sk8l+30GK597LtSoARdfrPsvWwZ33aXCla3SGYZhlExMsPJB84ERfP/CL6QSwEWB3zDg01vgvPMyLTtkSFq3VWqqZmriwQd1RVISOEevgG85cUIICYEGDXTTPffAz9+e0H8iImDXLt1Qsyb89Rd8/jns3AnvvAPt20N8PFx5pTYZGoZhlCKclIEHW6dOnSQ29sy5//LMJ59Aw4YQHZ1tsV69NGUTQLlyGjXYY/nL/D3+YarcfBVMncruPY4a1VJwCHfeE8jLkxyNArezPCWSs4IOwbvvwtChmR/g8GENlR87FiIjC/YcDcMoczjnlolIJ3/b4cU8rIJgwIAzihXA5Zfr34YNNZL93Xfhzvg7+IAh8N57cOgQNWuCm/8ZNGjAcz90pws/Uz1lF0dadoTvv89arAAqVYJZs9KL1a+/5vPkDMMwigcmWEWIt1nwr7/gl180kPCqq+DDs24k6cARmDRJC379Nfz1FyHLf+KjaiP44T9Labz2c+jSJXcHnDoVOnaEf/+74E/GMAyjiDHBKkLq1YMePTRn7qZNcOutmqHpw/dOEkwyvPCCNuu9+KIq2ltvUWfzD5S/62YICkIEtm3LxQGDglQhH3xQE+7aAGXDMEowJlhFjLdZcM4cn5Xnnw/nnAP79mlYoHMaCz9sGFStCsDRoxpL0bmzxljkiBtvhJkzNc3Giy9qBGFcXIGej2EYRlFhglXEeLug5s+HP//0rHROs18APPdcpmOpypXTAMGdO+GaK1NI2bxF+6d++CF7z+nqq+G773Sw8urVqnj33JN5FKGIHjshQaMN9+/XkEbDMIxigAlWEdOggaZqOn4c+vTR/iwALrhAx2/t3Qt9++pfH4KCYPZTW6gVepCF3wbSpHkAt3T8hXd6vMSmtgPgxx+zPmi3bupZjRqlOQyXLUuLsd+8GerUUU8uOBiqVIGmTaFVK6heXcPmvSQk5LJN0jAMo+AwwfIDM2dCu3awbl2aaC3+1nFr48+5KHQx039uzclzeqs4HDsGS5bA7bdT99yWzDn+N+qzjW005FVu4Wre4Y3fe2nn2OjRfPXpCa64IpNJjStU0KCOX36B8ePT1gcFqQEHD2rCw4oVoVEjFa0qVXRwspdnntEQx+7d4eWXfdTWMAyj8LFxWH5i1y7tulq7VjUjOTn99vps45+hr3B38nMEJHvm3XIOrruO1HGP8uuBpnz2GSz5NoVL+IJ7Fg4kJRXalN/CpmMNaN5c0wyefXbmx09I0O0H96fSrtEh2kcFENGxHOUql8va6Ntvh2nTIDExzZ6uXaF/f+1ga9ky630PH1YBPnAgfSaQDz9UsaxdG5o3h8aNISTkjNfPMIzCp7iNw0JEStQC1AZ+ABYDC4C6Z9onJiZGiiM7d4q0bSsCIk2aiIwdKzJ1qkhEmyTRDiWR5xgt0r69yK23iqxcmXVlS5eK1K4t8TSX6Jp/nNr/8stF4uPTF/3hBxHn5FQZ71Klisgdd4hs25aN0YcPi8yaJTJggEhISNrOEyaklfnwQ5GuXUWio0VatxapXDmtXEiISGpqWtnIyPRGBASIhIeLXH+9yPz5ebquhlEWSU0VOXlS5NgxkSNHRA4dEklJyV+dQKwUg+e+dylxHpZzLhAQEUl1zg0HGojI49ntUxw9LC9Hjmii23bt0qdueuO/Sdw0MpjatVL5PSGA8uVzUNkPP0CvXhxPCuDRfr/wnwWRHD+uHlyvXvDVV1osOVnzFHbrpl1Vq1fDihWagjAgQO1p1EjLjhmjwYsnT2psR0gIREVpwGFM6yNU/uVrjSAZNAjp159t22DpU9/w05Rl/ExXVhBFbf5iQ/kOUL8+1K7Ni4MXUqFqMA0bQu25U6i9bx219q0ncHO8Ttvs/U4++GBaqvtNmzThb0yMji2rVq0gb4NRzElJ0e/fyZP6/1lnpW3bvv1U3mgg7e0nNVW/Jt6yBw9qM7y3nhMn0paTJzWCt0IFLfvRRzrzT2pq+vpENM/ntddquaNH9SuanKxLUlLacvJkWnpP0DH906ZpuZQUrc/7t3LltN8nwEUX6Xl5y3iX5GRt6Lj/fi335ZeatyA5OfP4qC1b0n7LeaG4eVhB/jYgt4hIis/HSsAaf9lSEFSsqCkAfQkIgBtuDWbq67BsWQCvvQZ33pmDyrp3h1deIfTGG/n3150Z9d4PjPs4hjff1EDBpCSNqwgK0h9jUIa7v2KFJtPw/YLPmgU7dqQvN3Om/n344YpMmDAYBg/mjTfgwbrebq0+nkWp1qgSJBwF5xCBR6r6BkKOBNSuJk3ggalJjIhcBj/9REq3cwgQj5DPn5/2KwXtS2vaVHdq1oz4a8YhotGUZ63/ngrlU3EVwjSTcFiYPokqVy7zzY1btmjrbFJS2gMY9DtXu7a+U4A+iH//Pe0hmZSkLcGJibptwAC9rKAP4dhYXe99+J88qfVHR6e9cxw5kpYE2lcEvA/7CRPSUnG++KLud+yYBij5NpmfdRbs2ZP2uUcPPa/MeOABePJJ/f+nnzRXdFb06pUmWG+9pe9HmXHhhWmCdfx49uPyBw1KE6wtWzQnQGZUr57+84YNWZ+Tbw5s5/RaewkM1CUoSO9pCfNHzkihCpZz7nZgONAemC0iw322VQdeB/oCe4AHRGRWDuuNBv4LVPXsX+pwDh5+WLNjPPOMpggsl0330iluuEEjAidNosGoQbweF8e//10DEc/kkR4yihWo5xQVlX7df/6jb6YhIXr8w4dh+XJ9QLVunVauYkUVq2rV9Afatav2n8XEQEhIIHi8x6Qk9do2btSw/p07ddmzRyPpUwKCdcezz2b223DbBapNDSpcR7nG57N3L+w7GkLFPw4T+0dn+PZbqFWLXq+O8xHWHpTjODXYQx12cjMTuZnXANj7zydYO/hBmjaFult/JvCRh9T4ChV08Sp6YCBHx07gz0MV2LkTDr4xl6Mbt3P0eCCpKUKbsK30qLIaAgI4EnMey3qPoVIlqBKcSNVpE6lUO4yQOtU1aKVGDU1WXLMmyaEV2bvPsWeP9mPu2qXXbc8eHVh+6616BidP6jA8UJOCg/UeeJerrtIRCqAvGXPmqGAkJmo34d696hkHBqafLu3881WIMmP0aB1VAfDzzxoQlBWbNmmXI2gg6fvvZ17u+PG0/5OS0gedZuTUMA/P+e/enfbZOQgN1XOvXDn9fg0a6IPZ20IB+rB2TuOGvFSvrsliypVL+z77/u/bijFokHpS3nqcS/u/RYu0cmFh8PjjaV+boKC0+xUcnCZWoN28nTqliYp3CQhI/9sEvU7JyWnbvSIUGJj+nHr31mvsrcv3GpRGCrVJ0Dk3FEgFLgLKZxCs2WiU4g1ANPAZ0F1E1jjn6gDvZFLlVSKy06eOK4DeInJrdnYU5ybB7PC+oa5apWmcbs32LH1IStIn0/ffwyWX6PxcAYUbEHrokD50mzbN248mMVEDQWrVSgtMHDMm7QGakbDyqRz5eCEu4Xc4fpye797Ozp36492z4wTHU9PU/d9VnmJswDNw6BAfXP0eQ9/WwXDBgak0TPmdShwmjETCSOQrLvRqK+1aJ7FmQ+bvdDfyGq9xMwC/nv9POi48fdboIJIII5Ef6E4EqhrXnLed2YvrZVpnt1qb+OHSFyAkhGPV6xM2bkyW1+utyz5mWEdtXHhx9zXcPbFxpuWCAlI4+dBjONH2oot+epRtOwIIDoaA/XtxSSfU83UBDOu8gXsuXgflyvHTiQ7c8FKUPiQDUgk+eZSw8mkO6wv/2k/DxgEQFMT8n89i8x/BhIVBaMBJQgJTCA6GwCBHjbOEs7ukgnMkJTu+/K78qe+HO3GcwADxPIiFtq2FWjX1eXToeAiJySFaZ1AywXISF+BRDe8T3KskRqFR3JoEi6QPyzn3ONrXNNzzuQKwH2gnIhs962YA20Vk7BnqChGRk57/LwIuEpF/ZrdPSRUs0Jy4V16pwXPx8ae/iWXJH3+o2u3bB08/DffdV6h2FgYi6i1s26anc/KkNgeddVaa05LV8yoxMc17qVvX08wpwmefpDLhyUASEjKPyj/07H+pFKxtUAO+vovV64OoUweqpeyhAkepEJZKQGAA57bbx/C+OyA1lTVHm3DblPYcPgwH9qVwYNdJDp0oR6roS8LSNsPolPgt7N7N3X/bxNsL6qnDdeg3au9YTi12UZPdNGMzf2cGACkt2zBnwjpE9E375A23cfKkkEQwJwnhEj4nnHVa/11v812ja6lQAcLifqDq1H9TnX1UYz/V2Udt/jolwhw+rB4l6Jxq332X+QW8+mptDwZ1h33d6YwsXqx1gaYAe/HFzMu1aaOdSF7CwrTNLzNeeEEHuAPMmKEzn2ZGQIA2AXjPqV8/HZOYmRszZIhnXh/gt980utUreqdU1PP/m29q8wBoE8fbb2d+/GbNNNLVy9lnp3crfesePTqtLfHzz2HcuPRlfJevv05z+e64A1auTF+nlwsvhIce0v+3bIERI0638euv8/zCWtwEy199WK2AZK9YeVgBZD6ZVHqinXPPASnAceD6zAo5524GfQVulJ9eRz9z6aX6O1+/XtvVb7ghhzs2bAjTp2tnw4MPauqn7t0L1daCxjltYqxW7fR+vjMRFqbdW02apK+w/8BA+g/Uj4mJKoRHj+pz88QJKHfOLeDp5vpktG+NNTyLl8ZABwAigG+v8q4PBMojoudT1RkAACAASURBVAKbmAgVK84Az4vGxFThP95nR3wqbK4MRwLgaCU42QBOdIWTJwmsWpUrr/Q53JYGaqC34yd1EDAIROjcvxGde3rKxYVBtfZp7Vdej8T7IPTtw/vHP7TTyDdKwBuF4DseIjBQ23h9t/tGGfi2VZcrp+LhGwHgfSkODU1/kypUyPpB6vtmFhio+54699S0DrjUVN3u5dAhfcvJDN/B+CdOaEdRVniHboC2Va5alXk532gPUGHJSoR92zj37IGlS7M+vq8jERenYzEzwztxntfmhQuzrrMU4C8PqyfwvojU8SlzE3CtiPQq6OOXZA8LNMjhuuu0uW3Dhlx4WQD33gvPP6/uyNdf2zxZRunBK1heQQaN7PCGEnoXr8CVL69tzqCCtXlzWvSHtz7v/82apUVg7NiRPsrDl5AQfaP0smJF5vWJqLjUrq2f9+xJf3zfBfSFwSvEcXFpmQB8n9ciWl94uH4+elQ7HzNy/vl5bjotbh6WvwSrA/C9iIT5lBkN9BKRAQV9/JIuWCkpOtnwhg3wv//lwssCfQMcMAD+7/+01/mrrzQs3DAM4wwUN8HyV2qmjUCQc843NUIUJTxEvbAIDEzLjfv446e3QmRLcLC2sffvr/1ZffpoZJ1hGEYJo1AFyzkX5JwLRRv2A51zoc65IBE5CswDHnPOVXDO9QAGgafH2TiNK6/UloeEBO2ayhWhoTqoZMgQbd8/7zyNiZ46VTvH1q/XzvDVqzUD/M8/67rSNojDMIwSTWGHtY8HxmVY/aiIjPeMw3oDuBDYC4zN6TisXBx/ADCgRYsWN8XHxxdk1X5h9my45hoNJNiwIQ9jYJOSdHDXq69m3THtS40aGv118cXaQV/GB90aRlmjuDUJlrjUTHmhpPdheUlJ0Wi5des06vipp/KYduXYMfW4pk9Xl80bQRYYqKIUHKyx5L4jOVu21FBjbyiwYRilHhMsP1BaBAs0Q9HAgSpeISE6qXCjRjrm6MABDcgokOh1EU1nsHChCpU3BPiSS+D113Vwk2EYpRoTLD9QmgQLdADx+PHaRJjx9jVsqN1PYWGZ7po3kpJg8mQ96MGDKlZz5pS4cV2GYeSO4iZYNoFjCaRlSx2btWKFJha4914djN+unQ6Eff70LEH5IzhYD7R+vQZs/PmnZgp95RULzDAMo8go1R5WaQu6OBOLFukYwbAw9cLqZZ6yLn8kJWmaJ2+Km+HDYcoUcjb/iWEYJQnzsIoQEflERG6u4pveuBTTq5dGricmajamQiE4GCZO1Nxq5ctrzrVzztHgDcMwjEKkVAtWWeSZZ1RTpk/XKUAKjWuv1QmGmjXT+UZiYjRJaWazyBmGYRQAJliljBYtdJZT0GTXhdriGxmpqtivn2bR+PvfdTa97JJ6GoZh5BETrFLIww/rmN8lS2Du3EI+WLVq8Mkn2jRYp456XV26aDPhCy9YU6FhGAWGBV2UUl55BUaO1Azva9eePrNDoXDokM5r/tJL6ecEqlVL80q1bg2tWmmYY8uW+n9mUx8bhlEsKG5BF6VasLyUtnFYOSE5WedvXLPGD/M3Hj6sE9TNnat/Dx/OvFzlyhop0rs3DB6ss1QahlFsMMHyA2VRsEBnFLn4YtWF+Pi0qYCKlNRU2L49LclufLwu69enby50TtM+jRwJF12U5xlSDcMoOEyw/EBZFSzQeIjPP4crrtAZi30nh/U7W7fCggXwxRfwwQc68R5o8+Gjj8Jll5lwGYYfKW6CZU+DUs5zz6lIvfeeZlIqVl15jRrpwON33tFku95svuvX63wqHTrAxx9bNg3DMAATrFJPeDgsXqxTkixfrpMNP/NMMZzuqmZNuP9+VdSpU6F+fVi5EgYNgm7d4Ouvi5nBhmEUNSZYZYCuXSEuTp2WI0dUF9q2VWdm/PhipgMhIXDLLZopfuJE7Xj7+We48EKdm2vuXI0oMQyjzFGq+7DKclh7ZojAhx9qovWvvoLdu3X9vffCs8/617YsOXIEXn5Z3ULvpJMNG+q8KoMG6eBlm5/LMAqF4taHVaoFy0tZDrrIitRU+OgjDcZITvZD6HtuOXRI801NmgQbN6atb9BAQyE7d4ZOnTRlvc2MbBgFggmWHzDByprZszUtoAi89po6LsWa1FR1D+fMgU8/hZ07028PDoaICB2E1qGDZtyIitLZlA3DyBUmWH7ABCt7pkyBUaP0mb5wIfTs6W+LckhqqkaSfPed5jSMjU3vfXmpVEmFa8AAHaBssyUbRo4wwfIDJlhn5r77tB+rXj349Vc/DTIuCA4f1ujCFSvgl19UzDZvTtvunEYdXnstXHUVVK/uP1sNo5hjguUHTLDOTHKyZkj67ju44AIdy1tqWtG2b9dmxA8+0PQfJ07o+pAQGDgQ/vEPza4RHOxfOw2jmFHcBMvC2g1Ac9DOnq3Dob7+WhNNlJp3mfr1dYDyRx/Bnj0wcyb07auzJ8+Zo02FDRrAP/+pU6OUmhM3jNKFeVhGOr76Sp0NER1kfO+9cPnlpTSp+vbtOunk9Ok6ktpLw4YwdChcconO71Wxov9sNAw/Utw8rFItWDYOK29MmwZjx8KuXfq5bl1tLjz3XB2/27Spf+0rcETUs5oxA+bNgx070rYFBWnIfPfuOqtyTIzOkmk5Do0ygAmWHzAPK/ccP67P7+efhw0b0tYHBOhcWzff7D/bCpXUVA3W+PBDDZmMjdV1vlSqpCHzHTumiVnjxjaA2Sh1mGD5AROsvJOaqhNALl4M33yjcQugWZPuvtu/thUJhw7B99+rB7ZsmQqYrwfmpU4dHQ9wwQWl1A01yiImWH7ABKvgmDQJ7rhD/58wAR56qAw6Fjt3auz/smWa5/CHH2DfvvRlWrTQ+b3699e21GI1r4th5AwTLD9gglWwvPGGZsQQgSeegAcf9LdFfkZEBywvXKhRKwsWpOU9BKhQQT2vSy7RpVEj/9lqGLnABMsPmGAVPLNmwXXX6bP6lVfg1lv9bVExIjlZPa/PPtP0UatWpd/evDn06qXL2Wfr5zLnpholARMsP2CCVThMnQq33abP2lmzNHGEkQl//KEjsefPV+/r0KH026tX1+CNjh3TciA2b26RiIbfMcHyAyZYhceTT2o/VlAQ3HUX3H67ThZpZEFysk5OtnChphX5+ee08QO+ZIxE7NoVmjUzT8woUkyw/IAJVuEhomO2nnlGPwcEaH7ZkSPh/PPNSTgjIuqBLV2qgRxxcZrQ988/Ty9bo4aG0Pfsqcl8Y2IsnZRRqJhgFSE2cLjoiI2FF1+Ed9/VjEegDsGNN6rXVamSf+0rcfz1lwpYbKyOC8vMEwsL00S+556rAta1qwZ4GEYBYYLlB8zDKjr+/FPn1Xr9ddi6Vdd17AhffglnneVf20o0IvD777BkiS7ffpt+RDdotuLoaBWxzp2hSxdo1crcXCPPmGD5AROsoiclRUXqzjth0yadCPjrr6F2bX9bVor46y8Vr8WLdXDzihV64X2pWFEnsOzQQcUsKkonuCxf3j82GyUKEyw/YILlP/78E/r0gXXroHVrzZZRv76/rSqlHDmizYe+y/btp5cLCNCb4RWxjh11qVat6G02ijUmWH7ABMu/7Nql2YpWrtQEEIsWmWgVGbt2aV/Yr7+qB7ZihTYlZsyPCBpK36lTWnNix47WJ1bGMcHyAyZY/mffPk328OuvJlp+59gxWLNGIxJ//VWjEuPiNOOxLwEBEB6uAta5s0YlRkZCaKh/7DaKHBMsP2CCVTzIKFoLFujUU0YxIDlZRSw2VkPsf/lFXeKMfWJBQdoHFh2d1icWFaWDn41ShwmWHzDBKj74ilZYmA42HjPGuk+KJceOqee1dGlatvr16zOfkblBA/W+fJfWrUvpzJ9lBxMsP2CCVbzYtw9GjICPP9bPVarAffepeFmXSTHnyJG0vrAVK1TQVq1ScctIuXIaHur1xjp0UG/MZnAuMZhg+QETrOLJTz/Bww9r5CDolFLjxsENN1gChxJFSgps3qwCtnKlLitWQELC6WWd0/Zgb4Si92+dOkVutnFmTLD8gAlW8WbBAk3vtHSpfu7USXPF2kDjEs7Bgype3pRTv/6q/WTeVCi+1KqlwuXbpNimjc0j5mdMsPyACVbxRwTmzoV774UtW7Ql6auv7MW71HHypE5h7RWwuDhdMmawB+3/atUK2rdPW9q10+zKlr2jSCiRguWcqwAcE5FU51wroA3wuYhk8qpU/DDBKjns2KFBGevW6bPqm2+0P98oxYho86Fvc+KqVRAfn3mAR8WKGqnoK2Tt22tyYKNAKamCtQzoCVQDvgeWAidF5NrCNS9/WPLbksnu3dC3r754N26sKZ1atPC3VUaRk5io3tjq1Spg3mXnzszL162rTYm+Ita2rY0bywclVbCWi0hH59wdQHkRecY5Fyci0YVvYv4xD6vksX+/zib/88+af/DLL/VZZBjs2aMitnJleiFLTDy9bGAgtGyZvkmxXTudSiAwsOhtL2GUVMH6FRgJTARuEJE1zrlVItK+sA0sCEywSiZHjujcWt98A1Wr6oS93br52yqjWJKaqtnsV61KL2Tx8ZmnoQoNVe8rIkIFLCJCl8aNrX/Mh5IqWOcBo4HvReRp51wz4G4RubOwDSwITLBKLidOwNVXwwcfaB/8qFHwyCOWWMHIIceO6WBnr4CtWaPe2R9/ZF6+QgVNR+UVMK+gNWhQJmd7LpGClW4H5wKAiiKSSVhP8cQEq2STnAyjR8PLL2sffPXqMH483HabJVIw8sjBg2n9Y6tXq5CtWZN1/1jlyqcLWUQE1KtXqoWsRAqWc24WcCuQggZcVAZeFJFnC9e8gsEEq3SwYgXccw8sXKifo6Jg8mTo0cO/dhmliL1708TLV8j27Mm8fJUqKly+YhYeXmqErKQKVpyIRDvnrgU6AmOBZSJSIrrBTbBKDyLw0Udw9906Xgvg+uth0iSbk9AoRHbtShMv32XfvszLV6miwpVxadiwRAlZSRWsNUA0MAuYJCKLnXMrRCSqsA0sCEywSh+JifDvf8Mzz+hY1O7d4ZNPrG/LKEJEdNbntWvTBMz7f1ZCVrGiBntkFLJiOhi6pArWncD9wAqgP9AIeFtEehaueQWDCVbpZfVq6NdP+9DbtNGUTo0b+9sqo0wjooMJ16zREfBeEVu7Vj21zChfXr/A4eFpgta2rU6q6cfEmiVSsDLd0bkgEUkuYHsKBROs0s327Tpma9Uq7Tr45BOdLNcwih179qhw+QrZunWa4iUzgoN11HzbtumX1q2LZGqDEilYzrkqwDjgXM+qxcBjInKwEG0rMEywSj8HDuiYrcWL9WX17bdh6FB/W2UYOeTAARUur5B5RS2zjPdeGjU6XcjatIGaNQvMrJIqWHOB1cB0z6phQJSIlIhHgglW2eDECbjlFpju+ZY+/jg88ECx7BowjJxx9Chs2KBjybyCtm6dDojOLOs96DQHvgLWti307p2nzPclVbBOS8NkqZmM4ogIPPusTlciok2DL7wA553nb8sMowBJStI5yDIK2fr1cPjw6eUPH87TxJnFTbByOuzymHPuHBFZAuCc6wFkMsWoYfgX53T24rZtdWDx8uXQqxcMGgTPPWdJdI1SQnCw9mO1bq1fbi8i2qnrK2S7d5eaWZ5z6mFFAW8BVTyr9gP/EJGVhWhbgWEeVtkkMRGefx6eflpbVoKDdfzWww9r4gLDMLKnuHlYOWrdFxHvmKtIIFJEOgC9C9Uyw8gnYWHwr39pc/+IEdqK8uyzmrz700/9bZ1hGLklV93RInLIJ4fgPwvBHsMocOrWhTfegKVLdYDxrl0wYICKWUqKv60zDCOn5Cd+qtjnF3HODXDOvXrwYImIvjcKmU6dYMkSeOopjRx8/HEddJxVmjjDMIoX+RGsvI04LkJE5BMRublKlSpnLmyUCZyD++/XCSFr1NC/UVGwaJG/LTMM40xkK1jOucPOuUOZLIeBekVko2EUOH36aARhjx6aZKB3b20iTC4RuVsMo2ySrWCJSCURqZzJUklEbCYio0TTsKF6Vv/6l35+/HGIjIQPP9ToYMMwiheWA8Ao0wQFwWOPwYIF0LSpDlsZMkSDM777zt/WGYbhiwmWYaCDi9ev11mNa9WCn36Cc8/VfITx8f62zjAMMMEyjFOEhMDtt8OmTTBunI7j+uADnenhgQd03i3DMPyHCZZhZKBSJRg/Xj2r66/XsVpPPQVdu2qToWEY/sEEyzCyoF49eP11HbvVtCnExWky3ZdfhtRUf1tnGGUPEyzDOAPdu6tYDR8Ox4/DnXdqWPzmzf62zDDKFiZYhpEDKleGadNg3jwNyli0SEPgJ07UebgMwyh8TLAMIxcMGaKzml95pWaA/+c/oVUrFTMbdGwYhYsJlmHkkho14J134OOPISICtm7V4Iz27WHOHBt0bBiFhQmWYeSRAQNgxQqYMQOaNdNxXJdfDp07wzff+Ns6wyh9mGAZRj4IDITrrtNw9ylToE4dWLYMLrgArr1WpzIxDKNgMMEyjAIgJARuuw1++w2eeALKl4dZs6BNG52Ly8LgDSP/mGAZRgESFgYPPgirV0PfvrB/P9xwA5x3Hqxa5W/rDKNkY4JlGIVAs2bwxRfw9tsaBr9kCXTooFGFe/f62zrDKJmYYBlGIeGc9mNt2ACjRmmz4MSJ0KQJPPww7NvnbwsNo2RhgmUYhUzVqjBpEsTGwsUXw5Ej2s/VtCk8+igcOuRvCw2jZGCCZRhFRMeO8Pnn8MMPGkV46JAm2W3aFJ55RtM+GYaRNSZYhlHEdOsGX30FixdDz57aNHj//ToI+eOPbeCxYWSFCZZh+Ilzz1XR+uILaNdOk+kOGgT9+2u/l2EY6THBMgw/4hxcdBH8+iu8+CJUqaLNhu3aaUThgQP+ttAwig8lVrCcc1c753b72w7DKAiCgnTako0b4aabdNLIiROhZUt44QVITPS3hYbhf0qkYDnnAoHLgT/8bYthFCS1asGrr2p6p3PPhT17YPRoHdc1caIJl1G2KZGCBVwNvA9YwhujVNKhg8659cknEBMDf/2lTYTNm2vToUUUGmURJ4UYkuScux0YDrQHZovIcJ9t1YHXgb7AHuABEZmVgzoDgQ+AwcAvItLpTPt06tRJYmNj061LSkpi27ZtHLdffpkiNDSUBg0aEBwc7G9TcowIzJ8P48ap5wVQr56GxF9/vSbgNYzCwDm3LCfP2KKisAVrKOoFXQSUzyBYs1EP7wYgGvgM6C4ia5xzdYB3MqnyKk9dKSLytnMuNq+C9fvvv1OpUiXOOussnHN5O0GjRCEi7N27l8OHD9O0aVN/m5NrRNTjeuQRndYEdA6u55+HCy/0r21G6aS4CVahNgmKyDwR+RBIlz3NOVcBuBT4l4gcEZElwMfAMM9+O0WkVybLTiAc+Ltz7gugpXPupbzYdvz4cROrMoZzjrPOOqvEetXOwcCBsHy5TiDZuLEm1O3bF/r104S7hlGa8VcfVisgWUQ2+qxbAUScaUcRuV9E+orIxUC8iNyZWTnn3M3OuVjnXOzu3ZkHE5pYlT1Kwz0PCIArr9QJI596CipV0lD4qCi48UbYvt3fFhpG4eAvwaoIZMygdhColJtKsnNVReRVEekkIp1q1qyZBxMNo3gTGqoZMjZt0uS6AQHw+usamHH77fCHxdAapQx/CdYRoHKGdZWBw36wxTBKNLVqaXLdNWvgssvgxAmYPFmF65ZbICHB3xYaRsHgL8HaCAQ551r6rIsC1vjJHr+xbds2Bg0aRMuWLWnevDl33XUXJ0+ePK3cjh07uOyyy85YX79+/TiQx/QI48eP57nnnst026hRo4iOjiY8PJzy5csTHR1NdHQ0c+bMyVHd+bHLyBmtWsH772u/1lVXQXKyjulq2VIHI//+u78tNIz8UaiC5ZwLcs6FAoFAoHMu1DkXJCJHgXnAY865Cs65HsAgYEYBH3+Ac+7VgwcPFmS1BYaIMHToUAYPHkx8fDwbN27kyJEjPPTQQ+nKJScnU69evRyJw/z586latWqB2zp58mTi4uKYP38+zZs3Jy4ujri4uFMimpyc7Be7jNNp1w5mz4a1a+G663Qerv/9D1q31mwau3b520LDyBuF7WE9DBwDxgLXef5/2LNtJFAe2AXMBm4TkQL1sETkExG5uUqVKtkXdK5wljOwYMECQkNDGTFiBACBgYFMnDiRN954gylTpjBw4EB69+5Nnz59SEhIoF27dgAkJiZyxRVXEB4ezpAhQ+jatSvesP0mTZqwZ88eEhISaNu2LTfddBMRERH07duXY8eOAfDaa6/RuXNnoqKiuPTSS0nMY/qERYsW0bNnTwYOHEh4eDgAgwcPJiYmhoiICF599dVTZXNil1GwtGkDM2bAunUwbJh6XC+/rE2FNg+XURIp7LD28SLiMizjPdv2ichgEakgIo1yMmi4tLFmzRpiYmLSratcuTKNGjUiOTmZ5cuXM2fOHBYvXpyuzJQpU6hWrRpr165lwoQJLPOOJs1AfHw8o0aNYs2aNVStWpW5c+cCMHToUJYuXcqKFSto27Ytr7/+ep7PYfny5bz44ots3KgBn2+88QbLli0jNjaWl156ib2ZzAeflV1G4dCqFbz1lo7d6t9fJ5AcP17TPT33HNj7glFSKKmpmQoWkcJZ8smFF15I9erVT1u/ZMkSrrrqKgDatWtHZGRkpvs3bdqU6OhoAGJiYkjw9L6vXr2anj170r59e2bOnMmaNXl3bLt06ZJuEO5LL71EVFQUZ599Nn/88Qfx8fE5tssoXNq3h08/hW+/hXPOgb17YcyYNOE6bCFPRjHHBMuPhIeHn+YdHTp0iK1btxIUFESFChXyVX+5cuVO/R8YGHiqn2n48OFMmjSJVatWMW7cuHwNpPW1cdGiRXz99df8+OOPrFixgg4dOmRad1Z2GUVDz54qWvPn6yzIO3eqcDVurJ5XJk6xYRQLTLD8SJ8+fUhMTOStt94CICUlhdGjRzN8+HDCwsKy3K9Hjx689957AKxdu5ZVq1bl6riHDx+mbt26JCUlMXPmzLyfQAYOHjxItWrVCAsLY/369fz0008FVrdRsDgHl1wCsbHw2WfQowfs3699W40bw733wp9/+ttKw0hPqRas4h4l6Jzjgw8+4P3336dly5a0atWK0NBQnnzyyWz3GzlyJLt37yY8PJyHH36YiIgIzhhY4sOECRPo2rUrPXr0oE2bNvk9jVNcfPHFJCcn07ZtW8aOHcvZZ59dYHUbhYNzmtZpyRKd/fjii+HoUc1P2LQp3HWXCZdRfCjU5LfFhcyS365bt462bdv6yaL8kZKSQlJSEqGhofz2229ccMEFbNiwgZCQEH+bViIoyfe+KFi2DJ58EubN08+hoXDbbXDffVCnjn9tM4qWMpX81igcEhMTOeecc4iKimLIkCFMmTLFxMooMGJiYO5cjSocOlTn3po4Mc3jslyFhr8wwSqBVKpUidjYWFasWMHKlSu55JJLCrR+b1YL32XatGkFegyj+BMZqcK1fDkMGaLC9dJLGlV4002wYYO/LTTKGtYkaJQ57N7njZUr4fHHYc4cHbXhne7k/vuhWzd/W2cUBtYkWIQU96ALwyhJREbCe+/ptCY33wwhIfDRR9C9O5x7ro7xKgPvv4YfKdWClePUTIZh5JhWreC//4UtW+DBB6FqVfjuOxgwAKKjVdRSUvxtpVEaKdWCZRhG4VG7NjzxBGzdqpky6tXTZsMrr4SICJg2DTKZeMAw8owJlmEY+aJSJRg9GjZvhqlToUkTDci4/npo0QJefFHHdhlGfjHB8iPOOa677rpTn5OTk6lZsyZ/+9vfclWPNxM6QPfu3U+tHzNmDBEREYwZM4apU6eeyqiRGw4cOMCUKVNOfc7pvFw5pWvXrkRHR9OoUSNq1qx5KioxJ/kFC9oWI3+UK6cTRm7cqMl2w8N11uO774aGDbX5cMcOf1tplGhEpNQvMTExkpG1a9eetq6oqVChgkRFRUliYqKIiMyfP1+ioqKkf//+uaqncePGsnv37tPWV65cWZKTk/Nl4++//y4RERH5qiMnTJs2TUaNGnXa+qSkpAI/VnG492WBlBSRDz8UOfvstIzQwcEi//iHyMqV/rbOyAlArBSDZ7h3MQ8Lv02HBehMvJ999hkAs2fP5uqrrz61bd++fQwePJjIyEjOPvtsVq5cCcDevXvp27cvERER3HjjjYhPaFbFihUBGDhwIEeOHCEmJoZ333033WzCmzZt4oILLiAqKoqOHTvy22+/ceTIEfr06UPHjh1p3749H330EQBjx47lt99+Izo6mjFjxqSbl+v48eOMGDGC9u3b06FDBxYuXAjAm2++ydChQ7n44otp2bIl9913X67ux/jx4xk2bBg9evRg2LBhJCQk0LNnTzp27EjHjh354YcfANLZkt9jGgVPQAAMGgQ//gg//ACXXqrBGNOna8ThRRfB119bZKGRC/ytmIW5AAOAV1u0aHHam4PvW3ZhzS9yJipUqCArVqyQSy+9VI4dOyZRUVGycOHCUx7W7bffLuPHjxcRkW+++UaioqJEROSOO+6QRx99VEREPv30UwFOeVgVKlRIV7+XcePGybPPPisiIl26dJF58+aJiMixY8fk6NGjkpSUJAcPHhQRkd27d0vz5s0lNTX1NA/L9/Nzzz0nI0aMEBGRdevWScOGDeXYsWMybdo0adq0qRw4cECOHTsmjRo1kq1bt2Z7LXw9rHHjxknHjh1PeZ5Hjx6VY8eOiYjIxo0bxesx+9qSm2Oah+U/fvtN5I47RMLC0n4nHTuKvPuuSD4bA4xCAPOwig7JYVh7YUlWToiMjCQhIYHZs2fTr1+/dNuWLFnCsGHDAOjduzd79+7l0KFDfPvtt6f6vvr370+1atVyfE0OHz7M9u3bGTJkx8ijiwAAFx5JREFUCAChoaGEhYUhIjz44INERkZywQUXsH37dv76669s61qyZMkpO9q0aUPjxo1PTeTYp08fqlSpQmhoKOHh4WzZsiXHNoJ6iOXLlwcgKSmJm266ifbt23P55Zezdu3aTPfJ7zGNwqdZM82W8ccfGmFYq5Zm0rjySg2XnzzZAjSMrCnVglVSGDhwIPfee2+65sCiZubMmezevZtly5YRFxdH7dq18zVPVn7nvPKdZ2vixInUrl2bFStWEBsby8ksYqVtnq2SQ/XqGoSRkACvvALNm2uU4e23Q6NG8NBDFqBhnI4JVjHg+uuvZ9y4cbRv3z7d+p49e56ar2rRokXUqFGDypUrc+655zJr1iwAPv/8c/bv35/jY1WqVIkGDRrw4YcfAnDixAkSExM5ePAgtWrVIjg4mIULF57yTipVqsThLKai9bVv48aNbN26ldatW+fu5HPAwYMHqVu3LgEBAcyYMYMUG5VaaihfHm69VcPg58yBrl1h3z7NFt+4MVxzDfzyi7+tNIoLJljFgAYNGnDnnXeetn78+PEsW7aMyMhIxo4dy/Tp0wEYN24c3377LREREcybN49GjRrl6ngzZszgpZdeIjIyku7du7Nz506uvfZaYmNjad++PW+99dapebLOOussevToQbt27RgzZky6ekaOHElqairt27fnyiuv5M0330zn5RQUI0eOZPr06URFRbF+/fp8z8RsFD8CAzUo48cfdW6uyy6D1FSYPVtFrEcPeP99MKe5bGPJb40yh937ksGWLTBpErz2GnjTgTZsCHfeqdniLeNa4WPJbw3DMHJA48bw7LOwbZsGY7RqpcEaY8ZAgwY6IHnzZn9baRQlJlhGkeHNauG7rFq1yt9mGcWcihVh5EhYtw4++QR694YjRzTlU4sWMHgwLFpk47nKAkH+NqAwcc4NAAa0aNHC36YYwM8//+xvE4wSTEAA/O1vusTFwX/+o31cH32kS7t2GmV43XVg3Zylk1LtYeV0HJZhGCWL6Gh4803NFD9+PNSpA6tXa8Rh/fpwzz2waZO/rTQKmlItWIZhlG5q14Zx4zRAY/ZsnUzy4EH1vlq1gv794YsvNOLQKPmYYBmGUeIJCYGrroLvv4dly2DECF03fz5ccgm0aQMvvwyHDvnbUiM/mGAZhlGq6NgR3nhDowv//W8NhY+P13D4+vW1n2vdOn9baeQFEyw/s23bNgYNGkTLli1p3rw5d911V6aph3I691O/fv04cOBAnmzxzeiekVGjRhEdHU14eDjly5c/FeU3Z86cHNWdcV4twyhsatSAsWM19H3OHDjvPI0unDxZ5+rq3VsHIycl+dtSI6eYYHnJbp6QV19NK/fqq/mfU8SDiDB06FAGDx5MfHw8Gzdu5MiRIzz00EPpyiUnJ1OvXr0cicP8+fOpWrVqruzICZMnTyYuLo758+fTvHlz4uLiiIuLy/EEiiZYhr8ICtIsGosWwcqVOslkWBgsXAhXXKG5Cx95RD0yo3hjguVHFixYQGhoKCNGjAA0YevEiRN54403mDJlCgMHDqR379706dMn3dxPiYmJXHHFFYSHhzNkyBC6du2KN5OHd/bhhIQE2rZty0033URERAR9+/bl2LFjALz22mt07tyZqKgoLr30UhITE/Nk/9GjR7n++uvp0qULHTp0ODWH1po1a+jSpQvR0dFERkYSHx9/2rxahuEP2reHqVM1se5LL0HbtrBzJ0yYAE2awNChNkdXccYEy0t284TcfHNauZtvzv+cIh7WrFlDTExMunWVK1emUaNGJCcns3z5cubMmcPixYvTlZkyZQrVqlVj7dq1TJgwgWXLlmVaf3x8PKNGjWLNmjVUrVqVuXPnAjB06FCWLl3KihUraNu2La+//nqu7PbyxBNP0Lt3b3755RcWLlzImDFjOHr0KFOnTuWuu+4iLi6O2NhYGjRowFNPPXXKM3v22WfzdDzDKCiqVIE77oA1a9I8Lefggw/gwgs1SOPFFyGPretGIVGqBcs5N8A59+pBbyKyEsaFF15I9erVT1u/ZMkSrrrqKgDatWtHZGRkpvs3bdqU6OhoAGJiYkhISABg9erV9OzZk/bt2zNz5kzWrFmTJ/u+/PJLnnrqKaKjo+nVqxfHjx9n69atdOvWjSeffJKnn36aLVu2nJrXyjCKG85Br17w7rs6puuxxzQwY+NGTf1Uvz7ceKNGHhr+p1QLVnEfOBweHn6ad3To0CG2bt1KUFBQvrOSZzU/1PDhw5k0aRKrVq1i3LhxeZ73SkSYO3fuqf6srVu30rZtW6655ho+/vhjypcvT79+/ViwYEG+zsMwioK6deFf/9I5uubO1aCMxER4/XXo1Am6dNH/bYJJ/1GqBau406dPHxITE3nrrbcASElJYfTo0QwfPpywsLAs9+vRowfvvfceAGvXrs11Pr7Dhw9Tt25dkpKSTs1nlRcuuugiXn75ZbwZ/3/99VcANm/eTLNmzbjzzjsZNGgQK1euzHZeLcMoTgT9f3v3HhxVlSdw/PsTAr1GokGioxt5jAaVTtKBrMQBYeQxyICyIDo+EBUfy8rLcsZYlgWCOFpuaTm7LsMgDhJAQEhVREvU2RqXLIMWaOIQVMSUGRSJoDHGQEiEQH77x7kJnWBiEpL06/epugV9b6dzfn3S/bvn3HPP6e6uZb39NuzZ42bNSEyE9993ra0LL4TZs90ADtO1LGGFkIjwyiuvkJubS0pKCgMHDsTn8/Hkk0+2+HOzZs2irKyMQYMGMX/+fPx+P21pRT7++ONkZWUxfPjwhnWv2mPBggXU1taSnp6O3+9nwYIFAGzcuJHU1FQyMjL46KOPuP3221tcV8uYcHXppfDss1Ba6qaC+sUv3M3HS5dCIOAer1oF3ngm08lsPawIdOLECWpra/H5fJSUlDB27Fg+/fRTevToEeqiRYRIrnsTeh9+6O5uWb365MwZiYlwxx1uyPxpnAOGHVsPy5y26upqrrrqKgKBAFOmTGHp0qWWrIzpImlpbpqnr75y17SGDoWKCjd/4eWXuxuU166Fdl4aNi2wFpY5xezZs3nnnXca7bv//vsb7heLdFb3pqN98IG7v2vdupODMnr3hunT3XUv7xbKiBNuLSxLWCbmWN2bznL4sJs1fvnyxkPhr7wS7r0XbropstbqCreEZV2CxhjTQXr1cnMLFBS4Vtd990FCAmzfDnff7YbOz5zpjsdAW6HDWcIyxphOMHiwG0341VewciUMH+5aYMuXwxVXuFnlly612TTawhKWMcZ0ovh4uPNO2LbNTQX1wANw7rmwc6e7n+uCC9y1rvx8a3X9FEtYxhjTRQYNOnlf18svw5gxbjThSy/BqFGQkgJPPGEzxzfHElYIiQi33XZbw+Pjx4+TlJTEtdde26bXqZ+hHWDYsGEN+7Ozs/H7/WRnZ7Ns2bKGGTXaoumyIK1dl6u1srKyyMjIoG/fviQlJTWss1U/7+FPqV/yxJhI0rOnG4Dx17+69boWLIDkZCgpgfnz3ZIn48fDxo1w9GioSxtGVDVqN+A6YPkll1yiTe3evbvR45amYH/++ZPPe/75lp/bFvHx8RoIBLS6ulpVVd944w0NBAI6ceLENr1Ov379tKys7JT9CQkJevz48bYVqom9e/eq3+8/rddojZUrV+rs2bO75Oea1r0x4eD4cdW33lL9zW9Ue/Q4+Z2SmKg6Z45qYaFqXV3Xlgko0DD4Lq/forqFpWE++S24FYI3b94MwPr167nlllsajn333XdMnjyZ9PR0rrzySnZ5k5eVl5czbtw4/H4/99xzT8NcfgBnnXUWAJMmTaKqqorMzEw2bNjQaDXhzz77jLFjxxIIBBgyZAglJSVUVVUxZswYhgwZQlpaWsPaVk3XsQpel+uHH35gxowZpKWlMXjwYLZs2QJATk4O119/PePHjyclJYWHHnqoTe9JSUkJ48ePJzMzkxEjRrBnzx4AcnNzSU1NJRAIMHLkSI4dO8ajjz7Khg0byMjIYMOGDW1+/40JF926wTXXuJnjDxxwNycPHuxuSl6yBDIzISPD3aD8zTehLm2IhDpjdsWWmZl5yplDOJxlx8fHa1FRkU6dOlVramo0EAjoli1bGlpYc+bM0UWLFqmq6ttvv62BQEBVVefOnauPPfaYqqq+/vrrCjS0sOLj4xu9fr2FCxfq008/raqqQ4cO1by8PFVVramp0SNHjmhtba1WVlaqqmpZWZlefPHFWldXd0oLK/jxM888ozNmzFBV1U8++UQvuugiramp0ZUrV+qAAQP0+++/15qaGu3bt6/u27evxfciuKU0evRoLS4uVlXV7du366hRo1RVNTU1Vffv36+qqhUVFaf8XGuFQ90b01p//7vqvHmq5557stXVvbvqpEmqeXmqR4923u/GWlgmWHp6Op9//jnr169nwoQJjY5t27aN6dOnAzB69GjKy8s5dOgQW7dubbj2NXHiRBITE1v9+w4fPkxpaSlTpkwBwOfzceaZZ6KqPPLII6SnpzN27FhKS0v5+uuvW3ytbdu2NZTjsssuo1+/fhQXFwNuJvqzzz4bn8/HoEGD+OKLL1pVvqqqKt59911uvPFGMjIymDlzJgcOHADcLPV33nknL7zwAidOnGh1zMZEsowMt5hkaSnk5sK117q09dprblb5Cy90i1EWFkb/KMPuoS6Acd13Dz74IPn5+ZSXl4ekDGvXrqWsrIzCwkLi4uLo379/u9fJgubX4vopdXV1nHPOOezcufOUY8uWLWPHjh1s3ryZzMzMZldaNiYa9ewJN9zgtoMH3XyFOTnw0Ueuy3DJEvD73RD6adPccPloYy2sMHDXXXexcOFC0tLSGu0fMWJEw3pV+fn59OnTh4SEBEaOHMm6desAePPNN6moqGj17+rVqxfJycls2rQJgKNHj1JdXU1lZSXnnXcecXFxbNmypaFF1NI6VsHlKy4uZt++fVx66aVtC76JhIQEBgwYQG5uLuC6rIuKigB3bSsrK4vFixeTlJTEl19+aetsmZj0s5/B737n1uQqLIR586BPH3efV3a2G3E4YYK7HhZNk/BawgoDycnJzJs375T9ixYtorCwkPT0dB5++GFWrVoFwMKFC9m6dSt+v5+8vDz69u3bpt+3Zs0annvuOdLT0xk2bBgHDx5k2rRpFBQUkJaWxurVqxvWyWppHatZs2ZRV1dHWloaN910Ezk5OY1aVu21du1aVqxYQSAQwO/3NwwAyc7OJi0tjdTUVIYNG0YgEGDUqFHs3r3bBl2YmCTiZsyo7zLctAkmT4YzzoA334Sbb3bJbe/eUJe0Y9jktybmWN2baPftt24S3lWroLISiotdcmsrm/zWGGNMp+rTxw3EKCiA995rX7IKRzbownSZrKwsjja5bX/NmjWnXLszxnScNgwiDnsxnbBUFYmWU48IsGPHjlAXgVjoAjcmWsVsl6DP56O8vNy+wGKIqlJeXo7P5wt1UYwx7RCzLazk5GT2799PWVlZqItiupDP5yM5OTnUxTDGtEPMJqy4uDgGDBgQ6mIYY4xppZjtEjTGGBNZLGEZY4yJCJawjDHGRISYmOlCRMqA1k0X/uP6AN92UHEiRSzGDLEZdyzGDLEZd1tj7qeqSZ1VmLaKiYR1ukSkIJymJ+kKsRgzxGbcsRgzxGbckR6zdQkaY4yJCJawjDHGRARLWK2zPNQFCIFYjBliM+5YjBliM+6IjtmuYRljjIkI1sIyxhgTESxhGWOMiQiWsIwxxkQES1gtEJHeIvKKiBwRkS9E5NZQl6kjiUhPEVnhxXZYRHaKyK+Djo8RkT0iUi0iW0SkXyjL2xlEJEVEfhCRl4L23eq9J0dEZJOI9A5lGTuSiNwsIp94sZWIyAhvf9TWtYj0F5E3RKRCRA6KyBIR6e4dyxCRQi/uQhHJCHV520NE5ohIgYgcFZGcJsearVvvO+BFETnkvTe/7fLCt4ElrJb9ETgGnA9MA/4kIv7QFqlDdQe+BH4JnA3MBzZ6H/A+QB6wAOgNFAAbQlXQTvRH4P36B179Pg9Mx9V7NbA0NEXrWCLyK+A/gBlAL2Ak8I8YqOulwDfABUAG7u99loj0AF4FXgISgVXAq97+SPMV8HvgxeCdrajbRUAK0A8YBTwkIuO7oLzto6q2/cgGxOOS1cCgfWuAp0Jdtk6OexcwFfg34N0m70cNcFmoy9iBsd4MbMR9aF/y9j0JrAt6zsXe30GvUJe3A+J9F7j7R/ZHdV0DnwATgh4/jTspGQeU4o2W9o7tA8aHusynEevvgZzW1i0u0Y0LOv448HKo42husxZW8wYCx1W1OGhfERBNLaxGROR8XNwf4+Isqj+mqkeAEqIkfhFJABYDTbtAmsZdgnfi0nWl63gi0g34FyBJRD4Tkf1e19g/EeV1DfwncLOInCki/wz8GngLF98u9b6pPbuInrihhboVkURcq7Mo6Plh/R1nCat5ZwGHmuyrxHWlRB0RiQPWAqtUdQ8u/somT4um+B8HVqjq/ib7ozXu84E44AZgBK5rbDCuGzhaY663FfclfAjYj+sW20T0xw0tx3hW0OOmx8KSJazmVQEJTfYlAIdDUJZOJSJn4Lo7jwFzvN1RG793YX0s8IcfORytcdd4//63qh5Q1W+BZ4EJRG/M9X/bb+Gu48TjZitPxF3Li9q4g7QUY1XQ46bHwpIlrOYVA91FJCVoXwDXXRY1RESAFbgz8KmqWusd+hgXb/3z4nHXc6Ih/quB/sA+ETkIPAhMFZEPODXunwM9cX8PEUtVK3Cti+Dur/r/R3Nd9wb6AktU9aiqlgMrcYn6YyDd+wzUSyc64q7XbN16fxMHgo8T5t9xlrCa4fX15gGLRSReRIYD/4priUSTPwGXA9epak3Q/leAVBGZKiI+4FFcf/+eUBSygy3HfWgzvG0ZsBm4Btctep2IjPA+3IuBPFUN27PONlgJzBWR87zrFw8ArxPFde21JPcC94lIdxE5B7gDd60qHzgBzPOGd9f3LvxvSAp7GrzYfEA3oJuI+Lyh+z9Vt6uB+SKSKCKXAfcCOSEIoXVCPeojnDfc2dkm4Ahu9NCtoS5TB8fXD3eW/QOue6B+m+YdHwvswXUn5QP9Q13mTnofFuGNEvQe3+rV9xHcsOfeoS5jB8UZhxvi/T1wEHgO8EV7XeNOSvKBCtzihRuB871jg4FCL+4PgMGhLm87Y1zkfZaDt0U/Vbe43oMXcdf3vgZ+G+pYWtps8ltjjDERwboEjTHGRARLWMYYYyKCJSxjjDERwRKWMcaYiGAJyxhjTESwhGWMMSYiWMIypo1EpMr7t3+0rZFmTDizhGVM+/XH3WTcavULBxpj2s4SljHt9xQwwlup+QER6SYiT4vI+yKyS0RmAojI1SLyNxF5Ddjd9EVEpEpEnhCRIhHZ7i3zgojkiMgNwc8Ler3/E5FXReQfIvKUiEwTkfdE5EMRubhrwjema1nCMqb9Hgb+pqoZqvoH4G6gUlWvAK4A7hWRAd5zhwD3q+qPrasVD2xX1QBuKYx7W/G7A8C/4+aBnI5baHQo8Gdg7ukEZUy4su4JYzrOONzs3/WtorNxy48fA95T1b3N/Nwx3CS04Oa1+1Urftf7qnoAQERKgP/x9n+IW+rcmKhjCcuYjiPAXFX9S6OdIlfjJtJtTq2enNTzBCc/l8fxekG8dZ16BP3M0aD/1wU9rsM+1yZKWZegMe13mMars/4Ft4xFHICIDPSWKGmvz4FM7/+TcLOtGxOz7EzMmPbbBZwQkSLcGkL/hRs5+IG3KGAZMPk0Xv8F4FXv9d+i5VaaMVHPlhcxxhgTEaxL0BhjTESwhGWMMSYiWMIyxhgTESxhGWOMiQiWsIwxxkQES1jGGGMigiUsY4wxEeH/ATXLJhOQFfx3AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"3m7Ozb9KcT1Y","colab_type":"text"},"source":["# Plot "]},{"cell_type":"code","metadata":{"id":"areqNcghcT1Z","colab_type":"code","colab":{},"outputId":"2ea1cec7-7ed2-4be2-b0e3-b7313f94220d"},"source":["loss_ntk_scale_bn"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'train': [tensor(0.0020),\n","  tensor(0.0012),\n","  tensor(0.0008),\n","  tensor(0.0006),\n","  tensor(0.0005)],\n"," 'test': [tensor(0.0020),\n","  tensor(0.0014),\n","  tensor(0.0013),\n","  tensor(0.0011),\n","  tensor(0.0011)]}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"nh1iqAeucT1c","colab_type":"code","colab":{}},"source":["plot_dict = {'without scaling; without bn': loss_no_scale_no_bn,\n","            'without scaling; with bn': loss_no_scale_bn, \n","            'NTK scaling; without bn': loss_ntk_scale_no_bn, \n","            'NTK scaling; with bn': loss_ntk_scale_bn}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p82yPLiLcT1d","colab_type":"code","colab":{},"outputId":"df674051-2e0d-4230-e115-366a669104dc"},"source":["fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n","\n","# key_idx = 0\n","key_idx = 0\n","for title in list(plot_dict.keys()):\n","#     title = list(plot_dict.keys())[key_idx]\n","    loss_dict = plot_dict[title]\n","\n","    axs[key_idx].plot(step_list, loss_dict['train'], label = 'train', linewidth = 3 ,  linestyle =  '--')\n","    axs[key_idx].plot(step_list,loss_dict['test'], label = 'test', linewidth = 3)\n","    axs[key_idx].legend(frameon = False, fontsize = 12)\n","    \n","    if key_idx == 0:\n","        axs[key_idx].set_ylabel('Loss', fontsize = 15)\n","    axs[key_idx].set_title(title)\n","\n","    axs[key_idx].set_ylim([0, 0.003])   \n","    axs[key_idx].grid(True)\n","    \n","    axs[key_idx] = simpleaxis(axs[key_idx])\n","\n","    plt.suptitle('MLP 784-800-10; Full batch training using 64 MNIST images', fontsize = 17, y = 1.01)\n","    key_idx += 1\n","    \n","    \n","# plt.savefig(fig_save_path + 'fullbatch', format='png')\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x288 with 4 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"RQkTpFphcT1f","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OffNqsCJcT1h","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejrqtbu-cT1j","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUAWek9acT1m","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghrgkeIbcT1n","colab_type":"code","colab":{}},"source":["# plot_dict = {'without scaling; without bn': (loss_no_scale_no_bn, aclist_no_scale_no_bn),\n","#             'without scaling; with bn': (loss_no_scale_bn, aclist_no_scale_bn),\n","#             'NTK scaling; without bn': (loss_ntk_scale_no_bn, aclist_ntk_scale_no_bn),\n","#             'NTK scaling; with bn': (loss_ntk_scale_bn, aclist_ntk_scale_bn)}\n","\n","\n","\n","# fig, axs = plt.subplots(2, 4, figsize=(20, 6))\n","\n","# # key_idx = 0\n","# key_idx = 0\n","# for title in list(plot_dict.keys()):\n","# #     title = list(plot_dict.keys())[key_idx]\n","#     loss_dict, acc_dict = plot_dict[title]\n","\n","#     axs[0, key_idx].plot(step_list, loss_dict['train'], label = 'train', linewidth = 3 ,  linestyle =  '--')\n","#     axs[0, key_idx].plot(step_list,loss_dict['test'], label = 'test', linewidth = 3)\n","#     axs[0, key_idx].legend(frameon = False, fontsize = 12)\n","    \n","#     if key_idx == 0:\n","#         axs[0, key_idx].set_ylabel('Loss', fontsize = 15)\n","#     axs[0, key_idx].set_title(title)\n","\n","#     axs[1, key_idx].plot(step_list, acc_dict['train'], label = 'train', linewidth = 3, linestyle =  '--')\n","#     axs[1, key_idx].plot(step_list, acc_dict['test'], label = 'test', linewidth = 3)\n","#     axs[1, key_idx].legend(frameon = False, fontsize = 12)\n","#     axs[1, key_idx].set_xlabel('Iter num', fontsize = 15)\n","    \n","#     if key_idx == 0:\n","#         axs[1, key_idx].set_ylabel('Acc', fontsize = 15)\n","\n","#     axs[0, key_idx].set_ylim([0, 0.003])   \n","#     axs[1, key_idx].set_ylim([40, 101])    \n","#     axs[0, key_idx].grid(True)\n","#     axs[1, key_idx].grid(True)\n","    \n","#     axs[0, key_idx] = simpleaxis(axs[0, key_idx])\n","#     axs[1, key_idx] = simpleaxis(axs[1, key_idx])\n","\n","#     plt.suptitle('MLP 784-800-10; Full batch training using 64 MNIST images', fontsize = 17)\n","#     key_idx += 1\n","    \n","    \n","# plt.savefig(fig_save_path + 'fullbatch', format='png')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWQGm-IccT1p","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}